<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Controllers on Hugo 主题的 Learn 文档</title>
    <link>https://lijun.in/concepts/workloads/controllers/</link>
    <description>Recent content in Controllers on Hugo 主题的 Learn 文档</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-US</language>
    
	<atom:link href="https://lijun.in/concepts/workloads/controllers/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>ReplicaSet</title>
      <link>https://lijun.in/concepts/workloads/controllers/replicaset/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/concepts/workloads/controllers/replicaset/</guid>
      <description>ReplicaSet 是下一代的 Replication Controller。 ReplicaSet 和 Replication Controller 的唯一区别是选择器的支持。ReplicaSet 支持新的基于集合的选择器需求，这在标签用户指南中有描述。而 Replication Controller 仅支持基于相等选择器的需求。
怎样使用 ReplicaSet 大多数支持 Replication Controllers 的kubectl命令也支持 ReplicaSets。但rolling-update 命令是个例外。如果您想要滚动更新功能请考虑使用 Deployment。rolling-update 命令是必需的，而 Deployment 是声明性的，因此我们建议通过 rollout命令使用 Deployment。
虽然 ReplicaSets 可以独立使用，但今天它主要被Deployments 用作协调 Pod 创建、删除和更新的机制。 当您使用 Deployment 时，您不必担心还要管理它们创建的 ReplicaSet。Deployment 会拥有并管理它们的 ReplicaSet。
什么时候使用 ReplicaSet ReplicaSet 确保任何时间都有指定数量的 Pod 副本在运行。 然而，Deployment 是一个更高级的概念，它管理 ReplicaSet，并向 Pod 提供声明式的更新以及许多其他有用的功能。 因此，我们建议使用 Deployment 而不是直接使用 ReplicaSet，除非您需要自定义更新业务流程或根本不需要更新。
这实际上意味着，您可能永远不需要操作 ReplicaSet 对象：而是使用 Deployment，并在 spec 部分定义您的应用。
示例 codenew file=&amp;quot;controllers/frontend.yaml&amp;rdquo; &amp;gt;}}
将此清单保存到 frontend.yaml 中，并将其提交到 Kubernetes 集群，应该就能创建 yaml 文件所定义的 ReplicaSet 及其管理的 Pod。</description>
    </item>
    
    <item>
      <title>ReplicationController</title>
      <link>https://lijun.in/concepts/workloads/controllers/replicationcontroller/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/concepts/workloads/controllers/replicationcontroller/</guid>
      <description>现在推荐使用配置 ReplicaSet 的 Deployment 来建立副本管理机制。
ReplicationController 确保在任何时候都有特定数量的 pod 副本处于运行状态。 换句话说，ReplicationController 确保一个 pod 或一组同类的 pod 总是可用的。
ReplicationController 如何工作 当 pod 数量过多时，ReplicationController 会终止多余的 pod。当 pod 数量太少时，ReplicationController 将会启动新的 pod。 与手动创建的 pod 不同，由 ReplicationController 创建的 pod 在失败、被删除或被终止时会被自动替换。 例如，在中断性维护（如内核升级）之后，您的 pod 会在节点上重新创建。 因此，即使您的应用程序只需要一个 pod，您也应该使用 ReplicationController 创建 Pod。 ReplicationController 类似于进程管理器，但是 ReplicationController 不是监控单个节点上的单个进程，而是监控跨多个节点的多个 pod。
在讨论中，ReplicationController 通常缩写为 &amp;ldquo;rc&amp;rdquo;，并作为 kubectl 命令的快捷方式。
一个简单的示例是创建一个 ReplicationController 对象来可靠地无限期地运行 Pod 的一个实例。 更复杂的用例是运行一个多副本服务（如 web 服务器）的若干相同副本。
运行一个示例 ReplicationController 这个示例 ReplicationController 配置运行 nginx web 服务器的三个副本。
codenew file=&amp;quot;controllers/replication.yaml&amp;rdquo; &amp;gt;}}</description>
    </item>
    
    <item>
      <title>Deployments</title>
      <link>https://lijun.in/concepts/workloads/controllers/deployment/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/concepts/workloads/controllers/deployment/</guid>
      <description>apiVersion: extensions/v1beta1 # 接口版本 kind: Deployment # 接口类型 metadata: name: cango-demo # Deployment名称 namespace: cango-prd # 命名空间 labels: app: cango-demo # 标签 spec: replicas: 3 strategy: # 部署策略 rollingUpdate: # 由于replicas为3,则整个升级,pod个数在2-4个之间 maxSurge: 1 # 滚动升级时会先启动1个pod maxUnavailable: 1 # 滚动升级时允许的最大Unavailable的pod个数 template: metadata: labels: app: cango-demo # 模板名称必填 sepc: # 定义容器模板,该模板可以包含多个容器 containers: - name: cango-demo # 镜像名称 image: swr.cn-east-2.myhuaweicloud.com/cango-prd/cango-demo:0.0.1-SNAPSHOT # 镜像地址 command: [ &amp;#34;/bin/sh&amp;#34;,&amp;#34;-c&amp;#34;,&amp;#34;cat /etc/config/path/to/special-key&amp;#34; ] # 启动命令 args: # 启动参数 - &amp;#39;-storage.</description>
    </item>
    
    <item>
      <title>StatefulSets</title>
      <link>https://lijun.in/concepts/workloads/controllers/statefulset/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/concepts/workloads/controllers/statefulset/</guid>
      <description>StatefulSet 是用来管理有状态应用的工作负载 API 对象。
glossary_definition term_id=&amp;quot;statefulset&amp;rdquo; length=&amp;quot;all&amp;rdquo; &amp;gt;}}
使用 StatefulSets StatefulSets 对于需要满足以下一个或多个需求的应用程序很有价值：
 稳定的、唯一的网络标识符。 稳定的、持久的存储。 有序的、优雅的部署和缩放。 有序的、自动的滚动更新。  在上面，稳定意味着 Pod 调度或重调度的整个过程是有持久性的。如果应用程序不需要任何稳定的标识符或有序的部署、删除或伸缩，则应该使用由一组无状态的副本控制器提供的工作负载来部署应用程序，比如 Deployment 或者 ReplicaSet 可能更适用于您的无状态应用部署需要。
限制  给定 Pod 的存储必须由 [PersistentVolume 驱动](https://github.com/kubernetes/examples/tree/&amp;lt; param &amp;ldquo;githubbranch&amp;rdquo; &amp;gt;}}/staging/persistent-volume-provisioning/README.md) 基于所请求的 storage class 来提供，或者由管理员预先提供。 删除或者收缩 StatefulSet 并不会删除它关联的存储卷。这样做是为了保证数据安全，它通常比自动清除 StatefulSet 所有相关的资源更有价值。 StatefulSet 当前需要 headless 服务 来负责 Pod 的网络标识。您需要负责创建此服务。 当删除 StatefulSets 时，StatefulSet 不提供任何终止 Pod 的保证。为了实现 StatefulSet 中的 Pod 可以有序和优雅的终止，可以在删除之前将 StatefulSet 缩放为 0。 在默认 Pod 管理策略(OrderedReady) 时使用 滚动更新，可能进入需要 人工干预 才能修复的损坏状态。  组件 下面的示例演示了 StatefulSet 的组件。</description>
    </item>
    
    <item>
      <title>DaemonSet</title>
      <link>https://lijun.in/concepts/workloads/controllers/daemonset/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/concepts/workloads/controllers/daemonset/</guid>
      <description>DaemonSet 确保全部（或者某些）节点上运行一个 Pod 的副本。当有节点加入集群时， 也会为他们新增一个 Pod 。当有节点从集群移除时，这些 Pod 也会被回收。删除 DaemonSet 将会删除它创建的所有 Pod。
DaemonSet 的一些典型用法：
 在每个节点上运行集群存储 DaemonSet，例如 glusterd、ceph。 在每个节点上运行日志收集 DaemonSet，例如 fluentd、logstash。 在每个节点上运行监控 DaemonSet，例如 Prometheus Node Exporter、Flowmill、Sysdig 代理、collectd、Dynatrace OneAgent、AppDynamics 代理、Datadog 代理、New Relic 代理、Ganglia gmond 或者 Instana 代理。  一个简单的用法是在所有的节点上都启动一个 DaemonSet，将被作为每种类型的 daemon 使用。
一个稍微复杂的用法是单独对每种 daemon 类型使用多个 DaemonSet，但具有不同的标志， 并且对不同硬件类型具有不同的内存、CPU 要求。
编写 DaemonSet Spec 创建 DaemonSet 您可以在 YAML 文件中描述 DaemonSet。例如，下面的 daemonset.yaml 文件描述了一个运行 fluentd-elasticsearch Docker 镜像的 DaemonSet：
odenew file=&amp;quot;controllers/daemonset.yaml&amp;rdquo; &amp;gt;}}
 基于 YAML 文件创建 DaemonSet:  kubectl apply -f https://k8s.</description>
    </item>
    
    <item>
      <title>垃圾收集</title>
      <link>https://lijun.in/concepts/workloads/controllers/garbage-collection/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/concepts/workloads/controllers/garbage-collection/</guid>
      <description>Kubernetes 垃圾收集器的作用是删除某些曾经拥有所有者（owner）但现在不再拥有所有者的对象。
所有者和附属 某些 Kubernetes 对象是其它一些对象的所有者。例如，一个 ReplicaSet 是一组 Pod 的所有者。 具有所有者的对象被称为是所有者的 附属 。 每个附属对象具有一个指向其所属对象的 metadata.ownerReferences 字段。
有时，Kubernetes 会自动设置 ownerReference 的值。 例如，当创建一个 ReplicaSet 时，Kubernetes 自动设置 ReplicaSet 中每个 Pod 的 ownerReference 字段值。 在 Kubernetes 1.8 版本，Kubernetes 会自动为某些对象设置 ownerReference 的值，这些对象是由 ReplicationController、ReplicaSet、StatefulSet、DaemonSet、Deployment、Job 和 CronJob 所创建或管理。 也可以通过手动设置 ownerReference 的值，来指定所有者和附属之间的关系。
这里有一个配置文件，表示一个具有 3 个 Pod 的 ReplicaSet：
codenew file=&amp;quot;controllers/replicaset.yaml&amp;rdquo; &amp;gt;}}
如果创建该 ReplicaSet，然后查看 Pod 的 metadata 字段，能够看到 OwnerReferences 字段：
kubectl apply -f https://k8s.io/examples/controllers/replicaset.yaml kubectl get pods --output=yaml 输出显示了 Pod 的所有者是名为 my-repset 的 ReplicaSet：</description>
    </item>
    
    <item>
      <title>已完成资源的 TTL 控制器</title>
      <link>https://lijun.in/concepts/workloads/controllers/ttlafterfinished/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/concepts/workloads/controllers/ttlafterfinished/</guid>
      <description>feature-state for_k8s_version=&amp;quot;v1.12&amp;rdquo; state=&amp;quot;alpha&amp;rdquo; &amp;gt;}}
TTL 控制器提供了一种 TTL 机制来限制已完成执行的资源对象的生命周期。TTL 控制器目前只处理 Job，可能以后会扩展以处理将完成执行的其他资源，例如 Pod 和自定义资源。
Alpha 免责声明：此功能目前是 alpha 版，并且可以通过 kube-apiserver 和 kube-controller-manager 特性开关 TTLAfterFinished 启用。
TTL 控制器 TTL 控制器现在只支持 Job。集群操作员可以通过指定 Job 的 .spec.ttlSecondsAfterFinished 字段来自动清理已结束的作业（Complete 或 Failed），如下所示的示例。
TTL 控制器假设资源能在执行完成后的 TTL 秒内被清理，也就是当 TTL 过期后。当 TTL 控制器清理资源时，它将做级联删除操作，如删除资源对象的同时也删除其依赖对象。注意，当资源被删除时，由该资源的生命周期保证其终结器（finalizers）等被执行。
可以随时设置 TTL 秒。以下是设置 Job 的 .spec.ttlSecondsAfterFinished 字段的一些示例：
 在资源清单（manifest）中指定此字段，以便 Job 在完成后的某个时间被自动清除。 将此字段设置为存在的、已完成的资源，以采用此新功能。 在创建资源时使用 mutating admission webhook 动态设置该字段。集群管理员可以使用它对完成的资源强制执行 TTL 策略。 使用 mutating admission webhook 在资源完成后动态设置该字段，并根据资源状态、标签等选择不同的 TTL 值。  警告 更新 TTL 秒 请注意，在创建资源或已经执行结束后，仍可以修改其 TTL 周期，例如 Job 的 .</description>
    </item>
    
    <item>
      <title>CronJob</title>
      <link>https://lijun.in/concepts/workloads/controllers/cron-jobs/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/concepts/workloads/controllers/cron-jobs/</guid>
      <description>feature-state for_k8s_version=&amp;quot;v1.8&amp;rdquo; state=&amp;quot;beta&amp;rdquo; &amp;gt;}}
Cron Job 创建基于时间调度的 Jobs。
一个 CronJob 对象就像 crontab (cron table) 文件中的一行。它用 Cron 格式进行编写，并周期性地在给定的调度时间执行 Job。
所有 CronJob 的 schedule: 时间都是基于初始 Job 的主控节点的时区。
如果你的控制平面在 Pod 或是裸容器中运行了主控程序 (kube-controller-manager)， 那么为该容器设置的时区将会决定定时任务的控制器所使用的时区。
为 CronJob 资源创建清单时，请确保创建的名称不超过 52 个字符。这是因为 CronJob 控制器将自动在提供的作业名称后附加 11 个字符，并且存在一个限制，即作业名称的最大长度不能超过 63 个字符。
有关创建和使用 CronJob 的说明及规范文件的示例，请参见使用 CronJob 运行自动化任务。
CronJob 限制 CronJob 创建 Job 对象，每个 Job 的执行次数大约为一次。 我们之所以说 &amp;ldquo;大约&amp;rdquo;，是因为在某些情况下，可能会创建两个 Job，或者不会创建任何 Job。 我们试图使这些情况尽量少发生，但不能完全杜绝。因此，Job 应该是 幂等的。
如果 startingDeadlineSeconds 设置为很大的数值或未设置（默认），并且 concurrencyPolicy 设置为 Allow，则作业将始终至少运行一次。
对于每个 CronJob，CronJob 控制器 检查从上一次调度的时间点到现在所错过了调度次数。如果错过的调度次数超过 100 次，那么它就不会启动这个任务，并记录这个错误:</description>
    </item>
    
  </channel>
</rss>