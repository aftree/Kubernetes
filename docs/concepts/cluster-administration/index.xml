<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>😊 - 计算、存储和网络扩展 on Hugo 主题的 Learn 文档</title>
    <link>https://lijun.in/concepts/cluster-administration/</link>
    <description>Recent content in 😊 - 计算、存储和网络扩展 on Hugo 主题的 Learn 文档</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-US</language>
    
	<atom:link href="https://lijun.in/concepts/cluster-administration/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>集群管理概述</title>
      <link>https://lijun.in/concepts/cluster-administration/cluster-administration-overview/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/concepts/cluster-administration/cluster-administration-overview/</guid>
      <description>集群管理概述面向任何创建和管理 Kubernetes 集群的读者人群。 我们假设你对用户指南中的概念大概了解。
规划集群 查阅 安装 中的指导，获取如何规划、建立以及配置 Kubernetes 集群的示例。本文所列的文章称为发行版 。
在选择一个指南前，有一些因素需要考虑：
 你是打算在你的电脑上尝试 Kubernetes，还是要构建一个高可用的多节点集群？请选择最适合你需求的发行版。 如果你正在设计一个高可用集群，请了解在多个 zones 中配置集群。 您正在使用 类似 Google Kubernetes Engine 这样的被托管的Kubernetes集群, 还是管理您自己的集群? 你的集群是在本地还是 云（IaaS） 上？ Kubernetes 不能直接支持混合集群。作为代替，你可以建立多个集群。 如果你在本地配置 Kubernetes，需要考虑哪种网络模型最适合。 你的 Kubernetes 在 裸金属硬件 还是 虚拟机（VMs） 上运行？ 你只想运行一个集群，还是打算活动开发 Kubernetes 项目代码？如果是后者，请选择一个活动开发的发行版。某些发行版只提供二进制发布版，但提供更多的选择。 让你自己熟悉运行一个集群所需的组件 。  请注意：不是所有的发行版都被积极维护着。请选择测试过最近版本的 Kubernetes 的发行版。
管理集群   管理集群叙述了和集群生命周期相关的几个主题：创建一个新集群、升级集群的 master 和 worker 节点、执行节点维护（例如内核升级）以及升级活动集群的 Kubernetes API 版本。
  学习如何 管理节点.
  学习如何设定和管理集群共享的 资源配额 。
  集群安全   Certificates 描述了使用不同的工具链生成证书的步骤。</description>
    </item>
    
    <item>
      <title>证书</title>
      <link>https://lijun.in/concepts/cluster-administration/certificates/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/concepts/cluster-administration/certificates/</guid>
      <description>当使用客户端证书进行认证时，用户可以使用现有部署脚本，或者通过 easyrsa、openssl 或 cfssl 手动生成证书。
easyrsa 使用 easyrsa 能够手动地为集群生成证书。
  下载、解压并初始化 easyrsa3 的补丁版本。
curl -LO https://storage.googleapis.com/kubernetes-release/easy-rsa/easy-rsa.tar.gz tar xzf easy-rsa.tar.gz cd easy-rsa-master/easyrsa3 ./easyrsa init-pki    生成 CA（通过 --batch 参数设置自动模式。 通过 --req-cn 设置默认使用的 CN）
./easyrsa --batch &amp;quot;--req-cn=${MASTER_IP}@`date +%s`&amp;quot; build-ca nopass    生成服务器证书和密钥。 参数 --subject-alt-name 设置了访问 API 服务器时可能使用的 IP 和 DNS 名称。 MASTER_CLUSTER_IP 通常为 --service-cluster-ip-range 参数中指定的服务 CIDR 的 首个 IP 地址，--service-cluster-ip-range 同时用于 API 服务器和控制器管理器组件。 --days 参数用于设置证书的有效期限。 下面的示例还假设用户使用 cluster.local 作为默认的 DNS 域名。</description>
    </item>
    
    <item>
      <title>云驱动</title>
      <link>https://lijun.in/concepts/cluster-administration/cloud-providers/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/concepts/cluster-administration/cloud-providers/</guid>
      <description>本文介绍了如何管理运行在特定云驱动上的 Kubernetes 集群。
kubeadm kubeadm 是创建 kubernetes 集群的一种流行选择。 kubeadm 通过提供配置选项来指定云驱动的配置信息。例如，一个典型的适用于“树内”云驱动的 kubeadm 配置如下：
apiVersion: kubeadm.k8s.io/v1beta2 kind: InitConfiguration nodeRegistration: kubeletExtraArgs: cloud-provider: &amp;#34;openstack&amp;#34; cloud-config: &amp;#34;/etc/kubernetes/cloud.conf&amp;#34; --- apiVersion: kubeadm.k8s.io/v1beta2 kind: ClusterConfiguration kubernetesVersion: v1.13.0 apiServer: extraArgs: cloud-provider: &amp;#34;openstack&amp;#34; cloud-config: &amp;#34;/etc/kubernetes/cloud.conf&amp;#34; extraVolumes: - name: cloud hostPath: &amp;#34;/etc/kubernetes/cloud.conf&amp;#34; mountPath: &amp;#34;/etc/kubernetes/cloud.conf&amp;#34; controllerManager: extraArgs: cloud-provider: &amp;#34;openstack&amp;#34; cloud-config: &amp;#34;/etc/kubernetes/cloud.conf&amp;#34; extraVolumes: - name: cloud hostPath: &amp;#34;/etc/kubernetes/cloud.conf&amp;#34; mountPath: &amp;#34;/etc/kubernetes/cloud.conf&amp;#34; “树内”的云驱动通常需要在命令行中为 kube-apiserver、kube-controller-manager 和 kubelet 指定 --cloud-provider 和 --cloud-config。在 --cloud-config 中为每个供应商指定的文件的内容也同样需要写在下面。 对于所有外部云驱动，请遵循独立云存储库的说明，或浏览所有版本库清单
AWS 本节介绍在 Amazon Web Services 上运行 Kubernetes 时可以使用的所有配置。 如果希望使用此外部云驱动，其代码库位于 kubernetes/cloud-provider-aws</description>
    </item>
    
    <item>
      <title>管理资源</title>
      <link>https://lijun.in/concepts/cluster-administration/manage-deployment/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/concepts/cluster-administration/manage-deployment/</guid>
      <description>您已经部署了应用并通过服务暴露它。然后呢？Kubernetes 提供了一些工具来帮助管理您的应用部署，包括缩扩容和更新。我们将更深入讨论的特性包括配置文件和标签。
组织资源配置 许多应用需要创建多个资源，例如 Deployment 和 Service。可以通过将多个资源组合在同一个文件中（在 YAML 中以 --- 分隔）来简化对它们的管理。例如：
file=&amp;quot;application/nginx-app.yaml&amp;rdquo; &amp;gt;}}
可以用创建单个资源相同的方式来创建多个资源：
kubectl apply -f https://k8s.io/examples/application/nginx-app.yaml service/my-nginx-svc created deployment.apps/my-nginx created 资源将按照它们在文件中的顺序创建。因此，最好先指定服务，这样在控制器（例如 Deployment）创建 Pod 时能够确保调度器可以将与服务关联的多个 Pod 分散到不同节点。
kubectl create 也接受多个 -f 参数:
kubectl apply -f https://k8s.io/examples/application/nginx/nginx-svc.yaml -f https://k8s.io/examples/application/nginx/nginx-deployment.yaml 还可以指定目录路径，而不用添加多个单独的文件：
kubectl apply -f https://k8s.io/examples/application/nginx/ kubectl 将读取任何后缀为 .yaml，.yml 或者 .json 的文件。
建议的做法是，将同一个微服务或同一应用层相关的资源放到同一个文件中，将同一个应用相关的所有文件按组存放到同一个目录中。如果应用的各层使用 DNS 相互绑定，那么您可以简单地将堆栈的所有组件一起部署。
还可以使用 URL 作为配置源，便于直接使用已经提交到 Github 上的配置文件进行部署：
kubectl apply -f https://raw.githubusercontent.com/kubernetes/website/master/content/zh/examples/application/nginx/nginx-deployment.yaml deployment.apps/my-nginx created kubectl 中的批量操作 资源创建并不是 kubectl 可以批量执行的唯一操作。kubectl 还可以从配置文件中提取资源名，以便执行其他操作，特别是删除您之前创建的资源：</description>
    </item>
    
    <item>
      <title>集群网络系统</title>
      <link>https://lijun.in/concepts/cluster-administration/networking/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/concepts/cluster-administration/networking/</guid>
      <description>集群网络系统是 Kubernetes 的核心部分，但是想要准确了解它的工作原理可是个不小的挑战。下面列出的是网络系统的的四个主要问题：
 高度耦合的容器间通信：这个已经被 pods 和 localhost 通信解决了。 Pod 间通信：这个是本文档的重点要讲述的。 Pod 和 Service 间通信：这个已经在 services 里讲述过了。 外部和 Service 间通信：这个也已经在 services 讲述过了。  Kubernetes 的宗旨就是在应用之间共享机器。通常来说，共享机器需要两个应用之间不能使用相同的端口，但是在多个应用开发者之间去大规模地协调端口是件很困难的事情，尤其是还要让用户暴露在他们控制范围之外的集群级别的问题上。
动态分配端口也会给系统带来很多复杂度 - 每个应用都需要设置一个端口的参数，而 API 服务器还需要知道如何将动态端口数值插入到配置模块中，服务也需要知道如何找到对方等等。与其去解决这些问题，Kubernetes 选择了其他不同的方法。
Kubernetes 网络模型 每一个 Pod 都有它自己的IP地址，这就意味着你不需要显式地在每个 Pod 之间创建链接，你几乎不需要处理容器端口到主机端口之间的映射。这将创建一个干净的、向后兼容的模型，在这个模型里，从端口分配、命名、服务发现、负载均衡、应用配置和迁移的角度来看，Pod 可以被视作虚拟机或者物理主机。
Kubernetes 对所有网络设施的实施，都需要满足以下的基本要求（除非有设置一些特定的网络分段策略）：
 节点上的 pods 可以不通过 NAT 和其他任何节点上的 pods 通信 节点上的代理（比如：系统守护进程、kubelet） 可以和节点上的所有pods通信  备注：仅针对那些支持 Pods 在主机网络中运行的平台(比如：Linux) ：
 那些运行在节点的主机网络里的 pods 可以不通过 NAT 和所有节点上的 pods 通信  这个模型不仅不复杂，而且还和 Kubernetes 的实现廉价的从虚拟机向容器迁移的初衷相兼容，如果你的工作开始是在虚拟机中运行的，你的虚拟机有一个 IP ，这样就可以和其他的虚拟机进行通信，这是基本相同的模型。</description>
    </item>
    
    <item>
      <title>Kubernetes 控制面的指标</title>
      <link>https://lijun.in/concepts/cluster-administration/monitoring/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/concepts/cluster-administration/monitoring/</guid>
      <description>系统组件的指标可以让我们更好的看清系统内部究竟发生了什么，尤其对于构建仪表盘和告警都非常有用。
Kubernetes 控制面板中的指标是以 prometheus 格式发出的，而且是易于阅读的。
Kubernetes 的指标 在大多数情况下，指标在 HTTP 服务器的 /metrics 端点使用，对于默认情况下不暴露端点的组件，可以使用 --bind-address 参数启用。
举例下面这些组件：
 term_id=&amp;quot;kube-controller-manager&amp;rdquo; text=&amp;quot;kube-controller-manager&amp;rdquo; &amp;gt;}} term_id=&amp;quot;kube-proxy&amp;rdquo; text=&amp;quot;kube-proxy&amp;rdquo; &amp;gt;}} term_id=&amp;quot;kube-apiserver&amp;rdquo; text=&amp;quot;kube-apiserver&amp;rdquo; &amp;gt;}} term_id=&amp;quot;kube-scheduler&amp;rdquo; text=&amp;quot;kube-scheduler&amp;rdquo; &amp;gt;}} term_id=&amp;quot;kubelet&amp;rdquo; text=&amp;quot;kubelet&amp;rdquo; &amp;gt;}}  在生产环境中，你可能需要配置 Prometheus Server 或其他指标收集器来定期收集这些指标，并使它们在某种时间序列数据库中可用。
请注意 term_id=&amp;quot;kubelet&amp;rdquo; text=&amp;quot;kubelet&amp;rdquo; &amp;gt;}} 同样在 /metrics/cadvisor、/metrics/resource 和 /metrics/probes 等端点提供性能指标。这些指标的生命周期并不相同。
如果你的集群还使用了 term_id=&amp;quot;rbac&amp;rdquo; text=&amp;quot;RBAC&amp;rdquo; &amp;gt;}} ，那读取指标数据的时候，还需要通过具有 ClusterRole 的用户、组或者 ServiceAccount 来进行授权，才有权限访问 /metrics 。
举例：
apiVersion: rbac.authorization.k8s.io/v1	kind: ClusterRole	metadata:	name: prometheus	rules:	- nonResourceURLs:	- &amp;quot;/metrics&amp;quot;	verbs:	- get 指标的生命周期 内测版指标 → 稳定版指标 → 弃用指标 → 隐藏指标 → 删除</description>
    </item>
    
    <item>
      <title>日志架构</title>
      <link>https://lijun.in/concepts/cluster-administration/logging/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/concepts/cluster-administration/logging/</guid>
      <description>应用和系统日志可以让您了解集群内部的运行状况。日志对调试问题和监控集群活动非常有用。大部分现代化应用都有某种日志记录机制；同样地，大多数容器引擎也被设计成支持某种日志记录机制。针对容器化应用，最简单且受欢迎的日志记录方式就是写入标准输出和标准错误流。
但是，由容器引擎或 runtime 提供的原生功能通常不足以满足完整的日志记录方案。例如，如果发生容器崩溃、pod 被逐出或节点宕机等情况，您仍然想访问到应用日志。因此，日志应该具有独立的存储和生命周期，与节点、pod 或容器的生命周期相独立。这个概念叫 集群级的日志 。集群级日志方案需要一个独立的后台来存储、分析和查询日志。Kubernetes 没有为日志数据提供原生存储方案，但是您可以集成许多现有的日志解决方案到 Kubernetes 集群中。
集群级日志架构假定在集群内部或者外部有一个日志后台。如果您对集群级日志不感兴趣，您仍会发现关于如何在节点上存储和处理日志的描述对您是有用的。
Kubernetes 中的基本日志记录 本节，您会看到一个kubernetes 中生成基本日志的例子，该例子中数据被写入到标准输出。 这里通过一个特定的 pod 规约 演示创建一个容器，并令该容器每秒钟向标准输出写入数据。
用下面的命令运行 pod：
kubectl apply -f https://k8s.io/examples/debug/counter-pod.yaml 输出结果为：
pod/counter created 使用 kubectl logs 命令获取日志:
kubectl logs counter 输出结果为：
0: Mon Jan 1 00:00:00 UTC 2001 1: Mon Jan 1 00:00:01 UTC 2001 2: Mon Jan 1 00:00:02 UTC 2001 ... 一旦发生容器崩溃，您可以使用命令 kubectl logs 和参数 --previous 检索之前的容器日志。 如果 pod 中有多个容器，您应该向该命令附加一个容器名以访问对应容器的日志。 详见 kubectl logs 文档。</description>
    </item>
    
    <item>
      <title>配置 kubelet 垃圾回收策略</title>
      <link>https://lijun.in/concepts/cluster-administration/kubelet-garbage-collection/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/concepts/cluster-administration/kubelet-garbage-collection/</guid>
      <description>垃圾回收是 kubelet 的一个有用功能，它将清理未使用的镜像和容器。
Kubelet 将每分钟对容器执行一次垃圾回收，每五分钟对镜像执行一次垃圾回收。
不建议使用外部垃圾收集工具，因为这些工具可能会删除原本期望存在的容器进而破坏 kubelet 的行为。
镜像回收 Kubernetes 借助于 cadvisor 通过 imageManager 来管理所有镜像的生命周期。
镜像垃圾回收策略只考虑两个因素：HighThresholdPercent 和 LowThresholdPercent。
磁盘使用率超过上限阈值（HighThresholdPercent）将触发垃圾回收。
垃圾回收将删除最近最少使用的镜像，直到磁盘使用率满足下限阈值（LowThresholdPercent）。
容器回收 容器垃圾回收策略考虑三个用户定义变量。MinAge 是容器可以被执行垃圾回收的最小生命周期。MaxPerPodContainer 是每个 pod 内允许存在的死亡容器的最大数量。 MaxContainers 是全部死亡容器的最大数量。可以分别独立地通过将 MinAge 设置为 0，以及将 MaxPerPodContainer 和 MaxContainers 设置为小于 0 来禁用这些变量。
Kubelet 将处理无法辨识的、已删除的以及超出前面提到的参数所设置范围的容器。最老的容器通常会先被移除。 MaxPerPodContainer 和 MaxContainer 在某些场景下可能会存在冲突，例如在保证每个 pod 内死亡容器的最大数量（MaxPerPodContainer）的条件下可能会超过允许存在的全部死亡容器的最大数量（MaxContainer）。 MaxPerPodContainer 在这种情况下会被进行调整：最坏的情况是将 MaxPerPodContainer 降级为 1，并驱逐最老的容器。 此外，pod 内已经被删除的容器一旦年龄超过 MinAge 就会被清理。
不被 kubelet 管理的容器不受容器垃圾回收的约束。
用户配置 用户可以使用以下 kubelet 参数调整相关阈值来优化镜像垃圾回收：
  image-gc-high-threshold，触发镜像垃圾回收的磁盘使用率百分比。默认值为 85%。
  image-gc-low-threshold，镜像垃圾回收试图释放资源后达到的磁盘使用率百分比。默认值为 80%。
  我们还允许用户通过以下 kubelet 参数自定义垃圾收集策略：</description>
    </item>
    
    <item>
      <title>控制器管理器指标</title>
      <link>https://lijun.in/concepts/cluster-administration/controller-metrics/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/concepts/cluster-administration/controller-metrics/</guid>
      <description>控制器管理器指标为控制器管理器的性能和健康提供了重要的观测手段。
什么是控制器管理器度量 控制器管理器指标为控制器管理器的性能和健康提供了重要的观测手段。 这些度量包括常见的 Go 语言运行时度量，比如 go_routine 计数，以及控制器特定的度量，比如 etcd 请求延迟或 云提供商（AWS、GCE、OpenStack）的 API 延迟，这些参数可以用来测量集群的健康状况。
从 Kubernetes 1.7 版本开始，详细的云提供商指标可用于 GCE、AWS、Vsphere 和 OpenStack 的存储操作。 这些度量可用于监视持久卷操作的健康状况。
例如，在 GCE 中这些指标叫做：
cloudprovider_gce_api_request_duration_seconds { request = &amp;quot;instance_list&amp;quot;} cloudprovider_gce_api_request_duration_seconds { request = &amp;quot;disk_insert&amp;quot;} cloudprovider_gce_api_request_duration_seconds { request = &amp;quot;disk_delete&amp;quot;} cloudprovider_gce_api_request_duration_seconds { request = &amp;quot;attach_disk&amp;quot;} cloudprovider_gce_api_request_duration_seconds { request = &amp;quot;detach_disk&amp;quot;} cloudprovider_gce_api_request_duration_seconds { request = &amp;quot;list_disk&amp;quot;} 配置 在集群中，控制器管理器指标可从它所在的主机上的 http://localhost:10252/metrics 中获得。
这些指标是以 prometheus 格式 发出的，是人类可读的。
在生产环境中，您可能想配置 prometheus 或其他一些指标收集工具，以定期收集这些指标数据，并将它们应用到某种时间序列数据库中。</description>
    </item>
    
    <item>
      <title>Kubernetes 中的代理</title>
      <link>https://lijun.in/concepts/cluster-administration/proxies/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/concepts/cluster-administration/proxies/</guid>
      <description>本文讲述了 Kubernetes 中所使用的代理。
代理 用户在使用 Kubernetes 的过程中可能遇到几种不同的代理（proxy）：
  kubectl proxy：
 运行在用户的桌面或 pod 中 从本机地址到 Kubernetes apiserver 的代理 客户端到代理使用 HTTP 协议 代理到 apiserver 使用 HTTPS 协议 指向 apiserver 添加认证头信息    apiserver proxy：
 是一个建立在 apiserver 内部的“堡垒” 将集群外部的用户与群集 IP 相连接，这些IP是无法通过其他方式访问的 运行在 apiserver 进程内 客户端到代理使用 HTTPS 协议 (如果配置 apiserver 使用 HTTP 协议，则使用 HTTP 协议) 通过可用信息进行选择，代理到目的地可能使用 HTTP 或 HTTPS 协议 可以用来访问 Node、 Pod 或 Service 当用来访问 Service 时，会进行负载均衡    kube proxy：</description>
    </item>
    
    <item>
      <title>安装扩展（Addons）</title>
      <link>https://lijun.in/concepts/cluster-administration/addons/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/concepts/cluster-administration/addons/</guid>
      <description>Add-ons 扩展了 Kubernetes 的功能。
本文列举了一些可用的 add-ons 以及到它们各自安装说明的链接。
每个 add-ons 按字母顺序排序 - 顺序不代表任何优先地位。
网络和网络策略  ACI 通过 Cisco ACI 提供集成的容器网络和安全网络。 Calico 是一个安全的 L3 网络和网络策略提供者。 Canal 结合 Flannel 和 Calico，提供网络和网络策略。 Cilium 是一个 L3 网络和网络策略插件，能够透明的实施 HTTP/API/L7 策略。同时支持路由（routing）和叠加/封装（overlay/encapsulation）模式。 CNI-Genie 使 Kubernetes 无缝连接到一种 CNI 插件，例如：Flannel、Calico、Canal、Romana 或者 Weave。 Contiv 为多种用例提供可配置网络（使用 BGP 的原生 L3，使用 vxlan 的 overlay，经典 L2 和 Cisco-SDN/ACI）和丰富的策略框架。Contiv 项目完全开源。安装工具同时提供基于和不基于 kubeadm 的安装选项。 基于 Tungsten Fabric 的 Contrail，是一个开源的多云网络虚拟化和策略管理平台，Contrail 和 Tungsten Fabric 与业务流程系统（例如 Kubernetes、OpenShift、OpenStack和Mesos）集成在一起，并为虚拟机、容器或 Pod 以及裸机工作负载提供了隔离模式。 Flannel 是一个可以用于 Kubernetes 的 overlay 网络提供者。 Knitter 是为 kubernetes 提供复合网络解决方案的网络组件。 Multus 是一个多插件，可在 Kubernetes 中提供多种网络支持，以支持所有 CNI 插件（例如 Calico，Cilium，Contiv，Flannel），而且包含了在 Kubernetes 中基于 SRIOV、DPDK、OVS-DPDK 和 VPP 的工作负载。 NSX-T 容器插件（ NCP ）提供了 VMware NSX-T 与容器协调器（例如 Kubernetes）之间的集成，以及 NSX-T 与基于容器的 CaaS / PaaS 平台（例如关键容器服务（PKS）和 OpenShift）之间的集成。 Nuage 是一个 SDN 平台，可在 Kubernetes Pods 和非 Kubernetes 环境之间提供基于策略的联网，并具有可视化和安全监控。 Romana 是一个 pod 网络的层 3 解决方案，并且支持 NetworkPolicy API。Kubeadm add-on 安装细节可以在这里找到。 Weave Net 提供了在网络分组两端参与工作的网络和网络策略，并且不需要额外的数据库。  服务发现  CoreDNS 是一种灵活的，可扩展的 DNS 服务器，可以安装为集群内的 Pod 提供 DNS 服务。  可视化管理  Dashboard 是一个 Kubernetes 的 web 控制台界面。 Weave Scope 是一个图形化工具，用于查看你的 containers、 pods、services 等。 请和一个 Weave Cloud account 一起使用，或者自己运行 UI。  基础设施  KubeVirt 是可以让 Kubernetes 运行虚拟机的 add-ons。通常运行在裸机群集上。  遗留 Add-ons 还有一些其它 add-ons 归档在已废弃的 cluster/addons 路径中。</description>
    </item>
    
    <item>
      <title>联邦</title>
      <link>https://lijun.in/concepts/cluster-administration/federation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/concepts/cluster-administration/federation/</guid>
      <description>本页面阐明了为何以及如何使用联邦创建Kubernetes集群。
为何使用联邦 联邦可以使多个集群的管理简单化。它提供了两个主要构件模块：
 跨集群同步资源：联邦能够让资源在多个集群中同步。例如，你可以确保在多个集群中存在同样的部署。 跨集群发现：联邦能够在所有集群的后端自动配置DNS服务和负载均衡。例如，通过多个集群的后端，你可以确保全局的VIP或DNS记录可用。  联邦技术的其他应用场景：
 高可用性：通过跨集群分摊负载，自动配置DNS服务和负载均衡，联邦将集群失败所带来的影响降到最低。 避免供应商锁定：跨集群使迁移应用程序变得更容易，联邦服务避免了供应商锁定。  只有在多个集群的场景下联邦服务才是有帮助的。这里列出了一些你会使用多个集群的原因：
 降低延迟：在多个区域含有集群，可使用离用户最近的集群来服务用户，从而最大限度降低延迟。 故障隔离：对于故障隔离，也许有多个小的集群比有一个大的集群要更好一些（例如：一个云供应商的不同可用域里有多个集群）。详细信息请参阅多集群指南。 可伸缩性：对于单个kubernetes集群是有伸缩性限制的（但对于大多数用户来说并非如此。更多细节参考Kubernetes扩展和性能目标）。 混合云：可以有多个集群，它们分别拥有不同的云供应商或者本地数据中心。  注意事项 虽然联邦有很多吸引人的场景，但这里还是有一些需要关注的事项：
 增加网络的带宽和损耗：联邦控制面会监控所有的集群，来确保集群的当前状态与预期一致。那么当这些集群运行在一个或者多个云提供者的不同区域中，则会带来重大的网络损耗。 降低集群的隔离：当联邦控制面中存在一个故障时，会影响所有的集群。把联邦控制面的逻辑降到最小可以缓解这个问题。 无论何时，它都是kubernetes集群里控制面的代表。设计和实现也使其变得更安全,避免多集群运行中断。 完整性：联邦项目相对较新，还不是很成熟。不是所有资源都可用，且很多资源才刚刚开始。Issue 38893 列举了一些团队正忙于解决的系统已知问题。  混合云的能力 Kubernetes集群里的联邦包括运行在不同云供应商上的集群（例如，谷歌云、亚马逊），和本地部署的集群（例如，OpenStack）。只需在适当的云供应商和/或位置创建所需的所有集群，并将每个集群的API endpoint和凭据注册到您的联邦API服务中（详情参考联邦管理指南）。
在此之后，您的API资源就可以跨越不同的集群和云供应商。
建立联邦 若要能联合多个集群，首先需要建立一个联邦控制面。参照安装指南 建立联邦控制面。
API资源 控制面建立完成后，就可以开始创建联邦API资源了。 以下指南详细介绍了一些资源：
 Cluster ConfigMap DaemonSets Deployment Events Ingress Namespaces ReplicaSets Secrets Services  API参考文档列举了联邦API服务支持的所有资源。
级联删除 Kubernetes1.6版本支持联邦资源级联删除。使用级联删除，即当删除联邦控制面的一个资源时，也删除了所有底层集群中的相应资源。
当使用REST API时，级联删除功能不是默认开启的。若使用REST API从联邦控制面删除一个资源时，要开启级联删除功能，即需配置选项 DeleteOptions.orphanDependents=false。使用kubectl delete使级联删除功能默认开启。使用kubectl delete --cascade=false禁用级联删除功能。
注意：Kubernetes1.5版本开始支持联邦资源子集的级联删除。
单个集群的范围 对于IaaS供应商如谷歌计算引擎或亚马逊网络服务，一个虚拟机存在于一个域或可用域中。 我们建议一个Kubernetes集群里的所有虚机应该在相同的可用域里，因为：
 与单一的全局Kubernetes集群对比，该方式有较少的单点故障。 与跨可用域的集群对比，该方式更容易推断单区域集群的可用性属性。 当Kubernetes开发者设计一个系统(例如，对延迟、带宽或相关故障进行假设)，他们也会假设所有的机器都在一个单一的数据中心，或者以其他方式紧密相连。  每个可用区域里包含多个集群当然是可以的，但是总的来说我们认为集群数越少越好。 偏爱较少集群数的原因是：</description>
    </item>
    
  </channel>
</rss>