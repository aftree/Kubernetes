[
{
	"uri": "https://lijun.in/concepts/containers/overview/",
	"title": "容器概述",
	"tags": [],
	"description": "",
	"content": "容器是一种用来打包已经编译好的代码以及运行时需要的各个依赖项的技术。您运行的每个容器都是可以重复运行的；包含依赖项的标准化意味着您在任何地点运行它都会得到相同的结果。\n容器将应用程序和底层主机架构解耦，这使得在不同的云或OS环境中部署应用更加容易。\n容器镜像 容器镜像是一个现成的软件包，包含了运行应用程序时所需要的一切：代码和任何运行时所需的东西，应用程序和系统库，以及任何基本设置的默认值。\n根据设计，容器是不可变的：你不能更改已经在运行的容器中的代码。如果您有一个容器化的应用程序，想要做一些更改，您需要构建一个新的容器，来包含所做的更改，然后使用已经更新过的镜像来重新创建容器。\n##容器运行时\nterm_id=\u0026quot;container-runtime\u0026rdquo; length=\u0026quot;all\u0026rdquo; \u0026gt;}}\n  阅读有关容器镜像 阅读有关 Pods  "
},
{
	"uri": "https://lijun.in/",
	"title": "文档",
	"tags": [],
	"description": "",
	"content": "Kubernetes 文档\n"
},
{
	"uri": "https://lijun.in/reference/glossary/",
	"title": "😍 - 标准化词汇表",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lijun.in/tutorials/hello-minikube/",
	"title": "😎 - 你好 Minikube",
	"tags": [],
	"description": "",
	"content": "本教程向您展示如何使用 Minikube 和 Katacoda 在 Kubernetes 上运行一个简单的 “Hello World” Node.js 应用程序。Katacoda 提供免费的浏览器内 Kubernetes 环境。\n. note \u0026gt;}}\n如果您已在本地安装 Minikube，也可以按照本教程操作。\n. /note \u0026gt;}}\nheading \u0026ldquo;objectives\u0026rdquo; %}}  将 \u0026ldquo;Hello World\u0026rdquo; 应用程序部署到 Minikube。 运行应用程序。 查看应用日志  . heading \u0026ldquo;prerequisites\u0026rdquo; %}} 本教程提供了从以下文件构建的容器镜像：\n. codenew language=\u0026quot;js\u0026rdquo; file=\u0026quot;minikube/server.js\u0026rdquo; \u0026gt;}}\n. codenew language=\u0026quot;conf\u0026rdquo; file=\u0026quot;minikube/Dockerfile\u0026rdquo; \u0026gt;}}\n有关 docker build 命令的更多信息，请参阅 Docker 文档。\n创建 Minikube 集群   点击 启动终端\n. kat-button \u0026gt;}}\n. note \u0026gt;}}If you installed Minikube locally, run minikube start.. /note \u0026gt;}}\n   在浏览器中打开 Kubernetes dashboard：\nminikube dashboard    仅限 Katacoda 环境：在终端窗口的顶部，单击加号，然后单击 选择要在主机 1 上查看的端口。\n  仅限 Katacoda 环境：输入“30000”，然后单击 显示端口。\n  创建 Deployment Kubernetes Pod 是由一个或多个为了管理和联网而绑定在一起的容器构成的组。本教程中的 Pod 只有一个容器。Kubernetes Deployment 检查 Pod 的健康状况，并在 Pod 中的容器终止的情况下重新启动新的容器。Deployment 是管理 Pod 创建和扩展的推荐方法。\n  使用 kubectl create 命令创建管理 Pod 的 Deployment。该 Pod 根据提供的 Docker 镜像运行 Container。\nkubectl create deployment hello-node --image=k8s.gcr.io/echoserver:1.4    查看 Deployment：\nkubectl get deployments   输出结果类似于这样： ``` NAME READY UP-TO-DATE AVAILABLE AGE hello-node 1/1 1 1 1m ```   查看 Pod：\nkubectl get pods   输出结果类似于这样： ``` NAME READY STATUS RESTARTS AGE hello-node-5f76cf6ccf-br9b5 1/1 Running 0 1m ```   查看集群事件：\nkubectl get events    查看 kubectl 配置：\nkubectl config view   . note \u0026gt;}}有关 kubectl 命令的更多信息，请参阅 kubectl 概述。. /note \u0026gt;}}\n创建 Service 默认情况下，Pod 只能通过 Kubernetes 集群中的内部 IP 地址访问。要使得 hello-node 容器可以从 Kubernetes 虚拟网络的外部访问，您必须将 Pod 暴露为 Kubernetes Service。\n  使用 kubectl expose 命令将 Pod 暴露给公网：\nkubectl expose deployment hello-node --type=LoadBalancer --port=8080 The --type=LoadBalancer flag indicates that you want to expose your Service outside of the cluster.\n   查看您刚刚创建的服务:\nkubectl get services   输出结果类似于这样: ``` NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE hello-node LoadBalancer 10.108.144.78 \u0026lt;pending\u0026gt; 8080:30369/TCP 21s kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 23m ```  在支持负载均衡器的云服务提供商上，将提供一个外部 IP 来访问该服务。在 Minikube 上，LoadBalancer 使得服务可以通过命令 minikube service 访问。\n 运行下面的命令：\nminikube service hello-node   仅限 Katacoda 环境：单击加号，然后单击 选择要在主机 1 上查看的端口。   仅限 Katacoda 环境：请注意在 service 输出中与 8080 对应的长度为 5 位的端口号。此端口号是随机生成的，可能与您不同。在端口号文本框中输入您自己的端口号，然后单击显示端口。如果是上面那个例子，就需要输入 30369。\n这将打开一个浏览器窗口，为您的应用程序提供服务并显示 “Hello World” 消息。\n  启用插件 Minikube 有一组内置的 . glossary_tooltip text=\u0026quot;插件\u0026rdquo; term_id=\u0026quot;addons\u0026rdquo; \u0026gt;}}，可以在本地 Kubernetes 环境中启用、禁用和打开。\n  列出当前支持的插件：\nminikube addons list   输出结果类似于这样： ``` addon-manager: enabled dashboard: enabled default-storageclass: enabled efk: disabled freshpod: disabled gvisor: disabled helm-tiller: disabled ingress: disabled ingress-dns: disabled logviewer: disabled metrics-server: disabled nvidia-driver-installer: disabled nvidia-gpu-device-plugin: disabled registry: disabled registry-creds: disabled storage-provisioner: enabled storage-provisioner-gluster: disabled ```   启用插件，例如 metrics-server：\nminikube addons enable metrics-server   输出结果类似于这样： ``` metrics-server was successfully enabled ```   查看刚才创建的 Pod 和 Service：\nkubectl get pod,svc -n kube-system   输出结果类似于这样： ``` NAME READY STATUS RESTARTS AGE pod/coredns-5644d7b6d9-mh9ll 1/1 Running 0 34m pod/coredns-5644d7b6d9-pqd2t 1/1 Running 0 34m pod/metrics-server-67fb648c5 1/1 Running 0 26s pod/etcd-minikube 1/1 Running 0 34m pod/influxdb-grafana-b29w8 2/2 Running 0 26s pod/kube-addon-manager-minikube 1/1 Running 0 34m pod/kube-apiserver-minikube 1/1 Running 0 34m pod/kube-controller-manager-minikube 1/1 Running 0 34m pod/kube-proxy-rnlps 1/1 Running 0 34m pod/kube-scheduler-minikube 1/1 Running 0 34m pod/storage-provisioner 1/1 Running 0 34m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/metrics-server ClusterIP 10.96.241.45 \u0026lt;none\u0026gt; 80/TCP 26s service/kube-dns ClusterIP 10.96.0.10 \u0026lt;none\u0026gt; 53/UDP,53/TCP 34m service/monitoring-grafana NodePort 10.99.24.54 \u0026lt;none\u0026gt; 80:30002/TCP 26s service/monitoring-influxdb ClusterIP 10.111.169.94 \u0026lt;none\u0026gt; 8083/TCP,8086/TCP 26s ```   禁用 metrics-server：\nminikube addons disable metrics-server   输出结果类似于这样： ``` metrics-server was successfully disabled ```  清理 现在可以清理您在集群中创建的资源：\nkubectl delete service hello-node kubectl delete deployment hello-node 可选的，停止 Minikube 虚拟机（VM）：\nminikube stop 可选的，删除 Minikube 虚拟机（VM）：\nminikube delete . heading \u0026ldquo;whatsnext\u0026rdquo; %}}  进一步了解 Deployment 对象。 学习更多关于 部署应用。 学习更多关于 Service 对象。  "
},
{
	"uri": "https://lijun.in/concepts/services-networking/endpoint-slices/",
	"title": "Endpoint Slices",
	"tags": [],
	"description": "",
	"content": "feature-state for_k8s_version=\u0026quot;v1.17\u0026rdquo; state=\u0026quot;beta\u0026rdquo;\nEndpoint Slices 提供了一种简单的方法来跟踪 Kubernetes 集群中的网络端点（network endpoints）。它们为 Endpoints 提供了一种可伸缩和可拓展的替代方案。\nEndpoint Slice 资源 在 Kubernetes 中，EndpointSlice 包含对一组网络端点的引用。指定选择器后，EndpointSlice 控制器会自动为 Kubernetes 服务创建 EndpointSlice。这些 EndpointSlice 将包含对与服务选择器匹配的所有 Pod 的引用。EndpointSlice 通过唯一的服务和端口组合将网络端点组织在一起。\n例如，这里是 Kubernetes服务 example 的示例 EndpointSlice 资源。\napiVersion: discovery.k8s.io/v1beta1 kind: EndpointSlice metadata: name: example-abc labels: kubernetes.io/service-name: example addressType: IPv4 ports: - name: http protocol: TCP port: 80 endpoints: - addresses: - \u0026#34;10.1.2.3\u0026#34; conditions: ready: true hostname: pod-1 topology: kubernetes.io/hostname: node-1 topology.kubernetes.io/zone: us-west2-a 默认情况下，由 EndpointSlice 控制器管理的 Endpoint Slice 将有不超过 100 个 endpoints。低于此比例时，Endpoint Slices 应与 Endpoints 和服务进行 1：1 映射，并具有相似的性能。\n当涉及如何路由内部流量时，Endpoint Slices 可以充当 kube-proxy 的真实来源。启用该功能后，在服务的 endpoints 规模庞大时会有可观的性能提升。\n地址类型 EndpointSlice 支持三种地址类型：\n IPv4 IPv6 FQDN (完全合格的域名)  动机 Endpoints API 提供了一种简单明了的方法在 Kubernetes 中跟踪网络端点。不幸的是，随着 Kubernetes 集群与服务的增长，该 API 的局限性变得更加明显。最值得注意的是，这包含了扩展到更多网络端点的挑战。\n由于服务的所有网络端点都存储在单个 Endpoints 资源中，因此这些资源可能会变得很大。这影响了 Kubernetes 组件（尤其是主控制平面）的性能，并在 Endpoints 发生更改时导致大量网络流量和处理。Endpoint Slices 可帮助您缓解这些问题并提供可扩展的 附加特性（例如拓扑路由）平台。\nwhatsnext  启用 Endpoint Slices 阅读 Connecting Applications with Services  "
},
{
	"uri": "https://lijun.in/reference/setup-tools/kubeadm/",
	"title": "Kubeadm",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lijun.in/reference/setup-tools/kubeadm/kubeadm/",
	"title": "kubeadm 概述",
	"tags": [],
	"description": "",
	"content": "kubeadm 通过执行必要的操作来启动和运行一个最小可用的集群。它被故意设计为只关心启动集群，而不是准备节点环境的工作。同样的，诸如安装各种各样的可有可无的插件，例如 Kubernetes 控制面板、监控解决方案以及特定云提供商的插件，这些都不在它负责的范围。\n相反，我们期望由一个基于 kubeadm 从更高层设计的更加合适的工具来做这些事情；并且，理想情况下，使用 kubeadm 作为所有部署的基础将会使得创建一个符合期望的集群变得容易。\n接下可以做什么  kubeadm init 启动一个 Kubernetes 主节点   kubeadm join 启动一个 Kubernetes 工作节点并且将其加入到集群   kubeadm upgrade 更新一个 Kubernetes 集群到新版本   kubeadm config 如果你使用 kubeadm v1.7.x 或者更低版本，你需要对你的集群做一些配置以便使用 kubeadm upgrade 命令   kubeadm token 使用 kubeadm join 来管理令牌   kubeadm reset 还原之前使用 kubeadm init 或者 kubeadm join 对节点产生的改变   kubeadm version 打印出 kubeadm 版本   kubeadm alpha 预览一组可用的新功能以便从社区搜集反馈  "
},
{
	"uri": "https://lijun.in/reference/using-api/api-overview/",
	"title": "Kubernetes API 总览",
	"tags": [],
	"description": "",
	"content": "此页提供 Kubernetes API 的总览\nREST API 是 Kubernetes 的基础架构。组件之间的所有操作和通信，以及外部用户命令都是 API Server 处理的 REST API 调用。因此，Kubernetes 平台中的所有资源被视为 API 对象，并且在 [API](/docs/reference/generated/kubernetes-api/. param \u0026ldquo;version\u0026rdquo; \u0026gt;}}/) 中都有对应的定义项。\n大多数操作可以通过 kubectl 命令行界面或其他命令行工具执行，例如 kubeadm，它们本身也使用 API。但是，您也可以使用 REST 调用直接访问 API。\n如果您正在使用 Kubernetes API 编写应用程序，请考虑使用 客户端库。\nAPI 版本控制 为了消除字段或重组资源表示形式，Kubernetes 支持多个 API 版本，每个版本在不同的 API 路径下。例如：/api/v1 或者 /apis/extensions/v1beta1。\n版本是在 API 级别而非资源或字段级别配置的：\n 确保 API 呈现出清晰一致的系统资源和行为视图。 允许控制对已寿终正寝的 API 和/或实验性 API 的访问。  JSON 和 Protobuf 序列化模式在出现模式变更时均遵循这些准则。以下说明同时适用于这两种格式。\n. note \u0026gt;}} API 版本和软件版本是间接相关的。API 和发布版本建议 描述了 API 版本和软件版本之间的关系。 . /note \u0026gt;}}\n不同的 API 版本表示不同级别的稳定性和支持级别。您可以在 API 变更文档 中找到有关每个级别的条件的更多信息。\n以下是每个级别的摘要：\n  Alpha：\n 版本名称包含 alpha（例如，v1alpha1）。 该软件可能包含错误。启用功能可能会暴露错误。默认情况下，功能可能被禁用。 对功能的支持随时可能被删除，但不另行通知。 在以后的软件版本中，API 可能会以不兼容的方式更改，亦不另行通知。 由于存在更高的错误风险和缺乏长期支持，建议仅在短期测试集群中使用该软件。    Beta：\n 版本名称包含beta（例如，v2beta3）。 该软件已经过充分测试。启用功能被认为是安全的。默认情况下启用功能。 尽管细节可能会发生变更，对应功能不会被废弃。 在随后的 Beta 或稳定版本中，对象的模式和/或语义可能会以不兼容的方式更改。发生这种情况时，将提供迁移说明。迁移时可能需要删除、编辑和重新创建 API 对象。编辑过程可能需要一些思考。对于依赖该功能的应用程序，可能需要停机。 该软件仅建议用于非关键业务用途，因为在后续版本中可能会发生不兼容的更改。如果您有多个可以独立升级的群集，则可以放宽此限制。    . note \u0026gt;}}\n请试用 Beta 版功能并提供反馈。功能结束 Beta 版之后，再进行变更可能是不切实际的。\n. /note \u0026gt;}}\n 稳定版：  版本名称为 vX，其中X为整数。 功能特性的稳定版本会持续出现在许多后续版本的发行软件中。    API 组 API 组 使扩展 Kubernetes API 更容易。API 组在 REST 路径和序列化对象的 apiVersion 字段中指定。\n当前，有几个正在使用的 API 组：\n core（也称为 legacy）组，它位于 REST 路径/api/v1上，未指定为 apiVersion 字段的一部分，例如apiVersion: v1。 特定名称的组位于 REST 路径/apis/$GROUP_NAME/$VERSION下，并使用apiVersion:$GROUP_NAME/$VERSION（例如，apiVersion:batch/v1）。您可以在 Kubernetes API 参考 中找到受支持的 API Group 的完整列表。  有两种途径来使用 自定义资源 扩展 API，分别是：\n CustomResourceDefinition 提供基本的 CRUD 需求。 聚合器（Aggregator）具有完整的 Kubernetes API 语义，用以实现用户自己的 apiserver。  启用 API 组 默认情况下，某些资源和 API 组处于启用状态。您可以通过设置--runtime-config来启用或禁用它们。 --runtime-config 接受逗号分隔的值。例如：\n 要禁用 batch/v1，请配置--runtime-config=batch/v1=false 要启用 batch/2alpha1，请配置--runtime-config=batch/v2alpha1 该标志接受描述 apiserver 的运行时配置的以逗号分隔的key=value 对集合。  . note \u0026gt;}}\n启用或禁用组或资源时，需要重新启动 apiserver 和控制器管理器以刷新 --runtime-config 的更改。\n. /note \u0026gt;}}\n启用 extensions/v1beta1 组中具体资源 在 extensions/v1beta1 API 组中，DaemonSets，Deployments，StatefulSet, NetworkPolicies, PodSecurityPolicies 和 ReplicaSets 是默认禁用的。 例如：要启用 deployments 和 daemonsets，请设置 --runtime-config=extensions/v1beta1/deployments=true,extensions/v1beta1/daemonsets=true。\n. note \u0026gt;}}\n出于遗留原因，仅在 extensions / v1beta1 API 组中支持各个资源的启用/禁用。\n. /note \u0026gt;}}\n"
},
{
	"uri": "https://lijun.in/concepts/overview/what-is-kubernetes/",
	"title": "Kubernetes 是什么？",
	"tags": [],
	"description": "",
	"content": "此页面是 Kubernetes 的概述。\nKubernetes 是一个可移植的、可扩展的开源平台，用于管理容器化的工作负载和服务，可促进声明式配置和自动化。Kubernetes 拥有一个庞大且快速增长的生态系统。Kubernetes 的服务、支持和工具广泛可用。\n名称 Kubernetes 源于希腊语，意为 \u0026ldquo;舵手\u0026rdquo; 或 \u0026ldquo;飞行员\u0026rdquo;。Google 在 2014 年开源了 Kubernetes 项目。Kubernetes 建立在 Google 在大规模运行生产工作负载方面拥有十几年的经验的基础上，结合了社区中最好的想法和实践。\n言归正传 让我们回顾一下为什么 Kubernetes 如此有用。\n传统部署时代： 早期，组织在物理服务器上运行应用程序。无法为物理服务器中的应用程序定义资源边界，这会导致资源分配问题。例如，如果在物理服务器上运行多个应用程序，则可能会出现一个应用程序占用大部分资源的情况，结果可能导致其他应用程序的性能下降。一种解决方案是在不同的物理服务器上运行每个应用程序，但是由于资源利用不足而无法扩展，并且组织维护许多物理服务器的成本很高。\n虚拟化部署时代： 作为解决方案，引入了虚拟化功能，它允许您在单个物理服务器的 CPU 上运行多个虚拟机（VM）。虚拟化功能允许应用程序在 VM 之间隔离，并提供安全级别，因为一个应用程序的信息不能被另一应用程序自由地访问。\n因为虚拟化可以轻松地添加或更新应用程序、降低硬件成本等等，所以虚拟化可以更好地利用物理服务器中的资源，并可以实现更好的可伸缩性。\n每个 VM 是一台完整的计算机，在虚拟化硬件之上运行所有组件，包括其自己的操作系统。\n容器部署时代： 容器类似于 VM，但是它们具有轻量级的隔离属性，可以在应用程序之间共享操作系统（OS）。因此，容器被认为是轻量级的。容器与 VM 类似，具有自己的文件系统、CPU、内存、进程空间等。由于它们与基础架构分离，因此可以跨云和 OS 分发进行移植。\n容器因具有许多优势而变得流行起来。下面列出了容器的一些好处：\n 敏捷应用程序的创建和部署：与使用 VM 镜像相比，提高了容器镜像创建的简便性和效率。 持续开发、集成和部署：通过快速简单的回滚(由于镜像不可变性)，提供可靠且频繁的容器镜像构建和部署。 关注开发与运维的分离：在构建/发布时而不是在部署时创建应用程序容器镜像，从而将应用程序与基础架构分离。 可观察性不仅可以显示操作系统级别的信息和指标，还可以显示应用程序的运行状况和其他指标信号。 跨开发、测试和生产的环境一致性：在便携式计算机上与在云中相同地运行。 云和操作系统分发的可移植性：可在 Ubuntu、RHEL、CoreOS、本地、Google Kubernetes Engine 和其他任何地方运行。 以应用程序为中心的管理：提高抽象级别，从在虚拟硬件上运行 OS 到使用逻辑资源在 OS 上运行应用程序。 松散耦合、分布式、弹性、解放的微服务：应用程序被分解成较小的独立部分，并且可以动态部署和管理 - 而不是在一台大型单机上整体运行。 资源隔离：可预测的应用程序性能。 资源利用：高效率和高密度。  为什么需要 Kubernetes，它能做什么? 容器是打包和运行应用程序的好方式。在生产环境中，您需要管理运行应用程序的容器，并确保不会停机。例如，如果一个容器发生故障，则需要启动另一个容器。如果系统处理此行为，会不会更容易？\n这就是 Kubernetes 的救援方法！Kubernetes 为您提供了一个可弹性运行分布式系统的框架。Kubernetes 会满足您的扩展要求、故障转移、部署模式等。例如，Kubernetes 可以轻松管理系统的 Canary 部署。\nKubernetes 为您提供：\n 服务发现和负载均衡\nKubernetes 可以使用 DNS 名称或自己的 IP 地址公开容器，如果到容器的流量很大，Kubernetes 可以负载均衡并分配网络流量，从而使部署稳定。   存储编排\nKubernetes 允许您自动挂载您选择的存储系统，例如本地存储、公共云提供商等。   自动部署和回滚\n您可以使用 Kubernetes 描述已部署容器的所需状态，它可以以受控的速率将实际状态更改为所需状态。例如，您可以自动化 Kubernetes 来为您的部署创建新容器，删除现有容器并将它们的所有资源用于新容器。   自动二进制打包\nKubernetes 允许您指定每个容器所需 CPU 和内存（RAM）。当容器指定了资源请求时，Kubernetes 可以做出更好的决策来管理容器的资源。   自我修复\nKubernetes 重新启动失败的容器、替换容器、杀死不响应用户定义的运行状况检查的容器，并且在准备好服务之前不将其通告给客户端。   密钥与配置管理\nKubernetes 允许您存储和管理敏感信息，例如密码、OAuth 令牌和 ssh 密钥。您可以在不重建容器镜像的情况下部署和更新密钥和应用程序配置，也无需在堆栈配置中暴露密钥。  Kubernetes 不是什么 Kubernetes 不是传统的、包罗万象的 PaaS（平台即服务）系统。由于 Kubernetes 在容器级别而不是在硬件级别运行，因此它提供了 PaaS 产品共有的一些普遍适用的功能，例如部署、扩展、负载均衡、日志记录和监视。但是，Kubernetes 不是单一的，默认解决方案是可选和可插拔的。Kubernetes 提供了构建开发人员平台的基础，但是在重要的地方保留了用户的选择和灵活性。\nKubernetes：\n Kubernetes 不限制支持的应用程序类型。Kubernetes 旨在支持极其多种多样的工作负载，包括无状态、有状态和数据处理工作负载。如果应用程序可以在容器中运行，那么它应该可以在 Kubernetes 上很好地运行。 Kubernetes 不部署源代码，也不构建您的应用程序。持续集成(CI)、交付和部署（CI/CD）工作流取决于组织的文化和偏好以及技术要求。 Kubernetes 不提供应用程序级别的服务作为内置服务，例如中间件（例如，消息中间件）、数据处理框架（例如，Spark）、数据库（例如，mysql）、缓存、集群存储系统（例如，Ceph）。这样的组件可以在 Kubernetes 上运行，并且/或者可以由运行在 Kubernetes 上的应用程序通过可移植机制（例如，开放服务代理）来访问。   Kubernetes 不指定日志记录、监视或警报解决方案。它提供了一些集成作为概念证明，并提供了收集和导出指标的机制。 Kubernetes 不提供或不要求配置语言/系统（例如 jsonnet），它提供了声明性 API，该声明性 API 可以由任意形式的声明性规范所构成。 Kubernetes 不提供也不采用任何全面的机器配置、维护、管理或自我修复系统。 此外，Kubernetes 不仅仅是一个编排系统，实际上它消除了编排的需要。编排的技术定义是执行已定义的工作流程：首先执行 A，然后执行 B，再执行 C。相比之下，Kubernetes 包含一组独立的、可组合的控制过程，这些过程连续地将当前状态驱动到所提供的所需状态。从 A 到 C 的方式无关紧要，也不需要集中控制，这使得系统更易于使用且功能更强大、健壮、弹性和可扩展性。    查阅 Kubernetes 组件 开始 Kubernetes 入门?  "
},
{
	"uri": "https://lijun.in/reference/issues-security/issues/",
	"title": "Kubernetes 问题追踪",
	"tags": [],
	"description": "",
	"content": "要报告安全问题，请遵循 Kubernetes 安全问题公开流程。\n使用 GitHub Issues 跟踪 Kubernetes 编码工作和公开问题。\n CVE 相关问题  与安全性相关的公告请发送到 kubernetes-security-announce@googlegroups.com 邮件列表。\n"
},
{
	"uri": "https://lijun.in/concepts/workloads/pods/pod-overview/",
	"title": "Pod 概览",
	"tags": [],
	"description": "",
	"content": "本节提供了 Pod 的概览信息，Pod 是最小可部署的 Kubernetes 对象模型。\n理解 Pod Pod 是 Kubernetes 应用程序的基本执行单元，即它是 Kubernetes 对象模型中创建或部署的最小和最简单的单元。Pod 表示在 glossary_tooltip term_id=\u0026quot;cluster\u0026rdquo; \u0026gt;}} 上运行的进程。\nPod 封装了应用程序容器（或者在某些情况下封装多个容器）、存储资源、唯一网络 IP 以及控制容器应该如何运行的选项。 Pod 表示部署单元：Kubernetes 中应用程序的单个实例，它可能由单个glossary_tooltip text=\u0026quot;容器\u0026rdquo; term_id=\u0026quot;container\u0026rdquo; \u0026gt;}} 或少量紧密耦合并共享资源的容器组成。\nDocker 是 Kubernetes Pod 中最常用的容器运行时，但 Pod 也能支持其他的容器运行时。\nKubernetes 集群中的 Pod 可被用于以下两个主要用途：\n 运行单个容器的 Pod。\u0026ldquo;每个 Pod 一个容器\u0026quot;模型是最常见的 Kubernetes 用例；在这种情况下，可以将 Pod 看作单个容器的包装器，并且 Kubernetes 直接管理 Pod，而不是容器。 运行多个协同工作的容器的 Pod。 Pod 可能封装由多个紧密耦合且需要共享资源的共处容器组成的应用程序。 这些位于同一位置的容器可能形成单个内聚的服务单元 —— 一个容器将文件从共享卷提供给公众，而另一个单独的“挂斗”（sidecar）容器则刷新或更新这些文件。 Pod 将这些容器和存储资源打包为一个可管理的实体。 Kubernetes 博客 上有一些其他的 Pod 用例信息。更多信息请参考：   分布式系统工具包：容器组合的模式 容器设计模式  每个 Pod 表示运行给定应用程序的单个实例。如果希望横向扩展应用程序（例如，运行多个实例），则应该使用多个 Pod，每个应用实例使用一个 Pod 。在 Kubernetes 中，这通常被称为 副本。通常使用一个称为控制器的抽象来创建和管理一组副本 Pod。更多信息请参见 Pod 和控制器。\nPod 怎样管理多个容器 Pod 被设计成支持形成内聚服务单元的多个协作过程（作为容器）。 Pod 中的容器被自动的安排到集群中的同一物理或虚拟机上，并可以一起进行调度。 容器可以共享资源和依赖、彼此通信、协调何时以及何种方式终止它们。\n注意，在单个 Pod 中将多个并置和共同管理的容器分组是一个相对高级的使用方式。 只在容器紧密耦合的特定实例中使用此模式。 例如，您可能有一个充当共享卷中文件的 Web 服务器的容器，以及一个单独的 sidecar 容器，该容器从远端更新这些文件，如下图所示：\n  有些 Pod 具有 glossary_tooltip text=\u0026quot;初始容器\u0026rdquo; term_id=\u0026quot;init-container\u0026rdquo; \u0026gt;}} 和 glossary_tooltip text=\u0026quot;应用容器\u0026rdquo; term_id=\u0026quot;app-container\u0026rdquo; \u0026gt;}}。初始容器会在启动应用容器之前运行并完成。\nPod 为其组成容器提供了两种共享资源：网络 和 存储。\n网络 每个 Pod 分配一个唯一的 IP 地址。 Pod 中的每个容器共享网络命名空间，包括 IP 地址和网络端口。 Pod 内的容器 可以使用 localhost 互相通信。 当 Pod 中的容器与 Pod 之外 的实体通信时，它们必须协调如何使用共享的网络资源（例如端口）。\n存储 一个 Pod 可以指定一组共享存储glossary_tooltip text=\u0026quot;卷\u0026rdquo; term_id=\u0026quot;volume\u0026rdquo; \u0026gt;}}。 Pod 中的所有容器都可以访问共享卷，允许这些容器共享数据。 卷还允许 Pod 中的持久数据保留下来，以防其中的容器需要重新启动。 有关 Kubernetes 如何在 Pod 中实现共享存储的更多信息，请参考卷。\n使用 Pod 你很少在 Kubernetes 中直接创建单独的 Pod，甚至是单个存在的 Pod。 这是因为 Pod 被设计成了相对短暂的一次性的实体。 当 Pod 由您创建或者间接地由控制器创建时，它被调度在集群中的 glossary_tooltip term_id=\u0026quot;node\u0026rdquo; \u0026gt;}} 上运行。 Pod 会保持在该节点上运行，直到进程被终止、Pod 对象被删除、Pod 因资源不足而被 驱逐 或者节点失效为止。\n重启 Pod 中的容器不应与重启 Pod 混淆。Pod 本身不运行，而是作为容器运行的环境，并且一直保持到被删除为止。\nPod 本身并不能自愈。 如果 Pod 被调度到失败的节点，或者如果调度操作本身失败，则删除该 Pod；同样，由于缺乏资源或进行节点维护，Pod 在被驱逐后将不再生存。 Kubernetes 使用了一个更高级的称为 控制器 的抽象，由它处理相对可丢弃的 Pod 实例的管理工作。 因此，虽然可以直接使用 Pod，但在 Kubernetes 中，更为常见的是使用控制器管理 Pod。 有关 Kubernetes 如何使用控制器实现 Pod 伸缩和愈合的更多信息，请参考 Pod 和控制器。\nPod 和控制器 控制器可以为您创建和管理多个 Pod，管理副本和上线，并在集群范围内提供自修复能力。 例如，如果一个节点失败，控制器可以在不同的节点上调度一样的替身来自动替换 Pod。\n包含一个或多个 Pod 的控制器一些示例包括：\n Deployment StatefulSet DaemonSet  控制器通常使用您提供的 Pod 模板来创建它所负责的 Pod。\nPod 模板 Pod 模板是包含在其他对象中的 Pod 规范，例如 Replication Controllers、 Jobs 和 DaemonSets。 控制器使用 Pod 模板来制作实际使用的 Pod。 下面的示例是一个简单的 Pod 清单，它包含一个打印消息的容器。\napiVersion: v1 kind: Pod metadata: name: myapp-pod labels: app: myapp spec: containers: - name: myapp-container image: busybox command: [\u0026#39;sh\u0026#39;, \u0026#39;-c\u0026#39;, \u0026#39;echo Hello Kubernetes! \u0026amp;\u0026amp; sleep 3600\u0026#39;] Pod 模板就像饼干切割器，而不是指定所有副本的当前期望状态。 一旦饼干被切掉，饼干就与切割器没有关系。 没有“量子纠缠”。 随后对模板的更改或甚至切换到新的模板对已经创建的 Pod 没有直接影响。 类似地，由副本控制器创建的 Pod 随后可以被直接更新。 这与 Pod 形成有意的对比，Pod 指定了属于 Pod 的所有容器的当前期望状态。 这种方法从根本上简化了系统语义，增加了原语的灵活性。\n% heading \u0026ldquo;whatsnext\u0026rdquo;\n 详细了解 Pod 了解有关 Pod 行为的更多信息：  Pod 的终止 Pod 的生命周期    "
},
{
	"uri": "https://lijun.in/concepts/workloads/pods/",
	"title": "Pods",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lijun.in/concepts/workloads/controllers/replicaset/",
	"title": "ReplicaSet",
	"tags": [],
	"description": "",
	"content": "ReplicaSet 是下一代的 Replication Controller。 ReplicaSet 和 Replication Controller 的唯一区别是选择器的支持。ReplicaSet 支持新的基于集合的选择器需求，这在标签用户指南中有描述。而 Replication Controller 仅支持基于相等选择器的需求。\n怎样使用 ReplicaSet 大多数支持 Replication Controllers 的kubectl命令也支持 ReplicaSets。但rolling-update 命令是个例外。如果您想要滚动更新功能请考虑使用 Deployment。rolling-update 命令是必需的，而 Deployment 是声明性的，因此我们建议通过 rollout命令使用 Deployment。\n虽然 ReplicaSets 可以独立使用，但今天它主要被Deployments 用作协调 Pod 创建、删除和更新的机制。 当您使用 Deployment 时，您不必担心还要管理它们创建的 ReplicaSet。Deployment 会拥有并管理它们的 ReplicaSet。\n什么时候使用 ReplicaSet ReplicaSet 确保任何时间都有指定数量的 Pod 副本在运行。 然而，Deployment 是一个更高级的概念，它管理 ReplicaSet，并向 Pod 提供声明式的更新以及许多其他有用的功能。 因此，我们建议使用 Deployment 而不是直接使用 ReplicaSet，除非您需要自定义更新业务流程或根本不需要更新。\n这实际上意味着，您可能永远不需要操作 ReplicaSet 对象：而是使用 Deployment，并在 spec 部分定义您的应用。\n示例 codenew file=\u0026quot;controllers/frontend.yaml\u0026rdquo; \u0026gt;}}\n将此清单保存到 frontend.yaml 中，并将其提交到 Kubernetes 集群，应该就能创建 yaml 文件所定义的 ReplicaSet 及其管理的 Pod。\n$ kubectl create -f http://k8s.io/examples/controllers/frontend.yaml replicaset.apps/frontend created $ kubectl describe rs/frontend Name:\tfrontend Namespace:\tdefault Selector:\ttier=frontend,tier in (frontend) Labels:\tapp=guestbook tier=frontend Annotations:\t\u0026lt;none\u0026gt; Replicas:\t3 current / 3 desired Pods Status:\t3 Running / 0 Waiting / 0 Succeeded / 0 Failed Pod Template: Labels: app=guestbook tier=frontend Containers: php-redis: Image: gcr.io/google_samples/gb-frontend:v3 Port: 80/TCP Requests: cpu: 100m memory: 100Mi Environment: GET_HOSTS_FROM: dns Mounts: \u0026lt;none\u0026gt; Volumes: \u0026lt;none\u0026gt; Events: FirstSeen LastSeen Count From SubobjectPath Type Reason Message --------- -------- ----- ---- ------------- -------- ------ ------- 1m 1m 1 {replicaset-controller } Normal SuccessfulCreate Created pod: frontend-qhloh 1m 1m 1 {replicaset-controller } Normal SuccessfulCreate Created pod: frontend-dnjpy 1m 1m 1 {replicaset-controller } Normal SuccessfulCreate Created pod: frontend-9si5l $ kubectl get pods NAME READY STATUS RESTARTS AGE frontend-9si5l 1/1 Running 0 1m frontend-dnjpy 1/1 Running 0 1m frontend-qhloh 1/1 Running 0 1m 编写 ReplicaSet Spec 与所有其他 Kubernetes API 对象一样，ReplicaSet 也需要 apiVersion、kind、和 metadata 字段。有关使用清单的一般信息，请参见 使用 kubectl 管理对象。\nReplicaSet 也需要 .spec 部分。\nPod 模版 .spec.template 是 .spec 唯一需要的字段。.spec.template 是 Pod 模版。它和 Pod 的语法几乎完全一样，除了它是嵌套的并没有 apiVersion 和 kind。\n除了所需的 Pod 字段之外，ReplicaSet 中的 Pod 模板必须指定适当的标签和适当的重启策略。\n对于标签，请确保不要与其他控制器重叠。更多信息请参考 Pod 选择器。\n对于 重启策略，.spec.template.spec.restartPolicy 唯一允许的取值是 Always，这也是默认值.\n对于本地容器重新启动，ReplicaSet 委托给了节点上的代理去执行，例如Kubelet 或 Docker 去执行。\nPod 选择器 .spec.selector 字段是标签选择器。ReplicaSet 管理所有标签匹配与标签选择器的 Pod。它不区分自己创建或删除的 Pod 和其他人或进程创建或删除的pod。这允许在不影响运行中的 Pod 的情况下替换副本集。\n.spec.template.metadata.labels 必须匹配 .spec.selector，否则它将被 API 拒绝。\nKubernetes 1.9 版本中，API 版本 apps/v1 中的 ReplicaSet 类型的版本是当前版本并默认开启。API 版本 apps/v1beta2 被弃用。\n另外，通常您不应该创建标签与此选择器匹配的任何 Pod，或者直接与另一个 ReplicaSet 或另一个控制器（如 Deployment）标签匹配的任何 Pod。 如果你这样做，ReplicaSet 会认为它创造了其他 Pod。Kubernetes 并不会阻止您这样做。\n如果您最终使用了多个具有重叠选择器的控制器，则必须自己负责删除。\nReplicas 通过设置 .spec.replicas 您可以指定要同时运行多少个 Pod。 在任何时间运行的 Pod 数量可能高于或低于 .spec.replicas 指定的数量，例如在副本刚刚被增加或减少后、或者 Pod 正在被优雅地关闭、以及替换提前开始。\n如果您没有指定 .spec.replicas, 那么默认值为 1。\n使用 ReplicaSets 的具体方法 删除 ReplicaSet 和它的 Pod 要删除 ReplicaSet 和它的所有 Pod，使用kubectl delete 命令。 默认情况下，垃圾收集器 自动删除所有依赖的 Pod。\n当使用 REST API 或 client-go 库时，您必须在删除选项中将 propagationPolicy 设置为 Background 或 Foreground。例如：\nkubectl proxy --port=8080 curl -X DELETE \u0026#39;localhost:8080/apis/apps/v1/namespaces/default/replicasets/frontend\u0026#39; \\ \u0026gt; -d \u0026#39;{\u0026#34;kind\u0026#34;:\u0026#34;DeleteOptions\u0026#34;,\u0026#34;apiVersion\u0026#34;:\u0026#34;v1\u0026#34;,\u0026#34;propagationPolicy\u0026#34;:\u0026#34;Foreground\u0026#34;}\u0026#39; \\ \u0026gt; -H \u0026#34;Content-Type: application/json\u0026#34; 只删除 ReplicaSet 您可以只删除 ReplicaSet 而不影响它的 Pod，方法是使用kubectl delete 命令并设置 --cascade=false 选项。\n当使用 REST API 或 client-go 库时，您必须将 propagationPolicy 设置为 Orphan。例如：\nkubectl proxy --port=8080 curl -X DELETE \u0026#39;localhost:8080/apis/apps/v1/namespaces/default/replicasets/frontend\u0026#39; \\ \u0026gt; -d \u0026#39;{\u0026#34;kind\u0026#34;:\u0026#34;DeleteOptions\u0026#34;,\u0026#34;apiVersion\u0026#34;:\u0026#34;v1\u0026#34;,\u0026#34;propagationPolicy\u0026#34;:\u0026#34;Orphan\u0026#34;}\u0026#39; \\ \u0026gt; -H \u0026#34;Content-Type: application/json\u0026#34; 一旦删除了原来的 ReplicaSet，就可以创建一个新的来替换它。 由于新旧 ReplicaSet 的 .spec.selector 是相同的，新的 ReplicaSet 将接管老的 Pod。 但是，它不会努力使现有的 Pod 与新的、不同的 Pod 模板匹配。 若想要以可控的方式将 Pod 更新到新的 spec，就要使用 滚动更新的方式。\n将 Pod 从 ReplicaSet 中隔离 可以通过改变标签来从 ReplicaSet 的目标集中移除 Pod。这种技术可以用来从服务中去除 Pod，以便进行排错、数据恢复等。 以这种方式移除的 Pod 将被自动替换（假设副本的数量没有改变）。\n缩放 RepliaSet 通过更新 .spec.replicas 字段，ReplicaSet 可以被轻松的进行缩放。ReplicaSet 控制器能确保匹配标签选择器的数量的 Pod 是可用的和可操作的。\nReplicaSet 作为水平的 Pod 自动缩放器目标 ReplicaSet 也可以作为 水平的 Pod 缩放器 (HPA) 的目标。也就是说，ReplicaSet 可以被 HPA 自动缩放。 以下是 HPA 以我们在前一个示例中创建的副本集为目标的示例。\ncodenew file=\u0026quot;controllers/hpa-rs.yaml\u0026rdquo; \u0026gt;}}\n将这个列表保存到 hpa-rs.yaml 并提交到 Kubernetes 集群，就能创建它所定义的 HPA，进而就能根据复制的 Pod 的 CPU 利用率对目标 ReplicaSet进行自动缩放。\nkubectl create -f https://k8s.io/examples/controllers/hpa-rs.yaml 或者，可以使用 kubectl autoscale 命令完成相同的操作。 (而且它更简单！)\nkubectl autoscale rs frontend ReplicaSet 的替代方案 Deployment （推荐） Deployment 是一个高级 API 对象，它以 kubectl rolling-update 的方式更新其底层副本集及其Pod。 如果您需要滚动更新功能，建议使用 Deployment，因为 Deployment 与 kubectl rolling-update 不同的是：它是声明式的、服务器端的、并且具有其他特性。 有关使用 Deployment 来运行无状态应用的更多信息，请参阅 使用 Deployment 运行无状态应用。\n裸 Pod 与用户直接创建 Pod 的情况不同，ReplicaSet 会替换那些由于某些原因被删除或被终止的 Pod，例如在节点故障或破坏性的节点维护（如内核升级）的情况下。 因为这个好处，我们建议您使用 ReplicaSet，即使应用程序只需要一个 Pod。 想像一下，ReplicaSet 类似于进程监视器，只不过它在多个节点上监视多个 Pod，而不是在单个节点上监视单个进程。 ReplicaSet 将本地容器重启的任务委托给了节点上的某个代理（例如，Kubelet 或 Docker）去完成。\nJob 使用Job 代替ReplicaSet，可以用于那些期望自行终止的 Pod。\nDaemonSet 对于管理那些提供主机级别功能（如主机监控和主机日志）的容器，就要用DaemonSet 而不用 ReplicaSet。 这些 Pod 的寿命与主机寿命有关：这些 Pod 需要先于主机上的其他 Pod 运行，并且在机器准备重新启动/关闭时安全地终止。\n"
},
{
	"uri": "https://lijun.in/concepts/services-networking/service-topology/",
	"title": "Service 拓扑",
	"tags": [],
	"description": "",
	"content": "feature-state for_k8s_version=\u0026quot;v1.17\u0026rdquo; state=\u0026quot;alpha\u0026rdquo; \u0026gt;}}\nService 拓扑可以让一个服务基于集群的 Node 拓扑进行流量路由。例如，一个服务可以指定流量是被优先路由到一个和客户端在同一个 Node 或者在同一可用区域的端点。\n介绍 默认情况下，发往 ClusterIP 或者 NodePort 服务的流量可能会被路由到任意一个服务后端的地址上。从 Kubernetes 1.7 开始，可以将“外部”流量路由到节点上运行的 pod 上，但不支持 ClusterIP 服务，更复杂的拓扑 — 比如分区路由 — 也还不支持。通过允许 Service 创建者根据源 Node 和目的 Node 的标签来定义流量路由策略，Service 拓扑特性实现了服务流量的路由。\n通过对源 Node 和目的 Node 标签的匹配，运营者可以使用任何符合运营者要求的度量值来指定彼此“较近”和“较远”的节点组。例如，对于在公有云上的运营者来说，更偏向于把流量控制在同一区域内，因为区域间的流量是有费用成本的，而区域内的流量没有。其它常用需求还包括把流量路由到由 DaemonSet 管理的本地 Pod 上，或者把保持流量在连接同一机架交换机的 Node 上，以获得低延时。\n前提条件 为了启用拓扑感知服务路由功能，必须要满足以下一些前提条件：\n Kubernetes 的版本不低于 1.17 Kube-proxy 运行在 iptables 模式或者 IPVS 模式 启用 端点切片功能  启用 Service 拓扑 要启用 Service 拓扑，就要给 kube-apiserver 和 kube-proxy 启用 ServiceTopology 功能：\n--feature-gates=\u0026quot;ServiceTopology=true\u0026quot; 使用 Service 拓扑 如果集群启用了 Service 拓扑功能后，就可以在 Service 配置中指定 topologyKeys 字段，从而控制 Service 的流量路由。此字段是 Node 标签的优先顺序字段，将用于在访问这个 Service 时对端点进行排序。流量会被定向到第一个标签值和源 Node 标签值相匹配的 Node。如果这个 Service 没有匹配的后端 Node，那么第二个标签会被使用做匹配，以此类推，直到没有标签。\n如果没有匹配到，流量会被拒绝，就如同这个 Service 根本没有后端。这是根据有可用后端的第一个拓扑键来选择端点的。如果这个字段被配置了而没有后端可以匹配客户端拓扑，那么这个 Service 对那个客户端是没有后端的，链接应该是失败的。这个字段配置为 \u0026quot;*\u0026quot; 意味着任意拓扑。这个通配符值如果使用了，那么只有作为配置值列表中的最后一个才有用。\n如果 topologyKeys 没有指定或者为空，就没有启用这个拓扑功能。\n一个集群中，其 Node 的标签被打为其主机名，区域名和地区名。那么就可以设置 Service 的 topologyKeys 的值，像下面的做法一样定向流量了。\n 只定向到同一个 Node 上的端点，Node 上没有端点存在时就失败：配置 [\u0026quot;kubernetes.io/hostname\u0026quot;]。 偏向定向到同一个 Node 上的端点，回退同一区域的端点上，然后是同一地区，其它情况下就失败：配置 [\u0026quot;kubernetes.io/hostname\u0026quot;, \u0026quot;topology.kubernetes.io/zone\u0026quot;, \u0026quot;topology.kubernetes.io/region\u0026quot;]。这或许很有用，例如，数据局部性很重要的情况下。 偏向于同一区域，但如果此区域中没有可用的终结点，则回退到任何可用的终结点：配置 [\u0026quot;topology.kubernetes.io/zone\u0026quot;, \u0026quot;*\u0026quot;]。  约束条件   Service 拓扑和 externalTrafficPolicy=Local 是不兼容的，所以 Service 不能同时使用这两种特性。但是在同一个集群的不同 Service 上是可以分别使用这两种特性的，只要不在同一个 Service 上就可以。\n  有效的拓扑键目前只有：kubernetes.io/hostname，topology.kubernetes.io/zone 和 topology.kubernetes.io/region，但是未来会推广到其它的 Node 标签。\n  拓扑键必须是有效的标签，并且最多指定16个。\n  通配符：\u0026quot;*\u0026quot;，如果要用，那必须是拓扑键值的最后一个值。\n  whatsnext  阅读关于启用服务拓扑 阅读用 Services 连接应用程序  "
},
{
	"uri": "https://lijun.in/concepts/services-networking/service/",
	"title": "Services",
	"tags": [],
	"description": "",
	"content": "apiVersion: v1 kind: Service # string Required 资源类型 metadata: # Object Required 元数据 name: string # string Required Service名称 namespace: string # string Required 命名空间，默认default labels: # list Required 自定义标签属性列表 - name: string annotations: # list Required 自定义注解属性列表 - name: string spec: # Object Required 详细描述 selector: [] # list Required Label Selector配置，将选择具有指定Label标签的Pod作为管理范围 type: string # string Required Service的类型，指定Service的访问方式，默认为ClusterIP # ClusterIP：虚拟的服务IP地址，改地址用于Kubernetes集群内部的Pod访问，在Node上kube-proxy通过设置iptables规则进行转发 # NodePort: 使用宿主机的端口，使能够访问个Node的外部客户端通过Node的IP地址和端口号就能访问服务 # LoadBalancer: 使用外接负载均衡完成到服务的负载分发，需要在spec.status.loadBalancer字段指定外部负载均衡器的IP地址， # 并同时定义nodePort和clusterIP，用于公有云环境 clusterIP: string # string 虚拟服务IP地址，当type=ClusterIP时，如果不指定，则系统自动分配，也可以手动指定；当type=LoaderBalancer需要指定 sessionAffinity: string # string 是否支持Session，可选值为ClientIP，默认为空 # ClientIP: 表示将同一个客户端（根据IP地址决定）的访问请求都转发到同一个后端Pod ports: # list 需要暴露的端口列表 - name: string # 端口名称 protocol: string # 端口协议 支持tcp和udp，默认为tcp port: int # 服务监听的端口号 targetPort: int # 需要转发到后端Pod的端口号 nodePort: int # 当spec.type=NodePort时，指定映射到物理机的端口号 status: # 当spec.type=LoadBalancer时，设置外部负载均衡器的地址，用于公有云环境 loadBalancer: # 外部负载均衡器 ingress: # 外部负载均衡器 ip: string # 外部负载均衡器的IP地址 hostname: string # 外部负载均衡器的主机名 glossary_definition term_id=\u0026quot;service\u0026rdquo; length=\u0026quot;short\u0026rdquo; \u0026gt;}}\n使用Kubernetes，您无需修改应用程序即可使用不熟悉的服务发现机制。 Kubernetes为Pods提供自己的IP地址和一组Pod的单个DNS名称，并且可以在它们之间进行负载平衡。\n动机 Kubernetes glossary_tooltip term_id=\u0026quot;pod\u0026rdquo; text=\u0026quot;Pods\u0026rdquo; \u0026gt;}} 是有生命周期的。他们可以被创建，而且销毁不会再启动。 如果您使用 lossary_tooltip term_id =\u0026quot;deployment\u0026quot;\u0026gt;}} 来运行您的应用程序，则它可以动态创建和销毁 Pod。\n每个 Pod 都有自己的 IP 地址，但是在 Deployment 中，在同一时刻运行的 Pod 集合可能与稍后运行该应用程序的 Pod 集合不同。\n这导致了一个问题： 如果一组 Pod（称为“后端”）为群集内的其他 Pod（称为“前端”）提供功能，那么前端如何找出并跟踪要连接的 IP 地址，以便前端可以使用工作量的后端部分？\n进入 Services。\nService 资源 Kubernetes Service 定义了这样一种抽象：逻辑上的一组 Pod，一种可以访问它们的策略 —— 通常称为微服务。 这一组 Pod 能够被 Service 访问到，通常是通过 _tooltip text=\u0026quot;selector\u0026rdquo; term_id=\u0026quot;selector\u0026rdquo; \u0026gt;}} （查看下面了解，为什么你可能需要没有 selector 的 Service）实现的。\n举个例子，考虑一个图片处理 backend，它运行了3个副本。这些副本是可互换的 —— frontend 不需要关心它们调用了哪个 backend 副本。 然而组成这一组 backend 程序的 Pod 实际上可能会发生变化，frontend 客户端不应该也没必要知道，而且也不需要跟踪这一组 backend 的状态。 Service 定义的抽象能够解耦这种关联。\n云原生服务发现 如果您想要在应用程序中使用 Kubernetes 接口进行服务发现，则可以查询 ary_tooltip text=\u0026quot;API server\u0026rdquo; term_id=\u0026quot;kube-apiserver\u0026rdquo; \u0026gt;}} 的 endpoint 资源，只要服务中的Pod集合发生更改，端点就会更新。\n对于非本机应用程序，Kubernetes提供了在应用程序和后端Pod之间放置网络端口或负载均衡器的方法。\n定义 Service 一个 Service 在 Kubernetes 中是一个 REST 对象，和 Pod 类似。 像所有的 REST 对象一样， Service 定义可以基于 POST 方式，请求 API server 创建新的实例。\n例如，假定有一组 Pod，它们对外暴露了 9376 端口，同时还被打上 app=MyApp 标签。\napiVersion: v1 kind: Service metadata: name: my-service spec: selector: app: MyApp ports: - protocol: TCP port: 80 targetPort: 9376 上述配置创建一个名称为 \u0026ldquo;my-service\u0026rdquo; 的 Service 对象，它会将请求代理到使用 TCP 端口 9376，并且具有标签 \u0026quot;app=MyApp\u0026quot; 的 Pod 上。 Kubernetes 为该服务分配一个 IP 地址（有时称为 \u0026ldquo;集群IP\u0026rdquo; ），该 IP 地址由服务代理使用。 (请参见下面的 VIP 和 Service 代理). 服务选择器的控制器不断扫描与其选择器匹配的 Pod，然后将所有更新发布到也称为 “my-service” 的Endpoint对象。\n需要注意的是， Service 能够将一个接收 port 映射到任意的 targetPort。 默认情况下，targetPort 将被设置为与 port 字段相同的值。\nPod中的端口定义具有名称字段，您可以在服务的 targetTarget 属性中引用这些名称。 即使服务中使用单个配置的名称混合使用 Pod，并且通过不同的端口号提供相同的网络协议，此功能也可以使用。 这为部署和发展服务提供了很大的灵活性。 例如，您可以更改Pods在新版本的后端软件中公开的端口号，而不会破坏客户端。\n服务的默认协议是TCP。 您还可以使用任何其他 受支持的协议。\n由于许多服务需要公开多个端口，因此 Kubernetes 在服务对象上支持多个端口定义。 每个端口定义可以具有相同的 protocol，也可以具有不同的协议。\n没有 selector 的 Service 服务最常见的是抽象化对 Kubernetes Pod 的访问，但是它们也可以抽象化其他种类的后端。 实例:\n 希望在生产环境中使用外部的数据库集群，但测试环境使用自己的数据库。 希望服务指向另一个 lossary_tooltip term_id=\u0026quot;namespace\u0026rdquo; \u0026gt;}} 中或其它集群中的服务。 您正在将工作负载迁移到 Kubernetes。 在评估该方法时，您仅在 Kubernetes 中运行一部分后端。  在任何这些场景中，都能够定义没有 selector 的 Service。 实例:\napiVersion: v1 kind: Service metadata: name: my-service spec: ports: - protocol: TCP port: 80 targetPort: 9376 由于此服务没有选择器，因此 不会 自动创建相应的 Endpoint 对象。 您可以通过手动添加 Endpoint 对象，将服务手动映射到运行该服务的网络地址和端口：\napiVersion: v1 kind: Endpoints metadata: name: my-service subsets: - addresses: - ip: 192.0.2.42 ports: - port: 9376 端点 IPs 必须不可以 : 环回( IPv4 的 127.0.0.0/8 , IPv6 的 ::1/128 ）或本地链接（IPv4 的 169.254.0.0/16 和 224.0.0.0/24，IPv6 的 fe80::/64)。 端点 IP 地址不能是其他 Kubernetes Services 的群集 IP，因为 glossary_tooltip term_id =\u0026quot;kube-proxy\u0026quot;\u0026gt;}} 不支持将虚拟 IP 作为目标。\n访问没有 selector 的 Service，与有 selector 的 Service 的原理相同。 请求将被路由到用户定义的 Endpoint， YAML中为: 192.0.2.42:9376 (TCP)。\nExternalName Service 是 Service 的特例，它没有 selector，也没有使用 DNS 名称代替。 有关更多信息，请参阅本文档后面的ExternalName。\nEndpoint 切片 feature-state for_k8s_version=\u0026quot;v1.16\u0026rdquo; state=\u0026quot;alpha\u0026rdquo; \u0026gt;}}\nEndpoint 切片是一种 API 资源，可以为 Endpoint 提供更可扩展的替代方案。 尽管从概念上讲与 Endpoint 非常相似，但 Endpoint 切片允许跨多个资源分布网络端点。 默认情况下，一旦到达100个 Endpoint，该 Endpoint 切片将被视为“已满”，届时将创建其他 Endpoint 切片来存储任何其他 Endpoint。\nEndpoint 切片提供了附加的属性和功能，这些属性和功能在 Endpoint 切片中进行了详细描述。\nVIP 和 Service 代理 在 Kubernetes 集群中，每个 Node 运行一个 kube-proxy 进程。kube-proxy 负责为 Service 实现了一种 VIP（虚拟 IP）的形式，而不是 ExternalName 的形式。\n为什么不使用 DNS 轮询？ 时不时会有人问道，就是为什么 Kubernetes 依赖代理将入站流量转发到后端。 那其他方法呢？ 例如，是否可以配置具有多个A值（或IPv6为AAAA）的DNS记录，并依靠轮询名称解析？\n使用服务代理有以下几个原因：\n DNS 实现的历史由来已久，它不遵守记录 TTL，并且在名称查找结果到期后对其进行缓存。 有些应用程序仅执行一次 DNS 查找，并无限期地缓存结果。 即使应用和库进行了适当的重新解析，DNS 记录上的 TTL 值低或为零也可能会给 DNS 带来高负载，从而使管理变得困难。  版本兼容性 从Kubernetes v1.0开始，您已经可以使用 用户空间代理模式。 Kubernetes v1.1添加了 iptables 模式代理，在 Kubernetes v1.2 中，kube-proxy 的 iptables 模式成为默认设置。 Kubernetes v1.8添加了 ipvs 代理模式。\nuserspace 代理模式 这种模式，kube-proxy 会监视 Kubernetes master 对 Service 对象和 Endpoints 对象的添加和移除。 对每个 Service，它会在本地 Node 上打开一个端口（随机选择）。 任何连接到“代理端口”的请求，都会被代理到 Service 的backend Pods 中的某个上面（如 Endpoints 所报告的一样）。 使用哪个 backend Pod，是 kube-proxy 基于 SessionAffinity 来确定的。\n最后，它配置 iptables 规则，捕获到达该 Service 的 clusterIP（是虚拟 IP）和 Port 的请求，并重定向到代理端口，代理端口再代理请求到 backend Pod。\n默认情况下，用户空间模式下的kube-proxy通过循环算法选择后端。\n默认的策略是，通过 round-robin 算法来选择 backend Pod。\niptables 代理模式 这种模式，kube-proxy 会监视 Kubernetes 控制节点对 Service 对象和 Endpoints 对象的添加和移除。 对每个 Service，它会配置 iptables 规则，从而捕获到达该 Service 的 clusterIP 和端口的请求，进而将请求重定向到 Service 的一组 backend 中的某个上面。 对于每个 Endpoints 对象，它也会配置 iptables 规则，这个规则会选择一个 backend 组合。\n默认的策略是，kube-proxy 在 iptables 模式下随机选择一个 backend。\n使用 iptables 处理流量具有较低的系统开销，因为流量由 Linux netfilter 处理，而无需在用户空间和内核空间之间切换。 这种方法也可能更可靠。\n如果 kube-proxy 在 iptables 模式下运行，并且所选的第一个 Pod 没有响应，则连接失败。 这与用户空间模式不同：在这种情况下，kube-proxy 将检测到与第一个 Pod 的连接已失败，并会自动使用其他后端 Pod 重试。\n您可以使用 Pod readiness 探测器 验证后端 Pod 可以正常工作，以便 iptables 模式下的 kube-proxy 仅看到测试正常的后端。 这样做意味着您避免将流量通过 kube-proxy 发送到已知已失败的Pod。\nIPVS 代理模式 feature-state for_k8s_version=\u0026quot;v1.11\u0026rdquo; state=\u0026quot;stable\u0026rdquo; \u0026gt;}}\n在 ipvs 模式下，kube-proxy监视Kubernetes服务和端点，调用 netlink 接口相应地创建 IPVS 规则， 并定期将 IPVS 规则与 Kubernetes 服务和端点同步。 该控制循环可确保　IPVS　状态与所需状态匹配。 访问服务时，IPVS　将流量定向到后端Pod之一。\nIPVS代理模式基于类似于 iptables 模式的 netfilter 挂钩函数，但是使用哈希表作为基础数据结构，并且在内核空间中工作。 这意味着，与 iptables 模式下的 kube-proxy 相比，IPVS 模式下的 kube-proxy 重定向通信的延迟要短，并且在同步代理规则时具有更好的性能。与其他代理模式相比，IPVS 模式还支持更高的网络流量吞吐量。\nIPVS提供了更多选项来平衡后端Pod的流量。 这些是：\n rr: round-robin lc: least connection (smallest number of open connections) dh: destination hashing sh: source hashing sed: shortest expected delay nq: never queue  要在 IPVS 模式下运行 kube-proxy，必须在启动 kube-proxy 之前使 IPVS Linux 在节点上可用。\n当 kube-proxy 以 IPVS 代理模式启动时，它将验证 IPVS 内核模块是否可用。 如果未检测到 IPVS 内核模块，则 kube-proxy 将退回到以 iptables 代理模式运行。\n在这些代理模型中，绑定到服务IP的流量：在客户端不了解Kubernetes或服务或Pod的任何信息的情况下，将Port代理到适当的后端。 如果要确保每次都将来自特定客户端的连接传递到同一Pod，则可以通过将 service.spec.sessionAffinity 设置为 \u0026ldquo;ClientIP\u0026rdquo; (默认值是 \u0026ldquo;None\u0026rdquo;)，来基于客户端的IP地址选择会话关联。\n您还可以通过适当设置 service.spec.sessionAffinityConfig.clientIP.timeoutSeconds 来设置最大会话停留时间。 （默认值为 10800 秒，即 3 小时）。\n多端口 Service 对于某些服务，您需要公开多个端口。 Kubernetes允许您在Service对象上配置多个端口定义。 为服务使用多个端口时，必须提供所有端口名称，以使它们无歧义。 例如：\napiVersion: v1 kind: Service metadata: name: my-service spec: selector: app: MyApp ports: - name: http protocol: TCP port: 80 targetPort: 9376 - name: https protocol: TCP port: 443 targetPort: 9377 与一般的Kubernetes名称一样，端口名称只能包含 小写字母数字字符 和 -。 端口名称还必须以字母数字字符开头和结尾。\n例如，名称 123-abc 和 web 有效，但是 123_abc 和 -web 无效。\n选择自己的 IP 地址 在 Service 创建的请求中，可以通过设置 spec.clusterIP 字段来指定自己的集群 IP 地址。 比如，希望替换一个已经已存在的 DNS 条目，或者遗留系统已经配置了一个固定的 IP 且很难重新配置。\n用户选择的 IP 地址必须合法，并且这个 IP 地址在 service-cluster-ip-range CIDR 范围内，这对 API Server 来说是通过一个标识来指定的。 如果 IP 地址不合法，API Server 会返回 HTTP 状态码 422，表示值不合法。\n服务发现 Kubernetes 支持2种基本的服务发现模式 —— 环境变量和 DNS。\n环境变量 当 Pod 运行在 Node 上，kubelet 会为每个活跃的 Service 添加一组环境变量。 它同时支持 Docker links兼容 变量（查看 [makeLinkVariables](http://releases.k8s.io/\u0026lt; param \u0026ldquo;githubbranch\u0026rdquo; \u0026gt;}}/pkg/kubelet/envvars/envvars.go#L49)）、简单的 {SVCNAME}_SERVICE_HOST 和 {SVCNAME}_SERVICE_PORT 变量，这里 Service 的名称需大写，横线被转换成下划线。\n举个例子，一个名称为 \u0026quot;redis-master\u0026quot; 的 Service 暴露了 TCP 端口 6379，同时给它分配了 Cluster IP 地址 10.0.0.11，这个 Service 生成了如下环境变量：\nREDIS_MASTER_SERVICE_HOST=10.0.0.11 REDIS_MASTER_SERVICE_PORT=6379 REDIS_MASTER_PORT=tcp://10.0.0.11:6379 REDIS_MASTER_PORT_6379_TCP=tcp://10.0.0.11:6379 REDIS_MASTER_PORT_6379_TCP_PROTO=tcp REDIS_MASTER_PORT_6379_TCP_PORT=6379 REDIS_MASTER_PORT_6379_TCP_ADDR=10.0.0.11 当您具有需要访问服务的Pod时，并且您正在使用环境变量方法将端口和群集IP发布到客户端Pod时，必须在客户端Pod出现 之前 创建服务。 否则，这些客户端Pod将不会设定其环境变量。\n如果仅使用DNS查找服务的群集IP，则无需担心此设定问题。\nDNS 您可以（几乎总是应该）使用附加组件为Kubernetes集群设置DNS服务。\n支持群集的DNS服务器（例如CoreDNS）监视 Kubernetes API 中的新服务，并为每个服务创建一组 DNS 记录。 如果在整个群集中都启用了 DNS，则所有 Pod 都应该能够通过其 DNS 名称自动解析服务。\n例如，如果您在 Kubernetes 命名空间 \u0026quot;my-ns\u0026quot; 中有一个名为 \u0026quot;my-service\u0026quot; 的服务， 则控制平面和DNS服务共同为 \u0026quot;my-service.my-ns\u0026quot; 创建 DNS 记录。 \u0026quot;my-ns\u0026quot; 命名空间中的Pod应该能够通过简单地对 my-service 进行名称查找来找到它（ \u0026quot;my-service.my-ns\u0026quot; 也可以工作）。\n其他命名空间中的Pod必须将名称限定为 my-service.my-ns 。 这些名称将解析为为服务分配的群集IP。\nKubernetes 还支持命名端口的 DNS SRV（服务）记录。 如果 \u0026quot;my-service.my-ns\u0026quot; 服务具有名为 \u0026quot;http\u0026quot;　的端口，且协议设置为TCP， 则可以对 _http._tcp.my-service.my-ns 执行DNS SRV查询查询以发现该端口号, \u0026quot;http\u0026quot;以及IP地址。\nKubernetes DNS 服务器是唯一的一种能够访问 ExternalName 类型的 Service 的方式。 更多关于 ExternalName 信息可以查看DNS Pod 和 Service。\nHeadless Services 有时不需要或不想要负载均衡，以及单独的 Service IP。 遇到这种情况，可以通过指定 Cluster IP（spec.clusterIP）的值为 \u0026quot;None\u0026quot; 来创建 Headless Service。\n您可以使用 headless Service 与其他服务发现机制进行接口，而不必与 Kubernetes 的实现捆绑在一起。\n对这 headless Service 并不会分配 Cluster IP，kube-proxy 不会处理它们，而且平台也不会为它们进行负载均衡和路由。 DNS 如何实现自动配置，依赖于 Service 是否定义了 selector。\n配置 Selector 对定义了 selector 的 Headless Service，Endpoint 控制器在 API 中创建了 Endpoints 记录，并且修改 DNS 配置返回 A 记录（地址），通过这个地址直接到达 Service 的后端 Pod 上。\n不配置 Selector 对没有定义 selector 的 Headless Service，Endpoint 控制器不会创建 Endpoints 记录。 然而 DNS 系统会查找和配置，无论是：\n ExternalName 类型 Service 的 CNAME 记录 记录：与 Service 共享一个名称的任何 Endpoints，以及所有其它类型  发布服务 —— 服务类型 对一些应用（如 Frontend）的某些部分，可能希望通过外部Kubernetes 集群外部IP 地址暴露 Service。\nKubernetes ServiceTypes 允许指定一个需要的类型的 Service，默认是 ClusterIP 类型。\nType 的取值以及行为如下：\n  ClusterIP：通过集群的内部 IP 暴露服务，选择该值，服务只能够在集群内部可以访问，这也是默认的 ServiceType。\n  NodePort：通过每个 Node 上的 IP 和静态端口（NodePort）暴露服务。NodePort 服务会路由到 ClusterIP 服务，这个 ClusterIP 服务会自动创建。通过请求 \u0026lt;NodeIP\u0026gt;:\u0026lt;NodePort\u0026gt;，可以从集群的外部访问一个 NodePort 服务。\n  LoadBalancer：使用云提供商的负载局衡器，可以向外部暴露服务。外部的负载均衡器可以路由到 NodePort 服务和 ClusterIP 服务。\n  ExternalName：通过返回 CNAME 和它的值，可以将服务映射到 externalName 字段的内容（例如， foo.bar.example.com）。 没有任何类型代理被创建。\n您需要 CoreDNS 1.7 或更高版本才能使用 ExternalName 类型。\n  您也可以使用 Ingress 来暴露自己的服务。 Ingress 不是服务类型，但它充当集群的入口点。 它可以将路由规则整合到一个资源中，因为它可以在同一IP地址下公开多个服务。\nNodePort 类型 如果将 type 字段设置为 NodePort，则 Kubernetes 控制平面将在 --service-node-port-range 标志指定的范围内分配端口（默认值：30000-32767）。 每个节点将那个端口（每个节点上的相同端口号）代理到您的服务中。 您的服务在其 .spec.ports[*].nodePort 字段中要求分配的端口。\n如果您想指定特定的IP代理端口，则可以将 kube-proxy 中的 --nodeport-addresses 标志设置为特定的IP块。从Kubernetes v1.10开始支持此功能。\n该标志采用逗号分隔的IP块列表（例如10.0.0.0/8、192.0.2.0/25）来指定 kube-proxy 应该认为是此节点本地的IP地址范围。\n例如，如果您使用 --nodeport-addresses=127.0.0.0/8 标志启动 kube-proxy，则 kube-proxy 仅选择 NodePort Services 的环回接口。 --nodeport-addresses 的默认值是一个空列表。 这意味着 kube-proxy 应该考虑 NodePort 的所有可用网络接口。 （这也与早期的Kubernetes版本兼容）。\n如果需要特定的端口号，则可以在 nodePort 字段中指定一个值。 控制平面将为您分配该端口或向API报告事务失败。 这意味着您需要自己注意可能发生的端口冲突。 您还必须使用有效的端口号，该端口号在配置用于NodePort的范围内。\n使用 NodePort 可以让您自由设置自己的负载平衡解决方案，配置 Kubernetes 不完全支持的环境，甚至直接暴露一个或多个节点的IP。\n需要注意的是，Service 能够通过 \u0026lt;NodeIP\u0026gt;:spec.ports[*].nodePort 和 spec.clusterIp:spec.ports[*].port 而对外可见。\nLoadBalancer 类型 使用支持外部负载均衡器的云提供商的服务，设置 type 的值为 \u0026quot;LoadBalancer\u0026quot;，将为 Service 提供负载均衡器。 负载均衡器是异步创建的，关于被提供的负载均衡器的信息将会通过 Service 的 status.loadBalancer 字段被发布出去。 实例:\napiVersion: v1 kind: Service metadata: name: my-service spec: selector: app: MyApp ports: - protocol: TCP port: 80 targetPort: 9376 clusterIP: 10.0.171.239 loadBalancerIP: 78.11.24.19 type: LoadBalancer status: loadBalancer: ingress: - ip: 146.148.47.155 来自外部负载均衡器的流量将直接打到 backend Pod 上，不过实际它们是如何工作的，这要依赖于云提供商。\n在这些情况下，将根据用户设置的 loadBalancerIP 来创建负载均衡器。 某些云提供商允许设置 loadBalancerIP。如果没有设置 loadBalancerIP，将会给负载均衡器指派一个临时 IP。 如果设置了 loadBalancerIP，但云提供商并不支持这种特性，那么设置的 loadBalancerIP 值将会被忽略掉。\n如果您使用的是 SCTP，请参阅下面有关 LoadBalancer 服务类型的 caveat。\n在 Azure 上，如果要使用用户指定的公共类型 loadBalancerIP ，则首先需要创建静态类型的公共IP地址资源。 此公共IP地址资源应与群集中其他自动创建的资源位于同一资源组中。 例如，MC_myResourceGroup_myAKSCluster_eastus。\n将分配的IP地址指定为loadBalancerIP。 确保您已更新云提供程序配置文件中的securityGroupName。 有关对 CreatingLoadBalancerFailed 权限问题进行故障排除的信息， 请参阅 与Azure Kubernetes服务（AKS）负载平衡器一起使用静态IP地址或通过高级网络在AKS群集上创建LoadBalancerFailed。\n内部负载均衡器 在混合环境中，有时有必要在同一(虚拟)网络地址块内路由来自服务的流量。\n在水平分割 DNS 环境中，您需要两个服务才能将内部和外部流量都路由到您的 endpoints。 您可以通过向服务添加以下注释之一来实现此目的。 要添加的注释取决于您使用的云服务提供商。\ntabs name=\u0026quot;service_tabs\u0026rdquo; \u0026gt;}} tab name=\u0026quot;Default\u0026rdquo; %}}\n选择一个标签 /tab %}} tab name=\u0026quot;GCP\u0026rdquo; %}}\n[...] metadata: name: my-service annotations: cloud.google.com/load-balancer-type: \u0026#34;Internal\u0026#34; [...] 将 cloud.google.com/load-balancer-type: \u0026quot;internal\u0026quot; 节点用于版本1.7.0至1.7.3的主服务器。 有关更多信息，请参见 文档. /tab %}} tab name=\u0026quot;AWS\u0026rdquo; %}}\n[...] metadata: name: my-service annotations: service.beta.kubernetes.io/aws-load-balancer-internal: \u0026#34;true\u0026#34; [...] /tab %}} tab name=\u0026quot;Azure\u0026rdquo; %}}\n[...] metadata: name: my-service annotations: service.beta.kubernetes.io/azure-load-balancer-internal: \u0026#34;true\u0026#34; [...] /tab %}} tab name=\u0026quot;OpenStack\u0026rdquo; %}}\n[...] metadata: name: my-service annotations: service.beta.kubernetes.io/openstack-internal-load-balancer: \u0026#34;true\u0026#34; [...] /tab %}} tab name=\u0026quot;Baidu Cloud\u0026rdquo; %}}\n[...] metadata: name: my-service annotations: service.beta.kubernetes.io/cce-load-balancer-internal-vpc: \u0026#34;true\u0026#34; [...] /tab %}} /tabs \u0026gt;}}\nAWS TLS 支持 为了对在AWS上运行的集群提供部分TLS / SSL支持，您可以向 LoadBalancer 服务添加三个注释：\nmetadata: name: my-service annotations: service.beta.kubernetes.io/aws-load-balancer-ssl-cert: arn:aws:acm:us-east-1:123456789012:certificate/12345678-1234-1234-1234-123456789012 第一个指定要使用的证书的ARN。 它可以是已上载到 IAM 的第三方颁发者的证书，也可以是在 AWS Certificate Manager 中创建的证书。\nmetadata: name: my-service annotations: service.beta.kubernetes.io/aws-load-balancer-backend-protocol: (https|http|ssl|tcp) 第二个注释指定 Pod 使用哪种协议。 对于 HTTPS 和 SSL，ELB 希望 Pod 使用证书通过加密连接对自己进行身份验证。\nHTTP 和 HTTPS 选择第7层代理：ELB 终止与用户的连接，解析标头，并在转发请求时向 X-Forwarded-For 标头注入用户的 IP 地址（Pod 仅在连接的另一端看到 ELB 的 IP 地址）。\nTCP 和 SSL 选择第4层代理：ELB 转发流量而不修改报头。\n在某些端口处于安全状态而其他端口未加密的混合使用环境中，可以使用以下注释：\nmetadata: name: my-service annotations: service.beta.kubernetes.io/aws-load-balancer-backend-protocol: http service.beta.kubernetes.io/aws-load-balancer-ssl-ports: \u0026#34;443,8443\u0026#34; 从Kubernetes v1.9起可以使用 预定义的 AWS SSL 策略 为您的服务使用HTTPS或SSL侦听器。 要查看可以使用哪些策略，可以使用 aws 命令行工具：\naws elb describe-load-balancer-policies --query \u0026#39;PolicyDescriptions[].PolicyName\u0026#39; 然后，您可以使用 \u0026ldquo;service.beta.kubernetes.io/aws-load-balancer-ssl-negotiation-policy\u0026rdquo; 注解; 例如：\nmetadata: name: my-service annotations: service.beta.kubernetes.io/aws-load-balancer-ssl-negotiation-policy: \u0026#34;ELBSecurityPolicy-TLS-1-2-2017-01\u0026#34; AWS上的PROXY协议支持 为了支持在AWS上运行的集群，启用 PROXY协议, 您可以使用以下服务注释：\nmetadata: name: my-service annotations: service.beta.kubernetes.io/aws-load-balancer-proxy-protocol: \u0026#34;*\u0026#34; 从1.3.0版开始，此注释的使用适用于 ELB 代理的所有端口，并且不能进行其他配置。\nAWS上的ELB访问日志 有几个注释可用于管理AWS上ELB服务的访问日志。\n注释 service.beta.kubernetes.io/aws-load-balancer-access-log-enabled 控制是否启用访问日志。\n注解 service.beta.kubernetes.io/aws-load-balancer-access-log-emit-interval 控制发布访问日志的时间间隔（以分钟为单位）。 您可以指定5分钟或60分钟的间隔。\n注释 service.beta.kubernetes.io/aws-load-balancer-access-log-s3-bucket-name 控制存储负载均衡器访问日志的Amazon S3存储桶的名称。\n注释 service.beta.kubernetes.io/aws-load-balancer-access-log-s3-bucket-prefix 指定为Amazon S3存储桶创建的逻辑层次结构。\nmetadata: name: my-service annotations: service.beta.kubernetes.io/aws-load-balancer-access-log-enabled: \u0026#34;true\u0026#34; # Specifies whether access logs are enabled for the load balancer service.beta.kubernetes.io/aws-load-balancer-access-log-emit-interval: \u0026#34;60\u0026#34; # The interval for publishing the access logs. You can specify an interval of either 5 or 60 (minutes). service.beta.kubernetes.io/aws-load-balancer-access-log-s3-bucket-name: \u0026#34;my-bucket\u0026#34; # The name of the Amazon S3 bucket where the access logs are stored service.beta.kubernetes.io/aws-load-balancer-access-log-s3-bucket-prefix: \u0026#34;my-bucket-prefix/prod\u0026#34; # The logical hierarchy you created for your Amazon S3 bucket, for example `my-bucket-prefix/prod` AWS上的连接排空 可以将注释 service.beta.kubernetes.io/aws-load-balancer-connection-draining-enabled 设置为 \u0026quot;true\u0026quot; 的值来管理 ELB 的连接消耗。 注释 service.beta.kubernetes.io/aws-load-balancer-connection-draining-timeout 也可以用于设置最大时间（以秒为单位），以保持现有连接在注销实例之前保持打开状态。\nmetadata: name: my-service annotations: service.beta.kubernetes.io/aws-load-balancer-connection-draining-enabled: \u0026#34;true\u0026#34; service.beta.kubernetes.io/aws-load-balancer-connection-draining-timeout: \u0026#34;60\u0026#34; 其他ELB注释 还有其他一些注释，用于管理经典弹性负载均衡器，如下所述。\nmetadata: name: my-service annotations: service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout: \u0026#34;60\u0026#34; # The time, in seconds, that the connection is allowed to be idle (no data has been sent over the connection) before it is closed by the load balancer service.beta.kubernetes.io/aws-load-balancer-cross-zone-load-balancing-enabled: \u0026#34;true\u0026#34; # Specifies whether cross-zone load balancing is enabled for the load balancer service.beta.kubernetes.io/aws-load-balancer-additional-resource-tags: \u0026#34;environment=prod,owner=devops\u0026#34; # A comma-separated list of key-value pairs which will be recorded as # additional tags in the ELB. service.beta.kubernetes.io/aws-load-balancer-healthcheck-healthy-threshold: \u0026#34;\u0026#34; # The number of successive successful health checks required for a backend to # be considered healthy for traffic. Defaults to 2, must be between 2 and 10 service.beta.kubernetes.io/aws-load-balancer-healthcheck-unhealthy-threshold: \u0026#34;3\u0026#34; # The number of unsuccessful health checks required for a backend to be # considered unhealthy for traffic. Defaults to 6, must be between 2 and 10 service.beta.kubernetes.io/aws-load-balancer-healthcheck-interval: \u0026#34;20\u0026#34; # The approximate interval, in seconds, between health checks of an # individual instance. Defaults to 10, must be between 5 and 300 service.beta.kubernetes.io/aws-load-balancer-healthcheck-timeout: \u0026#34;5\u0026#34; # The amount of time, in seconds, during which no response means a failed # health check. This value must be less than the service.beta.kubernetes.io/aws-load-balancer-healthcheck-interval # value. Defaults to 5, must be between 2 and 60 service.beta.kubernetes.io/aws-load-balancer-extra-security-groups: \u0026#34;sg-53fae93f,sg-42efd82e\u0026#34; # A list of additional security groups to be added to the ELB feature-state for_k8s_version=\u0026quot;v1.15\u0026rdquo; state=\u0026quot;beta\u0026rdquo; \u0026gt;}}\nmetadata: name: my-service annotations: service.beta.kubernetes.io/aws-load-balancer-type: \u0026#34;nlb\u0026#34; NLB 仅适用于某些实例类。 有关受支持的实例类型的列表，请参见 Elastic Load Balancing 上的 AWS文档。\n与经典弹性负载平衡器不同，网络负载平衡器（NLB）将客户端的 IP 地址转发到该节点。 如果服务的 .spec.externalTrafficPolicy 设置为 Cluster ，则客户端的IP地址不会传达到终端 Pod。\n通过将 .spec.externalTrafficPolicy 设置为 Local，客户端IP地址将传播到终端 Pod，但这可能导致流量分配不均。 没有针对特定 LoadBalancer 服务的任何 Pod 的节点将无法通过自动分配的 .spec.healthCheckNodePort 进行 NLB 目标组的运行状况检查，并且不会收到任何流量。\n为了获得平均流量，请使用DaemonSet或指定 pod anti-affinity使其不在同一节点上。\n您还可以将NLB服务与 内部负载平衡器批注一起使用。\n为了使客户端流量能够到达 NLB 后面的实例，使用以下 IP 规则修改了节点安全组：\n   Rule Protocol Port(s) IpRange(s) IpRange Description     Health Check TCP NodePort(s) (.spec.healthCheckNodePort for .spec.externalTrafficPolicy = Local) VPC CIDR kubernetes.io/rule/nlb/health=\u0026lt;loadBalancerName\u0026gt;   Client Traffic TCP NodePort(s) .spec.loadBalancerSourceRanges (defaults to 0.0.0.0/0) kubernetes.io/rule/nlb/client=\u0026lt;loadBalancerName\u0026gt;   MTU Discovery ICMP 3,4 .spec.loadBalancerSourceRanges (defaults to 0.0.0.0/0) kubernetes.io/rule/nlb/mtu=\u0026lt;loadBalancerName\u0026gt;    为了限制哪些客户端IP可以访问网络负载平衡器，请指定 loadBalancerSourceRanges。\nspec: loadBalancerSourceRanges: - \u0026#34;143.231.0.0/16\u0026#34; 如果未设置 .spec.loadBalancerSourceRanges ，则 Kubernetes 允许从 0.0.0.0/0 到节点安全组的流量。 如果节点具有公共 IP 地址，请注意，非 NLB 流量也可以到达那些修改后的安全组中的所有实例。\n类型ExternalName 类型为 ExternalName 的服务将服务映射到 DNS 名称，而不是典型的选择器，例如 my-service 或者 cassandra。 您可以使用 spec.externalName 参数指定这些服务。\n例如，以下 Service 定义将 prod 名称空间中的 my-service 服务映射到 my.database.example.com：\napiVersion: v1 kind: Service metadata: name: my-service namespace: prod spec: type: ExternalName externalName: my.database.example.com ExternalName 接受 IPv4 地址字符串，但作为包含数字的 DNS 名称，而不是 IP 地址。 类似于 IPv4 地址的外部名称不能由 CoreDNS 或 ingress-nginx 解析，因为外部名称旨在指定规范的 DNS 名称。 要对 IP 地址进行硬编码，请考虑使用 headless Services。\n当查找主机 my-service.prod.svc.cluster.local 时，群集DNS服务返回 CNAME 记录，其值为 my.database.example.com。 访问 my-service 的方式与其他服务的方式相同，但主要区别在于重定向发生在 DNS 级别，而不是通过代理或转发。 如果以后您决定将数据库移到群集中，则可以启动其 Pod，添加适当的选择器或端点以及更改服务的类型。\n本部分感谢 Alen Komljen的 Kubernetes Tips - Part1 博客文章。\n外部 IP 如果外部的 IP 路由到集群中一个或多个 Node 上，Kubernetes Service 会被暴露给这些 externalIPs。 通过外部 IP（作为目的 IP 地址）进入到集群，打到 Service 的端口上的流量，将会被路由到 Service 的 Endpoint 上。 externalIPs 不会被 Kubernetes 管理，它属于集群管理员的职责范畴。\n根据 Service 的规定，externalIPs 可以同任意的 ServiceType 来一起指定。 在上面的例子中，my-service 可以在 \u0026ldquo;80.11.12.10:80\u0026rdquo;(externalIP:port) 上被客户端访问。\napiVersion: v1 kind: Service metadata: name: my-service spec: selector: app: MyApp ports: - name: http protocol: TCP port: 80 targetPort: 9376 externalIPs: - 80.11.12.10 不足之处 为 VIP 使用 userspace 代理，将只适合小型到中型规模的集群，不能够扩展到上千 Service 的大型集群。 查看 最初设计方案 获取更多细节。\n使用 userspace 代理，隐藏了访问 Service 的数据包的源 IP 地址。 这使得一些类型的防火墙无法起作用。 iptables 代理不会隐藏 Kubernetes 集群内部的 IP 地址，但却要求客户端请求必须通过一个负载均衡器或 Node 端口。\nType 字段支持嵌套功能 —— 每一层需要添加到上一层里面。 不会严格要求所有云提供商（例如，GCE 就没必要为了使一个 LoadBalancer 能工作而分配一个 NodePort，但是 AWS 需要 ），但当前 API 是强制要求的。\n虚拟IP实施 对很多想使用 Service 的人来说，前面的信息应该足够了。 然而，有很多内部原理性的内容，还是值去理解的。\n避免冲突 Kubernetes 最主要的哲学之一，是用户不应该暴露那些能够导致他们操作失败、但又不是他们的过错的场景。 这种场景下，让我们来看一下网络端口 —— 用户不应该必须选择一个端口号，而且该端口还有可能与其他用户的冲突。 这就是说，在彼此隔离状态下仍然会出现失败。\n为了使用户能够为他们的 Service 选择一个端口号，我们必须确保不能有2个 Service 发生冲突。 我们可以通过为每个 Service 分配它们自己的 IP 地址来实现。\n为了保证每个 Service 被分配到一个唯一的 IP，需要一个内部的分配器能够原子地更新 etcd 中的一个全局分配映射表，这个更新操作要先于创建每一个 Service。 为了使 Service 能够获取到 IP，这个映射表对象必须在注册中心存在，否则创建 Service 将会失败，指示一个 IP 不能被分配。 一个后台 Controller 的职责是创建映射表（从 Kubernetes 的旧版本迁移过来，旧版本中是通过在内存中加锁的方式实现），并检查由于管理员干预和清除任意 IP 造成的不合理分配，这些 IP 被分配了但当前没有 Service 使用它们。\nService IP 地址 不像 Pod 的 IP 地址，它实际路由到一个固定的目的地，Service 的 IP 实际上不能通过单个主机来进行应答。 相反，我们使用 iptables（Linux 中的数据包处理逻辑）来定义一个虚拟IP地址（VIP），它可以根据需要透明地进行重定向。 当客户端连接到 VIP 时，它们的流量会自动地传输到一个合适的 Endpoint。 环境变量和 DNS，实际上会根据 Service 的 VIP 和端口来进行填充。\nkube-proxy支持三种代理模式: 用户空间，iptables和IPVS；它们各自的操作略有不同。\nUserspace 作为一个例子，考虑前面提到的图片处理应用程序。 当创建 backend Service 时，Kubernetes master 会给它指派一个虚拟 IP 地址，比如 10.0.0.1。 假设 Service 的端口是 1234，该 Service 会被集群中所有的 kube-proxy 实例观察到。 当代理看到一个新的 Service， 它会打开一个新的端口，建立一个从该 VIP 重定向到新端口的 iptables，并开始接收请求连接。\n当一个客户端连接到一个 VIP，iptables 规则开始起作用，它会重定向该数据包到 Service代理 的端口。 Service代理 选择一个 backend，并将客户端的流量代理到 backend 上。\n这意味着 Service 的所有者能够选择任何他们想使用的端口，而不存在冲突的风险。 客户端可以简单地连接到一个 IP 和端口，而不需要知道实际访问了哪些 Pod。\niptables 再次考虑前面提到的图片处理应用程序。 当创建 backend Service 时，Kubernetes 控制面板会给它指派一个虚拟 IP 地址，比如 10.0.0.1。 假设 Service 的端口是 1234，该 Service 会被集群中所有的 kube-proxy 实例观察到。 当代理看到一个新的 Service， 它会配置一系列的 iptables 规则，从 VIP 重定向到 per-Service 规则。 该 per-Service 规则连接到 per-Endpoint 规则，该 per-Endpoint 规则会重定向（目标 NAT）到 backend。\n当一个客户端连接到一个 VIP，iptables 规则开始起作用。一个 backend 会被选择（或者根据会话亲和性，或者随机），数据包被重定向到这个 backend。 不像 userspace 代理，数据包从来不拷贝到用户空间，kube-proxy 不是必须为该 VIP 工作而运行，并且客户端 IP 是不可更改的。 当流量打到 Node 的端口上，或通过负载均衡器，会执行相同的基本流程，但是在那些案例中客户端 IP 是可以更改的。\nIPVS 在大规模集群（例如10,000个服务）中，iptables 操作会显着降低速度。 IPVS 专为负载平衡而设计，并基于内核内哈希表。 因此，您可以通过基于 IPVS 的 kube-proxy 在大量服务中实现性能一致性。 同时，基于 IPVS 的 kube-proxy 具有更复杂的负载平衡算法（最小连接，局部性，加权，持久性）。\nAPI Object Service 是Kubernetes REST API中的顶级资源。 您可以在以下位置找到有关API对象的更多详细信息： [Service 对象 API](/docs/reference/generated/kubernetes-api/\u0026lt; param \u0026ldquo;version\u0026rdquo; \u0026gt;}}/#service-v1-core).\nSupported protocols TCP feature-state for_k8s_version=\u0026quot;v1.0\u0026rdquo; state=\u0026quot;stable\u0026rdquo; \u0026gt;}}\n您可以将TCP用于任何类型的服务，这是默认的网络协议。\nUDP feature-state for_k8s_version=\u0026quot;v1.0\u0026rdquo; state=\u0026quot;stable\u0026rdquo; \u0026gt;}}\n您可以将UDP用于大多数服务。 对于 type=LoadBalancer 服务，对 UDP 的支持取决于提供此功能的云提供商。\nHTTP feature-state for_k8s_version=\u0026quot;v1.1\u0026rdquo; state=\u0026quot;stable\u0026rdquo; \u0026gt;}}\n如果您的云提供商支持它，则可以在 LoadBalancer 模式下使用服务来设置外部 HTTP/HTTPS 反向代理，并将其转发到该服务的 Endpoints。\n您还可以使用oltip term_id=\u0026quot;ingress\u0026rdquo; \u0026gt;}} 代替 Service 来公开HTTP / HTTPS服务。\nPROXY 协议 feature-state for_k8s_version=\u0026quot;v1.1\u0026rdquo; state=\u0026quot;stable\u0026rdquo; \u0026gt;}}\n如果您的云提供商支持它(例如, AWS)， 则可以在 LoadBalancer 模式下使用 Service 在 Kubernetes 本身之外配置负载均衡器，该负载均衡器将转发前缀为 [PROXY协议]PROXY protocol 的连接。\n负载平衡器将发送一系列初始字节，描述传入的连接，类似于此示例\nPROXY TCP4 192.0.2.202 10.0.42.7 12345 7\\r\\n 接下来是来自客户端的数据。\nSCTP feature-state for_k8s_version=\u0026quot;v1.12\u0026rdquo; state=\u0026quot;alpha\u0026rdquo; \u0026gt;}}\nKubernetes 支持 SCTP 作为 Service，Endpoint，NetworkPolicy 和 Pod 定义中的 协议 值作为alpha功能。 要启用此功能，集群管理员需要在apiserver上启用 SCTPSupport 功能门，例如 --feature-gates = SCTPSupport = true，…。\n启用功能门后，您可以将服务，端点，NetworkPolicy或Pod的 protocol 字段设置为 SCTP。 Kubernetes相应地为 SCTP 关联设置网络，就像为 TCP 连接一样。\n警告 支持多宿主SCTP关联 对多宿主 SCTP 关联的支持要求CNI插件可以支持将多个接口和 IP 地址分配给 Pod。 用于多宿主 SCTP 关联的 NAT 在相应的内核模块中需要特殊的逻辑。\nService 类型为 LoadBalancer 的服务 如果云提供商的负载平衡器实现支持将 SCTP 作为协议，则只能使用 类型 LoadBalancer 加上 协议 SCTP 创建服务。 否则，服务创建请求将被拒绝。 当前的云负载平衡器提供商（Azure，AWS，CloudStack，GCE，OpenStack）都缺乏对 SCTP 的支持。\nWindows 基于Windows的节点不支持SCTP。\nUserspace kube-proxy 当 kube-proxy 处于用户空间模式时，它不支持 SCTP 关联的管理。\n未来工作 未来我们能预见到，代理策略可能会变得比简单的 round-robin 均衡策略有更多细微的差别，比如 master 选举或分片。 我们也能想到，某些 Service 将具有 “真正” 的负载均衡器，这种情况下 VIP 将简化数据包的传输。\nKubernetes 项目打算为 L7（HTTP）Service 改进我们对它的支持。\nKubernetes 项目打算为 Service 实现更加灵活的请求进入模式，这些 Service 包含当前 ClusterIP、NodePort 和 LoadBalancer 模式，或者更多。\nwhatsnext  阅读 Connecting Applications with Services 阅读 Ingress 阅读 Endpoint Slices  "
},
{
	"uri": "https://lijun.in/tutorials/kubernetes-basics/expose/expose-intro/",
	"title": "Using a Service to Expose Your App",
	"tags": [],
	"description": "",
	"content": "  Objectives  Learn about a Service in Kubernetes Understand how labels and LabelSelector objects relate to a Service Expose an application outside a Kubernetes cluster using a Service   Overview of Kubernetes Services Kubernetes Pods are mortal. Pods in fact have a lifecycle. When a worker node dies, the Pods running on the Node are also lost. A ReplicaSet might then dynamically drive the cluster back to desired state via creation of new Pods to keep your application running. As another example, consider an image-processing backend with 3 replicas. Those replicas are exchangeable; the front-end system should not care about backend replicas or even if a Pod is lost and recreated. That said, each Pod in a Kubernetes cluster has a unique IP address, even Pods on the same Node, so there needs to be a way of automatically reconciling changes among Pods so that your applications continue to function.\nA Service in Kubernetes is an abstraction which defines a logical set of Pods and a policy by which to access them. Services enable a loose coupling between dependent Pods. A Service is defined using YAML (preferred) or JSON, like all Kubernetes objects. The set of Pods targeted by a Service is usually determined by a LabelSelector (see below for why you might want a Service without including selector in the spec).\nAlthough each Pod has a unique IP address, those IPs are not exposed outside the cluster without a Service. Services allow your applications to receive traffic. Services can be exposed in different ways by specifying a type in the ServiceSpec:\n ClusterIP (default) - Exposes the Service on an internal IP in the cluster. This type makes the Service only reachable from within the cluster. NodePort - Exposes the Service on the same port of each selected Node in the cluster using NAT. Makes a Service accessible from outside the cluster using \u0026lt;NodeIP\u0026gt;:\u0026lt;NodePort\u0026gt;. Superset of ClusterIP. LoadBalancer - Creates an external load balancer in the current cloud (if supported) and assigns a fixed, external IP to the Service. Superset of NodePort. ExternalName - Exposes the Service using an arbitrary name (specified by externalName in the spec) by returning a CNAME record with the name. No proxy is used. This type requires v1.7 or higher of kube-dns.  More information about the different types of Services can be found in the Using Source IP tutorial. Also see Connecting Applications with Services.\nAdditionally, note that there are some use cases with Services that involve not defining selector in the spec. A Service created without selector will also not create the corresponding Endpoints object. This allows users to manually map a Service to specific endpoints. Another possibility why there may be no selector is you are strictly using type: ExternalName.\n Summary  Exposing Pods to external traffic Load balancing traffic across multiple Pods Using labels   A Kubernetes Service is an abstraction layer which defines a logical set of Pods and enables external traffic exposure, load balancing and service discovery for those Pods.\n   Services and Labels     A Service routes traffic across a set of Pods. Services are the abstraction that allow pods to die and replicate in Kubernetes without impacting your application. Discovery and routing among dependent Pods (such as the frontend and backend components in an application) is handled by Kubernetes Services.\nServices match a set of Pods using labels and selectors, a grouping primitive that allows logical operation on objects in Kubernetes. Labels are key/value pairs attached to objects and can be used in any number of ways:\n Designate objects for development, test, and production Embed version tags Classify an object using tags   You can create a Service at the same time you create a Deployment by using\n--expose in kubectl.\n     Labels can be attached to objects at creation time or later on. They can be modified at any time. Let's expose our application now using a Service and apply some labels.\n  Start Interactive Tutorial›       "
},
{
	"uri": "https://lijun.in/concepts/storage/volumes/",
	"title": "Volumes",
	"tags": [],
	"description": "",
	"content": "容器中的文件在磁盘上是临时存放的，这给容器中运行的特殊应用程序带来一些问题。 首先，当容器崩溃时，kubelet 将重新启动容器，容器中的文件将会丢失——因为容器会以干净的状态重建。 其次，当在一个 Pod 中同时运行多个容器时，常常需要在这些容器之间共享文件。 Kubernetes 抽象出 Volume 对象来解决这两个问题。\n阅读本文前建议您熟悉一下 Pods。\n背景 Docker 也有 Volume 的概念，但对它只有少量且松散的管理。 在 Docker 中，Volume 是磁盘上或者另外一个容器内的一个目录。 直到最近，Docker 才支持对基于本地磁盘的 Volume 的生存期进行管理。 虽然 Docker 现在也能提供 Volume 驱动程序，但是目前功能还非常有限（例如，截至 Docker 1.7，每个容器只允许有一个 Volume 驱动程序，并且无法将参数传递给卷）。\n另一方面，Kubernetes 卷具有明确的生命周期——与包裹它的 Pod 相同。 因此，卷比 Pod 中运行的任何容器的存活期都长，在容器重新启动时数据也会得到保留。 当然，当一个 Pod 不再存在时，卷也将不再存在。也许更重要的是，Kubernetes 可以支持许多类型的卷，Pod 也能同时使用任意数量的卷。\n卷的核心是包含一些数据的目录，Pod 中的容器可以访问该目录。 特定的卷类型可以决定这个目录如何形成的，并能决定它支持何种介质，以及目录中存放什么内容。\n使用卷时, Pod 声明中需要提供卷的类型 (.spec.volumes 字段)和卷挂载的位置 (.spec.containers.volumeMounts 字段).\n容器中的进程能看到由它们的 Docker 镜像和卷组成的文件系统视图。 Docker 镜像 位于文件系统层次结构的根部，并且任何 Volume 都挂载在镜像内的指定路径上。 卷不能挂载到其他卷，也不能与其他卷有硬链接。 Pod 中的每个容器必须独立地指定每个卷的挂载位置。\nVolume 的类型 Kubernetes 支持下列类型的卷：\n awsElasticBlockStore azureDisk azureFile cephfs cinder configMap csi downwardAPI emptyDir fc (fibre channel) flexVolume flocker gcePersistentDisk gitRepo (deprecated) glusterfs hostPath iscsi local nfs persistentVolumeClaim projected portworxVolume quobyte rbd scaleIO secret storageos vsphereVolume  我们欢迎大家贡献其他的卷类型支持。\nawsElasticBlockStore awsElasticBlockStore 卷将 Amazon Web服务（AWS）EBS 卷 挂载到您的 Pod 中。 与 emptyDir 在删除 Pod 时会被删除不同，EBS 卷的内容在删除 Pod 时会被保留，卷只是被卸载掉了。 这意味着 EBS 卷可以预先填充数据，并且可以在 Pod 之间传递数据。\n您在使用 EBS 卷之前必须先创建它，可以使用 aws ec2 create-volume 命令进行创建；也可以使用 AWS API 进行创建。\n使用 awsElasticBlockStore 卷时有一些限制：\n Pod 正在运行的节点必须是 AWS EC2 实例。 这些实例需要与 EBS 卷在相同的地域（region）和可用区（availability-zone）。 EBS 卷只支持被挂载到单个 EC2 实例上。  创建 EBS 卷 在将 EBS 卷用到 Pod 上之前，您首先要创建它。\naws ec2 create-volume --availability-zone=eu-west-1a --size=10 --volume-type=gp2 确保该区域与您的群集所在的区域相匹配。（也要检查卷的大小和 EBS 卷类型都适合您的用途！）\nAWS EBS 配置示例 apiVersion: v1 kind: Pod metadata: name: test-ebs spec: containers: - image: k8s.gcr.io/test-webserver name: test-container volumeMounts: - mountPath: /test-ebs name: test-volume volumes: - name: test-volume # This AWS EBS volume must already exist. awsElasticBlockStore: volumeID: \u0026lt;volume-id\u0026gt; fsType: ext4 azureDisk azureDisk 用来在 Pod 上挂载 Microsoft Azure 数据盘（Data Disk） . 更多详情请参考这里。\nCSI迁移 feature-state for_k8s_version=\u0026quot;v1.15\u0026rdquo; state=\u0026quot;alpha\u0026rdquo; \u0026gt;}}\n启用azureDisk的CSI迁移功能后，它会将所有插件操作从现有的内建插件填添加disk.csi.azure.com容器存储接口（CSI）驱动程序中。 为了使用此功能，必须在群集上安装 Azure磁盘CSI驱动程序， 并且 CSIMigration 和 CSIMigrationAzureDisk Alpha功能 必须启用。\nazureFile azureFile 用来在 Pod 上挂载 Microsoft Azure 文件卷（File Volume） (SMB 2.1 和 3.0)。 更多详情请参考这里。\nCSI迁移 feature-state for_k8s_version=\u0026quot;v1.15\u0026rdquo; state=\u0026quot;alpha\u0026rdquo; \u0026gt;}}\n启用azureFile的CSI迁移功能后，它会将所有插件操作从现有的内建插件填添加file.csi.azure.com容器存储接口（CSI）驱动程序中。 为了使用此功能，必须在群集上安装 Azure文件CSI驱动程序， 并且 CSIMigration 和 CSIMigrationAzureFile Alpha功能 必须启用。\ncephfs cephfs 允许您将现存的 CephFS 卷挂载到 Pod 中。不像 emptyDir 那样会在删除 Pod 的同时也会被删除，cephfs 卷的内容在删除 Pod 时会被保留，卷只是被卸载掉了。 这意味着 CephFS 卷可以被预先填充数据，并且这些数据可以在 Pod 之间\u0026quot;传递\u0026rdquo;。CephFS 卷可同时被多个写者挂载。\n在您使用 Ceph 卷之前，您的 Ceph 服务器必须正常运行并且要使用的 share 被导出（exported）。\n更多信息请参考 CephFS 示例。\ncinder 先决条件：配置了OpenStack Cloud Provider 的 Kubernetes。 有关 cloudprovider 配置，请参考 cloud provider openstack。\ncinder 用于将 OpenStack Cinder 卷安装到 Pod 中。\nCinder Volume示例配置 apiVersion: v1 kind: Pod metadata: name: test-cinder spec: containers: - image: k8s.gcr.io/test-webserver name: test-cinder-container volumeMounts: - mountPath: /test-cinder name: test-volume volumes: - name: test-volume # This OpenStack volume must already exist. cinder: volumeID: \u0026lt;volume-id\u0026gt; fsType: ext4 CSI迁移 feature-state for_k8s_version=\u0026quot;v1.14\u0026rdquo; state=\u0026quot;alpha\u0026rdquo; \u0026gt;}}\n启用Cinder的CSI迁移功能后，它会将所有插件操作从现有的内建插件填添加 cinder.csi.openstack.org 容器存储接口（CSI）驱动程序中。 为了使用此功能，必须在群集上安装 Openstack Cinder CSI驱动程序， 并且 CSIMigration 和 CSIMigrationOpenStack Alpha功能 必须启用。\nconfigMap configMap 资源提供了向 Pod 注入配置数据的方法。 ConfigMap 对象中存储的数据可以被 configMap 类型的卷引用，然后被应用到 Pod 中运行的容器化应用。\n当引用 configMap 对象时，你可以简单的在 Volume 中通过它名称来引用。 还可以自定义 ConfigMap 中特定条目所要使用的路径。 例如，要将名为 log-config 的 ConfigMap 挂载到名为 configmap-pod 的 Pod 中，您可以使用下面的 YAML：\napiVersion: v1 kind: Pod metadata: name: configmap-pod spec: containers: - name: test image: busybox volumeMounts: - name: config-vol mountPath: /etc/config volumes: - name: config-vol configMap: name: log-config items: - key: log_level path: log_level log-config ConfigMap 是以卷的形式挂载的， 存储在 log_level 条目中的所有内容都被挂载到 Pod 的 \u0026ldquo;/etc/config/log_level\u0026rdquo; 路径下。 请注意，这个路径来源于 Volume 的 mountPath 和 log_level 键对应的 path。\n在使用 ConfigMap 之前您首先要创建它。\n容器以 subPath 卷挂载方式使用 ConfigMap 时，将无法接收 ConfigMap 的更新。\ndownwardAPI downwardAPI 卷用于使 downward API 数据对应用程序可用。 这种卷类型挂载一个目录并在纯文本文件中写入请求的数据。\n容器以挂载 subPath 卷的方式使用 downwardAPI 时，将不能接收到它的更新。\n更多详细信息请参考 downwardAPI 卷示例。\nemptyDir 当 Pod 指定到某个节点上时，首先创建的是一个 emptyDir 卷，并且只要 Pod 在该节点上运行，卷就一直存在。 就像它的名称表示的那样，卷最初是空的。 尽管 Pod 中的容器挂载 emptyDir 卷的路径可能相同也可能不同，但是这些容器都可以读写 emptyDir 卷中相同的文件。 当 Pod 因为某些原因被从节点上删除时，emptyDir 卷中的数据也会永久删除。\n容器崩溃并不会导致 Pod 被从节点上移除，因此容器崩溃时 emptyDir 卷中的数据是安全的。\nemptyDir 的一些用途：\n 缓存空间，例如基于磁盘的归并排序。 为耗时较长的计算任务提供检查点，以便任务能方便地从崩溃前状态恢复执行。 在 Web 服务器容器服务数据时，保存内容管理器容器获取的文件。  默认情况下， emptyDir 卷存储在支持该节点所使用的介质上；这里的介质可以是磁盘或 SSD 或网络存储，这取决于您的环境。 但是，您可以将 emptyDir.medium 字段设置为 \u0026quot;Memory\u0026quot;，以告诉 Kubernetes 为您安装 tmpfs（基于 RAM 的文件系统）。 虽然 tmpfs 速度非常快，但是要注意它与磁盘不同。 tmpfs 在节点重启时会被清除，并且您所写入的所有文件都会计入容器的内存消耗，受容器内存限制约束。\nPod 示例 apiVersion: v1 kind: Pod metadata: name: test-pd spec: containers: - image: k8s.gcr.io/test-webserver name: test-container volumeMounts: - mountPath: /cache name: cache-volume volumes: - name: cache-volume emptyDir: {} fc (光纤通道) fc 卷允许将现有的光纤通道卷挂载到 Pod 中。 可以使用卷配置中的参数 targetWWNs 来指定单个或多个目标 WWN。 如果指定多个 WWN，targetWWNs 期望这些 WWN 来自多路径连接。\n您必须配置 FC SAN Zoning，以便预先向目标 WWN 分配和屏蔽这些 LUN（卷），这样 Kubernetes 主机才可以访问它们。\n更多详情请参考 FC 示例。\nflocker Flocker 是一个开源的、集群化的容器数据卷管理器。 Flocker 提供了由各种存储后备支持的数据卷的管理和编排。\nflocker 卷允许将一个 Flocker 数据集挂载到 Pod 中。 如果数据集在 Flocker 中不存在，则需要首先使用 Flocker CLI 或 Flocker API 创建数据集。 如果数据集已经存在，那么 Flocker 将把它重新附加到 Pod 被调度的节点。 这意味着数据可以根据需要在 Pod 之间 \u0026ldquo;传递\u0026rdquo;。\n您在使用 Flocker 之前必须先安装运行自己的 Flocker。\n更多详情请参考 Flocker 示例。\ngcePersistentDisk gcePersistentDisk 卷能将谷歌计算引擎 (GCE) 持久盘（PD） 挂载到您的 Pod 中。 不像 emptyDir 那样会在删除 Pod 的同时也会被删除，持久盘卷的内容在删除 Pod 时会被保留，卷只是被卸载掉了。 这意味着持久盘卷可以被预先填充数据，并且这些数据可以在 Pod 之间\u0026quot;传递\u0026rdquo;。\n您在使用 PD 前，必须使用 gcloud 或者 GCE API 或 UI 创建它。\n使用 gcePersistentDisk 时有一些限制：\n 运行 Pod 的节点必须是 GCE VM 那些 VM 必须和持久盘属于相同的 GCE 项目和区域（zone）  PD 的一个特点是它们可以同时被多个消费者以只读方式挂载。 这意味着您可以用数据集预先填充 PD，然后根据需要并行地在尽可能多的 Pod 中提供该数据集。 不幸的是，PD 只能由单个使用者以读写模式挂载——即不允许同时写入。\n在由 ReplicationController 所管理的 Pod 上使用 PD 将会失败，除非 PD 是只读模式或者副本的数量是 0 或 1。\n创建持久盘（PD） 在 Pod 中使用 GCE 持久盘之前，您首先要创建它。\ngcloud compute disks create --size=500GB --zone=us-central1-a my-data-disk Pod 示例 apiVersion: v1 kind: Pod metadata: name: test-pd spec: containers: - image: k8s.gcr.io/test-webserver name: test-container volumeMounts: - mountPath: /test-pd name: test-volume volumes: - name: test-volume # This GCE PD must already exist. gcePersistentDisk: pdName: my-data-disk fsType: ext4 区域持久盘（Regional Persistent Disks） feature-state for_k8s_version=\u0026quot;v1.10\u0026rdquo; state=\u0026quot;beta\u0026rdquo; \u0026gt;}}\n区域持久盘 功能允许您创建能在同一区域的两个可用区中使用的持久盘。 要使用这个功能，必须以持久盘的方式提供卷；Pod 不支持直接引用这种卷。\n手动供应基于区域 PD 的 PersistentVolume 使用 为 GCE PD 定义的存储类 也可以动态供应。 在创建 PersistentVolume 之前，您首先要创建 PD。\ngcloud beta compute disks create --size=500GB my-data-disk --region us-central1 --replica-zones us-central1-a,us-central1-b PersistentVolume 示例：\napiVersion: v1 kind: PersistentVolume metadata: name: test-volume labels: failure-domain.beta.kubernetes.io/zone: us-central1-a__us-central1-b spec: capacity: storage: 400Gi accessModes: - ReadWriteOnce gcePersistentDisk: pdName: my-data-disk fsType: ext4 CSI迁移 feature-state for_k8s_version=\u0026quot;v1.14\u0026rdquo; state=\u0026quot;alpha\u0026rdquo; \u0026gt;}}\n启用 GCE PD 的 CSI 迁移功能后，它会将所有插件操作从现有的内建插件填添加 pd.csi.storage.gke.io 容器存储接口（ CSI ）驱动程序中。 为了使用此功能，必须在群集上安装 GCE PD CSI驱动程序， 并且 CSIMigration 和 CSIMigrationGCE Alpha功能 必须启用。\ngitRepo (已弃用) gitRepo 卷类型已经被废弃。如果需要在容器中提供 git 仓库，请将一个 EmptyDir 卷挂载到 InitContainer 中，使用 git 命令完成仓库的克隆操作，然后将 EmptyDir 卷挂载到 Pod 的容器中。\ngitRepo 卷是一个卷插件的例子。 该卷类型挂载了一个空目录，并将一个 Git 代码仓库克隆到这个目录中供您使用。 将来，这种卷可能被移动到一个更加解耦的模型中，而不是针对每个应用案例扩展 Kubernetes API。\n下面给出一个 gitRepo 卷的示例：\napiVersion: v1 kind: Pod metadata: name: server spec: containers: - image: nginx name: nginx volumeMounts: - mountPath: /mypath name: git-volume volumes: - name: git-volume gitRepo: repository: \u0026#34;git@somewhere:me/my-git-repository.git\u0026#34; revision: \u0026#34;22f1d8406d464b0c0874075539c1f2e96c253775\u0026#34; glusterfs glusterfs 卷能将 Glusterfs (一个开源的网络文件系统) 挂载到您的 Pod 中。 不像 emptyDir 那样会在删除 Pod 的同时也会被删除，glusterfs 卷的内容在删除 Pod 时会被保存，卷只是被卸载掉了。 这意味着 glusterfs 卷可以被预先填充数据，并且这些数据可以在 Pod 之间\u0026quot;传递\u0026rdquo;。GlusterFS 可以被多个写者同时挂载。\n在使用前您必须先安装运行自己的 GlusterFS。\n更多详情请参考 GlusterFS 示例。\nhostPath hostPath 卷能将主机节点文件系统上的文件或目录挂载到您的 Pod 中。 虽然这不是大多数 Pod 需要的，但是它为一些应用程序提供了强大的逃生舱。\n例如，hostPath 的一些用法有：\n 运行一个需要访问 Docker 引擎内部机制的容器；请使用 hostPath 挂载 /var/lib/docker 路径。 在容器中运行 cAdvisor 时，以 hostPath 方式挂载 /sys。 允许 Pod 指定给定的 hostPath 在运行 Pod 之前是否应该存在，是否应该创建以及应该以什么方式存在。  除了必需的 path 属性之外，用户可以选择性地为 hostPath 卷指定 type。\n支持的 type 值如下：\n   取值 行为      空字符串（默认）用于向后兼容，这意味着在安装 hostPath 卷之前不会执行任何检查。   DirectoryOrCreate 如果在给定路径上什么都不存在，那么将根据需要创建空目录，权限设置为 0755，具有与 Kubelet 相同的组和所有权。   Directory 在给定路径上必须存在的目录。   FileOrCreate 如果在给定路径上什么都不存在，那么将在那里根据需要创建空文件，权限设置为 0644，具有与 Kubelet 相同的组和所有权。   File 在给定路径上必须存在的文件。   Socket 在给定路径上必须存在的 UNIX 套接字。   CharDevice 在给定路径上必须存在的字符设备。   BlockDevice 在给定路径上必须存在的块设备。    当使用这种类型的卷时要小心，因为：\n 具有相同配置（例如从 podTemplate 创建）的多个 Pod 会由于节点上文件的不同而在不同节点上有不同的行为。 当 Kubernetes 按照计划添加资源感知的调度时，这类调度机制将无法考虑由 hostPath 使用的资源。 基础主机上创建的文件或目录只能由 root 用户写入。您需要在 特权容器 中以 root 身份运行进程，或者修改主机上的文件权限以便容器能够写入 hostPath 卷。  Pod 示例 apiVersion: v1 kind: Pod metadata: name: test-pd spec: containers: - image: k8s.gcr.io/test-webserver name: test-container volumeMounts: - mountPath: /test-pd name: test-volume volumes: - name: test-volume hostPath: # directory location on host path: /data # this field is optional type: Directory 应当注意,FileOrCreate 类型不会负责创建文件的父目录。如果挂载挂载文件的父目录不存在，pod 启动会失败。为了确保这种 type 能够工作，可以尝试把文件和它对应的目录分开挂载，如下所示：\nFileOrCreate pod 示例 apiVersion: v1 kind: Pod metadata: name: test-webserver spec: containers: - name: test-webserver image: k8s.gcr.io/test-webserver:latest volumeMounts: - mountPath: /var/local/aaa name: mydir - mountPath: /var/local/aaa/1.txt name: myfile volumes: - name: mydir hostPath: # 确保文件所在目录成功创建。 path: /var/local/aaa type: DirectoryOrCreate - name: myfile hostPath: path: /var/local/aaa/1.txt type: FileOrCreate iscsi iscsi 卷能将 iSCSI (基于 IP 的 SCSI) 挂载到您的 Pod 中。 不像 emptyDir 那样会在删除 Pod 的同时也会被删除，持久盘 卷的内容在删除 Pod 时会被保存，卷只是被卸载掉了。 这意味着 iscsi 卷可以被预先填充数据，并且这些数据可以在 Pod 之间\u0026quot;传递\u0026rdquo;。\n在您使用 iSCSI 卷之前，您必须拥有自己的 iSCSI 服务器，并在上面创建卷。\niSCSI 的一个特点是它可以同时被多个用户以只读方式挂载。 这意味着您可以用数据集预先填充卷，然后根据需要在尽可能多的 Pod 上提供它。不幸的是，iSCSI 卷只能由单个使用者以读写模式挂载——不允许同时写入。\n更多详情请参考 iSCSI 示例。\nlocal feature-state for_k8s_version=\u0026quot;v1.14\u0026rdquo; state=\u0026quot;stable\u0026rdquo; \u0026gt;}}\nalpha 版本的 PersistentVolume NodeAffinity 注释已被取消，将在将来的版本中废弃。 用户必须更新现有的使用该注解的 PersistentVolume，以使用新的 PersistentVolume NodeAffinity 字段。\nlocal 卷指的是所挂载的某个本地存储设备，例如磁盘、分区或者目录。\nlocal 卷只能用作静态创建的持久卷。尚不支持动态配置。\n相比 hostPath 卷，local 卷可以以持久和可移植的方式使用，而无需手动将 Pod 调度到节点，因为系统通过查看 PersistentVolume 所属节点的亲和性配置，就能了解卷的节点约束。\n然而，local 卷仍然取决于底层节点的可用性，并不是适合所有应用程序。 如果节点变得不健康，那么local 卷也将变得不可访问，并且使用它的 Pod 将不能运行。 使用 local 卷的应用程序必须能够容忍这种可用性的降低，以及因底层磁盘的耐用性特征而带来的潜在的数据丢失风险。\n下面是一个使用 local 卷和 nodeAffinity 的持久卷示例：\napiVersion: v1 kind: PersistentVolume metadata: name: example-pv spec: capacity: storage: 100Gi # volumeMode field requires BlockVolume Alpha feature gate to be enabled. volumeMode: Filesystem accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Delete storageClassName: local-storage local: path: /mnt/disks/ssd1 nodeAffinity: required: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/hostname operator: In values: - example-node 使用 local 卷时，需要使用 PersistentVolume 对象的 nodeAffinity 字段。 它使 Kubernetes 调度器能够将使用 local 卷的 Pod 正确地调度到合适的节点。\n现在，可以将 PersistentVolume 对象的 volumeMode 字段设置为 \u0026ldquo;Block\u0026rdquo;（而不是默认值 \u0026ldquo;Filesystem\u0026rdquo;），以将 local 卷作为原始块设备暴露出来。 volumeMode 字段需要启用 Alpha 功能 BlockVolume。\n当使用 local 卷时，建议创建一个 StorageClass，将 volumeBindingMode 设置为 WaitForFirstConsumer。 请参考 示例。 延迟卷绑定操作可以确保 Kubernetes 在为 PersistentVolumeClaim 作出绑定决策时，会评估 Pod 可能具有的其他节点约束，例如：如节点资源需求、节点选择器、Pod 亲和性和 Pod 反亲和性。\n您可以在 Kubernetes 之外单独运行静态驱动以改进对 local 卷的生命周期管理。 请注意，此驱动不支持动态配置。 有关如何运行外部 local 卷驱动的示例，请参考 local 卷驱动用户指南。\n如果不使用外部静态驱动来管理卷的生命周期，则用户需要手动清理和删除 local 类型的持久卷。\nnfs nfs 卷能将 NFS (网络文件系统) 挂载到您的 Pod 中。 不像 emptyDir 那样会在删除 Pod 的同时也会被删除，nfs 卷的内容在删除 Pod 时会被保存，卷只是被卸载掉了。 这意味着 nfs 卷可以被预先填充数据，并且这些数据可以在 Pod 之间\u0026quot;传递\u0026rdquo;。\n在您使用 NFS 卷之前，必须运行自己的 NFS 服务器并将目标 share 导出备用。\n要了解更多详情请参考 NFS 示例。\npersistentVolumeClaim persistentVolumeClaim 卷用来将持久卷（PersistentVolume）挂载到 Pod 中。 持久卷是用户在不知道特定云环境细节的情况下\u0026quot;申领\u0026quot;持久存储（例如 GCE PersistentDisk 或者 iSCSI 卷）的一种方法。\n更多详情请参考持久卷示例\nprojected projected 卷类型能将若干现有的卷来源映射到同一目录上。\n目前，可以映射的卷来源类型如下：\n secret downwardAPI configMap serviceAccountToken  所有的卷来源需要和 Pod 处于相同的命名空间。 更多详情请参考一体化卷设计文档。\n服务帐户令牌的映射是 Kubernetes 1.11 版本中引入的一个功能，并在 1.12 版本中被提升为 Beta 功能。 若要在 1.11 版本中启用此特性，需要显式设置 TokenRequestProjection 功能开关 为 True。\n包含 secret、downwardAPI 和 configmap 的 Pod 示例如下： apiVersion: v1 kind: Pod metadata: name: volume-test spec: containers: - name: container-test image: busybox volumeMounts: - name: all-in-one mountPath: \u0026#34;/projected-volume\u0026#34; readOnly: true volumes: - name: all-in-one projected: sources: - secret: name: mysecret items: - key: username path: my-group/my-username - downwardAPI: items: - path: \u0026#34;labels\u0026#34; fieldRef: fieldPath: metadata.labels - path: \u0026#34;cpu_limit\u0026#34; resourceFieldRef: containerName: container-test resource: limits.cpu - configMap: name: myconfigmap items: - key: config path: my-group/my-config 带有非默认许可模式设置的多个 secret 的 Pod 示例如下：\napiVersion: v1 kind: Pod metadata: name: volume-test spec: containers: - name: container-test image: busybox volumeMounts: - name: all-in-one mountPath: \u0026#34;/projected-volume\u0026#34; readOnly: true volumes: - name: all-in-one projected: sources: - secret: name: mysecret items: - key: username path: my-group/my-username - secret: name: mysecret2 items: - key: password path: my-group/my-password mode: 511 每个被投射的卷来源都在 spec 中的 sources 内列出。 参数几乎相同，除了两处例外：\n 对于 secret，secretName 字段已被变更为 name 以便与 ConfigMap 命名一致。 defaultMode 只能根据投射级别指定，而不是针对每个卷来源指定。不过，如上所述，您可以显式地为每个投射项设置 mode 值。  当开启 TokenRequestProjection 功能时，可以将当前 服务帐户的令牌注入 Pod 中的指定路径。 下面是一个例子：\napiVersion: v1 kind: Pod metadata: name: sa-token-test spec: containers: - name: container-test image: busybox volumeMounts: - name: token-vol mountPath: \u0026#34;/service-account\u0026#34; readOnly: true volumes: - name: token-vol projected: sources: - serviceAccountToken: audience: api expirationSeconds: 3600 path: token 示例 Pod 具有包含注入服务帐户令牌的映射卷。 例如，这个令牌可以被 Pod 容器用来访问 Kubernetes API服务器。 audience 字段包含令牌的预期受众。 令牌的接收者必须使用令牌的受众中指定的标识符来标识自己，否则应拒绝令牌。 此字段是可选的，默认值是 API 服务器的标识符。\nexpirationSeconds 是服务帐户令牌的有效期。 默认值为 1 小时，必须至少 10 分钟（600 秒）。 管理员还可以通过指定 API 服务器的 --service-account-max-token-expiration 选项来限制其最大值。 path 字段指定相对于映射卷的挂载点的相对路径。\n使用投射卷源作为 subPath 卷挂载的容器将不会接收这些卷源的更新。\nportworxVolume portworxVolume 是一个可伸缩的块存储层，能够以超聚合（hyperconverged）的方式与 Kubernetes 一起运行。 Portworx 支持对服务器上存储的指纹处理、基于存储能力进行分层以及跨多个服务器整合存储容量。 Portworx 可以以 in-guest 方式在虚拟机中运行，也可以在裸金属 Linux 节点上运行。\nportworxVolume 类型的卷可以通过 Kubernetes 动态创建，也可以在 Kubernetes Pod 内预先供应和引用。 下面是一个引用预先配置的 PortworxVolume 的示例 Pod：\napiVersion: v1 kind: Pod metadata: name: test-portworx-volume-pod spec: containers: - image: k8s.gcr.io/test-webserver name: test-container volumeMounts: - mountPath: /mnt name: pxvol volumes: - name: pxvol # This Portworx volume must already exist. portworxVolume: volumeID: \u0026#34;pxvol\u0026#34; fsType: \u0026#34;\u0026lt;fs-type\u0026gt;\u0026#34; 在 Pod 中使用 portworxVolume 之前，请确保有一个名为 pxvol 的 PortworxVolume 存在。\n更多详情和示例可以在这里找到。\nquobyte quobyte 卷允许将现有的 Quobyte 卷挂载到您的 Pod 中。\n在使用 Quobyte 卷之前，您首先要进行安装并创建好卷。\nQuobyte 支持 glossary_tooltip text=\u0026quot;容器存储接口\u0026rdquo; term_id=\u0026quot;csi\u0026rdquo; \u0026gt;}}。 推荐使用 CSI 插件以在 Kubernetes 中使用 Quobyte 卷。 Quobyte 的 GitHub 项目具有说明以及使用示例来部署 CSI 的 Quobyte。\nrbd rbd 卷允许将 Rados 块设备 卷挂载到您的 Pod 中. 不像 emptyDir 那样会在删除 Pod 的同时也会被删除，rbd 卷的内容在删除 Pod 时会被保存，卷只是被卸载掉了。 这意味着 rbd 卷可以被预先填充数据，并且这些数据可以在 Pod 之间\u0026quot;传递\u0026rdquo;。\n在使用 RBD 之前，您必须安装运行 Ceph。\nRBD 的一个特点是它可以同时被多个用户以只读方式挂载。 这意味着您可以用数据集预先填充卷，然后根据需要从尽可能多的 Pod 中并行地提供卷。 不幸的是，RBD 卷只能由单个使用者以读写模式安装——不允许同时写入。\n更多详情请参考 RBD 示例。\nscaleIO ScaleIO 是基于软件的存储平台，可以使用现有硬件来创建可伸缩的、共享的而且是网络化的块存储集群。 scaleIO 卷插件允许部署的 Pod 访问现有的 ScaleIO 卷（或者它可以动态地为持久卷申领提供新的卷，参见ScaleIO 持久卷)。\n在使用前，您必须有个安装完毕且运行正常的 ScaleIO 集群，并且创建好了存储卷。\n下面是配置了 ScaleIO 的 Pod 示例：\napiVersion: v1 kind: Pod metadata: name: pod-0 spec: containers: - image: k8s.gcr.io/test-webserver name: pod-0 volumeMounts: - mountPath: /test-pd name: vol-0 volumes: - name: vol-0 scaleIO: gateway: https://localhost:443/api system: scaleio protectionDomain: sd0 storagePool: sp1 volumeName: vol-0 secretRef: name: sio-secret fsType: xfs 更多详情，请参考 ScaleIO 示例。\nsecret secret 卷用来给 Pod 传递敏感信息，例如密码。您可以将 secret 存储在 Kubernetes API 服务器上，然后以文件的形式挂在到 Pod 中，无需直接与 Kubernetes 耦合。 secret 卷由 tmpfs（基于 RAM 的文件系统）提供存储，因此它们永远不会被写入非易失性（持久化的）存储器。\n使用前您必须在 Kubernetes API 中创建 secret。\n容器以 subPath 卷的方式挂载 Secret 时，它将感知不到 Secret 的更新。\nSecret 的更多详情请参考这里。\nstorageOS storageos 卷允许将现有的 StorageOS 卷挂载到您的 Pod 中。\nStorageOS 在 Kubernetes 环境中以容器的形式运行，这使得应用能够从 Kubernetes 集群中的任何节点访问本地或关联的存储。 为应对节点失效状况，可以复制数据。 若需提高利用率和降低成本，可以考虑瘦配置（Thin Provisioning）和数据压缩。\n作为其核心能力之一，StorageOS 为容器提供了可以通过文件系统访问的块存储。\nStorageOS 容器需要 64 位的 Linux，并且没有其他的依赖关系。 StorageOS 提供免费的开发者授权许可。\n您必须在每个希望访问 StorageOS 卷的或者将向存储资源池贡献存储容量的节点上运行 StorageOS 容器。 有关安装说明，请参阅 StorageOS 文档。\napiVersion: v1 kind: Pod metadata: labels: name: redis role: master name: test-storageos-redis spec: containers: - name: master image: kubernetes/redis:v1 env: - name: MASTER value: \u0026#34;true\u0026#34; ports: - containerPort: 6379 volumeMounts: - mountPath: /redis-master-data name: redis-data volumes: - name: redis-data storageos: # The `redis-vol01` volume must already exist within StorageOS in the `default` namespace. volumeName: redis-vol01 fsType: ext4 更多关于动态供应和持久卷申领的信息请参考 StorageOS 示例。\nvsphereVolume 前提条件：配备了 vSphere 云驱动的 Kubernetes。云驱动的配置方法请参考 vSphere 使用指南。\nvsphereVolume 用来将 vSphere VMDK 卷挂载到您的 Pod 中。 在卸载卷时，卷的内容会被保留。 vSphereVolume 卷类型支持 VMFS 和 VSAN 数据仓库。\n在挂载到 Pod 之前，您必须用下列方式之一创建 VMDK。\n创建 VMDK 卷 选择下列方式之一创建 VMDK。\ntabs name=\u0026quot;tabs_volumes\u0026rdquo; \u0026gt;}} tab name=\u0026quot;使用 vmkfstools 创建\u0026rdquo; %}}\n首先 ssh 到 ESX，然后使用下面的命令来创建 VMDK：\nvmkfstools -c 2G /vmfs/volumes/DatastoreName/volumes/myDisk.vmdk /tab %}} tab name=\u0026quot;使用 vmware-vdiskmanager 创建\u0026rdquo; %}}\n使用下面的命令创建 VMDK：\nvmware-vdiskmanager -c -t 0 -s 40GB -a lsilogic myDisk.vmdk /tab %}}\n/tabs \u0026gt;}}\nvSphere VMDK 配置示例 apiVersion: v1 kind: Pod metadata: name: test-vmdk spec: containers: - image: k8s.gcr.io/test-webserver name: test-container volumeMounts: - mountPath: /test-vmdk name: test-volume volumes: - name: test-volume # This VMDK volume must already exist. vsphereVolume: volumePath: \u0026#34;[DatastoreName] volumes/myDisk\u0026#34; fsType: ext4 更多示例可以在这里找到。\n使用 subPath 有时，在单个 Pod 中共享卷以供多方使用是很有用的。 volumeMounts.subPath 属性可用于指定所引用的卷内的子路径，而不是其根路径。\n下面是一个使用同一共享卷的、内含 LAMP 栈（Linux Apache Mysql PHP）的 Pod 的示例。 HTML 内容被映射到卷的 html 文件夹，数据库将被存储在卷的 mysql 文件夹中：\napiVersion: v1 kind: Pod metadata: name: my-lamp-site spec: containers: - name: mysql image: mysql env: - name: MYSQL_ROOT_PASSWORD value: \u0026#34;rootpasswd\u0026#34; volumeMounts: - mountPath: /var/lib/mysql name: site-data subPath: mysql - name: php image: php:7.0-apache volumeMounts: - mountPath: /var/www/html name: site-data subPath: html volumes: - name: site-data persistentVolumeClaim: claimName: my-lamp-site-data 使用带有扩展环境变量的 subPath feature-state for_k8s_version=\u0026quot;v1.15\u0026rdquo; state=\u0026quot;beta\u0026rdquo; \u0026gt;}}\n使用 subPathExpr 字段从 Downward API 环境变量构造 subPath 目录名。 在使用此特性之前，必须启用 VolumeSubpathEnvExpansion 功能开关。 subPath 和 subPathExpr 属性是互斥的。\n在这个示例中，Pod 基于 Downward API 中的 Pod 名称，使用 subPathExpr 在 hostPath 卷 /var/log/pods 中创建目录 pod1。 主机目录 /var/log/pods/pod1 挂载到了容器的 /logs 中。\napiVersion: v1 kind: Pod metadata: name: pod1 spec: containers: - name: container1 env: - name: POD_NAME valueFrom: fieldRef: apiVersion: v1 fieldPath: metadata.name image: busybox command: [ \u0026#34;sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;while [ true ]; do echo \u0026#39;Hello\u0026#39;; sleep 10; done | tee -a /logs/hello.txt\u0026#34; ] volumeMounts: - name: workdir1 mountPath: /logs subPathExpr: $(POD_NAME) restartPolicy: Never volumes: - name: workdir1 hostPath: path: /var/log/pods 资源 emptyDir 卷的存储介质（磁盘、SSD 等）是由保存 kubelet 根目录（通常是 /var/lib/kubelet）的文件系统的介质确定。 emptyDir 卷或者 hostPath 卷可以消耗的空间没有限制，容器之间或 Pod 之间也没有隔离。\n将来，我们希望 emptyDir 卷和 hostPath 卷能够使用 resource 规范来请求一定量的空间，并且能够为具有多种介质类型的集群选择要使用的介质类型。\nOut-of-Tree 卷插件 Out-of-Tree 卷插件包括容器存储接口（CSI）和 FlexVolume。 它们使存储供应商能够创建自定义存储插件，而无需将它们添加到 Kubernetes 代码仓库。\n在引入 CSI 和 FlexVolume 之前，所有卷插件（如上面列出的卷类型）都是 \u0026ldquo;in-tree\u0026rdquo; 的，这意味着它们是与 Kubernetes 的核心组件一同构建、链接、编译和交付的，并且这些插件都扩展了 Kubernetes 的核心 API。 这意味着向 Kubernetes 添加新的存储系统（卷插件）需要将代码合并到 Kubernetes 核心代码库中。\nCSI 和 FlexVolume 都允许独立于 Kubernetes 代码库开发卷插件，并作为扩展部署（安装）在 Kubernetes 集群上。\n对于希望创建 out-of-tree 卷插件的存储供应商，请参考这个 FAQ。\nCSI feature-state for_k8s_version=\u0026quot;v1.10\u0026rdquo; state=\u0026quot;beta\u0026rdquo; \u0026gt;}}\n容器存储接口 (CSI) 为容器编排系统（如 Kubernetes）定义标准接口，以将任意存储系统暴露给它们的容器工作负载。\n更多详情请阅读 CSI 设计方案。\nCSI 的支持在 Kubernetes v1.9 中作为 alpha 特性引入，在 Kubernetes v1.10 中转为 beta 特性，并在 Kubernetes v1.13 正式 GA。\nKubernetes v1.13中不支持 CSI 规范版本0.2和0.3，并将在以后的版本中删除。\nCSI驱动程序可能并非在所有Kubernetes版本中都兼容。 请查看特定CSI驱动程序的文档，以获取每个 Kubernetes 版本所支持的部署步骤以及兼容性列表。\n一旦在 Kubernetes 集群上部署了 CSI 兼容卷驱动程序，用户就可以使用 csi 卷类型来关联、挂载 CSI 驱动程序暴露出来的卷。\ncsi 卷类型不支持来自 Pod 的直接引用，只能通过 PersistentVolumeClaim 对象在 Pod 中引用。\n存储管理员可以使用以下字段来配置 CSI 持久卷：\n driver：指定要使用的卷驱动程序名称的字符串值。 这个值必须与 CSI 驱动程序在 GetPluginInfoResponse 中返回的值相对应；该接口定义在 CSI 规范中。 Kubernetes 使用所给的值来标识要调用的 CSI 驱动程序；CSI 驱动程序也使用该值来辨识哪些 PV 对象属于该 CSI 驱动程序。   volumeHandle：唯一标识卷的字符串值。 该值必须与CSI 驱动程序在 CreateVolumeResponse 的 volume_id 字段中返回的值相对应；接口定义在 CSI spec 中。 在所有对 CSI 卷驱动程序的调用中，引用该 CSI 卷时都使用此值作为 volume_id 参数。   readOnly：一个可选的布尔值，指示通过 ControllerPublished 关联该卷时是否设置该卷为只读。 默认值是 false。 该值通过 ControllerPublishVolumeRequest 中的 readonly 字段传递给 CSI 驱动程序。   fsType：如果 PV 的 VolumeMode 为 Filesystem，那么此字段指定挂载卷时应该使用的文件系统。 如果卷尚未格式化，并且支持格式化，此值将用于格式化卷。 此值可以通过 ControllerPublishVolumeRequest、NodeStageVolumeRequest 和 NodePublishVolumeRequest 的 VolumeCapability 字段传递给 CSI 驱动。   volumeAttributes：一个字符串到字符串的映射表，用来设置卷的静态属性。 该映射必须与 CSI 驱动程序返回的 CreateVolumeResponse 中的 volume.attributes 字段的映射相对应；CSI 规范 中有相应的定义。 该映射通过ControllerPublishVolumeRequest、NodeStageVolumeRequest、和 NodePublishVolumeRequest 中的 volume_attributes 字段传递给 CSI 驱动。   controllerPublishSecretRef：对包含敏感信息的 secret 对象的引用；该敏感信息会被传递给 CSI 驱动来完成 CSI ControllerPublishVolume 和 ControllerUnpublishVolume 调用。 此字段是可选的；在不需要 secret 时可以是空的。 如果 secret 对象包含多个 secret，则所有的 secret 都会被传递。   nodeStageSecretRef：对包含敏感信息的 secret 对象的引用，以传递给 CSI 驱动来完成 CSI NodeStageVolume 调用。 此字段是可选的，如果不需要 secret，则可能是空的。 如果 secret 对象包含多个 secret，则传递所有 secret。   nodePublishSecretRef：对包含敏感信息的 secret 对象的引用，以传递给 CSI 驱动来完成 CSI ``NodePublishVolume` 调用。 此字段是可选的，如果不需要 secret，则可能是空的。 如果 secret 对象包含多个 secret，则传递所有 secret。  CSI 原始块卷支持 feature-state for_k8s_version=\u0026quot;v1.14\u0026rdquo; state=\u0026quot;beta\u0026rdquo; \u0026gt;}}\n从 1.11 版本开始，CSI 引入了对原始块卷的支持。该特性依赖于在 Kubernetes 的之前版本中引入的原始块卷（Raw Block Volume）功能。 该特性将使具有外部 CSI 驱动程序的供应商能够在 Kubernetes 工作负载中实现原始块卷支持。\nCSI块卷支持功能已启用，但默认情况下启用。必须为此功能启用的两个功能是“ BlockVolume”和“ CSIBlockVolume”。\n--feature-gates=BlockVolume=true,CSIBlockVolume=true 学习怎样安装您的带有块卷支持的 PV/PVC。\nCSI临时卷 feature-state for_k8s_version=\u0026quot;v1.16\u0026rdquo; state=\u0026quot;beta\u0026rdquo; \u0026gt;}}\n此功能使 CSI 卷可以直接嵌入 Pod 规范中，而不是 PersistentVolume 中。 以这种方式指定的卷是临时的，不会在 Pod 重新启动后持续存在。\n实例：\nkind: Pod apiVersion: v1 metadata: name: my-csi-app spec: containers: - name: my-frontend image: busybox volumeMounts: - mountPath: \u0026#34;/data\u0026#34; name: my-csi-inline-vol command: [ \u0026#34;sleep\u0026#34;, \u0026#34;1000000\u0026#34; ] volumes: - name: my-csi-inline-vol csi: driver: inline.storage.kubernetes.io volumeAttributes: foo: bar 此功能需要启用 CSIInlineVolume 功能门。 从Kubernetes 1.16开始默认启用它。\nCSI 临时卷仅由一部分 CSI 驱动程序支持。 请在此处查看 CSI 驱动程序列表。\n＃开发人员资源 有关如何开发 CSI 驱动程序的更多信息，请参考kubernetes-csi文档\n从 in-tree 插件迁移到 CSI 驱动程序 feature-state for_k8s_version=\u0026quot;v1.14\u0026rdquo; state=\u0026quot;alpha\u0026rdquo; \u0026gt;}}\n启用 CSI 迁移功能后，会将针对现有 in-tree 插件的操作定向到相应的 CSI 插件（应安装和配置）。 该功能实现了必要的转换逻辑和填充以无缝方式重新路由操作。 因此，操作员在过渡到取代树内插件的CSI驱动程序时，无需对现有存储类，PV 或 PVC（指 in-tree 插件)进行任何配置更改。 在 Alpha 状态下，受支持的操作和功能包括供应/删除，附加/分离，安装/卸载和调整卷大小。 上面的 \u0026ldquo;卷类型\u0026rdquo; 部分列出了支持 CSI 迁移并已实现相应 CSI 驱动程序的树内插件。\nFlexVolume FlexVolume 是一个自 1.2 版本（在 CSI 之前）以来在 Kubernetes 中一直存在的 out-of-tree 插件接口。 它使用基于 exec 的模型来与驱动程序对接。 用户必须在每个节点（在某些情况下是主节点）上的预定义卷插件路径中安装 FlexVolume 驱动程序可执行文件。\nPod 通过 flexvolume in-tree 插件与 Flexvolume 驱动程序交互。 更多详情请参考这里。\n挂载卷的传播 挂载卷的传播能力允许将容器安装的卷共享到同一 Pod 中的其他容器，甚至共享到同一节点上的其他 Pod。\n卷的挂载传播特性由 Container.volumeMounts 中的 mountPropagation 字段控制。 它的值包括：\n None - 此卷挂载将不会感知到主机后续在此卷或其任何子目录上执行的挂载变化。 类似的，容器所创建的卷挂载在主机上是不可见的。这是默认模式。 该模式等同于 Linux 内核文档中描述的 private 挂载传播选项。   HostToContainer - 此卷挂载将会感知到主机后续针对此卷或其任何子目录的挂载操作。  换句话说，如果主机在此挂载卷中挂载任何内容，容器将能看到它被挂载在那里。\n类似的，配置了 Bidirectional 挂载传播选项的 Pod 如果在同一卷上挂载了内容，挂载传播设置为 HostToContainer 的容器都将能看到这一变化。\n该模式等同于 Linux 内核文档 中描述的 rslave 挂载传播选项。\n  Bidirectional - 这种卷挂载和 HostToContainer 挂载表现相同。\n另外，容器创建的卷挂载将被传播回至主机和使用同一卷的所有 Pod 的所有容器。\n该模式等同于 Linux 内核文档 中描述的 rshared 挂载传播选项。\n  Bidirectional 形式的挂载传播可能比较危险。 它可以破坏主机操作系统，因此它只被允许在特权容器中使用。 强烈建议您熟悉 Linux 内核行为。 此外，由 Pod 中的容器创建的任何卷挂载必须在终止时由容器销毁（卸载）。\n配置 在某些部署环境中，挂载传播正常工作前，必须在 Docker 中正确配置挂载共享（mount share），如下所示。\n编辑您的 Docker systemd 服务文件，按下面的方法设置 MountFlags：\nMountFlags=shared 或者，如果存在 MountFlags=slave 就删除掉。然后重启 Docker 守护进程：\nsudo systemctl daemon-reload sudo systemctl restart docker whatsnext  参考使用持久卷部署 WordPress 和 MySQL 示例。  "
},
{
	"uri": "https://lijun.in/tasks/administer-cluster/manage-resources/memory-default-namespace/",
	"title": "为命名空间配置默认的内存请求和限制",
	"tags": [],
	"description": "",
	"content": "本文介绍怎样给命名空间配置默认的内存请求和限制。如果在一个有默认内存限制的命名空间创建容器，该容器没有声明自己的内存限制时，将会被指定默认内存限制。Kubernetes 还为某些情况指定了默认的内存请求，本章后面会进行介绍。\n. heading \u0026ldquo;prerequisites\u0026rdquo; %}} . include \u0026ldquo;task-tutorial-prereqs.md\u0026rdquo; \u0026gt;}} . version-check \u0026gt;}}\n你的集群中的每个节点必须至少有2 GiB的内存。\n创建命名空间 创建一个命名空间，以便本练习中所建的资源与集群的其余资源相隔离。\nkubectl create namespace default-mem-example 创建 LimitRange 和 Pod 这里给出了一个限制范围对象的配置文件。该配置声明了一个默认的内存请求和一个默认的内存限制。\n. codenew file=\u0026quot;admin/resource/memory-defaults.yaml\u0026rdquo; \u0026gt;}}\n在 default-mem-example 命名空间创建限制范围：\nkubectl create -f https://k8s.io/examples/admin/resource/memory-defaults.yaml --namespace=default-mem-example 现在，如果在 default-mem-example 命名空间创建容器，并且该容器没有声明自己的内存请求和限制值，它将被指定一个默认的内存请求256 MiB和一个默认的内存限制512 Mib。\n. codenew file=\u0026quot;admin/resource/memory-defaults-pod.yaml\u0026rdquo; \u0026gt;}}\n创建 Pod\nkubectl create -f https://k8s.io/examples/admin/resource/memory-defaults-pod.yaml --namespace=default-mem-example 查看 Pod 的详情：\nkubectl get pod default-mem-demo --output=yaml --namespace=default-mem-example 输出内容显示该Pod的容器有一个256 MiB的内存请求和一个512 MiB的内存限制。这些都是限制范围声明的默认值。\ncontainers: - image: nginx imagePullPolicy: Always name: default-mem-demo-ctr resources: limits: memory: 512Mi requests: memory: 256Mi 删除你的 Pod：\nkubectl delete pod default-mem-demo --namespace=default-mem-example 声明容器的限制而不声明它的请求会怎么样？ 这里给出了包含一个容器的 Pod 的配置文件。该容器声明了内存限制，而没有声明内存请求：\n. codenew file=\u0026quot;admin/resource/memory-defaults-pod-2.yaml\u0026rdquo; \u0026gt;}}\n创建 Pod：\nkubectl create -f https://k8s.io/examples/admin/resource/memory-defaults-pod-2.yaml --namespace=default-mem-example 查看 Pod 的详情：\nkubectl get pod default-mem-demo-2 --output=yaml --namespace=default-mem-example 输出结果显示容器的内存请求被设置为它的内存限制相同的值。注意该容器没有被指定默认的内存请求值256Mi。\nresources: limits: memory: 1Gi requests: memory: 1Gi 声明容器的内存请求而不声明内存限制会怎么样？ 这里给出了一个包含一个容器的 Pod 的配置文件。该容器声明了内存请求，但没有内存限制：\n. codenew file=\u0026quot;admin/resource/memory-defaults-pod-3.yaml\u0026rdquo; \u0026gt;}}\n创建 Pod：\nkubectl create -f https://k8s.io/examples/admin/resource/memory-defaults-pod-3.yaml --namespace=default-mem-example 查看 Pod 声明：\nkubectl get pod default-mem-demo-3 --output=yaml --namespace=default-mem-example 输出结果显示该容器的内存请求被设置为了容器配置文件中声明的数值。容器的内存限制被设置为512Mi，即命名空间的默认内存限制。\nresources: limits: memory: 512Mi requests: memory: 128Mi 设置默认内存限制和请求的动机 如果你的命名空间有资源配额，那么默认内存限制是很有帮助的。下面是一个例子，通过资源配额为命名空间设置两项约束：\n 运行在命名空间中的每个容器必须有自己的内存限制。 命名空间中所有容器的内存使用量之和不能超过声明的限制值。  如果一个容器没有声明自己的内存限制，会被指定默认限制，然后它才会被允许在限定了配额的命名空间中运行。\n. heading \u0026ldquo;whatsnext\u0026rdquo; %}} 集群管理员参考   为命名空间配置默认的 CPU 请求和限制\n  为命名空间配置最小和最大内存限制\n  为命名空间配置最小和最大 CPU 限制\n  为命名空间配置内存和 CPU 配额\n  为命名空间配置 Pod 配额\n  为 API 对象配置配额\n  应用开发者参考   为容器和 Pod 分配内存资源\n  为容器和 Pod 分配 CPU 资源\n  为 Pod 配置服务数量\n  "
},
{
	"uri": "https://lijun.in/tasks/configure-pod-container/assign-memory-resource/",
	"title": "为容器和 Pod 分配内存资源",
	"tags": [],
	"description": "",
	"content": "此页面显示如何将内存 请求 （request）和内存 限制 （limit）分配给一个容器。我们保障容器拥有它请求数量的内存，但不允许使用超过限制数量的内存。\n. heading \u0026ldquo;prerequisites\u0026rdquo; %}} . include \u0026ldquo;task-tutorial-prereqs.md\u0026rdquo; \u0026gt;}} . version-check \u0026gt;}}\n您集群中的每个节点必须拥有至少 300 MiB 的内存。\n该页面上的一些步骤要求您在集群中运行 metrics-server 服务。如果您已经有在运行中的 metrics-server，则可以跳过这些步骤。\n如果您运行的是 Minikube，可以运行下面的命令启用 metrics-server：\nminikube addons enable metrics-server 要查看 metrics-server 或资源指标 API (metrics.k8s.io) 是否已经运行，请运行以下命令：\nkubectl get apiservices 如果资源指标 API 可用，则输出结果将包含对 metrics.k8s.io 的引用信息。\nNAME v1beta1.metrics.k8s.io 创建命名空间 创建一个命名空间，以便将本练习中创建的资源与集群的其余部分隔离。\nkubectl create namespace mem-example 指定内存请求和限制 要为容器指定内存请求，请在容器资源清单中包含 resources：requests 字段。 同理，要指定内存限制，请包含 resources：limits。\n在本练习中，您将创建一个拥有一个容器的 Pod。 容器将会请求 100 MiB 内存，并且内存会被限制在 200 MiB 以内。 这是 Pod 的配置文件：\n. codenew file=\u0026quot;pods/resource/memory-request-limit.yaml\u0026rdquo; \u0026gt;}}\n配置文件的 args 部分提供了容器启动时的参数。 \u0026quot;--vm-bytes\u0026quot;, \u0026quot;150M\u0026quot; 参数告知容器尝试分配 150 MiB 内存。\n开始创建 Pod：\nkubectl apply -f https://k8s.io/examples/pods/resource/memory-request-limit.yaml --namespace=mem-example 验证 Pod 中的容器是否已运行：\nkubectl get pod memory-demo --namespace=mem-example 查看 Pod 相关的详细信息：\nkubectl get pod memory-demo --output=yaml --namespace=mem-example 输出结果显示：该 Pod 中容器的内存请求为 100 MiB，内存限制为 200 MiB。\n... resources: limits: memory: 200Mi requests: memory: 100Mi ... 运行 kubectl top 命令，获取该 Pod 的指标数据：\nkubectl top pod memory-demo --namespace=mem-example 输出结果显示：Pod 正在使用的内存大约为 162,900,000 字节，约为 150 MiB。 这大于 Pod 请求的 100 MiB，但在 Pod 限制的 200 MiB之内。\nNAME CPU(cores) MEMORY(bytes) memory-demo \u0026lt;something\u0026gt; 162856960 删除 Pod：\nkubectl delete pod memory-demo --namespace=mem-example 超过容器限制的内存 当节点拥有足够的可用内存时，容器可以使用其请求的内存。但是，容器不允许使用超过其限制的内存。 如果容器分配的内存超过其限制，该容器会成为被终止的候选容器。如果容器继续消耗超出其限制的内存，则终止容器。 如果终止的容器可以被重启，则 kubelet 会重新启动它，就像其他任何类型的运行时失败一样。\n在本练习中，您将创建一个 Pod，尝试分配超出其限制的内存。 这是一个 Pod 的配置文件，其拥有一个容器，该容器的内存请求为 50 MiB，内存限制为 100 MiB：\n. codenew file=\u0026quot;pods/resource/memory-request-limit-2.yaml\u0026rdquo; \u0026gt;}}\n在配置文件的 args 部分中，您可以看到容器会尝试分配 250 MiB 内存，这远高于 100 MiB 的限制。\n创建 Pod：\nkubectl apply -f https://k8s.io/examples/pods/resource/memory-request-limit-2.yaml --namespace=mem-example 查看 Pod 相关的详细信息：\nkubectl get pod memory-demo-2 --namespace=mem-example 此时，容器可能正在运行或被杀死。重复前面的命令，直到容器被杀掉：\nNAME READY STATUS RESTARTS AGE memory-demo-2 0/1 OOMKilled 1 24s 获取容器更详细的状态信息：\nkubectl get pod memory-demo-2 --output=yaml --namespace=mem-example 输出结果显示：由于内存溢出（OOM），容器已被杀掉：\nlastState: terminated: containerID: docker://65183c1877aaec2e8427bc95609cc52677a454b56fcb24340dbd22917c23b10f exitCode: 137 finishedAt: 2017-06-20T20:52:19Z reason: OOMKilled startedAt: null 本练习中的容器可以被重启，所以 kubelet 会重启它。多次运行下面的命令，可以看到容器在反复的被杀死和重启：\nkubectl get pod memory-demo-2 --namespace=mem-example 输出结果显示：容器被杀掉、重启、再杀掉、再重启……：\nkubectl get pod memory-demo-2 --namespace=mem-example NAME READY STATUS RESTARTS AGE memory-demo-2 0/1 OOMKilled 1 37s  kubectl get pod memory-demo-2 --namespace=mem-example NAME READY STATUS RESTARTS AGE memory-demo-2 1/1 Running 2 40s 查看关于该 Pod 历史的详细信息：\nkubectl describe pod memory-demo-2 --namespace=mem-example 输出结果显示：该容器反复的在启动和失败：\n... Normal Created Created container with id 66a3a20aa7980e61be4922780bf9d24d1a1d8b7395c09861225b0eba1b1f8511 ... Warning BackOff Back-off restarting failed container 查看关于集群节点的详细信息：\nkubectl describe nodes 输出结果包含了一条练习中的容器由于内存溢出而被杀掉的记录：\nWarning OOMKilling Memory cgroup out of memory: Kill process 4481 (stress) score 1994 or sacrifice child 删除 Pod:\nkubectl delete pod memory-demo-2 --namespace=mem-example 超过整个节点容量的内存 内存请求和限制是与容器关联的，但将 Pod 视为具有内存请求和限制，也是很有用的。 Pod 的内存请求是 Pod 中所有容器的内存请求之和。 同理，Pod 的内存限制是 Pod 中所有容器的内存限制之和。\nPod 的调度基于请求。只有当节点拥有足够满足 Pod 内存请求的内存时，才会将 Pod 调度至节点上运行。\n在本练习中，你将创建一个 Pod，其内存请求超过了您集群中的任意一个节点所拥有的内存。 这是该 Pod 的配置文件，其拥有一个请求 1000 GiB 内存的容器，这应该超过了您集群中任何节点的容量。\n. codenew file=\u0026quot;pods/resource/memory-request-limit-3.yaml\u0026rdquo; \u0026gt;}}\n创建 Pod：\nkubectl apply -f https://k8s.io/examples/pods/resource/memory-request-limit-3.yaml --namespace=mem-example 查看 Pod 状态：\nkubectl get pod memory-demo-3 --namespace=mem-example 输出结果显示：Pod 处于 PENDING 状态。这意味着，该 Pod 没有被调度至任何节点上运行，并且它会无限期的保持该状态：\nkubectl get pod memory-demo-3 --namespace=mem-example NAME READY STATUS RESTARTS AGE memory-demo-3 0/1 Pending 0 25s 查看关于 Pod 的详细信息，包括事件：\nkubectl describe pod memory-demo-3 --namespace=mem-example 输出结果显示：由于节点内存不足，该容器无法被调度：\nEvents: ... Reason Message ------ ------- ... FailedScheduling No nodes are available that match all of the following predicates:: Insufficient memory (3). 内存单位 内存资源的基本单位是字节（byte）。您可以使用这些后缀之一，将内存表示为纯整数或定点整数：E、P、T、G、M、K、Ei、Pi、Ti、Gi、Mi、Ki。例如，下面是一些近似相同的值：\n128974848, 129e6, 129M , 123Mi 删除 Pod：\nkubectl delete pod memory-demo-3 --namespace=mem-example 如果你没有指定内存限制 如果你没有为一个容器指定内存限制，则自动遵循以下情况之一：\n  容器可无限制地使用内存。容器可以使用其所在节点所有的可用内存，进而可能导致该节点调用 OOM Killer。 此外，如果发生 OOM Kill，没有资源限制的容器将被杀掉的可行性更大。\n  运行的容器所在命名空间有默认的内存限制，那么该容器会被自动分配默认限制。 集群管理员可用使用 [LimitRange](/docs/reference/generated/kubernetes-api/. param \u0026ldquo;version\u0026rdquo; \u0026gt;}}/#limitrange-v1-core) 来指定默认的内存限制。\n  内存请求和限制的目的 通过为集群中运行的容器配置内存请求和限制，您可以有效利用集群节点上可用的内存资源。通过将 Pod 的内存请求保持在较低水平，您可以更好地安排 Pod 调度。通过让内存限制大于内存请求，您可以完成两件事：\n Pod 可以进行一些突发活动，从而更好的利用可用内存。 Pod 在突发活动期间，可使用的内存被限制为合理的数量。  清理 删除命名空间。下面的命令会删除你根据这个任务创建的所有 Pod：\nkubectl delete namespace mem-example . heading \u0026ldquo;whatsnext\u0026rdquo; %}} 应用开发者扩展阅读   为容器和 Pod 分配 CPU 资源\n  配置 Pod 的服务质量\n  集群管理员扩展阅读   为命名空间配置默认的内存请求和限制\n  为命名空间配置默认的 CPU 请求和限制\n  配置命名空间的最小和最大内存约束\n  配置命名空间的最小和最大 CPU 约束\n  为命名空间配置内存和 CPU 配额\n  配置命名空间下 Pod 总数\n  配置 API 对象配额\n  "
},
{
	"uri": "https://lijun.in/tasks/inject-data-application/define-command-argument-container/",
	"title": "为容器设置启动时要执行的命令及其入参",
	"tags": [],
	"description": "",
	"content": "本页将展示如何为 . glossary_tooltip term_id=\u0026quot;pod\u0026rdquo; \u0026gt;}} 中的容器设置启动时要执行的命令及其入参。\n. heading \u0026ldquo;prerequisites\u0026rdquo; %}} . include \u0026ldquo;task-tutorial-prereqs.md\u0026rdquo; \u0026gt;}} . version-check \u0026gt;}}\n创建 Pod 时设置命令及入参 创建 Pod 时，可以为其下的容器设置启动时要执行的命令及其入参。如果要设置命令，就填写在配置文件的 command 字段下，如果要设置命令的入参，就填写在配置文件的 args 字段下。一旦 Pod 创建完成，该命令及其入参就无法再进行更改了。\n如果在配置文件中设置了容器启动时要执行的命令及其入参，那么容器镜像中自带的命令与入参将会被覆盖而不再执行。如果配置文件中只是设置了入参，却没有设置其对应的命令，那么容器镜像中自带的命令会使用该新入参作为其执行时的入参。\n. note \u0026gt;}} 在有些容器运行时中，command 字段对应 entrypoint，请参阅下面的 注意。 . /note \u0026gt;}}\n本示例中，将创建一个只包含单个容器的 Pod。在 Pod 配置文件中设置了一个命令与两个入参：\n. codenew file=\u0026quot;pods/commands.yaml\u0026rdquo; \u0026gt;}}\n  基于 YAML 文件创建一个 Pod：\nkubectl apply -f https://k8s.io/examples/pods/commands.yaml    获取正在运行的 Pods：\nkubectl get pods ``\n查询结果显示在 command-demo 这个 Pod 下运行的容器已经启动完成。\n   如果要获取容器启动时执行命令的输出结果，可以通过 Pod 的日志进行查看：\nkubectl logs command-demo 日志中显示了 HOSTNAME 与 KUBERNETES_PORT 这两个环境变量的值：\ncommand-demo tcp://10.3.240.1:443   使用环境变量来设置入参 在上面的示例中，我们直接将一串字符作为命令的入参。除此之外，我们还可以将环境变量作为命令的入参。\nenv: - name: MESSAGE value: \u0026#34;hello world\u0026#34; command: [\u0026#34;/bin/echo\u0026#34;] args: [\u0026#34;$(MESSAGE)\u0026#34;] 这意味着你可以将那些用来设置环境变量的方法应用于设置命令的入参，其中包括了 ConfigMaps 与 Secrets。\n. note \u0026gt;}} 环境变量需要加上括号，类似于 \u0026quot;$(VAR)\u0026quot;。这是在 command 或 args 字段使用变量的格式要求。 . /note \u0026gt;}}\n通过 shell 来执行命令 有时候，您需要在 shell 脚本中运行命令。 例如，您要执行的命令可能由多个命令组合而成，或者它就是一个 shell 脚本。这时，就可以通过如下方式在 shell 中执行命令：\ncommand: [\u0026#34;/bin/sh\u0026#34;] args: [\u0026#34;-c\u0026#34;, \u0026#34;while true; do echo hello; sleep 10;done\u0026#34;] 注意 下表给出了 Docker 与 Kubernetes 中对应的字段名称。\n   描述 Docker 字段名称 Kubernetes 字段名称     容器执行的命令 Entrypoint command   传给命令的参数 Cmd args    如果要覆盖默认的 Entrypoint 与 Cmd，需要遵循如下规则：\n  如果在容器配置中没有设置 command 或者 args，那么将使用 Docker 镜像自带的命令及其入参。\n  如果在容器配置中只设置了 command 但是没有设置 args，那么容器启动时只会执行该命令，Docker 镜像中自带的命令及其入参会被忽略。\n  如果在容器配置中只设置了 args，那么 Docker 镜像中自带的命令会使用该新入参作为其执行时的入参。\n  如果在容器配置中同时设置了 command 与 args，那么 Docker 镜像中自带的命令及其入参会被忽略。容器启动时只会执行配置中设置的命令，并使用配置中设置的入参作为命令的入参。\n  下表涵盖了各类设置场景：\n   镜像 Entrypoint 镜像 Cmd 容器 command 容器 args 命令执行     [/ep-1] [foo bar] \u0026lt;not set\u0026gt; \u0026lt;not set\u0026gt; [ep-1 foo bar]   [/ep-1] [foo bar] [/ep-2] \u0026lt;not set\u0026gt; [ep-2]   [/ep-1] [foo bar] \u0026lt;not set\u0026gt; [zoo boo] [ep-1 zoo boo]   [/ep-1] [foo bar] [/ep-2] [zoo boo] [ep-2 zoo boo]    . heading \u0026ldquo;whatsnext\u0026rdquo; %}}  了解更多 pod 和容器的配置。 了解更多 在容器中运行命令。 请参阅 [有关容器的文档](/docs/reference/generated/kubernetes-api/. param \u0026ldquo;version\u0026rdquo; \u0026gt;}}/#container-v1-core)。  "
},
{
	"uri": "https://lijun.in/tasks/administer-cluster/network-policy-provider/calico-network-policy/",
	"title": "使用 Calico 作为 NetworkPolicy",
	"tags": [],
	"description": "",
	"content": "本页展示了两种在 Kubernetes 上快速创建 Calico 集群的方法。\n. heading \u0026ldquo;prerequisites\u0026rdquo; %}} 决定您想部署一个云 还是 本地 集群。\n在 Google Kubernetes Engine (GKE) 上创建一个 Calico 集群 先决条件: gcloud\n  启动一个带有 Calico 的 GKE 集群，只需加上flag --enable-network-policy。\n语法\ngcloud container clusters create [CLUSTER_NAME] --enable-network-policy 示例\ngcloud container clusters create my-calico-cluster --enable-network-policy   使用如下命令验证部署是否正确。\nkubectl get pods --namespace=kube-system Calico 的 pods 名以 calico 打头，检查确认每个 pods 状态为 Running。\n  使用 kubeadm 创建一个本地 Calico 集群 在15分钟内使用 kubeadm 得到一个本地单主机 Calico 集群，请参考 Calico 快速入门。\n. heading \u0026ldquo;whatsnext\u0026rdquo; %}} 集群运行后，您可以按照 声明 Network Policy 去尝试使用 Kubernetes NetworkPolicy。\n"
},
{
	"uri": "https://lijun.in/tasks/job/automated-tasks-with-cron-jobs/",
	"title": "使用 CronJob 运行自动化任务",
	"tags": [],
	"description": "",
	"content": "你可以利用 CronJobs 执行基于时间调度的任务。这些自动化任务和 Linux 或者 Unix 系统的 Cron 任务类似。\nCronJobs 在创建周期性以及重复性的任务时很有帮助，例如执行备份操作或者发送邮件。CronJobs 也可以在特定时间调度单个任务，例如你想调度低活跃周期的任务。\n. note \u0026gt;}}\n从集群版本1.8开始，batch/v2alpha1 API 组中的 CronJob 资源已经被废弃。 你应该切换到 API 服务器默认启用的 batch/v1beta1 API 组。本文中的所有示例使用了batch/v1beta1。 . /note \u0026gt;}}\nCronJobs 有一些限制和特点。 例如，在特定状况下，同一个 CronJob 可以创建多个任务。 因此，任务应该是幂等的。 查看更多限制，请参考 CronJobs。\n. heading \u0026ldquo;prerequisites\u0026rdquo; %}}  . include \u0026ldquo;task-tutorial-prereqs.md\u0026rdquo; \u0026gt;}} . version-check \u0026gt;}}   你需要一个版本 \u0026gt;=1.8 且工作正常的 Kubernetes 集群。对于更早的版本（ \u0026lt;1.8 ），你需要对 API 服务器设置 --runtime-config=batch/v2alpha1=true 来开启 batch/v2alpha1 API，(更多信息请查看 为你的集群开启或关闭 API 版本 ), 然后重启 API 服务器和控制管理器。  创建 CronJob CronJob 需要一个配置文件。 本例中 CronJob 的.spec 配置文件每分钟打印出当前时间和一个问好信息：\n. codenew file=\u0026quot;application/job/cronjob.yaml\u0026rdquo; \u0026gt;}}\n想要运行示例的 CronJob，可以下载示例文件并执行命令：\n$ kubectl create -f ./cronjob.yaml cronjob \u0026#34;hello\u0026#34; created 或者你也可以使用 kubectl run 来创建一个 CronJob 而不需要编写完整的配置：\n$ kubectl run hello --schedule=\u0026#34;*/1 * * * *\u0026#34; --restart=OnFailure --image=busybox -- /bin/sh -c \u0026#34;date; echo Hello from the Kubernetes cluster\u0026#34; cronjob \u0026#34;hello\u0026#34; created 创建好 CronJob 后，使用下面的命令来获取其状态：\n$ kubectl get cronjob hello NAME SCHEDULE SUSPEND ACTIVE LAST-SCHEDULE hello */1 * * * * False 0 \u0026lt;none\u0026gt; 就像你从命令返回结果看到的那样，CronJob 还没有调度或执行任何任务。大约需要一分钟任务才能创建好。\n$ kubectl get jobs --watch NAME DESIRED SUCCESSFUL AGE hello-4111706356 1 1 2s 现在你已经看到了一个运行中的任务被 “hello” CronJob 调度。你可以停止监视这个任务，然后再次查看 CronJob 就能看到它调度任务：\n$ kubectl get cronjob hello NAME SCHEDULE SUSPEND ACTIVE LAST-SCHEDULE hello */1 * * * * False 0 Mon, 29 Aug 2016 14:34:00 -0700 你应该能看到 “hello” CronJob 在 LAST-SCHEDULE 声明的时间点成功的调度了一次任务。有0个活跃的任务意味着任务执行完毕或者执行失败。\n现在，找到最后一次调度任务创建的 Pod 并查看一个 Pod 的标准输出。请注意任务名称和 Pod 名称是不同的。\n# Replace \u0026#34;hello-4111706356\u0026#34; with the job name in your system $ pods=$(kubectl get pods --selector=job-name=hello-4111706356 --output=jsonpath={.items..metadata.name}) $ echo $pods hello-4111706356-o9qcm $ kubectl logs $pods Mon Aug 29 21:34:09 UTC 2016 Hello from the Kubernetes cluster 删除 CronJob 当你不再需要 CronJob 时，可以用 kubectl delete cronjob 删掉它：\n$ kubectl delete cronjob hello cronjob \u0026#34;hello\u0026#34; deleted 删除 CronJob 会清除它创建的所有任务和 Pod，并阻止它创建额外的任务。你可以查阅 垃圾收集。\n编写 CronJob 声明信息 像 Kubernetes 的其他配置一样，CronJob 需要 apiVersion、 kind、 和 metadata 域。配置文件的一般信息，请参考 部署应用 和 使用 kubectl 管理资源.\nCronJob 配置也需要包括.spec.\n. note \u0026gt;}}\n对 CronJob 的所有改动，特别是它的 .spec，只会影响将来的运行实例。 . /note \u0026gt;}}\n时间安排 .spec.schedule 是 .spec 需要的域。它使用了 Cron 格式串，例如 0 * * * * or @hourly ，做为它的任务被创建和执行的调度时间。\n该格式也包含了扩展的 vixie cron 步长值。FreeBSD 手册中解释如下:\n 步长可被用于范围组合。范围后面带有 /\u0026lt;数字\u0026gt; 可以声明范围内的步幅数值。 例如，0-23/2 可被用在小时域来声明命令在其他数值的小时数执行（ V7 标准中对应的方法是0,2,4,6,8,10,12,14,16,18,20,22）。 步长也可以放在通配符后面，因此如果你想表达 \u0026ldquo;每两小时\u0026rdquo;，就用 */2 。\n . note \u0026gt;}}\n调度中的问号 (?) 和星号 * 含义相同，表示给定域的任何可用值。 . /note \u0026gt;}}\n任务模版 .spec.jobTemplate是任务的模版，它是必须的。它和 Job的语法完全一样，除了它是嵌套的没有 apiVersion 和 kind。 编写任务的 .spec ，请参考 编写任务的Spec。\n开始的最后期限 .spec.startingDeadlineSeconds 域是可选的。 它表示任务如果由于某种原因错过了调度时间，开始该任务的截止时间的秒数。过了截止时间，CronJob 就不会开始任务。 不满足这种最后期限的任务会被统计为失败任务。如果该域没有声明，那任务就没有最后期限。\nCronJob 控制器会统计错过了多少次调度。如果错过了100次以上的调度，CronJob 就不再调度了。当没有设置 .spec.startingDeadlineSeconds 时，CronJob 控制器统计从status.lastScheduleTime到当前的调度错过次数。 例如一个 CronJob 期望每分钟执行一次，status.lastScheduleTime是 5:00am，但现在是 7:00am。那意味着120次调度被错过了，所以 CronJob 将不再被调度。 如果设置了 .spec.startingDeadlineSeconds 域(非空)，CronJob 控制器统计从 .spec.startingDeadlineSeconds 到当前时间错过了多少次任务。 例如设置了 200，它会统计过去200秒内错过了多少次调度。在那种情况下，如果过去200秒内错过了超过100次的调度，CronJob 就不再调度。\n并发性规则 .spec.concurrencyPolicy 也是可选的。它声明了 CronJob 创建的任务执行时发生重叠如何处理。spec 仅能声明下列规则中的一种：\n Allow (默认)：CronJob 允许并发任务执行。 Forbid： CronJob 不允许并发任务执行；如果新任务的执行时间到了而老任务没有执行完，CronJob 会忽略新任务的执行。 Replace：如果新任务的执行时间到了而老任务没有执行完，CronJob 会用新任务替换当前正在运行的任务。  请注意，并发性规则仅适用于相同 CronJob 创建的任务。如果有多个 CronJob，它们相应的任务总是允许并发执行的。\n挂起 .spec.suspend域也是可选的。如果设置为 true ，后续发生的执行都会挂起。这个设置对已经开始的执行不起作用。默认是关闭的。\n. caution \u0026gt;}}\n在调度时间内挂起的执行都会被统计为错过的任务。当 .spec.suspend 从 true 改为 false 时，且没有 开始的最后期限，错过的任务会被立即调度。 . /caution \u0026gt;}}\n任务历史限制 .spec.successfulJobsHistoryLimit 和 .spec.failedJobsHistoryLimit是可选的。 这两个域声明了有多少执行完成和失败的任务会被保留。 默认设置为3和1。限制设置为0代表相应类型的任务完成后不会保留。\n"
},
{
	"uri": "https://lijun.in/setup/production-environment/tools/kubeadm/",
	"title": "使用 kubeadm 引导集群",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lijun.in/tutorials/kubernetes-basics/deploy-app/deploy-intro/",
	"title": "使用 kubectl 创建 Deployment",
	"tags": [],
	"description": "",
	"content": "  Objectives -- 目标 Learn about application Deployments. Deploy your first app on Kubernetes with kubectl.  --  学习了解应用的部署 使用 kubectl 在 Kubernetes 上部署第一个应用   Kubernetes Deployments -- Kubernetes 部署 Once you have a running Kubernetes cluster, you can deploy your containerized applications on top of it. To do so, you create a Kubernetes Deployment configuration. The Deployment instructs Kubernetes how to create and update instances of your application. Once you've created a Deployment, the Kubernetes master schedules mentioned application instances onto individual Nodes in the cluster. --  一旦运行了 Kubernetes 集群，就可以在其上部署容器化应用程序。 为此，您需要创建 Kubernetes  Deployment 配置。Deployment 指挥 Kubernetes 如何创建和更新应用程序的实例。创建 Deployment 后，Kubernetes master 将应用程序实例调度到集群中的各个节点上。 Once the application instances are created, a Kubernetes Deployment Controller continuously monitors those instances. If the Node hosting an instance goes down or is deleted, the Deployment controller replaces the instance with an instance on another Node in the cluster. This provides a self-healing mechanism to address machine failure or maintenance.\n-- 创建应用程序实例后，Kubernetes Deployment 控制器会持续监视这些实例。 如果托管实例的节点关闭或被删除，则 Deployment 控制器会将该实例替换为群集中另一个节点上的实例。 这提供了一种自我修复机制来解决机器故障维护问题。\nIn a pre-orchestration world, installation scripts would often be used to start applications, but they did not allow recovery from machine failure. By both creating your application instances and keeping them running across Nodes, Kubernetes Deployments provide a fundamentally different approach to application management. -- 在没有 Kubernetes 这种编排系统之前，安装脚本通常用于启动应用程序，但它们不允许从机器故障中恢复。通过创建应用程序实例并使它们在节点之间运行， Kubernetes Deployments 提供了一种与众不同的应用程序管理方法。\n Summary: -- 总结:  Deployments Kubectl    A Deployment is responsible for creating and updating instances of your application \n--  Deployment 负责创建和更新应用程序的实例 \n   Deploying your first app on Kubernetes -- 部署你在 Kubernetes 上的第一个应用程序     You can create and manage a Deployment by using the Kubernetes command line interface, Kubectl. Kubectl uses the Kubernetes API to interact with the cluster. In this module, you'll learn the most common Kubectl commands needed to create Deployments that run your applications on a Kubernetes cluster.\n-- 您可以使用 Kubernetes 命令行界面创建和管理 Deployment，Kubectl.Kubectl 使用 Kubernetes API 与集群进行交互。在本单元中，您将学习创建在 Kubernetes 集群上运行应用程序的 Deployment 所需的最常见的 Kubectl 命令。\nWhen you create a Deployment, you'll need to specify the container image for your application and the number of replicas that you want to run. You can change that information later by updating your Deployment; Modules 5 and 6 of the bootcamp discuss how you can scale and update your Deployments.\n-- 创建 Deployment 时，您需要指定应用程序的容器映像以及要运行的副本数。您可以稍后通过更新 Deployment 来更改该信息; 模块 5 和 6 讨论了如何扩展和更新 Deployments。\n  Applications need to be packaged into one of the supported container formats in order to be deployed on Kubernetes \n--  应用程序需要打包成一种受支持的容器格式，以便部署在 Kubernetes 上 \n   For our first Deployment, we'll use a Node.js application packaged in a Docker container. To create the Node.js application and deploy the Docker container, follow the instructions from the Hello Minikube tutorial.\nNow that you know what Deployments are, let's go to the online tutorial and deploy our first app!\n-- 对于我们的第一次部署，我们将使用打包在 Docker 容器中的 Node.js 应用程序。 要创建 Node.js 应用程序并部署 Docker 容器，请按照 你好 Minikube 教程.\n现在您已经了解了 Deployment 的内容，让我们转到在线教程并部署我们的第一个应用程序！\n  开始交互式教程 ›      "
},
{
	"uri": "https://lijun.in/tutorials/kubernetes-basics/create-cluster/cluster-intro/",
	"title": "使用 Minikube 创建集群",
	"tags": [],
	"description": "",
	"content": "  目标  了解 Kubernetes 集群。 了解 Minikube 。 使用在线终端开启一个 Kubernetes 集群。    Kubernetes 集群   Kubernetes 协调一个高可用计算机集群，每个计算机作为独立单元互相连接工作。 Kubernetes 中的抽象允许您将容器化的应用部署到群集，而无需将它们绑定到某个特定的独立计算机。为了使用这种新的部署模型，应用需要以将应用与单个主机分离的方式打包：它们需要被容器化。与过去的那种应用直接以包的方式深度与主机集成的部署模型相比，容器化应用更灵活、更可用。 Kubernetes 以更高效的方式跨群集自动分发和调度应用容器。 Kubernetes 是一个开源平台，并且可应用于生产环境。 一个 Kubernetes 集群包含两种类型的资源:   Master 调度整个集群  Nodes 负责运行应用   总结:   Kubernetes 集群  Minikube     Kubernetes 是一个生产级别的开源平台，可协调在计算机集群内和跨计算机集群的应用容器的部署（调度）和执行. \n   集群图      Master 负责管理整个集群。 Master 协调集群中的所有活动，例如调度应用、维护应用的所需状态、应用扩容以及推出新的更新。\n Node 是一个虚拟机或者物理机，它在 Kubernetes 集群中充当工作机器的角色 每个Node都有 Kubelet , 它管理 Node 而且是 Node 与 Master 通信的代理。 Node 还应该具有用于​​处理容器操作的工具，例如 Docker 或 rkt 。处理生产级流量的 Kubernetes 集群至少应具有三个 Node 。\n  Master 管理集群，Node 用于托管正在运行的应用。\n   在 Kubernetes 上部署应用时，您告诉 Master 启动应用容器。 Master 就编排容器在群集的 Node 上运行。 Node 使用 Master 暴露的 Kubernetes API 与 Master 通信。终端用户也可以使用 Kubernetes API 与集群交互。\n Kubernetes 既可以部署在物理机上也可以部署在虚拟机上。您可以使用 Minikube 开始部署 Kubernetes 集群。 Minikube 是一种轻量级的 Kubernetes 实现，可在本地计算机上创建 VM 并部署仅包含一个节点的简单集群。 Minikube 可用于 Linux ， macOS 和 Windows 系统。Minikube CLI 提供了用于引导群集工作的多种操作，包括启动、停止、查看状态和删除。在本教程里，您可以使用预装有 Minikube 的在线终端进行体验。\n既然您已经知道 Kubernetes 是什么，让我们转到在线教程并启动我们的第一个 Kubernetes 集群！\n  启动交互教程 ›       "
},
{
	"uri": "https://lijun.in/tasks/access-kubernetes-api/custom-resources/",
	"title": "使用自定义资源",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lijun.in/tutorials/stateless-application/expose-external-ip-address/",
	"title": "公开外部 IP 地址以访问集群中应用程序",
	"tags": [],
	"description": "",
	"content": "此页面显示如何创建公开外部 IP 地址的 Kubernetes 服务对象。\n. heading \u0026ldquo;prerequisites\u0026rdquo; %}}   安装 kubectl.\n  使用 Google Kubernetes Engine 或 Amazon Web Services 等云供应商创建 Kubernetes 群集。 本教程创建了一个外部负载均衡器，需要云供应商。\n  配置 kubectl 与 Kubernetes API 服务器通信。有关说明，请参阅云供应商文档。\n  . heading \u0026ldquo;objectives\u0026rdquo; %}}  运行 Hello World 应用程序的五个实例。 创建一个公开外部 IP 地址的 Service 对象。 使用 Service 对象访问正在运行的应用程序。  为一个在五个 pod 中运行的应用程序创建服务  在集群中运行 Hello World 应用程序：  . codenew file=\u0026quot;service/load-balancer-example.yaml\u0026rdquo; \u0026gt;}}\nkubectl apply -f https://k8s.io/examples/service/load-balancer-example.yaml 前面的命令创建一个 Deployment 对象和一个关联的 ReplicaSet对象。 ReplicaSet 有五个 Pod，每个都运行 Hello World 应用程序。\n 显示有关 Deployment 的信息：\n kubectl get deployments hello-world kubectl describe deployments hello-world     显示有关 ReplicaSet 对象的信息：\n kubectl get replicasets kubectl describe replicasets     创建公开 deployment 的 Service 对象：\n kubectl expose deployment hello-world --type=LoadBalancer --name=my-service     显示有关 Service 的信息：\n kubectl get services my-service    输出类似于：\n NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE my-service ClusterIP 10.3.245.137 104.198.205.71 8080/TCP 54s  注意：type=LoadBalancer 服务由外部云服务提供商提供支持，本例中不包含此部分，详细信息请参考此页\n注意：如果外部 IP 地址显示为 \u0026lt;pending\u0026gt;，请等待一分钟再次输入相同的命令。\n 显示有关 Service 的详细信息：\n kubectl describe services my-service    输出类似于：\n Name: my-service Namespace: default Labels: run=load-balancer-example Annotations: \u0026lt;none\u0026gt; Selector: run=load-balancer-example Type: LoadBalancer IP: 10.3.245.137 LoadBalancer Ingress: 104.198.205.71 Port: \u0026lt;unset\u0026gt; 8080/TCP NodePort: \u0026lt;unset\u0026gt; 32377/TCP Endpoints: 10.0.0.6:8080,10.0.1.6:8080,10.0.1.7:8080 + 2 more... Session Affinity: None Events: \u0026lt;none\u0026gt;  记下服务公开的外部 IP 地址（LoadBalancer Ingress)。 在本例中，外部 IP 地址是 104.198.205.71。还要注意 Port 和 NodePort 的值。 在本例中，Port 是 8080，NodePort 是32377。\n 在前面的输出中，您可以看到服务有几个端点： 10.0.0.6:8080、10.0.1.6:8080、10.0.1.7:8080 和另外两个， 这些都是正在运行 Hello World 应用程序的 pod 的内部地址。 要验证这些是 pod 地址，请输入以下命令：\n kubectl get pods --output=wide    输出类似于：\n NAME ... IP NODE hello-world-2895499144-1jaz9 ... 10.0.1.6 gke-cluster-1-default-pool-e0b8d269-1afc hello-world-2895499144-2e5uh ... 10.0.1.8 gke-cluster-1-default-pool-e0b8d269-1afc hello-world-2895499144-9m4h1 ... 10.0.0.6 gke-cluster-1-default-pool-e0b8d269-5v7a hello-world-2895499144-o4z13 ... 10.0.1.7 gke-cluster-1-default-pool-e0b8d269-1afc hello-world-2895499144-segjf ... 10.0.2.5 gke-cluster-1-default-pool-e0b8d269-cpuc   使用外部 IP 地址（LoadBalancer Ingress）访问 Hello World 应用程序:\n curl http://\u0026lt;external-ip\u0026gt;:\u0026lt;port\u0026gt;    其中 \u0026lt;external-ip\u0026gt; 是您的服务的外部 IP 地址（LoadBalancer Ingress）， \u0026lt;port\u0026gt; 是您的服务描述中的 port 的值。 如果您正在使用 minikube，输入 minikube service my-service 将在浏览器中自动打开 Hello World 应用程序。\n成功请求的响应是一条问候消息：\n Hello Kubernetes!  . heading \u0026ldquo;cleanup\u0026rdquo; %}} 要删除服务，请输入以下命令：\n kubectl delete services my-service  要删除正在运行 Hello World 应用程序的 Deployment，ReplicaSet 和 Pod，请输入以下命令：\n kubectl delete deployment hello-world  . heading \u0026ldquo;whatsnext\u0026rdquo; %}} 了解更多关于将应用程序与服务连接。\n"
},
{
	"uri": "https://lijun.in/reference/setup-tools/kubeadm/generated/",
	"title": "创建 Kubeadm",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lijun.in/tutorials/kubernetes-basics/create-cluster/",
	"title": "创建集群",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lijun.in/setup/production-environment/tools/kubeadm/install-kubeadm/",
	"title": "安装 kubeadm",
	"tags": [],
	"description": "",
	"content": "本页面显示如何安装 kubeadm 工具箱。 有关在执行此安装过程后如何使用 kubeadm 创建集群的信息，请参见使用 kubeadm 创建集群 页面。\n. heading \u0026ldquo;prerequisites\u0026rdquo; %}}  一台或多台运行着下列系统的机器：  Ubuntu 16.04+ Debian 9+ CentOS 7 Red Hat Enterprise Linux (RHEL) 7 Fedora 25+ HypriotOS v1.0.1+ Container Linux (测试 1800.6.0 版本)   每台机器 2 GB 或更多的 RAM (如果少于这个数字将会影响您应用的运行内存) 2 CPU 核或更多 集群中的所有机器的网络彼此均能相互连接(公网和内网都可以) 节点之中不可以有重复的主机名、MAC 地址或 product_uuid。请参见这里 了解更多详细信息。 开启机器上的某些端口。请参见这里 了解更多详细信息。 禁用交换分区。为了保证 kubelet 正常工作，您 必须 禁用交换分区。  确保每个节点上 MAC 地址和 product_uuid 的唯一性  您可以使用命令 ip link 或 ifconfig -a 来获取网络接口的 MAC 地址 可以使用 sudo cat /sys/class/dmi/id/product_uuid 命令对 product_uuid 校验  一般来讲，硬件设备会拥有唯一的地址，但是有些虚拟机的地址可能会重复。Kubernetes 使用这些值来唯一确定集群中的节点。 如果这些值在每个节点上不唯一，可能会导致安装失败。\n检查网络适配器 如果您有一个以上的网络适配器，同时您的 Kubernetes 组件通过默认路由不可达，我们建议您预先添加 IP 路由规则，这样 Kubernetes 集群就可以通过对应的适配器完成连接。\n确保 iptables 工具不使用 nftables 后端 在 Linux 中，nftables 当前可以作为内核 iptables 子系统的替代品。 iptables 工具可以充当兼容性层，其行为类似于 iptables 但实际上是在配置 nftables。 nftables 后端与当前的 kubeadm 软件包不兼容：它会导致重复防火墙规则并破坏 kube-proxy。\n如果您系统的 iptables 工具使用 nftables 后端，则需要把 iptables 工具切换到“旧版”模式来避免这些问题。 默认情况下，至少在 Debian 10 (Buster)、Ubuntu 19.04、Fedora 29 和较新的发行版本中会出现这种问题。RHEL 8 不支持切换到旧版本模式，因此与当前的 kubeadm 软件包不兼容。\n. tabs name=\u0026quot;iptables_legacy\u0026rdquo; \u0026gt;}} . tab name=\u0026quot;Debian 或 Ubuntu\u0026rdquo; %}}\nupdate-alternatives --set iptables /usr/sbin/iptables-legacy update-alternatives --set ip6tables /usr/sbin/ip6tables-legacy update-alternatives --set arptables /usr/sbin/arptables-legacy update-alternatives --set ebtables /usr/sbin/ebtables-legacy . /tab %}} . tab name=\u0026quot;Fedora\u0026rdquo; %}}\nupdate-alternatives --set iptables /usr/sbin/iptables-legacy . /tab %}} . /tabs \u0026gt;}}\n检查所需端口 控制平面节点    协议 方向 端口范围 作用 使用者     TCP 入站 6443* Kubernetes API 服务器 所有组件   TCP 入站 2379-2380 etcd server client API kube-apiserver, etcd   TCP 入站 10250 Kubelet API kubelet 自身、控制平面组件   TCP 入站 10251 kube-scheduler kube-scheduler 自身   TCP 入站 10252 kube-controller-manager kube-controller-manager 自身    工作节点    协议 方向 端口范围 作用 使用者     TCP 入站 10250 Kubelet API kubelet 自身、控制平面组件   TCP 入站 30000-32767 NodePort 服务** 所有组件    ** NodePort 服务 的默认端口范围。\n使用 * 标记的任意端口号都可以被覆盖，所以您需要保证所定制的端口是开放的。\n虽然控制平面节点已经包含了 etcd 的端口，您也可以使用自定义的外部 etcd 集群，或是指定自定义端口。\n您使用的 pod 网络插件 (见下) 也可能需要某些特定端口开启。由于各个 pod 网络插件都有所不同，请参阅他们各自文档中对端口的要求。\n安装 runtime 从 v1.6.0 版本起，Kubernetes 开始默认允许使用 CRI（容器运行时接口）。\n从 v1.14.0 版本起，kubeadm 将通过观察已知的 UNIX 域套接字来自动检测 Linux 节点上的容器运行时。 下表中是可检测到的正在运行的 runtime 和 socket 路径。\n   运行时 域套接字     Docker /var/run/docker.sock   containerd /run/containerd/containerd.sock   CRI-O /var/run/crio/crio.sock    如果同时检测到 docker 和 containerd，则优先选择 docker。 这是必然的，因为 docker 18.09 附带了 containerd 并且两者都是可以检测到的。 如果检测到其他两个或多个运行时，kubeadm 将以一个合理的错误信息退出。\n在非 Linux 节点上，默认使用 docker 作为容器 runtime。\n如果选择的容器 runtime 是 docker，则通过内置 dockershim CRI 在 kubelet 的内部实现其的应用。\n基于 CRI 的其他 runtimes 有：\n containerd （containerd 的内置 CRI 插件） cri-o frakti  请参考 CRI 安装指南 获取更多信息。\n安装 kubeadm、kubelet 和 kubectl 您需要在每台机器上安装以下的软件包：\n  kubeadm：用来初始化集群的指令。\n  kubelet：在集群中的每个节点上用来启动 pod 和容器等。\n  kubectl：用来与集群通信的命令行工具。\n  kubeadm 不能 帮您安装或者管理 kubelet 或 kubectl，所以您需要确保它们与通过 kubeadm 安装的控制平面的版本相匹配。 如果不这样做，则存在发生版本偏差的风险，可能会导致一些预料之外的错误和问题。 然而，控制平面与 kubelet 间的相差一个次要版本不一致是支持的，但 kubelet 的版本不可以超过 API 服务器的版本。 例如，1.7.0 版本的 kubelet 可以完全兼容 1.8.0 版本的 API 服务器，反之则不可以。\n有关安装 kubectl 的信息，请参阅安装和设置 kubectl文档。\n. warning \u0026gt;}}\n这些指南不包括系统升级时使用的所有 Kubernetes 程序包。这是因为 kubeadm 和 Kubernetes 有特殊的升级注意事项。 ./ warning \u0026gt;}}\n关于版本偏差的更多信息，请参阅以下文档：\n Kubernetes 版本与版本间的偏差策略 Kubeadm-specific 版本偏差策略  . tabs name=\u0026quot;k8s_install\u0026rdquo; \u0026gt;}} . tab name=\u0026quot;Ubuntu、Debian 或 HypriotOS\u0026rdquo; %}}\nsudo apt-get update \u0026amp;\u0026amp; sudo apt-get install -y apt-transport-https curl curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add - cat \u0026lt;\u0026lt;EOF | sudo tee /etc/apt/sources.list.d/kubernetes.list deb https://apt.kubernetes.io/ kubernetes-xenial main EOF sudo apt-get update sudo apt-get install -y kubelet kubeadm kubectl sudo apt-mark hold kubelet kubeadm kubectl . /tab %}} . tab name=\u0026quot;CentOS、RHEL 或 Fedora\u0026rdquo; %}}\ncat \u0026lt;\u0026lt;EOF \u0026gt; /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64 enabled=1 gpgcheck=1 repo_gpgcheck=1 gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg EOF # 将 SELinux 设置为 permissive 模式（相当于将其禁用） setenforce 0 sed -i \u0026#39;s/^SELINUX=enforcing$/SELINUX=permissive/\u0026#39; /etc/selinux/config yum install -y kubelet kubeadm kubectl --disableexcludes=kubernetes systemctl enable --now kubelet 请注意：\n  通过运行命令 setenforce 0 和 sed ... 将 SELinux 设置为 permissive 模式可以有效的将其禁用。 这是允许容器访问主机文件系统所必须的，例如正常使用 pod 网络。 您必须这么做，直到 kubelet 做出升级支持 SELinux 为止。\n  一些 RHEL/CentOS 7 的用户曾经遇到过问题：由于 iptables 被绕过而导致流量无法正确路由的问题。您应该确保 在 sysctl 配置中的 net.bridge.bridge-nf-call-iptables 被设置为 1。\ncat \u0026lt;\u0026lt;EOF \u0026gt; /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 EOF sysctl --system   确保在此步骤之前已加载了 br_netfilter 模块。这可以通过运行 lsmod | grep br_netfilter 来完成。要显示加载它，请调用 modprobe br_netfilter。 . /tab %}} . tab name=\u0026quot;Container Linux\u0026rdquo; %}}\n  安装 CNI 插件（大多数 pod 网络都需要）：\nCNI_VERSION=\u0026#34;v0.8.2\u0026#34; mkdir -p /opt/cni/bin curl -L \u0026#34;https://github.com/containernetworking/plugins/releases/download/${CNI_VERSION}/cni-plugins-linux-amd64-${CNI_VERSION}.tgz\u0026#34; | tar -C /opt/cni/bin -xz 安装 crictl（kubeadm/kubelet 容器运行时接口（CRI）所需）\nCRICTL_VERSION=\u0026#34;v1.16.0\u0026#34; mkdir -p /opt/bin curl -L \u0026#34;https://github.com/kubernetes-sigs/cri-tools/releases/download/${CRICTL_VERSION}/crictl-${CRICTL_VERSION}-linux-amd64.tar.gz\u0026#34; | tar -C /opt/bin -xz 安装 kubeadm、kubelet、kubectl 并添加 kubelet 系统服务：\nRELEASE=\u0026#34;$(curl -sSL https://dl.k8s.io/release/stable.txt)\u0026#34; mkdir -p /opt/bin cd /opt/bin curl -L --remote-name-all https://storage.googleapis.com/kubernetes-release/release/${RELEASE}/bin/linux/amd64/{kubeadm,kubelet,kubectl} chmod +x {kubeadm,kubelet,kubectl} curl -sSL \u0026#34;https://raw.githubusercontent.com/kubernetes/kubernetes/${RELEASE}/build/debs/kubelet.service\u0026#34; | sed \u0026#34;s:/usr/bin:/opt/bin:g\u0026#34; \u0026gt; /etc/systemd/system/kubelet.service mkdir -p /etc/systemd/system/kubelet.service.d curl -sSL \u0026#34;https://raw.githubusercontent.com/kubernetes/kubernetes/${RELEASE}/build/debs/10-kubeadm.conf\u0026#34; | sed \u0026#34;s:/usr/bin:/opt/bin:g\u0026#34; \u0026gt; /etc/systemd/system/kubelet.service.d/10-kubeadm.conf 开启并启动 kubelet：\nsystemctl enable --now kubelet . /tab %}} . /tabs \u0026gt;}}\nkubelet 现在每隔几秒就会重启，因为它陷入了一个等待 kubeadm 指令的死循环。\n在控制平面节点上配置 kubelet 使用的 cgroup 驱动程序 使用 docker 时，kubeadm 会自动为其检测 cgroup 驱动并在运行时对 /var/lib/kubelet/kubeadm-flags.env 文件进行配置。\n如果您使用不同的 CRI，您需要使用 cgroup-driver 值修改 /etc/default/kubelet 文件（对于 CentOS、RHEL、Fedora，修改 /etc/sysconfig/kubelet 文件），像这样：\nKUBELET_EXTRA_ARGS=--cgroup-driver=\u0026lt;value\u0026gt; 这个文件将由 kubeadm init 和 kubeadm join 使用以获取额外的用户自定义的 kubelet 参数。\n请注意，您 只 需要在您的 cgroup 驱动程序不是 cgroupfs 时这么做，因为它已经是 kubelet 中的默认值。\n需要重新启动 kubelet：\nsystemctl daemon-reload systemctl restart kubelet 自动检测其他容器运行时的 cgroup 驱动，例如在进程中工作的 CRI-O 和 containerd。\n故障排查 如果您在使用 kubeadm 时遇到困难，请参阅我们的故障排查文档。\n. heading \u0026ldquo;whatsnext\u0026rdquo; %}}  使用 kubeadm 创建集群  "
},
{
	"uri": "https://lijun.in/tasks/tools/install-kubectl/",
	"title": "安装并设置 kubectl",
	"tags": [],
	"description": "",
	"content": "在 Kubernetes 上使用 Kubernetes 命令行工具 kubectl 部署和管理应用程序。使用 kubectl，您可以检查集群资源；创建、删除和更新组件；查看您的新集群；并启动实例应用程序。\n. heading \u0026ldquo;prerequisites\u0026rdquo; %}} 您必须使用与集群小版本号差别为一的 kubectl 版本。例如，1.2版本的客户端应该与1.1版本、1.2版本和1.3版本的主节点一起使用。使用最新版本的 kubectl 有助于避免无法预料的问题。\n安装 kubectl 以下是一些安装 kubectl 的方法。\n使用本地软件包管理软件安装 kubectl 二进制文件 . tabs name=\u0026quot;kubectl_install\u0026rdquo; \u0026gt;}} . tab name=\u0026quot;Ubuntu, Debian or HypriotOS\u0026rdquo; codelang=\u0026quot;bash\u0026rdquo; \u0026gt;}} sudo apt-get update \u0026amp;\u0026amp; sudo apt-get install -y apt-transport-https curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add - echo \u0026ldquo;deb https://apt.kubernetes.io/ kubernetes-xenial main\u0026rdquo; | sudo tee -a /etc/apt/sources.list.d/kubernetes.list sudo apt-get update sudo apt-get install -y kubectl . /tab \u0026gt;}} . tab name=\u0026quot;CentOS, RHEL or Fedora\u0026rdquo; codelang=\u0026quot;bash\u0026rdquo; \u0026gt;}}cat \u0026laquo;EOF \u0026gt; /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64 enabled=1 gpgcheck=1 repo_gpgcheck=1 gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg EOF yum install -y kubectl . /tab \u0026gt;}} . /tabs \u0026gt;}}\n在 Ubuntu 上使用 snap 安装 kubectl 如果您使用的是 Ubuntu 或其他支持 snap 软件包管理器的Linux发行版，kubectl 可以作为一个 snap 应用程序使用。\n  切换到 snap 用户并运行安装命令：\nsudo snap install kubectl --classic   测试以确保您安装的版本是最新的：\nkubectl version   在 macOS 上用 Homebrew 安装 kubectl  如果您使用的是 macOS 系统并使用 Homebrew 包管理器，您可以通过 Homebrew 安装 kubectl。\n  运行安装命令：\nbrew install kubernetes-cli   测试以确保您安装的版本是最新的：\nkubectl version   在 macOS 上用 Macports 安装 kubectl 如果您使用的是 macOS 系统并使用 Macports 包管理器，您可以通过 Macports 安装 kubectl。\n  运行安装命令：\nport install kubectl   测试以确保您安装的版本是最新的：\nkubectl version   从 PSGallery 通过 Powershell 安装 kubectl 如果您使用的是 Windows 系统并使用 Powershell Gallery 软件包管理器，您可以使用 Powershell 安装和更新 kubectl。\n  运行安装命令（确保指定 DownloadLocation）：\nInstall-Script -Name install-kubectl -Scope CurrentUser -Force install-kubectl.ps1 [-DownloadLocation \u0026lt;path\u0026gt;] . note \u0026gt;}} 如果你没有指定 DownloadLocation，那么 kubectl 将安装在用户的临时目录中。 . /note \u0026gt;}}\n安装程序创建 $ HOME/.kube 并指示它创建配置文件\n  测试以确保您安装的版本是最新的：\nkubectl version . note \u0026gt;}} 通过重新运行步骤1中列出的两个命令来执行更新安装。 . /note \u0026gt;}}\n  在 Windows 上用 Chocolatey 安装 kubectl 如果您使用的是 Windows 系统并使用 Chocolatey 包管理器，您可以使用 Chocolatey 安装 kubectl。\n  运行安装命令：\nchoco install kubernetes-cli   测试以确保您安装的版本是最新的：\nkubectl version   切换到 %HOME% 目录：\n例如：cd C:\\users\\yourusername\n  创建 .kube 目录：\nmkdir .kube   切换到刚刚创建的 .kube 目录：\ncd .kube   配置 kubectl 以使用远程 Kubernetes 集群：\nNew-Item config -type file . note \u0026gt;}} 使用您偏爱的编辑器编辑配置文件，例如 Notepad。 . /note \u0026gt;}}\n  将 kubectl 作为 Google Cloud SDK 的一部分下载 kubectl 可以作为 Google Cloud SDK 的一部分进行安装。\n  安装 Google Cloud SDK.\n  运行以下命令安装 kubectl：\ngcloud components install kubectl   测试以确保您安装的版本是最新的：\nkubectl version   通过 curl 命令安装 kubectl 可执行文件 . tabs name=\u0026quot;kubectl_install_curl\u0026rdquo; \u0026gt;}} . tab name=\u0026quot;macOS\u0026rdquo; %}}\n  通过以下命令下载 kubectl 的最新版本：\ncurl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/darwin/amd64/kubectl 若需要下载特定版本的 kubectl，请将上述命令中的 $(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt) 部分替换成为需要下载的 kubectl 的具体版本即可。\n例如，如果需要下载 . param \u0026ldquo;fullversion\u0026rdquo; \u0026gt;}} 版本在 macOS 系统上,需要使用如下命令：\ncurl -LO https://storage.googleapis.com/kubernetes-release/release/. param \u0026quot;fullversion\u0026quot; \u0026gt;}}/bin/darwin/amd64/kubectl   修改所下载的 kubectl 二进制文件为可执行模式。\nchmod +x ./kubectl   将 kubectl 可执行文件放置到你的 PATH 目录下。\nsudo mv ./kubectl /usr/local/bin/kubectl   . /tab %}} . tab name=\u0026quot;Linux\u0026rdquo; %}}\n  通过以下命令下载 kubectl 的最新版本：\ncurl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl 若需要下载特定版本的 kubectl，请将上述命令中的 $(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt) 部分替换成为需要下载的 kubectl 的具体版本即可。\n例如，如果需要下载用于 Linux 的 . param \u0026ldquo;fullversion\u0026rdquo; \u0026gt;}} 版本，需要使用如下命令：\ncurl -LO https://storage.googleapis.com/kubernetes-release/release/. param \u0026quot;fullversion\u0026quot; \u0026gt;}}/bin/linux/amd64/kubectl   修改所下载的 kubectl 二进制文件为可执行模式。\nchmod +x ./kubectl   将 kubectl 可执行文件放置到你的 PATH 目录下。\nsudo mv ./kubectl /usr/local/bin/kubectl   . /tab %}} . tab name=\u0026quot;Windows\u0026rdquo; %}}\n  从[本链接](https://storage.googleapis.com/kubernetes-release/release/. param \u0026ldquo;fullversion\u0026rdquo; \u0026gt;}}/bin/windows/amd64/kubectl.exe)下载 kubectl 的最新版 . param \u0026ldquo;fullversion\u0026rdquo; \u0026gt;}}。\n或者如果您已经在系统中安装了 curl 工具，也可以通过以下命令下载：\ncurl -LO https://storage.googleapis.com/kubernetes-release/release/. param \u0026quot;fullversion\u0026quot; \u0026gt;}}/bin/windows/amd64/kubectl.exe   若要查找最新的稳定版本（例如脚本等），请查看 https://storage.googleapis.com/kubernetes-release/release/stable.txt.\n将 kubectl 可执行文件添加到你的 PATH 目录。 . /tab %}} . /tabs \u0026gt;}}  配置 kubectl kubectl 需要一个 kubeconfig 配置文件使其找到并访问 Kubernetes 集群。当您使用 kube-up.sh 脚本创建 Kubernetes 集群或者部署 Minikube 集群时，会自动生成 kubeconfig 配置文件。请参阅入门指南以了解更多创建集群相关的信息。如果您需要访问一个并非由您创建的集群，请参阅如何共享集群的访问。默认情况下，kubectl 配置文件位于 ~/.kube/config。\n检查 kubectl 的配置 通过获取集群状态检查 kubectl 是否被正确配置：\nkubectl cluster-info 如果您看到一个 URL 被返回，那么 kubectl 已经被正确配置，能够正常访问您的 Kubernetes 集群。\n如果您看到类似以下的信息被返回，那么 kubectl 没有被正确配置，无法正常访问您的 Kubernetes 集群。\nThe connection to the server \u0026lt;server-name:port\u0026gt; was refused - did you specify the right host or port? 例如，如果您打算在笔记本电脑（本地）上运行 Kubernetes 集群，则需要首先安装 minikube 等工具，然后重新运行上述命令。\n如果 kubectl cluster-info 能够返回 url 响应，但您无法访问您的集群，可以使用下面的命令检查配置是否正确：\nkubectl cluster-info dump 启用 shell 自动补全功能 kubectl 支持自动补全功能，可以节省大量输入！\n自动补全脚本由 kubectl 产生，您仅需要在您的 shell 配置文件中调用即可。\n以下仅提供了使用命令补全的常用示例，更多详细信息，请查阅 kubectl completion -h 帮助命令的输出。\nLinux 系统，使用 bash 在 CentOS Linux系统上，您可能需要安装默认情况下未安装的 bash-completion 软件包。\nyum install bash-completion -y 执行 source \u0026lt;(kubectl completion bash) 命令在您目前正在运行的 shell 中开启 kubectl 自动补全功能。\n可以将上述命令添加到 shell 配置文件中，这样在今后运行的 shell 中将自动开启 kubectl 自动补全：\necho \u0026#34;source \u0026lt;(kubectl completion bash)\u0026#34; \u0026gt;\u0026gt; ~/.bashrc macOS 系统，使用 bash macOS 系统需要先通过 Homebrew 安装 bash-completion：\n## 如果您运行的是 macOS 自带的 Bash 3.2，请运行： brew install bash-completion ## 如果您使用的是 Bash 4.1+，请运行： brew install bash-completion@2 请根据 Homebrew 输出的”注意事项（caveats）”部分的内容将 bash-completion 的路径添加到本地 .bashrc 文件中。\n如果您是按照 Homebrew 指示中的步骤安装的 kubectl，那么无需其他配置，kubectl 的自动补全功能已经被启用。\n如果您是手工下载并安装的 kubectl，那么您需要将 kubectl 自动补全添加到 bash-completion：\nkubectl completion bash \u0026gt; $(brew --prefix)/etc/bash_completion.d/kubectl 由于 Homebrew 项目与 Kubernetes 无关，所以并不能保证 bash-completion 总能够支持 kubectl 的自动补全功能。\n使用 Zsh 如果您使用的是 zsh,请编辑 ~/.zshrc 文件并添加以下代码以启用 kubectl 自动补全功能。\nif [ $commands[kubectl] ]; then source \u0026lt;(kubectl completion zsh) fi 如果您使用的是 Oh-My-Zsh，请编辑 ~/.zshrc 文件并更新 plugins= 行以包含 kubectl 插件。\nplugins=(kubectl) . heading \u0026ldquo;whatsnext\u0026rdquo; %}} 了解如何启动并对外暴露您的应用程序\n"
},
{
	"uri": "https://lijun.in/setup/production-environment/container-runtimes/",
	"title": "容器运行时",
	"tags": [],
	"description": "",
	"content": ". feature-state for_k8s_version=\u0026quot;v1.6\u0026rdquo; state=\u0026quot;stable\u0026rdquo; \u0026gt;}}\nKubernetes 使用容器运行时来实现在 pod 中运行容器。 这是各种运行时的安装说明。\n. caution \u0026gt;}}\n我们发现 runc 在运行容器，处理系统文件描述符时存在一个漏洞。 恶意容器可以利用此漏洞覆盖 runc 二进制文件的内容，并以此在主机系统的容器上运行任意的命令。\n请参考此链接以获取有关此问题的更多信息 [cve-2019-5736 : runc vulnerability ] (https://access.redhat.com/security/cve/cve-2019-5736) . /caution \u0026gt;}}\n适用性 . note \u0026gt;}}\n本文档是为在 Linux 上安装 CRI 的用户编写的。 对于其他操作系统，请查找特定于您平台的文档。 . /note \u0026gt;}}\n您应该以 root 身份执行本指南中的所有命令。 例如，使用 sudo 前缀命令，或者成为 root 并以该用户身份运行命令。\nCgroup 驱动程序 当某个 Linux 系统发行版使用 systemd 作为其初始化系统时，初始化进程会生成并使用一个 root 控制组 （cgroup），并充当 cgroup 管理器。 systemd 与 cgroup 集成紧密，并将为每个进程分配 cgroup。 您也可以配置容器运行时和 kubelet 使用 cgroupfs。 连同 systemd 一起使用 cgroupfs 意味着将有两个不同的 cgroup 管理器。\n控制组用来约束分配给进程的资源。 单个 cgroup 管理器将简化分配资源的视图，并且默认情况下将对可用资源和使用中的资源具有更一致的视图。 当有两个管理器时，最终将对这些资源产生两种视图。 在此领域我们已经看到案例，某些节点配置让 kubelet 和 docker 使用 cgroupfs，而节点上运行的其余进程则使用 systemd；这类节点在资源压力下会变得不稳定。\n更改设置，令容器运行时和 kubelet 使用 systemd 作为 cgroup 驱动，以此使系统更为稳定。 请注意在 docker 下设置 native.cgroupdriver=systemd 选项。\n. caution \u0026gt;}}\n强烈建议不要更改已加入集群的节点的 cgroup 驱动。 如果 kubelet 已经使用某 cgroup 驱动的语义创建了 pod，尝试更改运行时以使用别的 cgroup 驱动，为现有 Pods 重新创建 PodSandbox 时会产生错误。 重启 kubelet 也可能无法解决此类问题。 推荐将工作负载逐出节点，之后将节点从集群中删除并重新加入。 . /caution \u0026gt;}}\nDocker 在您的每台机器上安装 Docker。 推荐安装 19.03.4 版本，但是 1.13.1、17.03、17.06、17.09、18.06 和 18.09 版本也是可以的。 请跟踪 Kubernetes 发行说明中经过验证的 Docker 最新版本变化。\n使用以下命令在您的系统上安装 Docker：\n. tabs name=\u0026quot;tab-cri-docker-installation\u0026rdquo; \u0026gt;}} . tab name=\u0026quot;Ubuntu 16.04+\u0026rdquo; codelang=\u0026quot;bash\u0026rdquo; \u0026gt;}}\n安装 Docker CE 设置仓库 安装软件包以允许 apt 通过 HTTPS 使用存储库 apt-get update \u0026amp;\u0026amp; apt-get install\napt-transport-https ca-certificates curl software-properties-common\n新增 Docker 的 官方 GPG 秘钥 curl -fsSL https://download.docker.com/linux/ubuntu/gpg | apt-key add -\n添加 Docker apt 仓库 add-apt-repository\n\u0026ldquo;deb [arch=amd64] https://download.docker.com/linux/ubuntu\n$(lsb_release -cs)\nstable\u0026rdquo;\n安装 Docker CE apt-get update \u0026amp;\u0026amp; apt-get install docker-ce=18.06.2~ce~3-0~ubuntu\n设置 daemon cat \u0026gt; /etc/docker/daemon.json \u0026laquo;EOF { \u0026ldquo;exec-opts\u0026rdquo;: [\u0026ldquo;native.cgroupdriver=systemd\u0026rdquo;], \u0026ldquo;log-driver\u0026rdquo;: \u0026ldquo;json-file\u0026rdquo;, \u0026ldquo;log-opts\u0026rdquo;: { \u0026ldquo;max-size\u0026rdquo;: \u0026ldquo;100m\u0026rdquo; }, \u0026ldquo;storage-driver\u0026rdquo;: \u0026ldquo;overlay2\u0026rdquo; } EOF\nmkdir -p /etc/systemd/system/docker.service.d\n重启 docker. systemctl daemon-reload systemctl restart docker . /tab \u0026gt;}} . tab name=\u0026quot;CentOS/RHEL 7.4+\u0026rdquo; codelang=\u0026quot;bash\u0026rdquo; \u0026gt;}}\n安装 Docker CE 设置仓库 安装所需包 yum install yum-utils device-mapper-persistent-data lvm2\n新增 Docker 仓库。 yum-config-manager\n\u0026ndash;add-repo\nhttps://download.docker.com/linux/centos/docker-ce.repo\n安装 Docker CE. yum update \u0026amp;\u0026amp; yum install docker-ce-18.06.2.ce\n创建 /etc/docker 目录。 mkdir /etc/docker\n设置 daemon。 cat \u0026gt; /etc/docker/daemon.json \u0026laquo;EOF { \u0026ldquo;exec-opts\u0026rdquo;: [\u0026ldquo;native.cgroupdriver=systemd\u0026rdquo;], \u0026ldquo;log-driver\u0026rdquo;: \u0026ldquo;json-file\u0026rdquo;, \u0026ldquo;log-opts\u0026rdquo;: { \u0026ldquo;max-size\u0026rdquo;: \u0026ldquo;100m\u0026rdquo; }, \u0026ldquo;storage-driver\u0026rdquo;: \u0026ldquo;overlay2\u0026rdquo;, \u0026ldquo;storage-opts\u0026rdquo;: [ \u0026ldquo;overlay2.override_kernel_check=true\u0026rdquo; ] } EOF\nmkdir -p /etc/systemd/system/docker.service.d\n重启 Docker systemctl daemon-reload systemctl restart docker . /tab \u0026gt;}} . /tabs \u0026gt;}}\n请参阅官方 Docker 安装指南 来获取更多的信息。\nCRI-O 本节包含安装 CRI-O 作为 CRI 运行时的必要步骤。\n使用以下命令在系统中安装 CRI-O：\n准备环境 modprobe overlay modprobe br_netfilter # 设置必需的sysctl参数，这些参数在重新启动后仍然存在。 cat \u0026gt; /etc/sysctl.d/99-kubernetes-cri.conf \u0026lt;\u0026lt;EOF net.bridge.bridge-nf-call-iptables = 1 net.ipv4.ip_forward = 1 net.bridge.bridge-nf-call-ip6tables = 1 EOF sysctl --system . tabs name=\u0026quot;tab-cri-cri-o-installation\u0026rdquo; \u0026gt;}} . tab name=\u0026quot;Ubuntu 16.04\u0026rdquo; codelang=\u0026quot;bash\u0026rdquo; \u0026gt;}}\n安装必备软件 apt-get update apt-get install software-properties-common\nadd-apt-repository ppa:projectatomic/ppa apt-get update\n安装 CRI-O apt-get install cri-o-1.15\n. /tab \u0026gt;}} . tab name=\u0026quot;CentOS/RHEL 7.4+\u0026rdquo; codelang=\u0026quot;bash\u0026rdquo; \u0026gt;}}\n安装必备软件 yum-config-manager \u0026ndash;add-repo=https://cbs.centos.org/repos/paas7-crio-115-release/x86_64/os/\n安装 CRI-O yum install \u0026ndash;nogpgcheck cri-o\n. /tab \u0026gt;}} . /tabs \u0026gt;}}\n启动 CRI-O systemctl start crio 请参阅CRI-O 安装指南 来获取更多的信息。\ncontainerd 本节包含使用 containerd 作为 CRI 运行时的必要步骤。\n使用以下命令在系统上安装容器：\n准备环境 cat \u0026gt; /etc/modules-load.d/containerd.conf \u0026lt;\u0026lt;EOF overlay br_netfilter EOF modprobe overlay modprobe br_netfilter # 设置必需的sysctl参数，这些参数在重新启动后仍然存在。 cat \u0026gt; /etc/sysctl.d/99-kubernetes-cri.conf \u0026lt;\u0026lt;EOF net.bridge.bridge-nf-call-iptables = 1 net.ipv4.ip_forward = 1 net.bridge.bridge-nf-call-ip6tables = 1 EOF sysctl --system 安装 containerd . tabs name=\u0026quot;tab-cri-containerd-installation\u0026rdquo; \u0026gt;}} . tab name=\u0026quot;Ubuntu 16.04\u0026rdquo; codelang=\u0026quot;bash\u0026rdquo; \u0026gt;}}\n安装 containerd 设置仓库 安装软件包以允许 apt 通过 HTTPS 使用存储库 apt-get update \u0026amp;\u0026amp; apt-get install -y apt-transport-https ca-certificates curl software-properties-common\n安装 Docker 的官方 GPG 密钥 curl -fsSL https://download.docker.com/linux/ubuntu/gpg | apt-key add -\n新增 Docker apt 仓库。 add-apt-repository\n\u0026ldquo;deb [arch=amd64] https://download.docker.com/linux/ubuntu\n$(lsb_release -cs)\nstable\u0026rdquo;\n安装 containerd apt-get update \u0026amp;\u0026amp; apt-get install -y containerd.io\n配置 containerd mkdir -p /etc/containerd containerd config default \u0026gt; /etc/containerd/config.toml\n重启 containerd systemctl restart containerd . /tab \u0026gt;}} . tab name=\u0026quot;CentOS/RHEL 7.4+\u0026rdquo; codelang=\u0026quot;bash\u0026rdquo; \u0026gt;}}\n安装 containerd 设置仓库 安装所需包 yum install yum-utils device-mapper-persistent-data lvm2\n新增 Docker 仓库 yum-config-manager\n\u0026ndash;add-repo\nhttps://download.docker.com/linux/centos/docker-ce.repo\n安装 containerd yum update \u0026amp;\u0026amp; yum install containerd.io\n配置 containerd mkdir -p /etc/containerd containerd config default \u0026gt; /etc/containerd/config.toml\n重启 containerd systemctl restart containerd . /tab \u0026gt;}} . /tabs \u0026gt;}}\nsystemd 使用 systemd cgroup 驱动，在 /etc/containerd/config.toml 中设置 plugins.cri.systemd_cgroup = true。 当使用 kubeadm 时，请手动配置 kubelet 的 cgroup 驱动\n其他的 CRI 运行时：frakti 请参阅 Frakti 快速开始指南 来获取更多的信息。\n"
},
{
	"uri": "https://lijun.in/tutorials/kubernetes-basics/update/update-intro/",
	"title": "执行滚动更新",
	"tags": [],
	"description": "",
	"content": "  Objectives  Perform a rolling update using kubectl. -- 使用 kubectl 执行滚动更新。   Updating an application -- 更新应用程序 Users expect applications to be available all the time and developers are expected to deploy new versions of them several times a day. In Kubernetes this is done with rolling updates. Rolling updates allow Deployments' update to take place with zero downtime by incrementally updating Pods instances with new ones. The new Pods will be scheduled on Nodes with available resources.\n-- 用户希望应用程序始终可用，而开发人员则需要每天多次部署它们的新版本。在 Kubernetes 中，这些是通过滚动更新（Rolling Updates）完成的。 滚动更新 允许通过使用新的实例逐步更新 Pod 实例，零停机进行 Deployment 更新。新的 Pod 将在具有可用资源的节点上进行调度。\nIn the previous module we scaled our application to run multiple instances. This is a requirement for performing updates without affecting application availability. By default, the maximum number of Pods that can be unavailable during the update and the maximum number of new Pods that can be created, is one. Both options can be configured to either numbers or percentages (of Pods). In Kubernetes, updates are versioned and any Deployment update can be reverted to previous (stable) version.\n-- 在前面的模块中，我们将应用程序扩展为运行多个实例。这是在不影响应用程序可用性的情况下执行更新的要求。默认情况下，更新期间不可用的 pod 的最大值和可以创建的新 pod 数都是 1。这两个选项都可以配置为（pod）数字或百分比。 在 Kubernetes 中，更新是经过版本控制的，任何 Deployment 更新都可以恢复到以前的（稳定）版本。\n Summary: -- 摘要：  Updating an app -- 更新应用   Rolling updates allow Deployments' update to take place with zero downtime by incrementally updating Pods instances with new ones. \n-- 滚动更新允许通过使用新的实例逐步更新 Pod 实例从而实现 Deployments 更新，停机时间为零。\n   Rolling updates overview -- 滚动更新概述              Previous  Next     Similar to application Scaling, if a Deployment is exposed publicly, the Service will load-balance the traffic only to available Pods during the update. An available Pod is an instance that is available to the users of the application.\n-- 与应用程序扩展类似，如果公开了 Deployment，服务将在更新期间仅对可用的 pod 进行负载均衡。可用 Pod 是应用程序用户可用的实例。\nRolling updates allow the following actions:\n-- 滚动更新允许以下操作：\n Promote an application from one environment to another (via container image updates) Rollback to previous versions Continuous Integration and Continuous Delivery of applications with zero downtime -- 将应用程序从一个环境提升到另一个环境（通过容器镜像更新） 回滚到以前的版本 持续集成和持续交付应用程序，无需停机   If a Deployment is exposed publicly, the Service will load-balance the traffic only to available Pods during the update. \n-- 如果 Deployment 是公开的，则服务将仅在更新期间对可用的 pod 进行负载均衡。 \n   In the following interactive tutorial, we'll update our application to a new version, and also perform a rollback.\n--  在下面的交互式教程中，我们将应用程序更新为新版本，并执行回滚。\n  Start Interactive Tutorial › -- 启动交互教程›       "
},
{
	"uri": "https://lijun.in/concepts/extend-kubernetes/extend-cluster/",
	"title": "扩展 Kubernetes 集群",
	"tags": [],
	"description": "",
	"content": "Kubernetes 是高度可配置和可扩展的。因此，极少需要分发或提交补丁代码给 Kubernetes 项目。\n本文档介绍自定义 Kubernetes 集群的选项。本文档的目标读者 text=\u0026quot;cluster operators\u0026rdquo; term_id=\u0026quot;cluster-operator\u0026rdquo; \u0026gt;}} 是希望了解如何使 Kubernetes 集群满足其业务环境需求的集群运维人员。Kubernetes 项目的贡献者 text=\u0026quot;Contributors\u0026rdquo; term_id=\u0026quot;contributor\u0026rdquo; \u0026gt;}} 或潜在的平台开发人员 text=\u0026quot;Platform Developers\u0026rdquo; term_id=\u0026quot;platform-developer\u0026rdquo; \u0026gt;}} 也可以从本文找到有用的信息，如对已存在扩展点和模式的介绍，以及它们的权衡和限制。\n概述 定制方法可以大致分为 配置 和 扩展 。配置 只涉及更改标志参数、本地配置文件或 API 资源；扩展 涉及运行额外的程序或服务。本文档主要内容是关于扩展。\n配置 关于 配置文件 和 标志 的说明文档位于在线文档的参考部分，按照二进制组件各自描述：\n kubelet kube-apiserver kube-controller-manager kube-scheduler.  在托管的 Kubernetes 服务或受控安装的 Kubernetes 版本中，标志和配置文件可能并不总是可以更改的。而且当它们可以进行更改时，它们通常只能由集群管理员进行更改。此外，标志和配置文件在未来的 Kubernetes 版本中可能会发生变化，并且更改设置后它们可能需要重新启动进程。出于这些原因，只有在没有其他选择的情况下才使用它们。\n内置策略 API ，例如 ResourceQuota、PodSecurityPolicy、NetworkPolicy 和基于角色的权限控制 (RBAC)，是内置的 Kubernetes API。API 通常与托管的 Kubernetes 服务和受控的 Kubernetes 安装一起使用。 它们是声明性的，并使用与其他 Kubernetes 资源（如 Pod ）相同的约定，所以新的集群配置可以重复使用，并以与应用程序相同的方式进行管理。而且，当他们变稳定后，他们和其他 Kubernetes API 一样享受定义支持政策。出于这些原因，在合适的情况下它们优先于 配置文件 和 标志 被使用。\n扩展程序 扩展程序是指对 Kubernetes 进行扩展和深度集成的软件组件。它们适合用于支持新的类型和新型硬件。\n大多数集群管理员会使用托管的或统一分发的 Kubernetes 实例。因此，大多数 Kubernetes 用户需要安装扩展程序，而且还有少部分用户甚至需要编写新的扩展程序。\n扩展模式 Kubernetes 的设计是通过编写客户端程序来实现自动化的。任何读和（或）写 Kubernetes API 的程序都可以提供有用的自动化工作。自动化 程序可以运行在集群之中或之外。按照本文档的指导，您可以编写出高可用的和健壮的自动化程序。自动化程序通常适用于任何 Kubernetes 集群，包括托管集群和受管理安装的集群。\n控制器 模式是编写适合 Kubernetes 的客户端程序的一种特定模式。控制器通常读取一个对象的 .spec 字段，可能做出一些处理，然后更新对象的 .status 字段。\n一个控制器是 Kubernetes 的一个客户端。而当 Kubernetes 作为客户端调用远程服务时，它被称为 Webhook ，远程服务称为 Webhook 后端。 和控制器类似，Webhooks 增加了一个失败点。\n在 webhook 模型里，Kubernetes 向远程服务发送一个网络请求。在 二进制插件 模型里，Kubernetes 执行一个二进制（程序）。二进制插件被 kubelet（如 Flex 卷插件和网络插件)和 kubectl 所使用。\n下图显示了扩展点如何与 Kubernetes 控制平面进行交互。\n扩展点 下图显示了 Kubernetes 系统的扩展点。\n 用户通常使用 kubectl 与 Kubernetes API 进行交互。kubectl 插件扩展了 kubectl 二进制程序。它们只影响个人用户的本地环境，因此不能执行站点范围的策略。 apiserver 处理所有请求。apiserver 中的几种类型的扩展点允许对请求进行身份认证或根据其内容对其进行阻止、编辑内容以及处理删除操作。这些内容在API 访问扩展小节中描述。 apiserver 提供各种 资源 。 内置的资源种类 ，如 pods，由 Kubernetes 项目定义，不能更改。您还可以添加您自己定义的资源或其他项目已定义的资源，称为 自定义资源，如自定义资源部分所述。自定义资源通常与 API 访问扩展一起使用。 Kubernetes 调度器决定将 Pod 放置到哪个节点。有几种方法可以扩展调度器。这些内容在 Scheduler Extensions 小节中描述。 Kubernetes 的大部分行为都是由称为控制器的程序实现的，这些程序是 API-Server 的客户端。控制器通常与自定义资源一起使用。 kubelet 在主机上运行，并帮助 pod 看起来就像在集群网络上拥有自己的 IP 的虚拟服务器。网络插件让您可以实现不同的 pod 网络。 kubelet 也挂载和卸载容器的卷。新的存储类型可以通过存储插件支持。  如果您不确定从哪里开始扩展，此流程图可以提供帮助。请注意，某些解决方案可能涉及多种类型的扩展。\nAPI 扩展 用户自定义类型 如果您想定义新的控制器、应用程序配置对象或其他声明式 API，并使用 Kubernetes 工具（如 kubectl）管理它们，请考虑为 Kubernetes 添加一个自定义资源。\n不要使用自定义资源作为应用、用户或者监控数据的数据存储。\n有关自定义资源的更多信息，请查看自定义资源概念指南。\n将新的 API 与自动化相结合 自定义资源 API 和控制循环的组合称为 操作者模式。操作者模式用于管理特定的，通常是有状态的应用程序。这些自定义 API 和控制循环还可用于控制其他资源，例如存储或策略。\n改变内置资源 当您通过添加自定义资源来扩展 Kubernetes API 时，添加的资源始终属于新的 API 组。您不能替换或更改已有的 API 组。添加 API 不会直接影响现有 API（例如 Pod ）的行为，但是 API 访问扩展可以。\nAPI 访问扩展 当请求到达 Kubernetes API Server 时，它首先被要求进行用户认证，然后要进行授权检查，接着受到各种类型的准入控制的检查。有关此流程的更多信息，请参阅 Kubernetes API访问控制。\n上述每个步骤都提供了扩展点。\nKubernetes 有几个它支持的内置认证方法。它还可以位于身份验证代理之后，并将授权 header 中的令牌发送给远程服务进行验证（webhook）。所有这些方法都在身份验证文档中介绍。\n身份认证 身份认证将所有请求中的 header 或证书映射为发出请求的客户端的用户名。\nKubernetes 提供了几种内置的身份认证方法，如果这些方法不符合您的需求，可以使用身份认证 webhook 方法。\n授权 授权决定特定用户是否可以对 API 资源执行读取、写入以及其他操作。它只是在整个资源的层面上工作 \u0026ndash; 它不基于任意的对象字段进行区分。如果内置授权选项不能满足您的需求，授权 webhook 允许调用用户提供的代码来作出授权决定。\n动态准入控制 在请求被授权之后，如果是写入操作，它还将进入准入控制步骤。除了内置的步骤之外，还有几个扩展：\n 镜像策略 webhook 限制了哪些镜像可以在容器中运行。 为了进行灵活的准入控制决策，可以使用通用的 Admission webhook。Admission Webhooks 可以拒绝创建或更新操作。  基础设施扩展 存储插件 Flex Volumes 允许用户挂载无内置插件支持的卷类型，它通过 Kubelet 调用一个二进制插件来挂载卷。\n设备插件 设备插件允许节点通过设备插件发现新的节点资源（除了内置的 CPU 和内存之外）。\n网络插件 不同的网络结构可以通过节点级的网络插件支持。\n调度器扩展 调度器是一种特殊类型的控制器，用于监视 pod 并将其分配到节点。默认的调度器可以完全被替换，而继续使用其他 Kubernetes 组件，或者可以同时运行多个调度器。\n这是一个重要的任务，几乎所有的 Kubernetes 用户都发现他们不需要修改调度器。\n调度器也支持 webhook，它允许一个 webhook 后端（调度器扩展程序）为 pod 筛选节点和确定节点的优先级。\n  详细了解自定义资源 了解动态准入控制 详细了解基础设施扩展  网络插件 设备插件   了解 kubectl 插件 了解操作者模式  "
},
{
	"uri": "https://lijun.in/concepts/configuration/resource-bin-packing/",
	"title": "扩展资源的资源箱打包",
	"tags": [],
	"description": "",
	"content": "for_k8s_version=\u0026quot;1.16\u0026rdquo; state=\u0026quot;alpha\u0026rdquo; \u0026gt;}}\n可以将 kube-scheduler 配置为使用 RequestedToCapacityRatioResourceAllocation 优先级函数启用资源箱打包以及扩展资源。 优先级函数可用于根据自定义需求微调 kube-scheduler 。\n使用 RequestedToCapacityRatioResourceAllocation 启用装箱 在 Kubernetes 1.15 之前，Kube-scheduler 用于允许根据主要资源，如 CPU 和内存对容量之比的请求对节点进行评分。 Kubernetes 1.16 在优先级函数中添加了一个新参数，该参数允许用户指定资源以及每个资源的权重，以便根据容量之比的请求为节点评分。 这允许用户通过使用适当的参数来打包扩展资源，从而提高了大型集群中稀缺资源的利用率。 RequestedToCapacityRatioResourceAllocation 优先级函数的行为可以通过名为 requestedToCapacityRatioArguments 的配置选项进行控制。 这个论证由两个参数 shape 和 resources 组成。 Shape 允许用户根据 utilization 和 score 值将功能调整为要求最少或要求最高的功能。 资源由 name 和 weight 组成，name 指定评分时要考虑的资源，weight 指定每种资源的权重。\n以下是一个配置示例，该配置将 requestedToCapacityRatioArguments 设置为扩展资源 intel.com/foo 和 intel.com/bar 的装箱行为\n{ \u0026#34;kind\u0026#34; : \u0026#34;Policy\u0026#34;, \u0026#34;apiVersion\u0026#34; : \u0026#34;v1\u0026#34;, ... \u0026#34;priorities\u0026#34; : [ ... { \u0026#34;name\u0026#34;: \u0026#34;RequestedToCapacityRatioPriority\u0026#34;, \u0026#34;weight\u0026#34;: 2, \u0026#34;argument\u0026#34;: { \u0026#34;requestedToCapacityRatioArguments\u0026#34;: { \u0026#34;shape\u0026#34;: [ {\u0026#34;utilization\u0026#34;: 0, \u0026#34;score\u0026#34;: 0}, {\u0026#34;utilization\u0026#34;: 100, \u0026#34;score\u0026#34;: 10} ], \u0026#34;resources\u0026#34;: [ {\u0026#34;name\u0026#34;: \u0026#34;intel.com/foo\u0026#34;, \u0026#34;weight\u0026#34;: 3}, {\u0026#34;name\u0026#34;: \u0026#34;intel.com/bar\u0026#34;, \u0026#34;weight\u0026#34;: 5} ] } } } ], } 默认情况下禁用此功能\n调整 RequestedToCapacityRatioResourceAllocation 优先级函数 shape 用于指定 RequestedToCapacityRatioPriority 函数的行为。\n{\u0026#34;utilization\u0026#34;: 0, \u0026#34;score\u0026#34;: 0}, {\u0026#34;utilization\u0026#34;: 100, \u0026#34;score\u0026#34;: 10} 上面的参数在利用率为 0% 时给节点评分为0，在利用率为 100% 时给节点评分为10，因此启用了装箱行为。 要启用最少请求，必须按如下方式反转得分值。\n{\u0026#34;utilization\u0026#34;: 0, \u0026#34;score\u0026#34;: 100}, {\u0026#34;utilization\u0026#34;: 100, \u0026#34;score\u0026#34;: 0} resources 是一个可选参数，默认情况下设置为：\n\u0026#34;resources\u0026#34;: [ {\u0026#34;name\u0026#34;: \u0026#34;CPU\u0026#34;, \u0026#34;weight\u0026#34;: 1}, {\u0026#34;name\u0026#34;: \u0026#34;Memory\u0026#34;, \u0026#34;weight\u0026#34;: 1} ] 它可以用来添加扩展资源，如下所示：\n\u0026#34;resources\u0026#34;: [ {\u0026#34;name\u0026#34;: \u0026#34;intel.com/foo\u0026#34;, \u0026#34;weight\u0026#34;: 5}, {\u0026#34;name\u0026#34;: \u0026#34;CPU\u0026#34;, \u0026#34;weight\u0026#34;: 3}, {\u0026#34;name\u0026#34;: \u0026#34;Memory\u0026#34;, \u0026#34;weight\u0026#34;: 1} ] weight 参数是可选的，如果未指定，则设置为1。 同样， weight 不能设置为负值。\nRequestedToCapacityRatioResourceAllocation 优先级函数如何对节点评分 本部分适用于希望了解此功能的内部细节的人员。 以下是如何针对给定的一组值计算节点得分的示例。\nRequested Resources intel.com/foo : 2 Memory: 256MB CPU: 2 Resource Weights intel.com/foo : 5 Memory: 1 CPU: 3 FunctionShapePoint {{0, 0}, {100, 10}} Node 1 Spec Available: intel.com/foo : 4 Memory : 1 GB CPU: 8 Used: intel.com/foo: 1 Memory: 256MB CPU: 1 Node Score: intel.com/foo = resourceScoringFunction((2+1),4) = (100 - ((4-3)*100/4) = (100 - 25) = 75 = rawScoringFunction(75) = 7 Memory = resourceScoringFunction((256+256),1024) = (100 -((1024-512)*100/1024)) = 50 = rawScoringFunction(50) = 5 CPU = resourceScoringFunction((2+1),8) = (100 -((8-3)*100/8)) = 37.5 = rawScoringFunction(37.5) = 3 NodeScore = (7 * 5) + (5 * 1) + (3 * 3) / (5 + 1 + 3) = 5 Node 2 Spec Available: intel.com/foo: 8 Memory: 1GB CPU: 8 Used: intel.com/foo: 2 Memory: 512MB CPU: 6 Node Score: intel.com/foo = resourceScoringFunction((2+2),8) = (100 - ((8-4)*100/8) = (100 - 25) = 50 = rawScoringFunction(50) = 5 Memory = resourceScoringFunction((256+512),1024) = (100 -((1024-768)*100/1024)) = 75 = rawScoringFunction(75) = 7 CPU = resourceScoringFunction((2+6),8) = (100 -((8-8)*100/8)) = 100 = rawScoringFunction(100) = 10 NodeScore = (5 * 5) + (7 * 1) + (10 * 3) / (5 + 1 + 3) = 7 "
},
{
	"uri": "https://lijun.in/tutorials/kubernetes-basics/explore/explore-intro/",
	"title": "查看 pod 和工作节点",
	"tags": [],
	"description": "",
	"content": "  Objectives -- 目标  Learn about Kubernetes Pods. Learn about Kubernetes Nodes. Troubleshoot deployed applications. -- 了解 Kubernetes Pod。 了解 Kubernetes 工作节点。 对已部署的应用故障排除。   Kubernetes Pods -- Kubernetes Pods When you created a Deployment in Module 2, Kubernetes created a Pod to host your application instance. A Pod is a Kubernetes abstraction that represents a group of one or more application containers (such as Docker or rkt), and some shared resources for those containers. Those resources include:\n-- 在模块 2创建 Deployment 时, Kubernetes 添加了一个 Pod 来托管你的应用实例。Pod 是 Kubernetes 抽象出来的，表示一组一个或多个应用程序容器（如 Docker 或 rkt ），以及这些容器的一些共享资源。这些资源包括:\nShared storage, as Volumes Networking, as a unique cluster IP address Information about how to run each container, such as the container image version or specific ports to use  --  共享存储，当作卷 网络，作为唯一的集群 IP 地址 有关每个容器如何运行的信息，例如容器映像版本或要使用的特定端口。  A Pod models an application-specific \"logical host\" and can contain different application containers which are relatively tightly coupled. For example, a Pod might include both the container with your Node.js app as well as a different container that feeds the data to be published by the Node.js webserver. The containers in a Pod share an IP Address and port space, are always co-located and co-scheduled, and run in a shared context on the same Node.\n-- Pod 为特定于应用程序的“逻辑主机”建模，并且可以包含相对紧耦合的不同应用容器。例如，Pod 可能既包含带有 Node.js 应用的容器，也包含另一个不同的容器，用于提供 Node.js 网络服务器要发布的数据。Pod 中的容器共享 IP 地址和端口，始终位于同一位置并且共同调度，并在同一工作节点上的共享上下文中运行。\nPods are the atomic unit on the Kubernetes platform. When we create a Deployment on Kubernetes, that Deployment creates Pods with containers inside them (as opposed to creating containers directly). Each Pod is tied to the Node where it is scheduled, and remains there until termination (according to restart policy) or deletion. In case of a Node failure, identical Pods are scheduled on other available Nodes in the cluster.\n-- Pod是 Kubernetes 平台上的原子单元。 当我们在 Kubernetes 上创建 Deployment 时，该 Deployment 会在其中创建包含容器的 Pod （而不是直接创建容器）。每个 Pod 都与调度它的工作节点绑定，并保持在那里直到终止（根据重启策略）或删除。 如果工作节点发生故障，则会在群集中的其他可用工作节点上调度相同的 Pod。\n Summary: -- 总结:  Pods 工作节点 Kubectl main commands -- Kubectl 主要命令    A Pod is a group of one or more application containers (such as Docker or rkt) and includes shared storage (volumes), IP address and information about how to run them. \n--  Pod 是一组一个或多个应用程序容器（例如 Docker 或 rkt ），包括共享存储（卷), IP 地址和有关如何运行它们的信息。 \n   Pods overview -- Pod 概览     Nodes A Pod always runs on a Node. A Node is a worker machine in Kubernetes and may be either a virtual or a physical machine, depending on the cluster. Each Node is managed by the Master. A Node can have multiple pods, and the Kubernetes master automatically handles scheduling the pods across the Nodes in the cluster. The Master's automatic scheduling takes into account the available resources on each Node.\nEvery Kubernetes Node runs at least:\n Kubelet, a process responsible for communication between the Kubernetes Master and the Node; it manages the Pods and the containers running on a machine. A container runtime (like Docker, rkt) responsible for pulling the container image from a registry, unpacking the container, and running the application.  -- 工作节点 一个 pod 总是运行在 工作节点。工作节点是 Kubernetes 中的参与计算的机器，可以是虚拟机或物理计算机，具体取决于集群。每个工作节点由主节点管理。工作节点可以有多个 pod ，Kubernetes 主节点会自动处理在群集中的工作节点上调度 pod 。 主节点的自动调度考量了每个工作节点上的可用资源。\n每个 Kubernetes 工作节点至少运行:\n Kubelet，负责 Kubernetes 主节点和工作节点之间通信的过程; 它管理 Pod 和机器上运行的容器。 容器运行时（如 Docker ，rkt ）负责从仓库中提取容器镜像，解压缩容器以及运行应用程序。    Containers should only be scheduled together in a single Pod if they are tightly coupled and need to share resources such as disk. \n-- 如果它们紧耦合并且需要共享磁盘等资源，这些容器应在一个 Pod 中编排。\n   Node overview -- 工作节点概览     Troubleshooting with kubectl -- 使用 kubectl 进行故障排除 In Module 2, you used Kubectl command-line interface. You'll continue to use it in Module 3 to get information about deployed applications and their environments. The most common operations can be done with the following kubectl commands:\n-- 在模块 2,您使用了 Kubectl 命令行界面。 您将继续在第3单元中使用它来获取有关已部署的应用程序及其环境的信息。 最常见的操作可以使用以下 kubectl 命令完成：\nkubectl get - list resources kubectl describe - show detailed information about a resource kubectl logs - print the logs from a container in a pod kubectl exec - execute a command on a container in a pod  --  kubectl get - 列出资源 kubectl describe - 显示有关资源的详细信息 kubectl logs - 打印 pod 和其中容器的日志 kubectl exec - 在 pod 中的容器上执行命令  You can use these commands to see when applications were deployed, what their current statuses are, where they are running and what their configurations are.\n-- 您可以使用这些命令查看应用程序的部署时间，当前状态，运行位置以及配置。\nNow that we know more about our cluster components and the command line, let's explore our application.\n-- 现在我们了解了有关集群组件和命令行的更多信息，让我们来探索一下我们的应用程序。\n  A node is a worker machine in Kubernetes and may be a VM or physical machine, depending on the cluster. Multiple Pods can run on one Node. \n-- 工作节点是 Kubernetes 中的负责计算的机器，可能是VM或物理计算机，具体取决于群集。多个 Pod 可以在一个工作节点上运行。 \n   Start Interactive Tutorial › -- 开始交互式教程 ›       "
},
{
	"uri": "https://lijun.in/reference/command-line-tools-reference/feature-gates/",
	"title": "特性门控",
	"tags": [],
	"description": "",
	"content": "本页详述了管理员可以在不同的 Kubernetes 组件上指定的各种特性门控。\n关于特性各个阶段的说明，请参见特性阶段。\n概述 特性门控是描述 Kubernetes 特性的一组键值对。您可以在 Kubernetes 的每一个组件中使用 --feature-gates flag 来启用或禁用这些特性。\n每个 Kubernetes 组件都支持启用或禁用与该组件相关的一组特性门控。 使用 -h 参数来查看所有组件支持的完整特性门控。 要为诸如 kubelet 之类的组件设置特性门控，请使用 --feature-gates 参数，并向其传递一组特性：\n--feature-gates=\u0026#34;...,DynamicKubeletConfig=true\u0026#34; 下表总结了在不同的 Kubernetes 组件上可以设置的特性门控。\n 引入特性或更改其发布阶段后，\u0026ldquo;Since\u0026rdquo; 列将包含 Kubernetes 版本。 \u0026ldquo;Until\u0026rdquo; 列（如果不为空）包含最后一个 Kubernetes 版本，您仍可以在其中使用特性门控。 如果某个特性处于 Alpha 或 Beta 状态，您可以在 Alpha 和 Beta 特性门控表中找到该特性。 如果某个特性处于稳定状态，您可以在毕业和废弃特性门控表.中找到该特性的所有阶段。 毕业和废弃特性门控表 还列出了废弃的和已被移除的特性。  Alpha 和 Beta 的特性门控 . table caption=\u0026quot;处于 Alpha 或 Beta 状态的特性门控\u0026rdquo; \u0026gt;}}\n   特性 默认值 状态 开始(Since) 结束(Until)     APIListChunking false Alpha 1.8 1.8   APIListChunking true Beta 1.9    APIPriorityAndFairness false Alpha 1.17    APIResponseCompression false Alpha 1.7    AppArmor true Beta 1.4    BalanceAttachedNodeVolumes false Alpha 1.11    BlockVolume false Alpha 1.9 1.12   BlockVolume true Beta 1.13 -   BoundServiceAccountTokenVolume false Alpha 1.13    CPUManager false Alpha 1.8 1.9   CPUManager true Beta 1.10    CRIContainerLogRotation false Alpha 1.10 1.10   CRIContainerLogRotation true Beta 1.11    CSIBlockVolume false Alpha 1.11 1.13   CSIBlockVolume true Beta 1.14    CSIDriverRegistry false Alpha 1.12 1.13   CSIDriverRegistry true Beta 1.14    CSIInlineVolume false Alpha 1.15 1.15   CSIInlineVolume true Beta 1.16 -   CSIMigration false Alpha 1.14 1.16   CSIMigration true Beta 1.17    CSIMigrationAWS false Alpha 1.14    CSIMigrationAWS false Beta 1.17    CSIMigrationAWSComplete false Alpha 1.17    CSIMigrationAzureDisk false Alpha 1.15    CSIMigrationAzureDiskComplete false Alpha 1.17    CSIMigrationAzureFile false Alpha 1.15    CSIMigrationAzureFileComplete false Alpha 1.17    CSIMigrationGCE false Alpha 1.14 1.16   CSIMigrationGCE false Beta 1.17    CSIMigrationGCEComplete false Alpha 1.17    CSIMigrationOpenStack false Alpha 1.14    CSIMigrationOpenStackComplete false Alpha 1.17    CustomCPUCFSQuotaPeriod false Alpha 1.12    CustomResourceDefaulting false Alpha 1.15 1.15   CustomResourceDefaulting true Beta 1.16    DevicePlugins false Alpha 1.8 1.9   DevicePlugins true Beta 1.10    DryRun false Alpha 1.12 1.12   DryRun true Beta 1.13    DynamicAuditing false Alpha 1.13    DynamicKubeletConfig false Alpha 1.4 1.10   DynamicKubeletConfig true Beta 1.11    EndpointSlice false Alpha 1.16 1.16   EndpointSlice false Beta 1.17    EphemeralContainers false Alpha 1.16    ExpandCSIVolumes false Alpha 1.14 1.15   ExpandCSIVolumes true Beta 1.16    ExpandInUsePersistentVolumes false Alpha 1.11 1.14   ExpandInUsePersistentVolumes true Beta 1.15    ExpandPersistentVolumes false Alpha 1.8 1.10   ExpandPersistentVolumes true Beta 1.11    ExperimentalHostUserNamespaceDefaulting false Beta 1.5    EvenPodsSpread false Alpha 1.16    HPAScaleToZero false Alpha 1.16    HyperVContainer false Alpha 1.10    KubeletPodResources false Alpha 1.13 1.14   KubeletPodResources true Beta 1.15    LegacyNodeRoleBehavior true Alpha 1.16    LocalStorageCapacityIsolation false Alpha 1.7 1.9   LocalStorageCapacityIsolation true Beta 1.10    LocalStorageCapacityIsolationFSQuotaMonitoring false Alpha 1.15    MountContainers false Alpha 1.9    NodeDisruptionExclusion false Alpha 1.16    NonPreemptingPriority false Alpha 1.15    PodOverhead false Alpha 1.16 -   ProcMountType false Alpha 1.12    QOSReserved false Alpha 1.11    RemainingItemCount false Alpha 1.15    ResourceLimitsPriorityFunction false Alpha 1.9    RotateKubeletClientCertificate true Beta 1.8    RotateKubeletServerCertificate false Alpha 1.7 1.11   RotateKubeletServerCertificate true Beta 1.12    RunAsGroup true Beta 1.14    RuntimeClass false Alpha 1.12 1.13   RuntimeClass true Beta 1.14    SCTPSupport false Alpha 1.12    ServerSideApply false Alpha 1.14 1.15   ServerSideApply true Beta 1.16    ServiceNodeExclusion false Alpha 1.8    ServiceTopology false Alpha 1.17    StartupProbe false Alpha 1.16    StorageVersionHash false Alpha 1.14 1.14   StorageVersionHash true Beta 1.15    StreamingProxyRedirects false Beta 1.5 1.5   StreamingProxyRedirects true Beta 1.6    SupportNodePidsLimit false Alpha 1.14 1.14   SupportNodePidsLimit true Beta 1.15    SupportPodPidsLimit false Alpha 1.10 1.13   SupportPodPidsLimit true Beta 1.14    Sysctls true Beta 1.11    TaintBasedEvictions false Alpha 1.6 1.12   TaintBasedEvictions true Beta 1.13    TokenRequest false Alpha 1.10 1.11   TokenRequest true Beta 1.12    TokenRequestProjection false Alpha 1.11 1.11   TokenRequestProjection true Beta 1.12    TTLAfterFinished false Alpha 1.12    TopologyManager false Alpha 1.16    ValidateProxyRedirects false Alpha 1.12 1.13   ValidateProxyRedirects true Beta 1.14    VolumePVCDataSource false Alpha 1.15 1.15   VolumePVCDataSource true Beta 1.16    VolumeSnapshotDataSource false Alpha 1.12 1.16   VolumeSnapshotDataSource true Beta 1.17 -   WindowsGMSA false Alpha 1.14    WindowsGMSA true Beta 1.16    WinDSR false Alpha 1.14    WinOverlay false Alpha 1.14     . /table \u0026gt;}}\n已毕业和不推荐使用的特性门控 . table caption=\u0026quot;已毕业或不推荐使用的特性门控\u0026rdquo; \u0026gt;}}\n   特性 默认值 状态 开始(Since) 结束(Until)     Accelerators false Alpha 1.6 1.10   Accelerators - Deprecated 1.11 -   AdvancedAuditing false Alpha 1.7 1.7   AdvancedAuditing true Beta 1.8 1.11   AdvancedAuditing true GA 1.12 -   AffinityInAnnotations false Alpha 1.6 1.7   AffinityInAnnotations - Deprecated 1.8 -   AllowExtTrafficLocalEndpoints false Beta 1.4 1.6   AllowExtTrafficLocalEndpoints true GA 1.7 -   CSINodeInfo false Alpha 1.12 1.13   CSINodeInfo true Beta 1.14 1.16   CSINodeInfo true GA 1.17    AttachVolumeLimit false Alpha 1.11 1.11   AttachVolumeLimit true Beta 1.12 1.16   AttachVolumeLimit true GA 1.17 -   CSIPersistentVolume false Alpha 1.9 1.9   CSIPersistentVolume true Beta 1.10 1.12   CSIPersistentVolume true GA 1.13 -   CustomPodDNS false Alpha 1.9 1.9   CustomPodDNS true Beta 1.10 1.13   CustomPodDNS true GA 1.14 -   CustomResourcePublishOpenAPI false Alpha 1.14 1.14   CustomResourcePublishOpenAPI true Beta 1.15 1.15   CustomResourcePublishOpenAPI true GA 1.16 -   CustomResourceSubresources false Alpha 1.10 1.10   CustomResourceSubresources true Beta 1.11 1.15   CustomResourceSubresources true GA 1.16 -   CustomResourceValidation false Alpha 1.8 1.8   CustomResourceValidation true Beta 1.9 1.15   CustomResourceValidation true GA 1.16 -   CustomResourceWebhookConversion false Alpha 1.13 1.14   CustomResourceWebhookConversion true Beta 1.15 1.15   CustomResourceWebhookConversion true GA 1.16 -   DynamicProvisioningScheduling false Alpha 1.11 1.11   DynamicProvisioningScheduling - Deprecated 1.12 -   DynamicVolumeProvisioning true Alpha 1.3 1.7   DynamicVolumeProvisioning true GA 1.8 -   EnableEquivalenceClassCache false Alpha 1.8 1.14   EnableEquivalenceClassCache - Deprecated 1.15 -   ExperimentalCriticalPodAnnotation false Alpha 1.5 1.12   ExperimentalCriticalPodAnnotation false Deprecated 1.13 -   GCERegionalPersistentDisk true Beta 1.10 1.12   GCERegionalPersistentDisk true GA 1.13 -   HugePages false Alpha 1.8 1.9   HugePages true Beta 1.10 1.13   HugePages true GA 1.14 -   Initializers false Alpha 1.7 1.13   Initializers - Deprecated 1.14 -   KubeletConfigFile false Alpha 1.8 1.9   KubeletConfigFile - Deprecated 1.10 -   KubeletPluginsWatcher false Alpha 1.11 1.11   KubeletPluginsWatcher true Beta 1.12 1.12   KubeletPluginsWatcher true GA 1.13 -   MountPropagation false Alpha 1.8 1.9   MountPropagation true Beta 1.10 1.11   MountPropagation true GA 1.12 -   NodeLease false Alpha 1.12 1.13   NodeLease true Beta 1.14 1.16   NodeLease true GA 1.17 -   PersistentLocalVolumes false Alpha 1.7 1.9   PersistentLocalVolumes true Beta 1.10 1.13   PersistentLocalVolumes true GA 1.14 -   PodPriority false Alpha 1.8 1.10   PodPriority true Beta 1.11 1.13   PodPriority true GA 1.14 -   PodReadinessGates false Alpha 1.11 1.11   PodReadinessGates true Beta 1.12 1.13   PodReadinessGates true GA 1.14 -   PodShareProcessNamespace false Alpha 1.10 1.11   PodShareProcessNamespace true Beta 1.12 1.16   PodShareProcessNamespace true GA 1.17 -   PVCProtection false Alpha 1.9 1.9   PVCProtection - Deprecated 1.10 -   RequestManagement false Alpha 1.15 1.16   ResourceQuotaScopeSelectors false Alpha 1.11 1.11   ResourceQuotaScopeSelectors true Beta 1.12 1.16   ResourceQuotaScopeSelectors true GA 1.17 -   ScheduleDaemonSetPods false Alpha 1.11 1.11   ScheduleDaemonSetPods true Beta 1.12 1.16   ScheduleDaemonSetPods true GA 1.17 -   ServiceLoadBalancerFinalizer false Alpha 1.15 1.15   ServiceLoadBalancerFinalizer true Beta 1.16 1.16   ServiceLoadBalancerFinalizer true GA 1.17 -   StorageObjectInUseProtection true Beta 1.10 1.10   StorageObjectInUseProtection true GA 1.11 -   SupportIPVSProxyMode false Alpha 1.8 1.8   SupportIPVSProxyMode false Beta 1.9 1.9   SupportIPVSProxyMode true Beta 1.10 1.10   SupportIPVSProxyMode true GA 1.11 -   TaintNodesByCondition false Alpha 1.8 1.11   TaintNodesByCondition true Beta 1.12 1.16   TaintNodesByCondition true GA 1.17 -   VolumeScheduling false Alpha 1.9 1.9   VolumeScheduling true Beta 1.10 1.12   VolumeScheduling true GA 1.13 -   VolumeSubpath true GA 1.13 -   VolumeSubpathEnvExpansion false Alpha 1.14 1.14   VolumeSubpathEnvExpansion true Beta 1.15 1.16   VolumeSubpathEnvExpansion true GA 1.17 -   WatchBookmark false Alpha 1.15 1.15   WatchBookmark true Beta 1.16 1.16   WatchBookmark true GA 1.17 -    . /table \u0026gt;}}\n使用特性 特性阶段 处于 Alpha 、Beta 、 GA 阶段的特性。\nAlpha 特性代表：\n 默认禁用。 可能有错误，启用此特性可能会导致错误。 随时可能删除对此特性的支持，恕不另行通知。 在以后的软件版本中，API 可能会以不兼容的方式更改，恕不另行通知。 建议将其仅用于短期测试中，因为开启特性会增加错误的风险，并且缺乏长期支持。  Beta 特性代表：\n 默认禁用。 该特性已经经过良好测试。启用该特性是安全的。 尽管详细信息可能会更改，但不会放弃对整体特性的支持。 对象的架构或语义可能会在随后的 Beta 或稳定版本中以不兼容的方式更改。当发生这种情况时，我们将提供迁移到下一版本的说明。此特性可能需要删除、编辑和重新创建 API 对象。编辑过程可能需要慎重操作，因为这可能会导致依赖该特性的应用程序停机。 推荐仅用于非关键业务用途，因为在后续版本中可能会发生不兼容的更改。如果您具有多个可以独立升级的，则可以放宽此限制。  . note \u0026gt;}}\n请试用 Beta 特性并提供相关反馈！ 一旦特性结束 Beta 状态，我们就不太可能再对特性进行大幅修改。 . /note \u0026gt;}}\nGeneral Availability (GA) 特性也称为 稳定 特性，GA 特性代表着：\n 此特性会一直启用；你不能禁用它。 不再需要相应的特性门控。 对于许多后续版本，特性的稳定版本将出现在发行的软件中。  特性门控列表 每个特性门控均用于启用或禁用某个特定的特性：\n Accelerators：使用 Docker 时启用 Nvidia GPU 支持。 AdvancedAuditing：启用高级审查功能。 AffinityInAnnotations（ 已弃用 ）：启用 Pod 亲和力或反亲和力。 AllowExtTrafficLocalEndpoints：启用服务用于将外部请求路由到节点本地终端。 APIListChunking：启用 API 客户端以块的形式从 API 服务器检索（“LIST” 或 “GET”）资源。 APIPriorityAndFairness: Enable managing request concurrency with prioritization and fairness at each server. (Renamed from RequestManagement) APIPriorityAndFairness: 在每个服务器上启用优先级和公平性来管理请求并发。（由 RequestManagement 重命名而来） APIResponseCompression：压缩 “LIST” 或 “GET” 请求的 API 响应。 AppArmor：使用 Docker 时，在 Linux 节点上启用基于 AppArmor 机制的强制访问控制。请参见 AppArmor 教程 获取详细信息。   AttachVolumeLimit：启用卷插件用于报告可连接到节点的卷数限制。有关更多详细信息，请参见动态卷限制。 BalanceAttachedNodeVolumes：包括要在调度时进行平衡资源分配的节点上的卷数。scheduler 在决策时会优先考虑 CPU、内存利用率和卷数更近的节点。 BlockVolume：在 Pod 中启用原始块设备的定义和使用。有关更多详细信息，请参见原始块卷支持。 BoundServiceAccountTokenVolume：迁移 ServiceAccount 卷以使用由 ServiceAccountTokenVolumeProjection 组成的预计卷。有关更多详细信息，请参见 Service Account Token 卷。 CPUManager：启用容器级别的 CPU 亲和力支持，有关更多详细信息，请参见 CPU 管理策略。   CRIContainerLogRotation：为 cri 容器运行时启用容器日志轮换。 CSIBlockVolume：启用外部 CSI 卷驱动程序用于支持块存储。有关更多详细信息，请参见 csi 原始块卷支持。 CSIDriverRegistry：在 csi.storage.k8s.io 中启用与 CSIDriver API 对象有关的所有逻辑。 CSIInlineVolume：为 Pod 启用 CSI 内联卷支持。 CSIMigration：确保填充和转换逻辑能够将卷操作从内嵌插件路由到相应的预安装 CSI 插件。 CSIMigrationAWS：确保填充和转换逻辑能够将卷操作从 AWS-EBS 内嵌插件路由到 EBS CSI 插件。如果节点未安装和配置 EBS CSI 插件，则支持回退到内嵌 EBS 插件。这需要启用 CSIMigration 特性标志。 CSIMigrationAWSComplete：停止在 kubelet 和卷控制器中注册 EBS 内嵌插件，并启用 shims 和转换逻辑将卷操作从AWS-EBS 内嵌插件路由到 EBS CSI 插件。这需要启用 CSIMigration 和 CSIMigrationAWS 特性标志，并在群集中的所有节点上安装和配置 EBS CSI 插件。 CSIMigrationAzureDisk：确保填充和转换逻辑能够将卷操作从 Azure 磁盘内嵌插件路由到 Azure 磁盘 CSI 插件。如果节点未安装和配置 AzureDisk CSI 插件，支持回退到内建 AzureDisk 插件。这需要启用 CSIMigration 特性标志。 CSIMigrationAzureDiskComplete：停止在 kubelet 和卷控制器中注册 Azure 磁盘内嵌插件，并启用 shims 和转换逻辑以将卷操作从 Azure 磁盘内嵌插件路由到 AzureDisk CSI 插件。这需要启用 CSIMigration 和 CSIMigrationAzureDisk 特性标志，并在群集中的所有节点上安装和配置 AzureDisk CSI 插件。 CSIMigrationAzureFile：确保填充和转换逻辑能够将卷操作从 Azure 文件内嵌插件路由到 Azure 文件 CSI 插件。如果节点未安装和配置 AzureFile CSI 插件，支持回退到内嵌 AzureFile 插件。这需要启用 CSIMigration 特性标志。 CSIMigrationAzureFileComplete：停止在 kubelet 和卷控制器中注册 Azure-File 内嵌插件，并启用 shims 和转换逻辑以将卷操作从 Azure-File 内嵌插件路由到 AzureFile CSI 插件。这需要启用 CSIMigration 和 CSIMigrationAzureFile 特性标志，并在群集中的所有节点上安装和配置 AzureFile CSI 插件。   CSIMigrationGCE：使 shims 和转换逻辑能够将卷操作从 GCE-PD 内嵌插件路由到 PD CSI 插件。如果节点未安装和配置 PD CSI 插件，支持回退到内嵌 GCE 插件。这需要启用 CSIMigration 特性标志。 CSIMigrationGCEComplete：停止在 kubelet 和卷控制器中注册 GCE-PD 内嵌插件，并启用 shims 和转换逻辑以将卷操作从 GCE-PD 内嵌插件路由到 PD CSI 插件。这需要启用 CSIMigration 和 CSIMigrationGCE 特性标志，并在群集中的所有节点上安装和配置 PD CSI 插件。 CSIMigrationOpenStack：确保填充和转换逻辑能够将卷操作从 Cinder 内嵌插件路由到 Cinder CSI 插件。如果节点未安装和配置 Cinder CSI 插件，支持回退到内嵌 Cinder 插件。这需要启用 CSIMigration 特性标志。 CSIMigrationOpenStackComplete：停止在 kubelet 和卷控制器中注册 Cinder 内嵌插件，并启用 shims 和转换逻辑将卷操作从 Cinder 内嵌插件路由到 Cinder CSI 插件。这需要启用 CSIMigration 和 CSIMigrationOpenStack 特性标志，并在群集中的所有节点上安装和配置 Cinder CSI 插件。 CSINodeInfo：在 csi.storage.k8s.io 中启用与 CSINodeInfo API 对象有关的所有逻辑。 CSIPersistentVolume：启用发现并挂载通过 CSI（容器存储接口）兼容卷插件配置的卷。有关更多详细信息，请参见 csi 卷类型。   CustomCPUCFSQuotaPeriod：使节点能够更改 CPUCFSQuotaPeriod。 CustomPodDNS：使用其 dnsConfig 属性启用 Pod 的自定义 DNS 设置。有关更多详细信息，请参见 Pod 的 DNS 配置。 CustomResourceDefaulting：为 OpenAPI v3 验证架构中的默认值启用 CRD 支持。 CustomResourcePublishOpenAPI：启用 CRD OpenAPI 规范的发布。 CustomResourceSubresources：对于从 CustomResourceDefinition 中创建的资源启用 /status 和 /scale 子资源。 CustomResourceValidation：对于从 CustomResourceDefinition 中创建的资源启用基于架构的验证。 CustomResourceWebhookConversion：对于从 CustomResourceDefinition 中创建的资源启用基于 Webhook 的转换。 对正在运行的 Pod 进行故障排除。   DevicePlugins：在节点上启用基于 device-plugins 的资源供应。 DryRun：启用服务器端 dry run 请求，以便无需提交即可测试验证、合并和差异化。 DynamicAuditing：确保动态审查。 DynamicKubeletConfig：启用 kubelet 的动态配置。请参阅重新配置 kubelet。 DynamicProvisioningScheduling：扩展默认 scheduler 以了解卷拓扑并处理 PV 配置。此特性已在 v1.12 中完全被 VolumeScheduling 特性取代。 DynamicVolumeProvisioning（ 已弃用 ）：启用持久化卷到 Pod 的动态预配置。   EnableAggregatedDiscoveryTimeout （ 已弃用 ）：对聚集的发现调用启用五秒钟超时设置。 EnableEquivalenceClassCache：调度 Pod 时，使 scheduler 缓存节点的等效项。 EphemeralContainers：启用添加 . glossary_tooltip text=\u0026quot;临时容器\u0026rdquo; term_id=\u0026quot;ephemeral-container\u0026rdquo; \u0026gt;}} 到正在运行的 Pod 的特性。 EvenPodsSpread：使 Pod 能够在拓扑域之间平衡调度。请参阅 Pod 拓扑扩展约束。 ExpandInUsePersistentVolumes：启用扩展使用中的 PVC。请查阅 调整使用中的 PersistentVolumeClaim 的大小。 ExpandPersistentVolumes：启用持久卷的扩展。请查阅扩展永久卷声明。 ExperimentalCriticalPodAnnotation：启用将特定 Pod 注解为 critical 的方式，用于确保其调度。从 v1.13 开始，Pod 优先级和抢占功能已弃用此特性。   ExperimentalHostUserNamespaceDefaultingGate：启用主机默认的用户命名空间。这适用于使用其他主机命名空间、主机安装的容器，或具有特权或使用特定的非命名空间功能（例如MKNODE、SYS_MODULE等）的容器。如果在 Docker 守护程序中启用了用户命名空间重新映射，则启用此选项。 EndpointSlice：启用端点切片以实现更多可扩展的网络端点。需要启用相应的 API 和控制器，请参阅启用端点切片。 GCERegionalPersistentDisk：在 GCE 上启用区域 PD 特性。 HugePages: 启用分配和使用预分配的 huge pages。   HyperVContainer：为 Windows 容器启用Hyper-V 隔离。 HPAScaleToZero：使用自定义指标或外部指标时，可将 HorizontalPodAutoscaler 资源的 minReplicas 设置为 0。 KubeletConfigFile：启用从使用配置文件指定的文件中加载 kubelet 配置。有关更多详细信息，请参见通过配置文件设置 kubelet 参数。 KubeletPluginsWatcher：启用基于探针的插件监视应用程序，使 kubelet 能够发现插件，例如 CSI 卷驱动程序。 KubeletPodResources：启用 kubelet 的 pod 资源 grpc 端点。有关更多详细信息，请参见支持设备监控。 LegacyNodeRoleBehavior：禁用此选项后，服务负载均衡中的旧版操作和节点中断将忽略 node-role.kubernetes.io/master 标签，而使用特性指定的标签。   LocalStorageCapacityIsolation：启用本地临时存储的消耗，以及 emptyDir 卷 的 sizeLimit 属性。 LocalStorageCapacityIsolationFSQuotaMonitoring：如果为本地临时存储启用了 LocalStorageCapacityIsolation，并且 emptyDir 卷 的后备文件系统支持项目配额，并且启用了这些配额，请使用项目配额来监视 emptyDir 卷的存储消耗而不是遍历文件系统，以此获得更好的性能和准确性。 MountContainers：在主机上启用将应用程序容器用作卷安装程序。 MountPropagation：启用将一个容器安装的共享卷共享到其他容器或 Pod。有关更多详细信息，请参见 mount propagation。 NodeDisruptionExclusion：启用节点标签 node.kubernetes.io/exclude-disruption，以防止在区域故障期间驱逐节点。   NodeLease：启用新的租赁 API 以报告节点心跳，可用作节点运行状况信号。 NonPreemptingPriority：为 PriorityClass 和 Pod 启用 NonPreempting 选项。 PersistentLocalVolumes：在 Pod 中启用 “本地” 卷类型的使用。如果请求 “本地” 卷，则必须指定 Pod 亲和力。 PodOverhead：启用 PodOverhead 特性以解决 Pod 开销。 PodPriority：根据优先级启用 Pod 的调度和抢占。 PodReadinessGates：启用 PodReadinessGate 字段的设置以扩展 Pod 准备状态评估。有关更多详细信息，请参见 Pod readiness 特性门控。   PodShareProcessNamespace：在 Pod 中启用 shareProcessNamespace 的设置，以便在 Pod 中运行的容器之间共享单个进程命名空间。更多详细信息，请参见在 Pod 中的容器之间共享进程命名空间。 ProcMountType：启用对容器的 ProcMountType 的控制。 PVCProtection：启用防止任何 Pod 仍使用 PersistentVolumeClaim(PVC) 删除的特性。可以在此处中找到更多详细信息。 QOSReserved：允许在 QoS 级别进行资源预留，以防止处于较低 QoS 级别的 Pod 突发进入处于较高 QoS 级别的请求资源（仅适用于内存）。 ResourceLimitsPriorityFunction：启用 scheduler 优先级特性，该特性将最低可能得 1 分配给至少满足输入 Pod 的 cpu 和内存限制之一的节点，目的是打破得分相同的节点之间的联系。   RequestManagement：在每个服务器上启用具有优先级和公平性的管理请求并发性。 ResourceQuotaScopeSelectors：启用资源配额范围选择器。 RotateKubeletClientCertificate：在 kubelet 上启用客户端 TLS 证书的轮换。有关更多详细信息，请参见 kubelet 配置。 RotateKubeletServerCertificate：在 kubelet 上启用服务器 TLS 证书的轮换。有关更多详细信息，请参见 kubelet 配置。 RunAsGroup：启用对容器初始化过程中设置的主要组 ID 的控制。 RuntimeClass：启用 RuntimeClass 特性用于选择容器运行时配置。 ScheduleDaemonSetPods：启用 DaemonSet Pods 由默认调度程序而不是 DaemonSet 控制器进行调度。   SCTPSupport：在 “服务”、“端点”、“NetworkPolicy” 和 “Pod” 定义中，将 SCTP 用作 “协议” 值。 ServerSideApply：在 API 服务器上启用服务器端应用（SSA） 路径。 ServiceLoadBalancerFinalizer：为服务负载均衡启用终结器保护。 ServiceNodeExclusion：启用从云提供商创建的负载均衡中排除节点。如果节点标记有 alpha.service-controller.kubernetes.io/exclude-balancer 键或 node.kubernetes.io/exclude-from-external-load-balancers，则可以排除节点。 ServiceTopology: 启用服务拓扑可以让一个服务基于集群的节点拓扑进行流量路由。有关更多详细信息，请参见Service 拓扑 StartupProbe：在 kubelet 中启用 startup 探针。 StorageObjectInUseProtection：如果仍在使用 PersistentVolume 或 PersistentVolumeClaim 对象，则将其推迟。   StorageVersionHash：允许 apiserver 在发现中公开存储版本的哈希值。 StreamingProxyRedirects：指示 API 服务器拦截（并遵循）从后端（kubelet）进行重定向以处理流请求。流请求的例子包括 exec、attach 和 port-forward 请求。 SupportIPVSProxyMode：启用使用 IPVS 提供内服务负载平衡。有关更多详细信息，请参见服务代理。 SupportPodPidsLimit：启用支持限制 Pod 中的进程 PID。 Sysctls：启用对可以为每个 Pod 设置的命名空间内核参数（sysctls）的支持。有关更多详细信息，请参见 sysctls。   TaintBasedEvictions：根据节点上的污点和 Pod 上的容忍度启用从节点驱逐 Pod 的特性。有关更多详细信息，请参见污点和容忍度。 TaintNodesByCondition：根据节点条件启用自动在节点标记污点。 TokenRequest：在服务帐户资源上启用 TokenRequest 端点。 TokenRequestProjection：启用通过 projected 卷 将服务帐户令牌注入到 Pod 中的特性。 TopologyManager：启用一种机制来协调 Kubernetes 不同组件的细粒度硬件资源分配。详见 控制节点上的拓扑管理策略。 TTLAfterFinished：完成执行后，允许 TTL 控制器清理资源。   VolumePVCDataSource：启用对将现有 PVC 指定数据源的支持。 VolumeScheduling：启用卷拓扑感知调度，并使 PersistentVolumeClaim（PVC）绑定调度决策；当与 PersistentLocalVolumes 特性门控一起使用时，还可以使用 PersistentLocalVolumes 卷类型。 VolumeSnapshotDataSource：启用卷快照数据源支持。   VolumeSubpathEnvExpansion：启用 subPathExpr 字段用于将环境变量扩展为 subPath。 WatchBookmark：启用对监测 bookmark 事件的支持。 WindowsGMSA：允许将 GMSA 凭据规范从 Pod 传递到容器运行时。 WinDSR：允许 kube-proxy 为 Windows 创建 DSR 负载均衡。 WinOverlay：允许 kube-proxy 在 Windows 的 overlay 模式下运行。  . heading \u0026ldquo;whatsnext\u0026rdquo; %}}  Kubernetes 的 弃用策略 介绍了项目已移除的特性部件和组件的方法。  "
},
{
	"uri": "https://lijun.in/concepts/overview/working-with-objects/kubernetes-objects/",
	"title": "理解 Kubernetes 对象",
	"tags": [],
	"description": "",
	"content": "本页说明了 Kubernetes 对象在 Kubernetes API 中是如何表示的，以及如何在 .yaml 格式的文件中表示。\n理解 Kubernetes 对象 在 Kubernetes 系统中，Kubernetes 对象 是持久化的实体。Kubernetes 使用这些实体去表示整个集群的状态。特别地，它们描述了如下信息：\n 哪些容器化应用在运行（以及在哪个 Node 上） 可以被应用使用的资源 关于应用运行时表现的策略，比如重启策略、升级策略，以及容错策略  Kubernetes 对象是 “目标性记录” —— 一旦创建对象，Kubernetes 系统将持续工作以确保对象存在。通过创建对象，本质上是在告知 Kubernetes 系统，所需要的集群工作负载看起来是什么样子的，这就是 Kubernetes 集群的 期望状态（Desired State）。\n操作 Kubernetes 对象 —— 无论是创建、修改，或者删除 —— 需要使用 Kubernetes API。比如，当使用 kubectl 命令行接口时，CLI 会执行必要的 Kubernetes API 调用，也可以在程序中使用 客户端库 直接调用 Kubernetes API。\n对象规约（Spec）与状态（Status） 每个 Kubernetes 对象包含两个嵌套的对象字段，它们负责管理对象的配置：对象 spec 和 对象 status 。 spec 是必需的，它描述了对象的 期望状态（Desired State） —— 希望对象所具有的特征。 status 描述了对象的 实际状态（Actual State） ，它是由 Kubernetes 系统提供和更新的。在任何时刻，Kubernetes 控制面一直努力地管理着对象的实际状态以与期望状态相匹配。\n例如，Kubernetes Deployment 对象能够表示运行在集群中的应用。 当创建 Deployment 时，可能需要设置 Deployment 的规约，以指定该应用需要有 3 个副本在运行。 Kubernetes 系统读取 Deployment 规约，并启动我们所期望的该应用的 3 个实例 —— 更新状态以与规约相匹配。 如果那些实例中有失败的（一种状态变更），Kubernetes 系统通过修正来响应规约和状态之间的不一致 —— 这种情况，会启动一个新的实例来替换。\n关于对象 spec、status 和 metadata 的更多信息，查看 Kubernetes API 约定。\n描述 Kubernetes 对象 当创建 Kubernetes 对象时，必须提供对象的规约，用来描述该对象的期望状态，以及关于对象的一些基本信息（例如名称）。 当使用 Kubernetes API 创建对象时（或者直接创建，或者基于kubectl），API 请求必须在请求体中包含 JSON 格式的信息。 大多数情况下，需要在 .yaml 文件中为 kubectl 提供这些信息。 kubectl 在发起 API 请求时，将这些信息转换成 JSON 格式。\n这里有一个 .yaml 示例文件，展示了 Kubernetes Deployment 的必需字段和对象规约：\nfile=\u0026quot;application/deployment.yaml\u0026rdquo; \u0026gt;}}\n使用类似于上面的 .yaml 文件来创建 Deployment，一种方式是使用 kubectl 命令行接口（CLI）中的 kubectl apply 命令， 将 .yaml 文件作为参数。下面是一个示例：\nkubectl apply -f https://k8s.io/examples/application/deployment.yaml --record 输出类似如下这样：\ndeployment.apps/nginx-deployment created 必需字段 在想要创建的 Kubernetes 对象对应的 .yaml 文件中，需要配置如下的字段：\n apiVersion - 创建该对象所使用的 Kubernetes API 的版本 kind - 想要创建的对象的类型 metadata - 帮助识别对象唯一性的数据，包括一个 name 字符串、UID 和可选的 namespace  您也需要提供对象的 spec 字段。对象 spec 的精确格式对每个 Kubernetes 对象来说是不同的，包含了特定于该对象的嵌套字段。[Kubernetes API 参考](/docs/reference/generated/kubernetes-api/\u0026lt; param \u0026ldquo;version\u0026rdquo; \u0026gt;}}/)能够帮助我们找到任何我们想创建的对象的 spec 格式。 例如，可以从 [这里](/docs/reference/generated/kubernetes-api/\u0026lt; param \u0026ldquo;version\u0026rdquo; \u0026gt;}}/#podspec-v1-core) 查看 Pod 的 spec 格式， 并且可以从 [这里](/docs/reference/generated/kubernetes-api/\u0026lt; param \u0026ldquo;version\u0026rdquo; \u0026gt;}}/#deploymentspec-v1-apps) 查看 Deployment 的 spec 格式。\n  Kubernetes API 概述 提供关于 API 概念的进一步阐述 了解最重要的 Kubernetes 基本对象，例如 Pod。 了解 Kubernetes 中的控制器。  "
},
{
	"uri": "https://lijun.in/tasks/administer-cluster/kubeadm/",
	"title": "用 kubeadm 进行管理",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lijun.in/concepts/extend-kubernetes/compute-storage-net/network-plugins/",
	"title": "网络插件",
	"tags": [],
	"description": "",
	"content": "state=\u0026quot;alpha\u0026rdquo; \u0026gt;}}\nAlpha 特性迅速变化。\nKubernetes中的网络插件有几种类型：\n CNI 插件： 遵守 appc/CNI 规约，为互操作性设计。 Kubenet 插件：使用 bridge 和 host-local CNI 插件实现了基本的 cbr0。  安装 kubelet 有一个单独的默认网络插件，以及一个对整个集群通用的默认网络。 它在启动时探测插件，记住找到的内容，并在 pod 生命周期的适当时间执行所选插件（这仅适用于 Docker，因为 rkt 管理自己的 CNI 插件）。 在使用插件时，需要记住两个 Kubelet 命令行参数：\n cni-bin-dir： Kubelet 在启动时探测这个目录中的插件 network-plugin： 要使用的网络插件来自 cni-bin-dir。它必须与从插件目录探测到的插件报告的名称匹配。对于 CNI 插件，其值为 \u0026ldquo;cni\u0026rdquo;。  网络插件要求 除了提供[NetworkPlugin 接口](https://github.com/kubernetes/kubernetes/tree/param \u0026ldquo;fullversion\u0026rdquo; \u0026gt;}}/pkg/kubelet/dockershim/network/plugins.go)来配置和清理 pod 网络之外，该插件还可能需要对 kube-proxy 的特定支持。 iptables 代理显然依赖于 iptables，插件可能需要确保 iptables 能够监控容器的网络通信。 例如，如果插件将容器连接到 Linux 网桥，插件必须将 net/bridge/bridge-nf-call-iptables 系统参数设置为1，以确保 iptables 代理正常工作。 如果插件不使用 Linux 网桥（而是类似于 Open vSwitch 或者其它一些机制），它应该确保为代理对容器通信执行正确的路由。\n默认情况下，如果未指定 kubelet 网络插件，则使用 noop 插件，该插件设置 net/bridge/bridge-nf-call-iptables=1，以确保简单的配置（如带网桥的 Docker ）与 iptables 代理正常工作。\nCNI 通过给 Kubelet 传递 --network-plugin=cni 命令行选项来选择 CNI 插件。 Kubelet 从 --cni-conf-dir （默认是 /etc/cni/net.d） 读取文件并使用该文件中的 CNI 配置来设置每个 pod 的网络。 CNI 配置文件必须与 CNI 规约匹配，并且配置引用的任何所需的 CNI 插件都必须存在于 --cni-bin-dir（默认是 /opt/cni/bin）。\n如果这个目录中有多个 CNI 配置文件，则使用按文件名的字典顺序排列的第一个配置文件。\n除了配置文件指定的 CNI 插件外，Kubernetes 还需要标准的 CNI lo 插件，最低版本是0.2.0。\n支持 hostPort CNI 网络插件支持 hostPort。 您可以使用官方 portmap 插件，它由 CNI 插件团队提供，或者使用您自己的带有 portMapping 功能的插件。\n如果你想要启动 hostPort 支持，则必须在 cni-conf-dir 指定 portMappings capability。 例如：\n{ \u0026#34;name\u0026#34;: \u0026#34;k8s-pod-network\u0026#34;, \u0026#34;cniVersion\u0026#34;: \u0026#34;0.3.0\u0026#34;, \u0026#34;plugins\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;calico\u0026#34;, \u0026#34;log_level\u0026#34;: \u0026#34;info\u0026#34;, \u0026#34;datastore_type\u0026#34;: \u0026#34;kubernetes\u0026#34;, \u0026#34;nodename\u0026#34;: \u0026#34;127.0.0.1\u0026#34;, \u0026#34;ipam\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;host-local\u0026#34;, \u0026#34;subnet\u0026#34;: \u0026#34;usePodCidr\u0026#34; }, \u0026#34;policy\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;k8s\u0026#34; }, \u0026#34;kubernetes\u0026#34;: { \u0026#34;kubeconfig\u0026#34;: \u0026#34;/etc/cni/net.d/calico-kubeconfig\u0026#34; } }, { \u0026#34;type\u0026#34;: \u0026#34;portmap\u0026#34;, \u0026#34;capabilities\u0026#34;: {\u0026#34;portMappings\u0026#34;: true} } ] } 支持流量整形 CNI 网络插件还支持 pod 入口和出口流量整形。 您可以使用 CNI 插件团队提供的 bandwidth 插件， 也可以使用您自己的具有带宽控制功能的插件。\n如果您想要启用流量整形支持，你必须将 bandwidth 插件添加到 CNI 配置文件 （默认是 /etc/cni/net.d）。\n{ \u0026#34;name\u0026#34;: \u0026#34;k8s-pod-network\u0026#34;, \u0026#34;cniVersion\u0026#34;: \u0026#34;0.3.0\u0026#34;, \u0026#34;plugins\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;calico\u0026#34;, \u0026#34;log_level\u0026#34;: \u0026#34;info\u0026#34;, \u0026#34;datastore_type\u0026#34;: \u0026#34;kubernetes\u0026#34;, \u0026#34;nodename\u0026#34;: \u0026#34;127.0.0.1\u0026#34;, \u0026#34;ipam\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;host-local\u0026#34;, \u0026#34;subnet\u0026#34;: \u0026#34;usePodCidr\u0026#34; }, \u0026#34;policy\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;k8s\u0026#34; }, \u0026#34;kubernetes\u0026#34;: { \u0026#34;kubeconfig\u0026#34;: \u0026#34;/etc/cni/net.d/calico-kubeconfig\u0026#34; } }, { \u0026#34;type\u0026#34;: \u0026#34;bandwidth\u0026#34;, \u0026#34;capabilities\u0026#34;: {\u0026#34;bandwidth\u0026#34;: true} } ] } 现在，您可以将 kubernetes.io/ingress-bandwidth 和 kubernetes.io/egress-bandwidth 注解添加到 pod 中。 例如：\napiVersion: v1 kind: Pod metadata: annotations: kubernetes.io/ingress-bandwidth: 1M kubernetes.io/egress-bandwidth: 1M ... kubenet Kubenet 是一个非常基本的、简单的网络插件，仅适用于 Linux。 它本身并不实现更高级的功能，如跨节点网络或网络策略。 它通常与云驱动一起使用，云驱动为节点间或单节点环境中的通信设置路由规则。\nKubenet 创建名为 cbr0 的网桥，并为每个 pod 创建了一个 veth 对，每个 pod 的主机端都连接到 cbr0。 这个 veth 对的 pod 端会被分配一个 IP 地址，该 IP 地址隶属于节点所被分配的 IP 地址范围内。节点的 IP 地址范围则通过配置或控制器管理器来设置。 cbr0 被分配一个 MTU，该 MTU 匹配主机上已启用的正常接口的最小 MTU。\n使用此插件还需要一些其他条件：\n 需要标准的 CNI bridge、lo 以及 host-local 插件，最低版本是0.2.0。Kubenet 首先在 /opt/cni/bin 中搜索它们。 指定 cni-bin-dir 以提供其它的搜索路径。首次找到的匹配将生效。 Kubelet 必须和 --network-plugin=kubenet 参数一起运行，才能启用该插件。 Kubelet 还应该和 --non-masquerade-cidr=\u0026lt;clusterCidr\u0026gt; 参数一起运行，以确保超出此范围的 IP 流量将使用 IP 伪装。 节点必须被分配一个 IP 子网，通过kubelet 命令行的 --pod-cidr 选项或控制器管理器的命令行选项 --allocate-node-cidrs=true --cluster-cidr=\u0026lt;cidr\u0026gt; 来设置。  自定义 MTU（使用 kubenet） 要获得最佳的网络性能，必须确保 MTU 的取值配置正确。 网络插件通常会尝试推断出一个合理的 MTU，但有时候这个逻辑不会产生一个最优的 MTU。 例如，如果 Docker 网桥或其他接口有一个小的 MTU, kubenet 当前将选择该 MTU。 或者如果您正在使用 IPSEC 封装，则必须减少 MTU，并且这种计算超出了大多数网络插件的能力范围。\n如果需要，您可以使用 network-plugin-mtu kubelet 选项显式的指定 MTU。 例如：在 AWS 上 eth0 MTU 通常是 9001，因此您可以指定 --network-plugin-mtu=9001。 如果您正在使用 IPSEC ，您可以减少它以允许封装开销，例如 --network-plugin-mtu=8873。\n此选项会传递给网络插件； 当前 仅 kubenet 支持 network-plugin-mtu。\n使用总结  --network-plugin=cni 用来表明我们要使用 cni 网络插件，实际的 CNI 插件可执行文件位于 --cni-bin-dir（默认是 /opt/cni/bin）下， CNI 插件配置位于 --cni-conf-dir（默认是 /etc/cni/net.d）下。 --network-plugin=kubenet 用来表明我们要使用 kubenet 网络插件，CNI bridge 和 host-local 插件位于 /opt/cni/bin 或 cni-bin-dir 中。 --network-plugin-mtu=9001 指定了我们使用的 MTU，当前仅被 kubenet 网络插件使用。   "
},
{
	"uri": "https://lijun.in/tasks/access-application-cluster/web-ui-dashboard/",
	"title": "网页界面 (Dashboard)",
	"tags": [],
	"description": "",
	"content": "Dashboard 是基于网页的 Kubernetes 用户界面。您可以使用 Dashboard 将容器应用部署到 Kubernetes 集群中，也可以对容器应用排错，还能管理集群资源。您可以使用 Dashboard 获取运行在集群中的应用的概览信息，也可以创建或者修改 Kubernetes 资源（如 Deployment，Job，DaemonSet 等等）。例如，您可以对 Deployment 实现弹性伸缩、发起滚动升级、重启 Pod 或者使用向导创建新的应用。\nDashboard 同时展示了 Kubernetes 集群中的资源状态信息和所有报错信息。\n部署 Dashboard UI 默认情况下不会部署 Dashboard。可以通过以下命令部署：\nkubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0/aio/deploy/recommended.yaml 访问 Dashboard UI 为了保护您的集群数据，默认情况下，Dashboard 会使用最少的 RBAC 配置进行部署。 当前，Dashboard 仅支持使用 Bearer 令牌登录。 要为此样本演示创建令牌，您可以按照创建示例用户上的指南进行操作。\n. warning \u0026gt;}} 在教程中创建的样本用户将具有管理特权，并且仅用于教育目的。 . /warning \u0026gt;}}\n命令行代理 您可以使用 kubectl 命令行工具访问 Dashboard，命令如下：\nkubectl proxy kubectl 会使得 Dashboard 可以通过 http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/ 访问。\nUI 只能 通过执行这条命令的机器进行访问。更多选项参见 kubectl proxy --help。\n. note \u0026gt;}} Kubeconfig 身份验证方法不支持外部身份提供程序或基于 x509 证书的身份验证。 . /note \u0026gt;}}\n欢迎界面 当访问空集群的 Dashboard 时，您会看到欢迎界面。页面包含一个指向此文档的链接，以及一个用于部署第一个应用程序的按钮。此外，您可以看到在默认情况下有哪些默认系统应用运行在 kube-system 命名空间 中，比如 Dashboard 自己。\n部署容器化应用 通过一个简单的部署向导，您可以使用 Dashboard 将容器化应用作为一个 Deployment 和可选的 Service 进行创建和部署。可以手工指定应用的详细配置，或者上传一个包含应用配置的 YAML 或 JSON 文件。\n点击任何页面右上角的 创建 按钮以开始。\n指定应用的详细配置 部署向导需要您提供以下信息：\n 应用名称（必填）：应用的名称。内容为应用名称的标签 会被添加到任何将被部署的 Deployment 和 Service。  在选定的 Kubernetes 命名空间 中，应用名称必须唯一。必须由小写字母开头，以数字或者小写字母结尾，并且只含有小写字母、数字和中划线（-）。小于等于24个字符。开头和结尾的空格会被忽略。\n 容器镜像（必填）：公共镜像仓库上的 Docker 容器镜像 或者私有镜像仓库（通常是 Google Container Registery 或者 Docker Hub）的 URL。容器镜像参数说明必须以冒号结尾。   pod 的数量（必填）：您希望应用程序部署的 Pod 的数量。值必须为正整数。  系统会创建一个 Deployment 用于保证集群中运行了期望的 Pod 数量。\n 服务（可选）：对于部分应用（比如前端），您可能想对外暴露一个 Service ，这个 Service（外部 Service）可能用的是集群之外的公网 IP 地址。对于外部 Service 的情况，需要开放一个或者多个端口来满足。更多信息请参考 这里。  其它只能对集群内部可见的 Service 称为内部 Service。\n不管哪种 Service 类型，如果您选择创建一个 Service，而且容器在一个端口上开启了监听（入向的），那么您需要定义两个端口。创建的 Service 会把（入向的）端口映射到容器可见的目标端口。该 Service 会把流量路由到您部署的 Pod。支持的协议有 TCP 和 UDP。这个 Service 的内部 DNS 解析名就是之前您定义的应用名称的值。\n如果需要，您可以打开 高级选项 部分，这里您可以定义更多设置：\n 描述：这里您输入的文本会作为一个 注解 添加到 Deployment，并显示在应用的详细信息中。   标签：应用默认使用的 标签 是应用名称和版本。您可以为 Deployment、Service（如果有）定义额外的标签，比如 release（版本）、environment（环境）、tier（层级）、partition（分区） 和 release track（版本跟踪）。  例子：\nrelease=1.0 tier=frontend environment=pod track=stable  命名空间：Kubernetes 支持多个虚拟集群依附于同一个物理集群。这些虚拟集群被称为 命名空间，可以让您将资源划分为逻辑命名的组。  Dashboard 通过下拉菜单提供所有可用的命名空间，并允许您创建新的命名空间。命名空间的名称最长可以包含 63 个字母或数字和中横线（-），但是不能包含大写字母。\n命名空间的名称不能只包含数字。如果名字被设置成一个数字，比如 10，pod 就\n在 namespace 创建成功的情况下，默认会使用新创建的命名空间。如果创建失败，那么第一个命名空间会被选中。\n 镜像拉取 Secret：如果要使用私有的 Docker 容器镜像，需要拉取 secret 凭证。  Dashboard 通过下拉菜单提供所有可用的 secret，并允许您创建新的 secret。secret 名称必须遵循 DNS 域名语法，比如 new.image-pull.secret。secret 的内容必须是 base64 编码的，并且在一个 .dockercfg 文件中声明。secret 名称最大可以包含 253 个字符。\n在镜像拉取 secret 创建成功的情况下，默认会使用新创建的 secret。如果创建失败，则不会使用任何 secret。\n CPU 需求（核数）和内存需求（MiB）：您可以为容器定义最小的 资源限制。默认情况下，Pod 没有 CPU 和内存限制。   运行命令和运行命令参数：默认情况下，您的容器会运行 Docker 镜像的默认 入口命令。您可以使用 command 选项覆盖默认值。   以特权运行：这个设置决定了在 特权容器 中运行的进程是否像主机中使用 root 运行的进程一样。特权容器可以使用诸如操纵网络堆栈和访问设备的功能。   环境变量：Kubernetes 通过 环境变量 暴露 Service。您可以构建环境变量，或者将环境变量的值作为参数传递给您的命令。它们可以被应用用于查找 Service。值可以通过 $(VAR_NAME) 语法关联其他变量。  上传 YAML 或者 JSON 文件 Kubernetes 支持声明式配置。所有的配置都存储在遵循 Kubernetes API 架构的 YAML 或者 JSON 配置文件中。\n作为一种替代在部署向导中指定应用详情的方式，您可以在 YAML 或者 JSON 文件中定义应用，并且使用 Dashboard 上传文件：\n使用 Dashboard 以下各节描述了 Kubernetes Dashboard UI 视图；包括它们提供的内容，以及怎么使用它们。\n导航栏 当在集群中定义 Kubernetes 对象时，Dashboard 会在初始视图中显示它们。默认情况下只会显示 默认 命名空间中的对象，可以通过更改导航栏菜单中的命名空间筛选器进行改变。\nDashboard 展示大部分 Kubernetes 对象，并将它们分组放在几个菜单类别中。\n管理概述 集群和命名空间管理的视图, Dashboard 会列出节点、命名空间和持久卷，并且有它们的详细视图。节点列表视图包含从所有节点聚合的 CPU 和内存使用的度量值。详细信息视图显示了一个节点的度量值，它的规格、状态、分配的资源、事件和这个节点上运行的 Pod。\n负载 显示选中的命名空间中所有运行的应用。视图按照负载类型（如 Deployment、ReplicaSet、StatefulSet 等）罗列应用，并且每种负载都可以单独查看。列表总结了关于负载的可执行信息，比如一个 ReplicaSet 的准备状态的 Pod 数量，或者目前一个 Pod 的内存使用量。\n工作负载的详情视图展示了对象的状态、详细信息和相互关系。例如，ReplicaSet 所控制的 Pod，或者 Deployment 关联的 新 ReplicaSet 和 Pod 水平扩展控制器。\n服务 展示允许暴露给外网服务和允许集群内部发现的 Kubernetes 资源。因此，Service 和 Ingress 视图展示他们关联的 Pod、给集群连接使用的内部端点和给外部用户使用的外部端点。\n存储 存储视图展示持久卷申领（PVC）资源，这些资源被应用程序用来存储数据。\n配置 展示的所有 Kubernetes 资源是在集群中运行的应用程序的实时配置。通过这个视图可以编辑和管理配置对象，并显示那些默认隐藏的 secret。\n日志查看器 Pod 列表和详细信息页面可以链接到 Dashboard 内置的日志查看器。查看器可以钻取属于同一个 Pod 的不同容器的日志。\n. heading \u0026ldquo;whatsnext\u0026rdquo; %}} 更多信息，参见 Kubernetes Dashboard 项目页面.\n"
},
{
	"uri": "https://lijun.in/concepts/architecture/nodes/",
	"title": "节点",
	"tags": [],
	"description": "",
	"content": "在 Kubernetes 中，节点（Node）是执行工作的机器，以前叫做 minion。根据你的集群环境，节点可以是一个虚拟机或者物理机器。每个节点都包含用于运行 pods 的必要服务，并由主控组件管理。节点上的服务包括 容器运行时、kubelet 和 kube-proxy。查阅架构设计文档中 Kubernetes 节点 一节获取更多细节。\n节点状态 一个节点的状态包含以下信息:\n 地址 条件 容量与可分配 信息  可以使用以下命令显示节点状态和有关节点的其他详细信息：\nkubectl describe node \u0026lt;insert-node-name-here\u0026gt; 下面对每个章节进行详细描述。\n地址 这些字段组合的用法取决于你的云服务商或者裸机配置。\n HostName：由节点的内核指定。可以通过 kubelet 的 --hostname-override 参数覆盖。 ExternalIP：通常是可以外部路由的节点 IP 地址（从集群外可访问）。 InternalIP：通常是仅可在集群内部路由的节点 IP 地址。  条件 conditions 字段描述了所有 Running 节点的状态。条件的示例包括：\n   节点条件 描述     OutOfDisk True 表示节点的空闲空间不足以用于添加新 pods, 否则为 False   Ready 表示节点是健康的并已经准备好接受 pods；False 表示节点不健康而且不能接受 pods；Unknown 表示节点控制器在最近 40 秒内没有收到节点的消息   MemoryPressure True 表示节点存在内存压力 \u0026ndash; 即节点内存用量低，否则为 False   PIDPressure True 表示节点存在进程压力 \u0026ndash; 即进程过多；否则为 False   DiskPressure True 表示节点存在磁盘压力 \u0026ndash; 即磁盘可用量低，否则为 False   NetworkUnavailable True 表示节点网络配置不正确；否则为 False    节点条件使用一个 JSON 对象表示。例如，下面的响应描述了一个健康的节点。\n\u0026#34;conditions\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;Ready\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;True\u0026#34;, \u0026#34;reason\u0026#34;: \u0026#34;KubeletReady\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;kubelet is posting ready status\u0026#34;, \u0026#34;lastHeartbeatTime\u0026#34;: \u0026#34;2019-06-05T18:38:35Z\u0026#34;, \u0026#34;lastTransitionTime\u0026#34;: \u0026#34;2019-06-05T11:41:27Z\u0026#34; } ] 如果 Ready 条件处于状态 Unknown 或者 False 的时间超过了 pod-eviction-timeout（一个传递给 kube-controller-manager 的参数），节点上的所有 Pods 都会被节点控制器计划删除。默认的删除超时时长为5 分钟。某些情况下，当节点不可访问时，apiserver 不能和其上的 kubelet 通信。删除 pods 的决定不能传达给 kubelet，直到它重新建立和 apiserver 的连接为止。与此同时，被计划删除的 pods 可能会继续在分区节点上运行。\n在 1.5 版本之前的 Kubernetes 里，节点控制器会将不能访问的 pods 从 apiserver 中强制删除。但在 1.5 或更高的版本里，在节点控制器确认这些 pods 已经在集群停止运行前不会强制删除它们。你可以看到这些处于 Terminating 或者 Unknown 状态的 pods 可能在无法访问的节点上运行。为了防止 kubernetes 不能从底层基础设施中推断出一个节点是否已经永久的离开了集群，集群管理员可能需要手动删除这个节点对象。从 Kubernetes 删除节点对象将导致 apiserver 删除节点上所有运行的 Pod 对象并释放它们的名字。\n节点生命周期控制器会自动创建代表条件的污点。 当调度器将 Pod 分配给节点时，调度器会考虑节点上的污点，但是 Pod 可以容忍的污点除外。\n容量与可分配 描述节点上的可用资源：CPU、内存和可以调度到节点上的 pods 的最大数量。\ncapacity 块中的字段指示节点拥有的资源总量。allocatable 块指示节点上可供普通 Pod 消耗的资源量。\n可以在学习如何在节点上保留计算资源的同时阅读有关容量和可分配资源的更多信息。\n信息 关于节点的通用信息，例如内核版本、Kubernetes 版本（kubelet 和 kube-proxy 版本）、Docker 版本（如果使用了）和操作系统名称。这些信息由 kubelet 从节点上搜集而来。\n管理 与 pods 和 services 不同，节点并不是在 Kubernetes 内部创建的：它是被外部的云服务商创建，例如 Google Compute Engine 或者你的集群中的物理或者虚拟机。这意味着当 Kubernetes 创建一个节点时，它其实仅仅创建了一个对象来代表这个节点。创建以后，Kubernetes 将检查这个节点是否可用。例如，如果你尝试使用如下内容创建一个节点：\n{ \u0026#34;kind\u0026#34;: \u0026#34;Node\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;v1\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;10.240.79.157\u0026#34;, \u0026#34;labels\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;my-first-k8s-node\u0026#34; } } } Kubernetes 会在内部创一个 Node 对象（用以表示节点），并基于 metadata.name 字段执行健康检查，对节点进行验证。如果节点可用，意即所有必要服务都已运行，它就符合了运行一个 pod 的条件；否则它将被所有的集群动作忽略直到变为可用。\n当前，有 3 个组件同 Kubernetes 节点接口交互：节点控制器、kubelet 和 kubectl。\n节点控制器 节点控制器是一个 Kubernetes master 组件，管理节点的方方面面。\n节点控制器在节点的生命周期中扮演了多个角色。第一个是当节点注册时为它分配一个 CIDR block（如果打开了 CIDR 分配）。\n第二个是使用云服务商提供了可用节点列表保持节点控制器内部的节点列表更新。如果在云环境下运行，任何时候当一个节点不健康时节点控制器将询问云服务节点的虚拟机是否可用。如果不可用，节点控制器会将这个节点从它的节点列表删除。\n第三个是监控节点的健康情况。节点控制器负责在节点不能访问时（也即是节点控制器因为某些原因没有收到心跳，例如节点宕机）将它的 NodeStatus 的 NodeReady 状态更新为 ConditionUnknown。后续如果节点持续不可访问，节点控制器将删除节点上的所有 pods（使用优雅终止）。（默认情况下 40s 开始报告 ConditionUnknown，在那之后 5m 开始删除 pods。）节点控制器每隔 --node-monitor-period 秒检查每个节点的状态。\n心跳机制 Kubernetes 节点发送的心跳有助于确定节点的可用性。 心跳有两种形式：NodeStatus 和 Lease 对象。 每个节点在 kube-node-lease\u0026quot;命名空间\u0026rdquo; 中都有一个关联的 Lease 对象。 Lease 是一种轻量级的资源，可在集群扩展时提高节点心跳机制的性能。\nkubelet 负责创建和更新 NodeStatus 和 Lease 对象。\n 当状态发生变化时，或者在配置的时间间隔内没有更新时，kubelet 会更新 NodeStatus。 NodeStatus 更新的默认间隔为 5 分钟（比无法访问的节点的 40 秒默认超时时间长很多）。 kubelet 会每 10 秒（默认更新间隔时间）创建并更新其 Lease 对象。Lease 更新独立于 NodeStatus 更新而发生。  可靠性 在 Kubernetes 1.4 中我们更新了节点控制器逻辑以更好地处理大批量节点访问 master 出问题的情况（例如 master 的网络出了问题）。从 1.4 开始，节点控制器在决定删除 pod 之前会检查集群中所有节点的状态。\n大部分情况下，节点控制器把驱逐频率限制在每秒 --node-eviction-rate 个（默认为 0.1）。这表示它每 10 秒钟内至多从一个节点驱逐 Pods。\n当一个可用区域中的节点变为不健康时，它的驱逐行为将发生改变。节点控制器会同时检查可用区域中不健康（NodeReady 状态为 ConditionUnknown 或 ConditionFalse）的节点的百分比。如果不健康节点的部分超过 --unhealthy-zone-threshold （默认为 0.55），驱逐速率将会减小：如果集群较小（意即小于等于 --large-cluster-size-threshold 个 节点 - 默认为 50），驱逐操作将会停止，否则驱逐速率将降为每秒 --secondary-node-eviction-rate 个（默认为 0.01）。在单个可用区域实施这些策略的原因是当一个可用区域可能从 master 分区时其它的仍然保持连接。如果你的集群没有跨越云服务商的多个可用区域，那就只有一个可用区域整个集群）。\n在多个可用区域分布你的节点的一个关键原因是当整个可用区域故障时，工作负载可以转移到健康的可用区域。因此，如果一个可用区域中的所有节点都不健康时，节点控制器会以正常的速率 --node-eviction-rate 进行驱逐操作。在所有的可用区域都不健康（也即集群中没有健康节点）的极端情况下，节点控制器将假设 master 的连接出了某些问题，它将停止所有驱逐动作直到一些连接恢复。\n从 Kubernetes 1.6 开始，NodeController 还负责驱逐运行在拥有 NoExecute 污点的节点上的 pods，如果这些 pods 没有容忍这些污点。此外，作为一个默认禁用的 alpha 特性，NodeController 还负责根据节点故障（例如节点不可访问或没有 ready）添加污点。请查看这个文档了解关于 NoExecute 污点和这个 alpha 特性。\n从版本 1.8 开始，可以使节点控制器负责创建代表节点条件的污点。这是版本 1.8 的 Alpha 功能。\n节点自注册 当 kubelet 标志 --register-node 为 true （默认）时，它会尝试向 API 服务注册自己。这是首选模式，被绝大多数发行版选用。\n对于自注册模式，kubelet 使用下列参数启动：\n --kubeconfig - 用于向 apiserver 验证自己的凭据路径。 --cloud-provider - 如何从云服务商读取关于自己的元数据。 --register-node - 自动向 API 服务注册。 --register-with-taints - 使用 taints 列表（逗号分隔的 \u0026lt;key\u0026gt;=\u0026lt;value\u0026gt;:\u0026lt;effect\u0026gt;）注册节点。当 register-node 为 false 时无效。 --node-ip - 节点 IP 地址。 --node-labels - 在集群中注册节点时要添加的标签（请参阅 NodeRestriction 准入插件 在 1.13+ 中实施的标签限制）。 --node-status-update-frequency - 指定 kubelet 向 master 发送状态的频率。  启用节点授权模式 和 NodeRestriction 准入插件时，仅授权小组件创建或修改其自己的节点资源。\n手动节点管理 集群管理员可以创建及修改节点对象。\n如果管理员希望手动创建节点对象，请设置 kubelet 标记 --register-node=false。\n管理员可以修改节点资源（忽略 --register-node 设置）。修改包括在节点上设置 labels 及标记它为不可调度。\n节点上的 labels 可以和 pods 的节点 selectors 一起使用来控制调度，例如限制一个 pod 只能在一个符合要求的节点子集上运行。\n标记一个节点为不可调度的将防止新建 pods 调度到那个节点之上，但不会影响任何已经在它之上的 pods。这是重启节点等操作之前的一个有用的准备步骤。例如，标记一个节点为不可调度的，执行以下命令：\nkubectl cordon $NODENAME 请注意，被 daemonSet 控制器创建的 pods 将忽略 Kubernetes 调度器，且不会遵照节点上不可调度的属性。这个假设基于守护程序属于节点机器，即使在准备重启而隔离应用的时候。\n节点容量 节点的容量（cpu 数量和内存容量）是节点对象的一部分。通常情况下，在创建节点对象时，它们会注册自己并报告自己的容量。如果你正在执行手动节点管理，那么你需要在添加节点时手动设置节点容量。\nKubernetes 调度器保证一个节点上有足够的资源供其上的所有 pods 使用。它会检查节点上所有容器要求的总和不会超过节点的容量。这包括由 kubelet 启动的所有容器，但不包括由 container runtime 直接启动的容器，也不包括在容器外部运行的任何进程。\n如果要为非 Pod 进程显式保留资源。请按照本教程为系统守护程序保留资源。\n节点拓扑 如果启用了 TopologyManager 功能开关，则 kubelet 可以在做出资源分配决策时使用拓扑提示。\nAPI 对象 节点是 Kubernetes REST API 的顶级资源。更多关于 API 对象的细节可以在这里找到：[节点 API 对象](/docs/reference/generated/kubernetes-api/ param \u0026ldquo;version\u0026rdquo; \u0026gt;}}/#node-v1-core)。\n 了解有关节点组件的信息。 阅读有关节点级拓扑的信息：控制节点上的拓扑管理策略。  "
},
{
	"uri": "https://lijun.in/concepts/policy/resource-quotas/",
	"title": "资源配额",
	"tags": [],
	"description": "",
	"content": "当多个用户或团队共享具有固定节点数目的集群时，人们会担心有人使用超过其基于公平原则所分配到的资源量。\n资源配额是帮助管理员解决这一问题的工具。\n资源配额，通过 ResourceQuota 对象来定义，对每个命名空间的资源消耗总量提供限制。它可以限制命名空间中某种类型的对象的总数目上限，也可以限制命令空间中的 Pod 可以使用的计算资源的总上限。\n资源配额的工作方式如下：\n 不同的团队可以在不同的命名空间下工作，目前这是非约束性的，在未来的版本中可能会通过 ACL (Access Control List 访问控制列表) 来实现强制性约束。 集群管理员可以为每个命名空间创建一个或多个资源配额对象。 当用户在命名空间下创建资源（如 Pod、Service 等）时，Kubernetes 的配额系统会跟踪集群的资源使用情况，以确保使用的资源用量不超过资源配额中定义的硬性资源限额。 如果资源创建或者更新请求违反了配额约束，那么该请求会报错（HTTP 403 FORBIDDEN），并在消息中给出有可能违反的约束。 如果命名空间下的计算资源 （如 cpu 和 memory）的配额被启用，则用户必须为这些资源设定请求值（request）和约束值（limit），否则配额系统将拒绝 Pod 的创建。 提示: 可使用 LimitRanger 准入控制器来为没有设置计算资源需求的 Pod 设置默认值。 若想避免这类问题，请参考演练中的示例。  下面是使用命名空间和配额构建策略的示例：\n 在具有 32 GiB 内存和 16 核 CPU 资源的集群中，允许 A 团队使用 20 GiB 内存 和 10 核的 CPU 资源，允许 B 团队使用 10 GiB 内存和 4 核的 CPU 资源，并且预留 2 GiB 内存和 2 核的 CPU 资源供将来分配。 限制 \u0026ldquo;testing\u0026rdquo; 命名空间使用 1 核 CPU 资源和 1GiB 内存。允许 \u0026ldquo;production\u0026rdquo; 命名空间使用任意数量。  在集群容量小于各命名空间配额总和的情况下，可能存在资源竞争。资源竞争时，Kubernetes 系统会遵循先到先得的原则。\n不管是资源竞争还是配额的修改，都不会影响已经创建的资源使用对象。\n启用资源配额 资源配额的支持在很多 Kubernetes 版本中是默认开启的。当 apiserver --enable-admission-plugins= 参数中包含 ResourceQuota 时，资源配额会被启用。\n当命名空间中存在一个 ResourceQuota 对象时，对于该命名空间而言，资源配额就是开启的。\n计算资源配额 用户可以对给定命名空间下的可被请求的计算资源总量进行限制。\n配额机制所支持的资源类型：\n   资源名称 描述     limits.cpu 所有非终止状态的 Pod，其 CPU 限额总量不能超过该值。   limits.memory 所有非终止状态的 Pod，其内存限额总量不能超过该值。   requests.cpu 所有非终止状态的 Pod，其 CPU 需求总量不能超过该值。   requests.memory 所有非终止状态的 Pod，其内存需求总量不能超过该值。    扩展资源的资源配额 除上述资源外，在 Kubernetes 1.10 版本中，还添加了对扩展资源的支持。\n由于扩展资源不可超量分配，因此没有必要在配额中为同一扩展资源同时指定 requests 和 limits。对于扩展资源而言，目前仅允许使用前缀为 requests. 的配额项。\n以 GPU 拓展资源为例，如果资源名称为 nvidia.com/gpu，并且要将命名空间中请求的 GPU 资源总数限制为 4，则可以如下定义配额：\n requests.nvidia.com/gpu: 4  有关更多详细信息，请参阅查看和设置配额。\n存储资源配额 用户可以对给定命名空间下的存储资源总量进行限制。\n此外，还可以根据相关的存储类（Storage Class）来限制存储资源的消耗。\n   资源名称 描述     requests.storage 所有 PVC，存储资源的需求总量不能超过该值。   persistentvolumeclaims 在该命名空间中所允许的 PVC 总量。   \u0026lt;storage-class-name\u0026gt;.storageclass.storage.k8s.io/requests.storage 在所有与 storage-class-name 相关的持久卷声明中，存储请求的总和不能超过该值。   \u0026lt;storage-class-name\u0026gt;.storageclass.storage.k8s.io/persistentvolumeclaims 在与 storage-class-name 相关的所有持久卷声明中，命名空间中可以存在的持久卷声明总数。    例如，如果一个操作人员针对 gold 存储类型与 bronze 存储类型设置配额，操作人员可以定义如下配额：\n gold.storageclass.storage.k8s.io/requests.storage: 500Gi bronze.storageclass.storage.k8s.io/requests.storage: 100Gi  在 Kubernetes 1.8 版本中，本地临时存储的配额支持已经是 Alpha 功能：\n   资源名称 描述     requests.ephemeral-storage 在命名空间的所有 Pod 中，本地临时存储请求的总和不能超过此值。   limits.ephemeral-storage 在命名空间的所有 Pod 中，本地临时存储限制值的总和不能超过此值。    对象数量配额 Kubernetes 1.9 版本增加了使用以下语法对所有标准的、命名空间域的资源类型进行配额设置的支持。\n count/\u0026lt;resource\u0026gt;.\u0026lt;group\u0026gt;  这是用户可能希望利用对象计数配额来管理的一组资源示例。\n count/persistentvolumeclaims count/services count/secrets count/configmaps count/replicationcontrollers count/deployments.apps count/replicasets.apps count/statefulsets.apps count/jobs.batch count/cronjobs.batch count/deployments.extensions  Kubernetes 1.15 版本增加了对使用相同语法来约束自定义资源的支持。 例如，要对 example.com API 组中的自定义资源 widgets 设置配额，请使用 count/widgets.example.com。\n当使用 count/* 资源配额时，如果对象存在于服务器存储中，则会根据配额管理资源。 这些类型的配额有助于防止存储资源耗尽。例如，用户可能想根据服务器的存储能力来对服务器中 Secret 的数量进行配额限制。集群中存在过多的 Secret 实际上会导致服务器和控制器无法启动！用户可以选择对 Job 进行配额管理，以防止配置不当的 CronJob 在某命名空间中创建太多作业而导致集群拒绝服务。\n在 Kubernetes 1.9 版本之前，可以在有限的一组资源上实施一般性的对象数量配额。 此外，还可以进一步按资源的类型设置其配额。\n支持以下类型：\n   资源名称 描述     configmaps 在该命名空间中允许存在的 ConfigMap 总数上限。   persistentvolumeclaims 在该命名空间中允许存在的 PVC 的总数上限。   pods 在该命名空间中允许存在的非终止状态的 pod 总数上限。Pod 终止状态等价于 Pod 的 .status.phase in (Failed, Succeeded) = true   replicationcontrollers 在该命名空间中允许存在的 RC 总数上限。   resourcequotas 在该命名空间中允许存在的资源配额总数上限。   services 在该命名空间中允许存在的 Service 总数上限。   services.loadbalancers 在该命名空间中允许存在的 LoadBalancer 类型的服务总数上限。   services.nodeports 在该命名空间中允许存在的 NodePort 类型的服务总数上限。   secrets 在该命名空间中允许存在的 Secret 总数上限。    例如，pods 配额统计某个命名空间中所创建的、非终止状态的 Pod 个数并确保其不超过某上限值。用户可能希望在某命名空间中设置 pods 配额，以避免有用户创建很多小的 Pod，从而耗尽集群所能提供的 Pod IP 地址。\n配额作用域 每个配额都有一组相关的作用域（scope），配额只会对作用域内的资源生效。配额机制仅统计所列举的作用域的交集中的资源用量。\n当一个作用域被添加到配额中后，它会对作用域相关的资源数量作限制。 如配额中指定了允许（作用域）集合之外的资源，会导致验证错误。\n   作用域 描述     Terminating 匹配所有 spec.activeDeadlineSeconds 不小于 0 的 Pod。   NotTerminating 匹配所有 spec.activeDeadlineSeconds 是 nil 的 Pod。   BestEffort 匹配所有 Qos 是 BestEffort 的 Pod。   NotBestEffort 匹配所有 Qos 不是 BestEffort 的 Pod。    BestEffort 作用域限制配额跟踪以下资源：pods\nTerminating、NotTerminating 和 NotBestEffort 这三种作用域限制配额跟踪以下资源：\n cpu limits.cpu limits.memory memory pods requests.cpu requests.memory  基于优先级类（PriorityClass）来设置资源配额 feature-state for_k8s_version=\u0026quot;1.12\u0026rdquo; state=\u0026quot;beta\u0026rdquo; \u0026gt;}}\nPod 可以创建为特定的优先级。 通过使用配额规约中的 scopeSelector 字段，用户可以根据 Pod 的优先级控制其系统资源消耗。\n仅当配额规范中的 scopeSelector 字段选择到某 Pod 时，配额机制才会匹配和计量 Pod 的资源消耗。\n本示例创建一个配额对象，并将其与具有特定优先级的 Pod 进行匹配。 该示例的工作方式如下：\n 集群中的 Pod 可取三个优先级类之一，即 \u0026ldquo;low\u0026rdquo;、\u0026ldquo;medium\u0026rdquo;、\u0026ldquo;high\u0026rdquo;。 为每个优先级创建一个配额对象。  将以下 YAML 保存到文件 quota.yml 中。\napiVersion: v1 kind: List items: - apiVersion: v1 kind: ResourceQuota metadata: name: pods-high spec: hard: cpu: \u0026#34;1000\u0026#34; memory: 200Gi pods: \u0026#34;10\u0026#34; scopeSelector: matchExpressions: - operator : In scopeName: PriorityClass values: [\u0026#34;high\u0026#34;] - apiVersion: v1 kind: ResourceQuota metadata: name: pods-medium spec: hard: cpu: \u0026#34;10\u0026#34; memory: 20Gi pods: \u0026#34;10\u0026#34; scopeSelector: matchExpressions: - operator : In scopeName: PriorityClass values: [\u0026#34;medium\u0026#34;] - apiVersion: v1 kind: ResourceQuota metadata: name: pods-low spec: hard: cpu: \u0026#34;5\u0026#34; memory: 10Gi pods: \u0026#34;10\u0026#34; scopeSelector: matchExpressions: - operator : In scopeName: PriorityClass values: [\u0026#34;low\u0026#34;] 使用 kubectl create 命令运行以下操作。\nkubectl create -f ./quota.yml resourcequota/pods-high created resourcequota/pods-medium created resourcequota/pods-low created 使用 kubectl describe quota 操作验证配额的 Used 值为 0。\nkubectl describe quota Name: pods-high Namespace: default Resource Used Hard -------- ---- ---- cpu 0 1k memory 0 200Gi pods 0 10 Name: pods-low Namespace: default Resource Used Hard -------- ---- ---- cpu 0 5 memory 0 10Gi pods 0 10 Name: pods-medium Namespace: default Resource Used Hard -------- ---- ---- cpu 0 10 memory 0 20Gi pods 0 10 创建优先级为 \u0026ldquo;high\u0026rdquo; 的 Pod。 将以下 YAML 保存到文件 high-priority-pod.yml 中。\napiVersion: v1 kind: Pod metadata: name: high-priority spec: containers: - name: high-priority image: ubuntu command: [\u0026#34;/bin/sh\u0026#34;] args: [\u0026#34;-c\u0026#34;, \u0026#34;while true; do echo hello; sleep 10;done\u0026#34;] resources: requests: memory: \u0026#34;10Gi\u0026#34; cpu: \u0026#34;500m\u0026#34; limits: memory: \u0026#34;10Gi\u0026#34; cpu: \u0026#34;500m\u0026#34; priorityClassName: high 使用 kubectl create 运行以下操作。\nkubectl create -f ./high-priority-pod.yml 确认 \u0026ldquo;high\u0026rdquo; 优先级配额 pods-high 的 \u0026ldquo;Used\u0026rdquo; 统计信息已更改，并且其他两个配额未更改。\nkubectl describe quota Name: pods-high Namespace: default Resource Used Hard -------- ---- ---- cpu 500m 1k memory 10Gi 200Gi pods 1 10 Name: pods-low Namespace: default Resource Used Hard -------- ---- ---- cpu 0 5 memory 0 10Gi pods 0 10 Name: pods-medium Namespace: default Resource Used Hard -------- ---- ---- cpu 0 10 memory 0 20Gi pods 0 10 scopeSelector 在 operator 字段中支持以下值：\n In NotIn Exist DoesNotExist  请求与限制 分配计算资源时，每个容器可以为 CPU 或内存指定请求和约束。 配额可以针对二者之一进行设置。\n如果配额中指定了 requests.cpu 或 requests.memory 的值，则它要求每个容器都显式给出对这些资源的请求。同理，如果配额中指定了 limits.cpu 或 limits.memory 的值，那么它要求每个容器都显式设定对应资源的限制。\n查看和设置配额 Kubectl 支持创建、更新和查看配额：\nkubectl create namespace myspace cat \u0026lt;\u0026lt;EOF \u0026gt; compute-resources.yaml apiVersion: v1 kind: ResourceQuota metadata: name: compute-resources spec: hard: requests.cpu: \u0026#34;1\u0026#34; requests.memory: 1Gi limits.cpu: \u0026#34;2\u0026#34; limits.memory: 2Gi requests.nvidia.com/gpu: 4 EOF kubectl create -f ./compute-resources.yaml --namespace=myspace cat \u0026lt;\u0026lt;EOF \u0026gt; object-counts.yaml apiVersion: v1 kind: ResourceQuota metadata: name: object-counts spec: hard: configmaps: \u0026#34;10\u0026#34; persistentvolumeclaims: \u0026#34;4\u0026#34; pods: \u0026#34;4\u0026#34; replicationcontrollers: \u0026#34;20\u0026#34; secrets: \u0026#34;10\u0026#34; services: \u0026#34;10\u0026#34; services.loadbalancers: \u0026#34;2\u0026#34; EOF kubectl create -f ./object-counts.yaml --namespace=myspace kubectl get quota --namespace=myspace NAME AGE compute-resources 30s object-counts 32s kubectl describe quota compute-resources --namespace=myspace Name: compute-resources Namespace: myspace Resource Used Hard -------- ---- ---- limits.cpu 0 2 limits.memory 0 2Gi requests.cpu 0 1 requests.memory 0 1Gi requests.nvidia.com/gpu 0 4 kubectl describe quota object-counts --namespace=myspace Name: object-counts Namespace: myspace Resource Used Hard -------- ---- ---- configmaps 0 10 persistentvolumeclaims 0 4 pods 0 4 replicationcontrollers 0 20 secrets 1 10 services 0 10 services.loadbalancers 0 2 kubectl 还使用语法 count/\u0026lt;resource\u0026gt;.\u0026lt;group\u0026gt; 支持所有标准的、命名空间域的资源的对象计数配额：\nkubectl create namespace myspace kubectl create quota test --hard=count/deployments.extensions=2,count/replicasets.extensions=4,count/pods=3,count/secrets=4 --namespace=myspace kubectl run nginx --image=nginx --replicas=2 --namespace=myspace kubectl describe quota --namespace=myspace Name: test Namespace: myspace Resource Used Hard -------- ---- ---- count/deployments.extensions 1 2 count/pods 2 3 count/replicasets.extensions 1 4 count/secrets 1 4 配额和集群容量 资源配额与集群资源总量是完全独立的。它们通过绝对的单位来配置。所以，为集群添加节点时，资源配额不会自动赋予每个命名空间消耗更多资源的能力。\n有时可能需要资源配额支持更复杂的策略，比如：\n 在几个团队中按比例划分总的集群资源。 允许每个租户根据需要增加资源使用量，但要有足够的限制以防止资源意外耗尽。 探测某个命名空间的需求，添加物理节点并扩大资源配额值。  这些策略可以通过将资源配额作为一个组成模块、手动编写一个控制器来监控资源使用情况，并结合其他信号调整命名空间上的硬性资源配额来实现。\n注意：资源配额对集群资源总体进行划分，但它对节点没有限制：来自不同命名空间的 Pod 可能在同一节点上运行。\n默认情况下限制特定优先级的资源消耗 有时候可能希望当且仅当某名字空间中存在匹配的配额对象时，才可以创建特定优先级（例如 \u0026ldquo;cluster-services\u0026rdquo;）的 Pod。\n通过这种机制，操作人员能够将限制某些高优先级类仅出现在有限数量的命名空间中，而并非每个命名空间默认情况下都能够使用这些优先级类。\n要实现此目的，应使用 kube-apiserver 标志 --admission-control-config-file 传递如下配置文件的路径：\ntabs name=\u0026quot;example1\u0026rdquo; \u0026gt;}} tab name=\u0026quot;apiserver.config.k8s.io/v1\u0026rdquo; %}}\napiVersion: apiserver.config.k8s.io/v1 kind: AdmissionConfiguration plugins: - name: \u0026#34;ResourceQuota\u0026#34; configuration: apiVersion: apiserver.config.k8s.io/v1 kind: ResourceQuotaConfiguration limitedResources: - resource: pods matchScopes: - scopeName: PriorityClass operator: In values: [\u0026#34;cluster-services\u0026#34;] /tab %}} tab name=\u0026quot;apiserver.k8s.io/v1alpha1\u0026rdquo; %}}\n# 在 Kubernetes 1.17 中已不被推荐使用，请使用 apiserver.config.k8s.io/v1 apiVersion: apiserver.k8s.io/v1alpha1 kind: AdmissionConfiguration plugins: - name: \u0026#34;ResourceQuota\u0026#34; configuration: # 在 Kubernetes 1.17 中已不被推荐使用，请使用 apiserver.config.k8s.io/v1, ResourceQuotaConfiguration apiVersion: resourcequota.admission.k8s.io/v1beta1 kind: Configuration limitedResources: - resource: pods matchScopes: - scopeName: PriorityClass operator: In values: [\u0026#34;cluster-services\u0026#34;] /tab %}} /tabs \u0026gt;}}\n现在，仅当命名空间中存在匹配的 scopeSelector 的配额对象时，才允许使用 \u0026ldquo;cluster-services\u0026rdquo; Pod。\n示例：\nscopeSelector: matchExpressions: - scopeName: PriorityClass operator: In values: [\u0026#34;cluster-services\u0026#34;] 有关更多信息，请参见 LimitedResources 和优先级类配额支持的设计文档。\n示例 查看如何使用资源配额的详细示例。\n 查看资源配额设计文档了解更多信息。\n"
},
{
	"uri": "https://lijun.in/tutorials/kubernetes-basics/scale/scale-intro/",
	"title": "运行应用程序的多个实例",
	"tags": [],
	"description": "",
	"content": "  Objectives -- 目的  Scale an app using kubectl. -- 用 kubectl 扩缩应用程序   Scaling an application -- 扩缩应用程序 In the previous modules we created a Deployment, and then exposed it publicly via a Service. The Deployment created only one Pod for running our application. When traffic increases, we will need to scale the application to keep up with user demand.\n-- 在之前的模块中，我们创建了一个 Deployment，然后通过 Service让其可以开放访问。Deployment 仅为跑这个应用程序创建了一个 Pod。 当流量增加时，我们需要扩容应用程序满足用户需求。\nScaling is accomplished by changing the number of replicas in a Deployment\n-- 扩缩 是通过改变 Deployment 中的副本数量来实现的。\n Summary: -- 小结:  Scaling a Deployment -- 扩缩一个 Deployment    You can create from the start a Deployment with multiple instances using the --replicas parameter for the kubectl run command \n--  在运行 kubectl run 命令时，你可以通过设置 --replicas 参数来设置 Deployment 的副本数。\n   Scaling overview -- 扩缩概述          Previous  Next     Scaling out a Deployment will ensure new Pods are created and scheduled to Nodes with available resources. Scaling in will reduce the number of Pods to the new desired state. Kubernetes also supports autoscaling of Pods, but it is outside of the scope of this tutorial. Scaling to zero is also possible, and it will terminate all Pods of the specified Deployment.\n-- 扩展 Deployment 将创建新的 Pods，并将资源调度请求分配到有可用资源的节点上，收缩 会将 Pods 数量减少至所需的状态。Kubernetes 还支持 Pods 的自动缩放，但这并不在本教程的讨论范围内。将 Pods 数量收缩到0也是可以的，但这会终止 Deployment 上所有已经部署的 Pods。\nRunning multiple instances of an application will require a way to distribute the traffic to all of them. Services have an integrated load-balancer that will distribute network traffic to all Pods of an exposed Deployment. Services will monitor continuously the running Pods using endpoints, to ensure the traffic is sent only to available Pods.\n-- 运行应用程序的多个实例需要在它们之间分配流量。服务 (Service)有一种负载均衡器类型，可以将网络流量均衡分配到外部可访问的 Pods 上。服务将会一直通过端点来监视 Pods 的运行，保证流量只分配到可用的 Pods 上。\n Scaling is accomplished by changing the number of replicas in a Deployment.\n-- 扩缩是通过改变 Deployment 中的副本数量来实现的。\n   Once you have multiple instances of an Application running, you would be able to do Rolling updates without downtime. We'll cover that in the next module. Now, let's go to the online terminal and scale our application.\n-- 一旦有了多个应用实例，就可以没有宕机地滚动更新。我们将会在下面的模块中介绍这些。现在让我们使用在线终端来体验一下应用程序的扩缩过程。\n  Start Interactive Tutorial › -- 开始互动教程 ›       "
},
{
	"uri": "https://lijun.in/concepts/extend-kubernetes/api-extension/apiserver-aggregation/",
	"title": "通过聚合层扩展 Kubernetes API",
	"tags": [],
	"description": "",
	"content": "聚合层允许 Kubernetes 通过额外的 API 进行扩展，而不局限于 Kubernetes 核心 API 提供的功能。\n概述 聚合层使您的集群可以安装其他 Kubernetes 风格的 API。这些 API 可以是预编译的、第三方的解决方案提供的例如service-catalog、或者用户创建的类似apiserver-builder一样的API可以帮助你上手。\n聚合层在 kube-apiserver 进程内运行。在扩展资源注册之前，聚合层不做任何事情。要注册 API，用户必须添加一个 APIService 对象，用它来申领 Kubernetes API 中的 URL 路径。自此以后，聚合层将会把发给该 API 路径的所有内容（例如 /apis/myextension.mycompany.io/v1/…）代理到已注册的 APIService。\n正常情况下，APIService 会实现为运行于集群中某 Pod 内的 extension-apiserver。如果需要对增加的资源进行动态管理，extension-apiserver 经常需要和一个或多个控制器一起使用。因此，apiserver-builder 同时提供用来管理新资源的 API 框架和控制器框架。另外一个例子，当安装了 service-catalog 时，它会为自己提供的服务提供 extension-apiserver 和控制器。\nextension-apiserver 与 kube-apiserver 之间的连接应具有低延迟。 特别是，发现请求需要在五秒钟或更短的时间内从 kube-apiserver 往返。 如果您的部署无法实现此目的，则应考虑如何进行更改。目前，在 kube-apiserver 上设置 EnableAggregatedDiscoveryTimeout=false 功能开关将禁用超时限制。它将在将来的版本中被删除。\n 阅读配置聚合层 文档，了解如何在自己的环境中启用聚合器（aggregator）。 然后安装扩展的 api-server 来开始使用聚合层。 也可以学习怎样 使用自定义资源定义扩展 Kubernetes API。  "
},
{
	"uri": "https://lijun.in/concepts/configuration/overview/",
	"title": "配置最佳实践",
	"tags": [],
	"description": "",
	"content": "本文档重点介绍并整合了整个用户指南、入门文档和示例中介绍的配置最佳实践。\n这是一份活文件。 如果您认为某些内容不在此列表中但可能对其他人有用，请不要犹豫，提交问题或提交 PR。\n一般配置提示  定义配置时，请指定最新的稳定 API 版本。   在推送到集群之前，配置文件应存储在版本控制中。 这允许您在必要时快速回滚配置更改。 它还有助于集群重新创建和恢复。   使用 YAML 而不是 JSON 编写配置文件。虽然这些格式几乎可以在所有场景中互换使用，但 YAML 往往更加用户友好。   只要有意义，就将相关对象分组到一个文件中。 一个文件通常比几个文件更容易管理。 请参阅[guestbook-all-in-one.yaml](https://github.com/kubernetes/examples/tree/param \u0026ldquo;githubbranch\u0026rdquo; \u0026gt;}}/guestbook/all-in-one/guestbook-all-in-one.yaml) 文件作为此语法的示例。   另请注意，可以在目录上调用许多kubectl命令。 例如，你可以在配置文件的目录中调用kubectl apply。   除非必要，否则不指定默认值：简单的最小配置会降低错误的可能性。   将对象描述放在注释中，以便更好地进行内省。  “Naked”Pods 与 ReplicaSet，Deployment 和 Jobs  如果您能避免，不要使用 naked Pods（即，Pod 未绑定到ReplicaSet 或Deployment）。 如果节点发生故障，将不会重新安排 Naked Pods。  Deployment，它创建一个 ReplicaSet 以确保所需数量的 Pod 始终可用，并指定替换 Pod 的策略(例如 RollingUpdate)，除了一些显式的restartPolicy: Never场景之外，几乎总是优先考虑直接创建 Pod。 Job 也可能是合适的。\n服务   在其相应的后端工作负载（Deployment 或 ReplicaSet）之前，以及在需要访问它的任何工作负载之前创建服务。 当 Kubernetes 启动容器时，它提供指向启动容器时正在运行的所有服务的环境变量。 例如，如果存在名为foo当服务，则所有容器将在其初始环境中获取以下变量。\nFOO_SERVICE_HOST=\u0026lt;the host the Service is running on\u0026gt; FOO_SERVICE_PORT=\u0026lt;the port the Service is running on\u0026gt;   这确实意味着订购要求 - 必须在Pod本身之前创建Pod想要访问的任何Service，否则将不会填充环境变量。 DNS没有此限制。\n 一个可选（尽管强烈推荐）cluster add-on是 DNS 服务器。DNS 服务器为新的Services监视 Kubernetes API，并为每个创建一组 DNS 记录。 如果在整个集群中启用了 DNS，则所有Pods应该能够自动对Services进行名称解析。   除非绝对必要，否则不要为 Pod 指定hostPort。 将 Pod 绑定到hostPort时，它会限制 Pod 可以调度的位置数，因为每个\u0026lt;hostIP, hostPort, protocol\u0026gt;组合必须是唯一的。如果您没有明确指定hostIP和protocol，Kubernetes将使用0.0.0.0作为默认hostIP和TCP作为默认protocol。  如果您只需要访问端口以进行调试，则可以使用apiserver proxy或kubectl port-forward。\n如果您明确需要在节点上公开 Pod 的端口，请在使用hostPort之前考虑使用NodePort 服务。\n 避免使用hostNetwork，原因与hostPort相同。   当您不需要kube-proxy负载平衡时，使用 [无头服务](/docs/concepts/services-networking/service/#headless- services) (具有None的ClusterIP)以便于服务发现。  使用标签  定义并使用标签来识别应用程序或部署的__semantic attributes__，例如{ app: myapp, tier: frontend, phase: test, deployment: v3 }。 您可以使用这些标签为其他资源选择合适的 Pod；例如，一个选择所有tier: frontend Pod 的服务，或者app: myapp的所有phase: test组件。 有关此方法的示例，请参阅[留言板](https://github.com/kubernetes/examples/tree/ param \u0026ldquo;githubbranch\u0026rdquo; \u0026gt;}}/guestbook/) 。  通过从选择器中省略特定发行版的标签，可以使服务跨越多个部署。 部署可以在不停机的情况下轻松更新正在运行的服务。\n部署描述了对象的期望状态，并且如果对该规范的更改是_applied_，则部署控制器以受控速率将实际状态改变为期望状态。\n 您可以操纵标签进行调试。 由于 Kubernetes 控制器（例如 ReplicaSet）和服务使用选择器标签与 Pod 匹配，因此从 Pod 中删除相关标签将阻止其被控制器考虑或由服务提供服务流量。 如果删除现有 Pod 的标签，其控制器将创建一个新的 Pod 来取代它。 这是在\u0026quot;隔离\u0026quot;环境中调试先前\u0026quot;实时\u0026quot;Pod 的有用方法。 要以交互方式删除或添加标签，请使用kubectl label。  容器镜像 当 kubelet尝试拉取指定的镜像时，imagePullPolicy和镜像标签会生效。\n imagePullPolicy: IfNotPresent：仅当镜像在本地不存在时镜像才被拉取。   imagePullPolicy: Always：每次启动 pod 的时候都会拉取镜像。   imagePullPolicy 省略时，镜像标签为 :latest 或不存在，使用 Always 值。   imagePullPolicy 省略时，指定镜像标签并且不是 :latest，使用 IfNotPresent 值。   imagePullPolicy: Never：假设镜像已经存在本地，不会尝试拉取镜像。  要确保容器始终使用相同版本的镜像，你可以指定其 摘要, 例如sha256:45b23dee08af5e43a7fea6c4cf9c25ccf269ee113168c19722f87876677c5cb2。 摘要唯一地标识出镜像的指定版本，因此除非您更改摘要值，否则 Kubernetes 永远不会更新它。\n在生产中部署容器时应避免使用 :latest 标记，因为更难跟踪正在运行的镜像版本，并且更难以正确回滚。\n底层镜像提供程序的缓存语义甚至使 imagePullPolicy: Always变得高效。 例如，对于 Docker，如果镜像已经存在，则拉取尝试很快，因为镜像层都被缓存并且不需要镜像下载。\n使用 kubectl  使用kubectl apply -f \u0026lt;directory\u0026gt;。 它在\u0026lt;directory\u0026gt;中的所有.yaml，.yml和.json文件中查找 Kubernetes 配置，并将其传递给apply。   使用标签选择器进行get和delete操作，而不是特定的对象名称。 请参阅标签选择器和有效使用标签部分。   使用kubectl run和kubectl expose来快速创建单容器部署和服务。 有关示例，请参阅使用服务访问集群中的应用程序。  "
},
{
	"uri": "https://lijun.in/tasks/access-kubernetes-api/configure-aggregation-layer/",
	"title": "配置聚合层",
	"tags": [],
	"description": "",
	"content": "配置 聚合层 允许 Kubernetes apiserver 使用其它 API 扩展，这些 API 不是核心 Kubernetes API 的一部分。\n. heading \u0026ldquo;prerequisites\u0026rdquo; %}} . include \u0026ldquo;task-tutorial-prereqs.md\u0026rdquo; \u0026gt;}} . version-check \u0026gt;}}\n. note \u0026gt;}}\n要使聚合层在您的环境中正常工作以支持代理服务器和扩展 apiserver 之间的相互 TLS 身份验证，需要满足一些设置要求。Kubernetes 和 kube-apiserver 具有多个 CA，因此请确保代理是由聚合层 CA 签名的，而不是由主 CA 签名的。\n. caution \u0026gt;}}\n对不同的客户端类型重复使用相同的 CA 会对群集的功能产生负面影响。有关更多信息，请参见 CA重用和冲突。\n. /caution \u0026gt;}} . /note \u0026gt;}}\n认证流程 与自定义资源定义（CRD）不同，除标准的 Kubernetes apiserver 外，Aggregation API 还涉及另一个服务器：扩展 apiserver。Kubernetes apiserver 将需要与您的扩展 apiserver 通信，并且您的扩展 apiserver 也需要与 Kubernetes apiserver 通信。为了确保此通信的安全，Kubernetes apiserver 使用 x509 证书向扩展 apiserver 认证。\n本节介绍身份认证和鉴权流程的工作方式以及如何配置它们。\n大致流程如下：\n Kubernetes apiserver：对发出请求的用户身份认证，并对请求的 API 路径执行鉴权。 Kubernetes apiserver：将请求转发到扩展 apiserver 扩展 apiserver：认证来自 Kubernetes apiserver 的请求 扩展 apiserver：对来自原始用户的请求鉴权 扩展 apiserver：执行  本节的其余部分详细描述了这些步骤。\n该流程可以在下图中看到。\n.\n以上泳道的来源可以在本文档的源码中找到。\nKubernetes Apiserver 认证和授权 由扩展 apiserver 服务的对 API 路径的请求以与所有API请求相同的方式开始：与 Kubernetes apiserver 的通信。该路径已通过扩展 apiserver 在 Kubernetes apiserver 中注册。\n用户与 Kubernetes apiserver 通信，请求访问 path 。Kubernetes apiserver 使用它的标准认证和授权配置来对用户认证，以及对特定 path 的鉴权。\n有关对 Kubernetes 集群认证的概述，请参见 对集群认证。有关对Kubernetes群集资源的访问鉴权的概述，请参见 鉴权概述。\n到目前为止，所有内容都是标准的 Kubernetes API 请求，认证与鉴权。\nKubernetes apiserver 现在准备将请求发送到扩展 apiserver。\nKubernetes Apiserver 代理请求 Kubernetes apiserver 现在将请求发送或代理到注册以处理该请求的扩展 apiserver。为此，它需要了解几件事：\n  Kubernetes apiserver 应该如何向扩展 apiserver 认证，以通知扩展 apiserver 通过网络发出的请求来自有效的 Kubernetes apiserver？\n  Kubernetes apiserver 应该如何通知扩展 apiserver 原始请求已通过认证的用户名和组？\n  为提供这两条信息，您必须使用若干标志来配置 Kubernetes apiserver。\nKubernetes Apiserver 客户端认证 Kubernetes apiserver 通过 TLS 连接到扩展 apiserver，并使用客户端证书认证。您必须在启动时使用提供的标志向 Kubernetes apiserver 提供以下内容：\n 通过 --proxy-client-key-file 指定私钥文件 通过 --proxy-client-cert-file 签名的客户端证书文件 通过 --requestheader-client-ca-file 签署客户端证书文件的 CA 证书 通过 --requestheader-allowed-names 在签署的客户证书中有效的公用名（CN）  Kubernetes apiserver 将使用由 \u0026ndash;proxy-client-*-file 指示的文件来验证扩展 apiserver。为了使合规的扩展 apiserver 能够将该请求视为有效，必须满足以下条件：\n 连接必须使用由 CA 签署的客户端证书，该证书的证书位于 --requestheader-client-ca-file 中。 连接必须使用客户端证书，该客户端证书的 CN 是 --requestheader-allowed-names 中列出的证书之一。 **注意：**您可以将此选项设置为空白，即为--requestheader-allowed-names。这将向扩展 apiserver 指示任何 CN 是可接受的。  使用这些选项启动时，Kubernetes apiserver 将：\n 使用它们向扩展 apiserver 认证。 在名为 extension-apiserver-authentication 的 kube-system 命名空间中创建一个 configmap，它将在其中放置 CA 证书和允许的 CN。反过来，扩展 apiserver 可以检索这些内容以验证请求。  请注意，Kubernetes apiserver 使用相同的客户端证书对所有扩展 apiserver 认证。它不会为每个扩展 apiserver 创建一个客户端证书，而是创建一个证书作为 Kubernetes apiserver 认证。所有扩展 apiserver 请求都重复使用相同的请求。\n原始请求用户名和组 当 Kubernetes apiserver 将请求代理到扩展 apiserver 时，它将向扩展 apiserver 通知原始请求已成功通过其验证的用户名和组。它在其代理请求的 http 标头中提供这些。您必须将要使用的标头名称告知 Kubernetes apiserver。\n 通过--requestheader-username-headers 标明用来保存用户名的头部 通过--requestheader-group-headers 标明用来保存 group 的头部 通过--requestheader-extra-headers-prefix 标明用来保存拓展信息前缀的头部  这些标头名称也放置在extension-apiserver-authentication 的 configmap 中，因此扩展 apiserver 可以检索和使用它们。\n扩展 Apiserver 认证 扩展 apiserver 在收到来自 Kubernetes apiserver 的代理请求后，必须验证该请求确实确实来自有效的身份验证代理，该认证代理由 Kubernetes apiserver 履行。扩展 apiserver 通过以下方式对其认证：\n1.如上所述，从kube-system中的 configmap 中检索以下内容： * 客户端 CA 证书 * 允许名称（CN）列表 * 用户名，组和其他信息的头部\n2.使用以下证书检查 TLS 连接是否已通过认证： * 由其证书与检索到的 CA 证书匹配的 CA 签名。 * 在允许的 CN 列表中有一个 CN，除非列表为空，在这种情况下允许所有 CN。 * 从适当的头部中提取用户名和组\n如果以上均通过，则该请求是来自合法认证代理（在本例中为 Kubernetes apiserver）的有效代理请求。\n请注意，扩展 apiserver 实现负责提供上述内容。默认情况下，许多扩展 apiserver 实现利用 k8s.io/apiserver/ 软件包来做到这一点。也有一些实现可能支持使用命令行选项来覆盖这些配置。\n为了具有检索 configmap 的权限，扩展 apiserver 需要适当的角色。在 kube-system 名字空间中有一个默认角色extension-apiserver-authentication-reader 可用于设置。\n扩展 Apiserver 对请求鉴权 扩展 apiserver 现在可以验证从标头检索的user/group是否有权执行给定请求。通过向 Kubernetes apiserver 发送标准 SubjectAccessReview 请求来实现。\n为了使扩展 apiserver 本身被鉴权可以向 Kubernetes apiserver 提交 SubjectAccessReview 请求，它需要正确的权限。Kubernetes 包含一个具有相应权限的名为system：auth-delegator 的默认 ClusterRole，可以将其授予扩展 apiserver 的服务帐户。\n扩展 Apiserver 执行 如果SubjectAccessReview通过，则扩展 apiserver 执行请求。\n启用 Kubernetes Apiserver 标志 通过以下 kube-apiserver 标志启用聚合层。您的服务提供商可能已经为您完成了这些工作：\n--requestheader-client-ca-file=\u0026lt;path to aggregator CA cert\u0026gt; --requestheader-allowed-names=front-proxy-client --requestheader-extra-headers-prefix=X-Remote-Extra- --requestheader-group-headers=X-Remote-Group --requestheader-username-headers=X-Remote-User --proxy-client-cert-file=\u0026lt;path to aggregator proxy cert\u0026gt; --proxy-client-key-file=\u0026lt;path to aggregator proxy key\u0026gt;  CA-重用和冲突 Kubernetes apiserver 有两个客户端 CA 选项：\n --client-ca-file --requestheader-client-ca-file  这些功能中的每个功能都是独立的；如果使用不正确，可能彼此冲突。\n  --client-ca-file：当请求到达 Kubernetes apiserver 时，如果启用了此选项，则 Kubernetes apiserver 会检查请求的证书。如果它是由 --client-ca-file 引用的文件中的 CA 证书之一签名的，并且用户是公用名CN=的值，而组是组织O= 的取值，则该请求被视为合法请求。请参阅 关于 TLS 身份验证的文档。\n  --requestheader-client-ca-file：当请求到达 Kubernetes apiserver 时，如果启用此选项，则 Kubernetes apiserver 会检查请求的证书。如果它是由文件引用中的 \u0026ndash;requestheader-client-ca-file 所签署的 CA 证书之一签名的，则该请求将被视为潜在的合法请求。然后，Kubernetes apiserver 检查通用名称CN=是否是 \u0026ndash;requestheader-allowed-names 提供的列表中的名称之一。如果名称允许，则请求被批准；如果不是，则请求被拒绝。\n  如果同时提供了 --client-ca-file 和--requestheader-client-ca-file，则首先检查 --requestheader-client-ca-file CA，然后再检查--client-ca-file。通常，这些选项中的每一个都使用不同的 CA（根 CA 或中间 CA）。常规客户端请求与 \u0026ndash;client-ca-file 相匹配，而聚合请求与 --requestheader-client-ca-file 相匹配。但是，如果两者都使用同一个 CA，则通常会通过 --client-ca-file 传递的客户端请求将失败，因为 CA 将与 --requestheader-client-ca-file 中的 CA 匹配，但是通用名称 CN= 将不匹配 --requestheader-allowed-names 中可接受的通用名称之一。这可能导致您的 kubelet 和其他控制平面组件以及最终用户无法向 Kubernetes apiserver 认证。\n因此，请对用于控制平面组件和最终用户鉴权的 --client-ca-file 选项和用于聚合 apiserver 鉴权的 --requestheader-client-ca-file 选项使用不同的 CA 证书。\n. warning \u0026gt;}}\n除非您了解风险和保护 CA 用法的机制，否则 不要 重用在不同上下文中使用的 CA。\n. /warning \u0026gt;}}\n如果您未在运行 API 服务器的主机上运行 kube-proxy，则必须确保使用以下 kube-apiserver 标志启用系统： \u0026ndash;enable-aggregator-routing=true\n注册 APIService 对象 您可以动态配置将哪些客户端请求代理到扩展 apiserver。以下是注册示例：\napiVersion: apiregistration.k8s.io/v1 kind: APIService metadata: name: \u0026lt; 注释对象名称 \u0026gt; spec: group: \u0026lt; 拓展 Apiserver 的 API group 名称 \u0026gt; version: \u0026lt; 拓展 Apiserver 的 API version\u0026gt; groupPriorityMinimum: \u0026lt; APIService 对对应 group 的优先级, 参考 API 文档 \u0026gt; versionPriority: \u0026lt; 优先考虑 version 在 group 中的排序, 参考 API 文档 \u0026gt; service: namespace: \u0026lt; 拓展 Apiserver 服务的 namespace \u0026gt; name: \u0026lt; 拓展 Apiserver 服务的 name \u0026gt; caBundle: \u0026lt; PEM 编码的 CA 证书，用于对 Webhook 服务器的证书签名 \u0026gt; 调用扩展 apiserver 一旦 Kubernetes apiserver 确定应将请求发送到扩展 apiserver，它需要知道如何调用它。\nservice 部分是对扩展 apiserver 的服务的引用。 服务的 namespace 和 name 是必需的。port 是可选的，默认为 443。 path 配置是可选的，默认为/。\n下面是为可在端口 1234 上调用的扩展 apiserver 的配置示例 服务位于子路径 /my-path 下，并针对 ServerName my-service-name.my-service-namespace.svc 使用自定义的 CA 包来验证 TLS 连接 使用自定义 CA 捆绑包的my-service-name.my-service-namespace.svc。\napiVersion: apiregistration.k8s.io/v1 kind: APIService ... spec: ... service: namespace: my-service-namespace name: my-service-name port: 1234 caBundle: \u0026#34;Ci0tLS0tQk...\u0026lt;base64-encoded PEM bundle\u0026gt;...tLS0K\u0026#34; ... . heading \u0026ldquo;whatsnext\u0026rdquo; %}}  使用聚合层 建立扩展 api-server。 有关高级概述，请参阅 使用聚合层扩展 Kubernetes API。 了解如何 使用自定义资源扩展 Kubernetes API。  "
},
{
	"uri": "https://lijun.in/concepts/containers/images/",
	"title": "镜像",
	"tags": [],
	"description": "",
	"content": "创建 Docker 镜像并将其推送到仓库，然后在 Kubernetes pod 中引用它。\n容器的 image 属性支持与 docker 命令相同的语法，包括私有仓库和标签。\n升级镜像 默认的镜像拉取策略是 IfNotPresent，在镜像已经存在的情况下，kubelet 将不再去拉取镜像。如果总是想要拉取镜像，您可以执行以下操作：\n 设置容器的 imagePullPolicy 为 Always。 省略 imagePullPolicy，并使用 :latest 作为要使用的镜像的标签。 省略 imagePullPolicy 和要使用的镜像标签。 启用 AlwaysPullImages 准入控制器（admission controller）。  注意应避免使用 :latest 标签，参见配置镜像最佳实践 获取更多信息。\n使用清单（manifest）构建多架构镜像 Docker CLI 现在支持以下命令 docker manifest 以及 create、annotate、push 等子命令。这些命令可用于构建和推送清单。您可以使用 docker manifest inspect 来查看清单。\n请在此处查看 docker 清单文档： https://docs.docker.com/edge/engine/reference/commandline/manifest/\n查看有关如何在构建工具中使用清单的示例： https://cs.k8s.io/?q=docker%20manifest%20(create%7Cpush%7Cannotate)\u0026amp;i=nope\u0026amp;files=\u0026amp;repos=\n这些命令依赖于 Docker CLI 并仅在 Docker CLI 上实现。需要编辑 $HOME/.docker/config.json 并将 experimental 设置为 enabled，或者仅在调用 CLI 命令时将 DOCKER_CLI_EXPERIMENTAL 环境变量设置为 enabled。\n请使用 Docker 18.06 或更高版本，低版本存在错误或不支持实验性命令行选项。导致容器问题示例 https://github.com/docker/cli/issues/1135。\n如果在上传旧清单时遇到麻烦，只需删除 $HOME/.docker/manifests 中旧的清单即可重新开始。\n对于 Kubernetes，通常使用带有后缀 -$(ARCH) 的镜像。为了向后兼容，请生成带有后缀的旧镜像。想法是生成具有所有 arch(es) 清单的 pause 镜像，并生成 pause-amd64 镜像，该镜像向后兼容较早的配置或者可能已对带有后缀的镜像进行硬编码的 YAML 文件。\n使用私有仓库 从私有仓库读取镜像时可能需要密钥。 凭证可以用以下方式提供:\n 使用 Google Container Registry  每个集群 在 Google Compute Engine 或 Google Kubernetes Engine 上自动配置 所有 Pod 均可读取项目的私有仓库   使用 Amazon Elastic Container Registry（ECR）  使用 IAM 角色和策略来控制对 ECR 仓库的访问 自动刷新 ECR 登录凭据   使用 Oracle Cloud Infrastructure Registry（OCIR）  使用 IAM 角色和策略来控制对 OCIR 仓库的访问   使用 Azure Container Registry (ACR) 使用 IBM Cloud Container Registry 配置节点用于私有仓库进行身份验证  所有 Pod 均可读取任何已配置的私有仓库 需要集群管理员配置节点   预拉镜像  所有 Pod 都可以使用节点上缓存的任何镜像 需要所有节点的 root 访问权限才能进行设置   在 Pod 上指定 ImagePullSecrets  只有提供自己密钥的 Pod 才能访问私有仓库    下面将详细描述每一项。\n使用 Google Container Registry Kuberetes 运行在 Google Compute Engine (GCE) 时原生支持 Google Container Registry (GCR)。如果 kubernetes 集群运行在 GCE 或者 Google Kubernetes Engine，使用镜像全名(例如 gcr.io/my_project/image:tag) 即可。\n集群中所有 pod 都会有读取这个仓库镜像的权限。\nkubelet 将使用实例的 Google service account 向 GCR 认证。实例的 Google service account 拥有 https://www.googleapis.com/auth/devstorage.read_only，所以它可以从项目的 GCR 拉取，但不能推送。\n使用 Amazon Elastic Container Registry 当 Node 是 AWS EC2 实例时，Kubernetes 原生支持 Amazon Elastic Container Registry。\n在 pod 定义中，使用镜像全名即可 (例如 ACCOUNT.dkr.ecr.REGION.amazonaws.com/imagename:tag)\n集群中所有可以创建 Pod 的用户都将能够运行使用 ECR 仓库中任何镜像的 Pod。\nkubelet 将获取并定期刷新 ECR 凭据。它需要以下权限才能执行此操作：\n ecr:GetAuthorizationToken ecr:BatchCheckLayerAvailability ecr:GetDownloadUrlForLayer ecr:GetRepositoryPolicy ecr:DescribeRepositories ecr:ListImages ecr:BatchGetImage  要求：\n 必须使用 kubelet v1.2.0 及以上版本。（例如 运行 /usr/bin/kubelet --version=true）。 如果 Node 在区域 A，而镜像仓库在另一个区域 B，需要 v1.3.0 及以上版本。 区域中必须提供 ECR。  故障排除：\n 验证是否满足以上要求。 获取工作站的 $REGION (例如 us-west-2) 凭证，使用凭证 SSH 到主机手动运行 Docker。它行得通吗？ 验证 kubelet 是否使用参数 --cloud-provider=aws 运行。 检查 kubelet 日志(例如 journalctl -u kubelet)是否有类似的行：  plugins.go:56] Registering credential provider: aws-ecr-key provider.go:91] Refreshing cache for provider: *aws_credentials.ecrProvider    使用 Azure Container Registry (ACR) 当使用 Azure Container Registry 时，可以使用管理员用户或者 service principal 进行身份验证。任何一种情况，认证都通过标准的 Docker 授权完成。本指南假设使用 azure-cli 命令行工具。\n首先，需要创建仓库并获取凭证，完整的文档请参考 Azure container registry 文档。\n创建好容器仓库后，可以使用以下凭证登录：\n DOCKER_USER : service principal，或管理员用户名称 DOCKER_PASSWORD: service principal 密码，或管理员用户密码 DOCKER_REGISTRY_SERVER: ${some-registry-name}.azurecr.io DOCKER_EMAIL: ${some-email-address}  填写以上变量后，就可以 配置 Kubernetes Secret 并使用它来部署 Pod。\n使用 IBM Cloud Container Registry IBM Cloud Container Registry 提供了一个多租户私有镜像仓库，可以使用它来安全地存储和共享 Docker 仓库。默认情况下，集成的 Vulnerability Advisor 会扫描私有仓库中的镜像，以检测安全问题和潜在的漏洞。IBM Cloud 帐户中的用户可以访问您的镜像，也可以创建令牌来授予对仓库命名空间的访问权限。\n要安装 IBM Cloud Container Registry CLI 插件并为镜像创建命名空间，请参阅 IBM Cloud Container Registry 入门。\n可以使用 IBM Cloud Container Registry 将容器从 IBM Cloud 公共镜像 和私有镜像部署到 IBM Cloud Kubernetes Service 集群的默认命名空间。要将容器部署到其他命名空间，或使用来自其他 IBM Cloud Container 的仓库区域或 IBM Cloud 帐户的镜像，请创建 Kubernetes imagePullSecret。有关更多信息，请参阅从镜像构建容器。\n配置 Node 对私有仓库认证 如果在 Google Kubernetes Engine 上运行集群，每个节点上都会有 .dockercfg 文件，它包含 Google Container Registry 的凭证。不需要使用以下方法。\n如果在 AWS EC2 上运行集群且准备使用 EC2 Container Registry (ECR)，每个 node 上的 kubelet 会管理和更新 ECR 的登录凭证。不需要使用以下方法。\n该方法适用于能够对节点进行配置的情况。该方法在 GCE 及在其它能自动配置节点的云平台上并不适合。\n截至目前，Kubernetes 仅支持 docker config 的 auths 和 HttpHeaders 部分。这意味着不支持凭据助手（credHelpers 或 credsStore）。\nDocker 将私有仓库的密钥存放在 $HOME/.dockercfg 或 $HOME/.docker/config.json 文件中。Kubelet 上，docker 会使用 root 用户 $HOME 路径下的密钥。\n {--root-dir:-/var/lib/kubelet}/config.json {cwd of kubelet}/config.json ${HOME}/.docker/config.json /.docker/config.json {--root-dir:-/var/lib/kubelet}/.dockercfg {cwd of kubelet}/.dockercfg ${HOME}/.dockercfg /.dockercfg  可能必须在环境变量文件中为 kubelet 显式设置 HOME=/root。\n推荐如下步骤来为 node 配置私有仓库。以下示例在 PC 或笔记本电脑中操作：\n 对于想要使用的每一种凭证，运行 docker login [server]，它会更新 $HOME/.docker/config.json。 使用编辑器查看 $HOME/.docker/config.json，保证文件中包含了想要使用的凭证。 获取 node 列表，例如  如果想要 node 名称，nodes=$(kubectl get nodes -o jsonpath='{range.items[*].metadata}{.name} {end}') 如果想要 node IP ，nodes=$(kubectl get nodes -o jsonpath='{range .items[*].status.addresses[?(@.type==\u0026quot;ExternalIP\u0026quot;)]}{.address} {end}')   将本地的 .docker/config.json 拷贝到每个节点 root 用户目录下  例如： for n in $nodes; do scp ~/.docker/config.json root@$n:/root/.docker/config.json; done    创建使用私有仓库的 pod 来验证，例如：\nkubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: v1 kind: Pod metadata: name: private-image-test-1 spec: containers: - name: uses-private-image image: $PRIVATE_IMAGE_NAME imagePullPolicy: Always command: [ \u0026#34;echo\u0026#34;, \u0026#34;SUCCESS\u0026#34; ] EOF pod/private-image-test-1 created 如果一切正常，一段时间后，可以看到:\nkubectl logs private-image-test-1 SUCCESS 如果失败，则可以看到：\nkubectl describe pods/private-image-test-1 | grep \u0026#34;Failed\u0026#34; Fri, 26 Jun 2015 15:36:13 -0700 Fri, 26 Jun 2015 15:39:13 -0700 19 {kubelet node-i2hq} spec.containers{uses-private-image} failed Failed to pull image \u0026#34;user/privaterepo:v1\u0026#34;: Error: image user/privaterepo:v1 not found 必须保证集群中所有的节点都有相同的 .docker/config.json 文件。否则, pod 会在一些节点上正常运行而在另一些节点上无法启动。例如，如果使用 node 自动缩放，那么每个实例模板都需要包含 .docker/config.json，或者挂载一个包含这个文件的驱动器。\n在 .docker/config.json 中配置了私有仓库密钥后，所有 pod 都会能读取私有仓库中的镜像。\n提前拉取镜像 如果在 Google Kubernetes Engine 上运行集群，每个节点上都会有 .dockercfg 文件，它包含 Google Container Registry 的凭证。不需要使用以下方法。\n该方法适用于能够对节点进行配置的情况。该方法在 GCE 及在其它能自动配置节点的云平台上并不适合。\n默认情况下，kubelet 会尝试从指定的仓库拉取每一个镜像。但是，如果容器属性 imagePullPolicy 设置为 IfNotPresent 或者 Never，则会使用本地镜像（优先、唯一、分别）。\n如果依赖提前拉取镜像代替仓库认证，必须保证集群所有的节点提前拉取的镜像是相同的。\n可以用于提前载入指定的镜像以提高速度，或者作为私有仓库认证的一种替代方案。\n所有的 pod 都可以使用 node 上缓存的镜像。\n在 pod 上指定 ImagePullSecrets Google Kubernetes Engine、GCE 及其他自动创建 node 的云平台上，推荐使用本方法。\nKubernetes 支持在 pod 中指定仓库密钥。\n使用 Docker Config 创建 Secret 运行以下命令，将大写字母代替为合适的值：\nkubectl create secret docker-registry \u0026lt;name\u0026gt; --docker-server=DOCKER_REGISTRY_SERVER --docker-username=DOCKER_USER --docker-password=DOCKER_PASSWORD --docker-email=DOCKER_EMAIL 如果已经有 Docker 凭证文件，则可以将凭证文件作为 Kubernetes secret 导入而不是使用上面的命令。根据现有 Docker 凭证创建 Secret 解释了如何安装。如果使用多个私有容器仓库，这将特别有用，因为 kubectl create secret docker-registry 创建了一个仅适用于单个私有仓库的 Secret。\nPod 只能引用和它相同命名空间的 ImagePullSecrets，所以需要为每一个命名空间做配置。\n引用 Pod 上的 imagePullSecrets 现在，在创建 pod 时，可以在 pod 定义中增加 imagePullSecrets 部分来引用 secret。\ncat \u0026lt;\u0026lt;EOF \u0026gt; pod.yaml apiVersion: v1 kind: Pod metadata: name: foo namespace: awesomeapps spec: containers: - name: foo image: janedoe/awesomeapp:v1 imagePullSecrets: - name: myregistrykey EOF cat \u0026lt;\u0026lt;EOF \u0026gt;\u0026gt; ./kustomization.yaml resources: - pod.yaml EOF 对每一个使用私有仓库的 pod，都需要做以上操作。\n但是，可以通过在 serviceAccount 资源中设置 imagePullSecrets 来自动设置 imagePullSecrets。检查 将 ImagePullSecrets 添加 Service Account 以获取详细说明。\n可以将其与每个节点 .docker/config.json 结合使用。凭据将被合并。这种方法适用于 Google Kubernetes Engine。\n使用场景 配置私有仓库有多种方案，以下是一些常用场景和建议的解决方案。\n 集群运行非专有（例如 开源镜像）镜像。镜像不需要隐藏。  使用 Docker hub 上的公有镜像 无需配置 在 GCE/GKE 上会自动使用高稳定性和高速的 Docker hub 的本地 mirror    集群运行一些专有镜像，这些镜像对外部公司需要隐藏，对集群用户可见  使用自主的私有 Docker registry。  可以放置在 Docker Hub,或者其他地方。  按照上面的描述，在每个节点手动配置 .docker/config.json。     或者，在防火墙内运行一个内置的私有仓库，并开放读取权限。  不需要配置 Kubenretes。   或者，在 GCE/GKE 上时，使用项目的 Google Container Registry。  使用集群自动伸缩比手动配置 node 工作的更好。   或者，在更改集群 node 配置不方便时，使用 imagePullSecrets。    使用专有镜像的集群，有更严格的访问控制。  保证开启 AlwaysPullImages admission controller。否则，所有的 pod 都可以使用镜像。 将敏感数据存储在 \u0026ldquo;Secret\u0026rdquo; 资源中，而不是打包在镜像里。    多租户集群下，每个租户需要自己的私有仓库。  开启保证 AlwaysPullImages admission controller。否则，所有租户的所有的 pod 都可以使用镜像。 私有仓库开启认证。 为每个租户获取仓库凭证，放置在 secret 中，并发布到每个租户的命名空间下。 租户将 secret 增加到每个命名空间下的 imagePullSecrets 中。    如果需要访问多个仓库，则可以为每个仓库创建一个 secret。Kubelet 将任何 imagePullSecrets 合并为单个虚拟 .docker/config.json 文件。\n"
},
{
	"uri": "https://lijun.in/concepts/policy/limit-range/",
	"title": "限制范围",
	"tags": [],
	"description": "",
	"content": "默认情况下， Kubernetes 集群上的容器运行使用的计算资源 没有限制。 使用资源配额，集群管理员可以以命名空间为单位，限制其资源的使用与创建。 在命名空间中，一个 Pod 或 Container 最多能够使用命名空间的资源配额所定义的 CPU 和内存用量。有人担心，一个 Pod 或 Container 会垄断所有可用的资源。LimitRange 是在命名空间内限制资源分配（给多个 Pod 或 Container）的策略对象。\n一个 LimitRange 对象提供的限制能够做到：\n 在一个命名空间中实施对每个 Pod 或 Container 最小和最大的资源使用量的限制。 在一个命名空间中实施对每个 PersistentVolumeClaim 能申请的最小和最大的存储空间大小的限制。 在一个命名空间中实施对一种资源的申请值和限制值的比值的控制。 设置一个命名空间中对计算资源的默认申请/限制值，并且自动的在运行时注入到多个 Container 中。  启用 LimitRange 对 LimitRange 的支持默认在多数 Kubernetes 发行版中启用。当 apiserver 的 --enable-admission-plugins 标志的参数包含 LimitRanger 准入控制器时即启用。\n当一个命名空间中有 LimitRange 时，实施该 LimitRange 所定义的限制。\nLimitRange 的名称必须是合法的 DNS 子域名。\n限制范围总览  管理员在一个命名空间内创建一个 LimitRange 对象。 用户在命名空间内创建 Pod ，Container 和 PersistentVolumeClaim 等资源。 LimitRanger 准入控制器对所有没有设置计算资源需求的 Pod 和 Container 设置默认值与限制值，并跟踪其使用量以保证没有超出命名空间中存在的任意 LimitRange 对象中的最小、最大资源使用量以及使用量比值。 若创建或更新资源（Pod, Container, PersistentVolumeClaim）违反了 LimitRange 的约束，向 API 服务器的请求会失败，并返回 HTTP 状态码 403 FORBIDDEN 与描述哪一项约束被违反的消息。 若命名空间中的 LimitRange 启用了对 cpu 和 memory 的限制，用户必须指定这些值的需求使用量与限制使用量。否则，系统将会拒绝创建 Pod。 LimitRange 的验证仅在 Pod 准入阶段进行，不对正在运行的 Pod 进行验证。  能够使用限制范围创建策略的例子有：\n 在一个有两个节点，8 GiB 内存与16个核的集群中，限制一个命名空间的 Pod 申请 100m 单位，最大 500m 单位的 CPU，以及申请 200Mi，最大 600Mi 的内存。 为 spec 中没有 cpu 和内存需求值的 Container 定义默认 CPU 限制值与需求值 150m，内存默认需求值 300Mi。  在命名空间的总限制值小于 Pod 或 Container 的限制值的总和的情况下，可能会产生资源竞争。在这种情况下，将不会创建 Container 或 Pod。\n竞争和对 LimitRange 的改变都不会影响任何已经创建了的资源。\n示例  查看如何配置每个命名空间最小和最大的 CPU 约束。 查看如何配置每个命名空间最小和最大的内存约束。 查看如何配置每个命名空间默认的 CPU 申请值和限制值。 查看如何配置每个命名空间默认的内存申请值和限制值。 查看如何配置每个命名空间最小和最大存储使用量。 查看配置每个命名空间的配额的详细例子。   查看 LimitRanger 设计文档获取更多信息。\n"
},
{
	"uri": "https://lijun.in/concepts/cluster-administration/cluster-administration-overview/",
	"title": "集群管理概述",
	"tags": [],
	"description": "",
	"content": "集群管理概述面向任何创建和管理 Kubernetes 集群的读者人群。 我们假设你对用户指南中的概念大概了解。\n规划集群 查阅 安装 中的指导，获取如何规划、建立以及配置 Kubernetes 集群的示例。本文所列的文章称为发行版 。\n在选择一个指南前，有一些因素需要考虑：\n 你是打算在你的电脑上尝试 Kubernetes，还是要构建一个高可用的多节点集群？请选择最适合你需求的发行版。 如果你正在设计一个高可用集群，请了解在多个 zones 中配置集群。 您正在使用 类似 Google Kubernetes Engine 这样的被托管的Kubernetes集群, 还是管理您自己的集群? 你的集群是在本地还是 云（IaaS） 上？ Kubernetes 不能直接支持混合集群。作为代替，你可以建立多个集群。 如果你在本地配置 Kubernetes，需要考虑哪种网络模型最适合。 你的 Kubernetes 在 裸金属硬件 还是 虚拟机（VMs） 上运行？ 你只想运行一个集群，还是打算活动开发 Kubernetes 项目代码？如果是后者，请选择一个活动开发的发行版。某些发行版只提供二进制发布版，但提供更多的选择。 让你自己熟悉运行一个集群所需的组件 。  请注意：不是所有的发行版都被积极维护着。请选择测试过最近版本的 Kubernetes 的发行版。\n管理集群   管理集群叙述了和集群生命周期相关的几个主题：创建一个新集群、升级集群的 master 和 worker 节点、执行节点维护（例如内核升级）以及升级活动集群的 Kubernetes API 版本。\n  学习如何 管理节点.\n  学习如何设定和管理集群共享的 资源配额 。\n  集群安全   Certificates 描述了使用不同的工具链生成证书的步骤。\n  Kubernetes 容器环境 描述了 Kubernetes 节点上由 Kubelet 管理的容器的环境。\n  控制到 Kubernetes API 的访问描述了如何为用户和 service accounts 建立权限许可。\n  用户认证阐述了 Kubernetes 中的认证功能，包括许多认证选项。\n  授权从认证中分离出来，用于控制如何处理 HTTP 请求。\n  使用 Admission Controllers 阐述了在认证和授权之后拦截到 Kubernetes API 服务的请求的插件。\n  在 Kubernetes Cluster 中使用 Sysctls 描述了管理员如何使用 sysctl 命令行工具来设置内核参数。\n  审计描述了如何与 Kubernetes 的审计日志交互。\n  保护 kubelet  Master 节点通信 TLS 引导 Kubelet 认证/授权  可选集群服务   DNS 与 SkyDNS 集成描述了如何将一个 DNS 名解析到一个 Kubernetes service。\n  记录和监控集群活动阐述了 Kubernetes 的日志如何工作以及怎样实现。\n  "
},
{
	"uri": "https://lijun.in/setup/release/",
	"title": "💖 - Kubernetes 发行说明和版本偏差",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lijun.in/reference/issues-security/",
	"title": "😍 - Kubernetes 问题和安全",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lijun.in/reference/using-api/",
	"title": "😍 - 使用 Kubernetes API",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lijun.in/tutorials/kubernetes-basics/",
	"title": "😎 - 学习 Kubernetes 基础知识",
	"tags": [],
	"description": "",
	"content": "  Kubernetes 基础 本教程介绍了 Kubernetes 集群编排系统的基础知识。每个模块包含关于 Kubernetes 主要特性和概念的一些背景信息，并包括一个在线互动教程。这些互动教程让您可以自己管理一个简单的集群及其容器化应用程序。\n使用互动教程，您可以学习：\n 在集群上部署容器化应用程序 弹性部署 使用新的软件版本，更新容器化应用程序 调试容器化应用程序  教程 Katacoda 在您的浏览器中运行一个虚拟终端，在浏览器中运行 Minikube，这是一个可在任何地方小规模本地部署的 Kubernetes 集群。不需要安装任何软件或进行任何配置；每个交互性教程都直接从您的网页浏览器上运行。\n  Kubernetes 可以为您做些什么? 通过现代的 Web 服务，用户希望应用程序能够 24/7 全天候使用，开发人员希望每天可以多次发布部署新版本的应用程序。 容器化可以帮助软件包达成这些目标，使应用程序能够以简单快速的方式发布和更新，而无需停机。Kubernetes 帮助您确保这些容器化的应用程序在您想要的时间和地点运行，并帮助应用程序找到它们需要的资源和工具。Kubernetes 是一个可用于生产的开源平台，根据 Google 容器集群方面积累的经验，以及来自社区的最佳实践而设计。\n  Kubernetes 基础模块  1. 创建一个 Kubernetes 集群     2. 部署应用程序     3. 应用程序探索     4. 应用外部可见     5. 应用可扩展     6. 应用更新          "
},
{
	"uri": "https://lijun.in/tasks/tools/",
	"title": "😝 - 安装工具",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lijun.in/concepts/overview/working-with-objects/object-management/",
	"title": "Kubernetes 对象管理",
	"tags": [],
	"description": "",
	"content": "kubectl 命令行工具支持多种不同的方式来创建和管理 Kubernetes 对象。本文档概述了不同的方法。阅读 Kubectl book 来了解 kubectl 管理对象的详细信息。\n管理技巧 应该只使用一种技术来管理 Kubernetes 对象。混合和匹配技术作用在同一对象上将导致未定义行为。\n   Management technique Operates on Recommended environment Supported writers Learning curve     Imperative commands Live objects Development projects 1+ Lowest   Imperative object configuration Individual files Production projects 1 Moderate   Declarative object configuration Directories of files Production projects 1+ Highest    命令式命令 使用命令式命令时，用户可以在集群中的活动对象上进行操作。用户将操作传给 kubectl 命令作为参数或标志。\n这是开始或者在集群中运行一次性任务的最简单方法。因为这个技术直接在活动对象上操作，所以它不提供以前配置的历史记录。\n例子 通过创建 Deployment 对象来运行 nginx 容器的实例：\nkubectl run nginx --image nginx 使用不同的语法来达到同样的上面的效果：\nkubectl create deployment nginx --image nginx 权衡 与对象配置相比的优点：\n 命令简单，易学且易于记忆。 命令仅需一步即可对集群进行更改。  与对象配置相比的缺点：\n 命令不与变更审查流程集成。 命令不提供与更改关联的审核跟踪。 除了实时内容外，命令不提供记录源。 命令不提供用于创建新对象的模板。  命令式对象配置 在命令式对象配置中，kubectl 命令指定操作（创建，替换等），可选标志和至少一个文件名。指定的文件必须包含 YAML 或 JSON 格式的对象的完整定义。\n有关对象定义的详细信息，请查看 [API 参考](/docs/reference/generated/kubernetes-api/\u0026lt; param \u0026ldquo;version\u0026rdquo; \u0026gt;}}/)。\nreplace 命令式命令将现有规范替换为新提供的规范，并删除对配置文件中缺少的对象的所有更改。此方法不应与规范独立于配置文件进行更新的资源类型一起使用。比如类型为 LoadBalancer 的服务，它的 externalIPs 字段就是独立于集群配置进行更新。\n例子 创建配置文件中定义的对象：\nkubectl create -f nginx.yaml 删除两个配置文件中定义的对象：\nkubectl delete -f nginx.yaml -f redis.yaml 通过覆盖活动配置来更新配置文件中定义的对象：\nkubectl replace -f nginx.yaml 权衡 与命令式命令相比的优点：\n 对象配置可以存储在源控制系统中，比如 Git。 对象配置可以与流程集成，例如在推送和审计之前检查更新。 对象配置提供了用于创建新对象的模板。  与命令式命令相比的缺点：\n 对象配置需要对对象架构有基本的了解。 对象配置需要额外的步骤来编写 YAML 文件。  与声明式对象配置相比的优点：\n 命令式对象配置行为更加简单易懂。 从 Kubernetes 1.5 版本开始，命令式对象配置更加成熟。  与声明式对象配置相比的缺点：\n 命令式对象配置更适合文件，而非目录。 对活动对象的更新必须反映在配置文件中，否则会在下一次替换时丢失。  声明式对象配置 使用声明式对象配置时，用户对本地存储的对象配置文件进行操作，但是用户未定义要对该文件执行的操作。kubectl 会自动检测每个文件的创建、更新和删除操作。这使得配置可以在目录上工作，根据目录中配置文件对不同的对象执行不同的操作。\n声明式对象配置保留其他编写者所做的修改，即使这些更改并未合并到对象配置文件中。可以通过使用 patch API 操作仅写入观察到的差异，而不是使用 replace API 操作来替换整个对象配置来实现。\n例子 处理 configs 目录中的所有对象配置文件，创建并更新活动对象。可以首先使用 diff 子命令查看将要进行的更改，然后在进行应用：\nkubectl diff -f configs/ kubectl apply -f configs/ 递归处理目录：\nkubectl diff -R -f configs/ kubectl apply -R -f configs/ 权衡 与命令式对象配置相比的优点：\n 对活动对象所做的更改即使未合并到配置文件中，也会被保留下来。 声明性对象配置更好地支持对目录进行操作并自动检测每个文件的操作类型（创建，修补，删除）。  与命令式对象配置相比的缺点：\n 声明式对象配置难于调试并且出现异常时结果难以理解。 使用 diff 产生的部分更新会创建复杂的合并和补丁操作。    使用命令式命令管理 Kubernetes 对象 使用对象配置管理 Kubernetes 对象（命令式） 使用对象配置管理 Kubernetes 对象（声明式） 使用 Kustomize（声明式）管理 Kubernetes 对象 Kubectl 命令参考 Kubectl Book [Kubernetes API 参考](/docs/reference/generated/kubernetes-api/\u0026lt; param \u0026ldquo;version\u0026rdquo; \u0026gt;}}/)  "
},
{
	"uri": "https://lijun.in/tasks/access-kubernetes-api/setup-extension-api-server/",
	"title": "设置一个扩展的 API server",
	"tags": [],
	"description": "",
	"content": "设置一个扩展的 API server 来使用聚合层以让 Kubernetes apiserver 使用其它 API 进行扩展，这些 API 不是核心 Kubernetes API 的一部分。\n. heading \u0026ldquo;prerequisites\u0026rdquo; %}}  您需要拥有一个运行的 Kubernetes 集群。 您必须 配置聚合层 并且启用 apiserver 的相关参数。  . heading \u0026ldquo;prerequisites\u0026rdquo; %}} . include \u0026ldquo;task-tutorial-prereqs.md\u0026rdquo; \u0026gt;}} . version-check \u0026gt;}}\n设置一个扩展的 api-server 来使用聚合层 以下步骤描述如何 在一个高层次 设置一个扩展的 apiserver。无论您使用的是 YAML 配置还是使用 API，这些步骤都适用。目前我们正在尝试区分出两者的区别。有关使用 YAML 配置的具体示例，您可以在 Kubernetes 库中查看 sample-apiserver。\n或者，您可以使用现有的第三方解决方案，例如 apiserver-builder，它将生成框架并自动执行以下所有步骤。\n 确保启用了 APIService API（检查 --runtime-config）。默认应该是启用的，除非被特意关闭了。 您可能需要制定一个 RBAC 规则，以允许您添加 APIService 对象，或让您的集群管理员创建一个。（由于 API 扩展会影响整个集群，因此不建议在实时集群中对 API 扩展进行测试/开发/调试） 创建 Kubernetes 命名空间，扩展的 api-service 将运行在该命名空间中。 创建（或获取）用来签署服务器证书的 CA 证书，扩展 api-server 中将使用该证书做 HTTPS 连接。 为 api-server 创建一个服务端的证书（或秘钥）以使用 HTTPS。这个证书应该由上述的 CA 签署。同时应该还要有一个 Kube DNS 名称的 CN，这是从 Kubernetes 服务派生而来的，格式为 \u0026lt;service name\u0026gt;.\u0026lt;service name namespace\u0026gt;.svc。 使用命名空间中的证书（或秘钥）创建一个 Kubernetes secret。 为扩展 api-server 创建一个 Kubernetes deployment，并确保以卷的方式挂载了 secret。它应该包含对扩展 api-server 镜像的引用。Deployment 也应该在同一个命名空间中。 确保您的扩展 apiserver 从该卷中加载了那些证书，并在 HTTPS 握手过程中使用它们。 在您的命令空间中创建一个 Kubernetes service account。 为资源允许的操作创建 Kubernetes 集群角色。 以您命令空间中的 service account 创建一个 Kubernetes 集群角色绑定，绑定到您刚创建的角色上。 以您命令空间中的 service account 创建一个 Kubernetes 集群角色绑定，绑定到 system:auth-delegator 集群角色，以将 auth 决策委派给 Kubernetes 核心 API 服务器。 以您命令空间中的 service account 创建一个 Kubernetes 集群角色绑定，绑定到 extension-apiserver-authentication-reader 角色。这将让您的扩展 api-server 能够访问 extension-apiserver-authentication configmap。 创建一个 Kubernetes apiservice。上述的 CA 证书应该使用 base64 编码，剥离新行并用作 apiservice 中的 spec.caBundle。这不应该是命名空间化的。如果使用了 kube-aggregator API，那么只需要传入 PEM 编码的 CA 绑定，因为 base 64 编码已经完成了。 使用 kubectl 来获得您的资源。它应该返回 \u0026ldquo;找不到资源\u0026rdquo;。这意味着一切正常，但您目前还没有创建该资源类型的对象。  . heading \u0026ldquo;whatsnext\u0026rdquo; %}}  如果你还未配置，请 配置聚合层 并启用 apiserver 的相关参数。 高级概述，请参阅 使用聚合层扩展 Kubernetes API。 了解如何 使用 Custom Resource Definition 扩展 Kubernetes API。  "
},
{
	"uri": "https://lijun.in/concepts/configuration/configmap/",
	"title": "ConfigMap",
	"tags": [],
	"description": "",
	"content": "n term_id=\u0026quot;configmap\u0026rdquo; length=\u0026quot;all\u0026rdquo; \u0026gt;}}\nConfigMap 并不提供保密或者加密功能。如果你想存储的数据是机密的，请使用 text=\u0026quot;Secret\u0026rdquo; term_id=\u0026quot;secret\u0026rdquo; \u0026gt;}} ，或者使用其他第三方工具来保证你的数据的私密性，而不是用 ConfigMap。\n动机 使用 ConfigMap 来将你的配置数据和应用程序代码分开。\n比如，假设你正在开发一个应用，它可以在你自己的电脑上（用于开发）和在云上（用于实际流量）运行。你的代码里有一段是用于查看环境变量 DATABASE_HOST，在本地运行时，你将这个变量设置为 localhost，在云上，你将其设置为引用 Kubernetes 集群中的公开数据库 text=\u0026quot;Service\u0026rdquo; term_id=\u0026quot;service\u0026rdquo; \u0026gt;}} 中的组件。\n这让您可以获取在云中运行的容器镜像，并且如果有需要的话，在本地调试完全相同的代码。\nConfigMap 对象 ConfigMap 是一个 API 对象，让你可以存储其他对象所需要使用的配置。和其他 Kubernetes 对象都有一个 spec 不同的是，ConfigMap 使用 data 块来存储元素（键名）和它们的值。\nConfigMap 的名字必须是一个合法的 DNS 子域名。\nConfigMaps 和 Pods 您可以写一个引用 ConfigMap 的 Pod 的 spec，并根据 ConfigMap 中的数据在该 Pod 中配置容器。这个 Pod 和 ConfigMap 必须要在同一个 text=\u0026quot;命名空间\u0026rdquo; term_id=\u0026quot;namespace\u0026rdquo; \u0026gt;}} 中。\n这是一个 ConfigMap 的示例，它的一些键只有一个值，其他键的值看起来像是配置的片段格式。\napiVersion: v1 kind: ConfigMap metadata: Name: game-demo data: # 类属性键；每一个键都映射到一个简单的值 player_initial_lives: 3 ui_properties_file_name: \u0026#34;user-interface.properties\u0026#34; # # 类文件键 game.properties: | enemy.types=aliens,monsters player.maximum-lives=5 user-interface.properties: | color.good=purple color.bad=yellow allow.textmode=true 您可以使用四种方式来使用 ConfigMap 配置 Pod 中的容器：\n 容器 entrypoint 的命令行参数 容器的环境变量 在只读卷里面添加一个文件，让应用来读取 编写代码在 Pod 中运行，使用 Kubernetes API 来读取 ConfigMap  这些不同的方法适用于不同的数据使用方式。对前三个方法， text=\u0026quot;kubelet\u0026rdquo; term_id=\u0026quot;kubelet\u0026rdquo; \u0026gt;}} 使用 ConfigMap 中的数据在 Pod 中启动容器。\n第四种方法意味着你必须编写代码才能读取 ConfigMap 和它的数据。然而，由于您是直接使用 Kubernetes API，因此只要 ConfigMap 发生更改，您的应用就能够通过订阅来获取更新，并且在这样的情况发生的时候做出反应。通过直接进入 Kubernetes API，这个技术也可以让你能够获取到不同的命名空间里的 ConfigMap。\n这是一个 Pod 的示例，它通过使用 game-demo 中的值来配置一个 Pod：\napiVersion: v1 kind: Pod metadata: name: configmap-demo-pod spec: containers: - name: demo image: game.example/demo-game env: # 定义环境变量 - name: PLAYER_INITIAL_LIVES # 请注意这里和 ConfigMap 中的键名是不一样的 valueFrom: configMapKeyRef: name: game-demo # 这个值来自 ConfigMap key: player_initial_lives # 需要取值的键 - name: UI_PROPERTIES_FILE_NAME valueFrom: configMapKeyRef: name: game-demo key: ui_properties_file_name volumeMounts: - name: config mountPath: \u0026#34;/config\u0026#34; readOnly: true volumes: # 您可以在 Pod 级别设置卷，然后将其挂载到 Pod 内的容器中 - name: config configMap: # 提供你想要挂载的 ConfigMap 的名字 name: game-demo ConfigMap 不会区分单行属性值和多行类似文件的值，重要的是 Pods 和其他对象如何使用这些值。比如，定义一个卷，并将它作为 /config 文件夹安装到 demo 容器内，并创建四个文件：\n /config/player_initial_lives /config/ui_properties_file_name /config/game.properties /config/user-interface.properties  如果您要确保 /config 只包含带有 .properties 扩展名的文件，可以使用两个不同的 ConfigMaps，并在 spec 中同时引用这两个 ConfigMaps 来创建 Pod。第一个 ConfigMap 定义了 player_initial_lives 和 ui_properties_file_name，第二个 ConfigMap 定义了 kubelet 放进 /config 的文件。\nConfigMap 最常见的用法是为同一命名空间里某 Pod 中运行的容器执行配置。您也可以单独使用 ConfigMap。\n比如，您可能会遇到基于 ConfigMap 来调整其行为的 text=\u0026quot;插件\u0026rdquo; term_id=\u0026quot;addons\u0026rdquo; \u0026gt;}} 或者 text=\u0026quot;operator\u0026rdquo; term_id=\u0026quot;operator-pattern\u0026rdquo; \u0026gt;}}。\n  阅读 Secret。 阅读 配置 Pod 来使用 ConfigMap。 阅读 Twelve-Factor 应用 来了解将代码和配置分开的动机。  "
},
{
	"uri": "https://lijun.in/concepts/workloads/controllers/",
	"title": "Controllers",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lijun.in/reference/setup-tools/kubeadm/kubeadm-init/",
	"title": "kubeadm init",
	"tags": [],
	"description": "",
	"content": "此命令初始化一个 Kubernetes 控制平面节点。\n. include \u0026ldquo;generated/kubeadm_init.md\u0026rdquo; \u0026gt;}}\nInit 命令的工作流程 kubeadm init 命令通过执行下列步骤来启动一个 Kubernetes 控制平面节点。\n 在做出变更前运行一系列的预检项来验证系统状态。一些检查项目仅仅触发警告，其它的则会被视为错误并且退出 kubeadm，除非问题得到解决或者用户指定了 --ignore-preflight-errors=\u0026lt;list-of-errors\u0026gt; 参数。  生成一个自签名的 CA 证书 (或者使用现有的证书，如果提供的话) 来为集群中的每一个组件建立身份标识。如果用户已经通过 --cert-dir 配置的证书目录（默认为 /etc/kubernetes/pki）提供了他们自己的 CA 证书以及/或者密钥， 那么将会跳过这个步骤，正如文档使用自定义证书所述。如果指定了 --apiserver-cert-extra-sans 参数, APIServer 的证书将会有额外的 SAN 条目，如果必要的话，将会被转为小写。  将 kubeconfig 文件写入 /etc/kubernetes/ 目录以便 kubelet、控制器管理器和调度器用来连接到 API 服务器，它们每一个都有自己的身份标识，同时生成一个名为 admin.conf 的独立的 kubeconfig 文件，用于管理操作。  为 API 服务器、控制器管理器和调度器生成静态 Pod 的清单文件。假使没有提供一个外部的 etcd 服务的话，也会为 etcd 生成一份额外的静态 Pod 清单文件。  静态 Pod 的清单文件被写入到 /etc/kubernetes/manifests 目录; kubelet 会监视这个目录以便在系统启动的时候创建 Pod。\n一旦控制平面的 Pod 都运行起来， kubeadm init 的工作流程就继续往下执行。\n 创建一份 ConfigMap 提供添加集群节点所需的信息，并为该 ConfigMap 设置相关的 RBAC 访问规则。 使得 Bootstrap Tokens 可以访问 CSR 签名 API。 对新的 CSR 请求配置为自动签发。  查阅kubeadm join文档以获取更多信息。\n 通过 API 服务器安装一个 DNS 服务器 (CoreDNS) 和 kube-proxy 附加组件。 在 1.11 版本以及更新版本的 Kubernetes 中 CoreDNS 是默认的 DNS 服务器。 要安装 kube-dns 而不是 CoreDNS，必须在 kubeadm ClusterConfiguration 中配置 DNS 插件。有关配置的更多信息，请参见下面的 Using kubeadm init with a configuration file 一节。 请注意，尽管已部署 DNS 服务器，但直到安装 CNI 时才调度它。  在 kubeadm 中使用 init phases Kubeadm 允许您使用 kubeadm init phase 命令分阶段创建控制平面节点。\n要查看阶段和子阶段的有序列表，可以调用 kubeadm init --help。 该列表将位于帮助屏幕的顶部，每个阶段旁边都有一个描述。 注意，通过调用 kubeadm init，所有阶段和子阶段都将按照此确切顺序执行。\n某些阶段具有唯一的标志，因此，如果要查看可用选项的列表，请添加 --help，例如：\nsudo kubeadm init phase control-plane controller-manager --help 您也可以使用 --help 查看特定父阶段的子阶段列表：\nsudo kubeadm init phase control-plane --help kubeadm init 还公开了一个名为 --skip-phases 的参数，该参数可用于跳过某些阶段。参数接受阶段名称列表，并且这些名称可以从上面的有序列表中获取。\n例如：\nsudo kubeadm init phase control-plane all --config=configfile.yaml sudo kubeadm init phase etcd local --config=configfile.yaml # 您现在可以修改控制平面和 etcd 清单文件 sudo kubeadm init --skip-phases=control-plane,etcd --config=configfile.yaml 该示例将执行的操作是基于 configfile.yaml 中的配置在 /etc/kubernetes/manifests 中写入控制平面和 etcd 的清单文件。这允许您修改文件，然后使用 --skip-phases 跳过这些阶段。通过调用最后一个命令，您将使用自定义清单文件创建一个控制平面节点。\n结合一份配置文件来使用 kubeadm init . caution \u0026gt;}}\n. /caution \u0026gt;}}\n通过一份配置文件而不是使用命令行参数来配置 kubeadm init 命令是可能的，但是一些更加高级的功能只能够通过配置文件设定。这份配置文件通过 --config 选项参数指定。\n可以使用kubeadm config print命令打印出默认配置。\n推荐使用kubeadm config migrate命令将旧的 v1beta1 版本的配置迁移到 v1beta2 版本。\n获取 v1beta2 版本配置中每个字段的细节说明，查看我们的API 参考页面。\n添加 kube-proxy 参数 kubeadm 配置中有关 kube-proxy 的说明请查看：\n kube-proxy  使用 kubeadm 启用 IPVS 模式的说明请查看：\n IPVS  向控制平面组件传递自定义的命令行参数 有关向控制平面组件传递命令行参数的说明请查看： 控制平面命令行参数\n使用自定义的镜像 默认情况下, kubeadm 会从 k8s.gcr.io 仓库拉取镜像。如果请求的 Kubernetes 版本是 CI label (例如 ci/latest)，则使用 gcr.io/kubernetes-ci-images。\n您可以通过使用带有配置文件的 kubeadm来重写此操作。\n允许的自定义功能有：\n 使用其他的 imageRepository 来代替 k8s.gcr.io。 将 useHyperKubeImage 设置为 true，使用 HyperKube 镜像。 为 etcd 或 DNS 附件提供特定的 imageRepository 和 imageTag。  请注意配置文件中的配置项 kubernetesVersion 或者命令行参数 --kubernetes-version 会影响到镜像的版本。\n将控制平面证书上传到集群 通过将参数 --upload-certs 添加到 kubeadm init，您可以将控制平面证书临时上传到集群中的 Secret。请注意，此 Secret 将在 2 小时后自动过期。证书使用 32 字节密钥加密，可以使用 --certificate-key 指定。通过将 --control-plane 和 --certificate-key 传递给 kubeadm join，可以在添加其他控制平面节点时使用相同的密钥下载证书。\n以下阶段命令可用于证书到期后重新上传证书：\nkubeadm init phase upload-certs --upload-certs --certificate-key=SOME_VALUE 如果未将参数 --certificate-key 传递给 kubeadm init 和 kubeadm init phase upload-certs，则会自动生成一个新密钥。\n以下命令可用于按需生成新密钥：\nkubeadm alpha certs certificate-key 使用自定义的证书 默认情况下, kubeadm 会生成运行一个集群所需的全部证书。 你可以通过提供你自己的证书来改变这个行为策略。\n如果要这样做, 你必须将证书文件放置在通过 --cert-dir 命令行参数或者配置文件里的 CertificatesDir 配置项指明的目录中。默认的值是 /etc/kubernetes/pki。\n如果在运行 kubeadm init 之前存在给定的证书和私钥对，则 kubeadm 将不会重写它们。例如，这意味着您可以将现有的 CA 复制到 /etc/kubernetes/pki/ca.crt 和 /etc/kubernetes/pki/ca.key 中，而 kubeadm 将使用此 CA 对其余证书进行签名。\n外部 CA 模式 如果只提供了 ca.crt 文件但是没有提供 ca.key 文件也是可以的 (这只对 CA 根证书可用，其它证书不可用)。 如果所有的其它证书和 kubeconfig 文件已就绪， kubeadm 检测到满足以上条件就会激活 \u0026ldquo;外部 CA\u0026rdquo; 模式。kubeadm 将会在没有 CA 密钥文件的情况下继续执行。\n否则, kubeadm 将独立运行 controller-manager，附加一个 --controllers=csrsigner 的参数，并且指明 CA 证书和密钥。\n管理 kubeadm 为 kubelet 提供的 systemd 配置文件 kubeadm 包自带了关于 systemd 如何运行 kubelet 的配置文件。请注意 kubeadm 客户端命令行工具永远不会修改这份 systemd 配置文件。这份 systemd 配置文件属于 kubeadm DEB/RPM 包。\n有关更多信息，请阅读管理 systemd 的 kubeadm 内嵌文件。\n结合 CRI 运行时使用 kubeadm 默认情况下，kubeadm 尝试检测您的容器运行环境。有关此检测的更多详细信息，请参见kubeadm CRI 安装指南。\n设置节点的名称 默认情况下, kubeadm 基于机器的主机地址分配一个节点名称。你可以使用 --node-name 参数覆盖此设置。 注意覆盖主机名称可能会干扰到云服务提供商。\n在没有互联网连接的情况下运行 kubeadm 要在没有互联网连接的情况下运行 kubeadm，您必须提前拉取所需的控制平面镜像。\n您可以使用 kubeadm config images 子命令列出并拉取镜像：\nkubeadm config images list kubeadm config images pull kubeadm 需要的所有镜像，例如 k8s.gcr.io/kube-*、k8s.gcr.io/etcd 和 k8s.gcr.io/pause 都支持多种架构。\nkubeadm 自动化 不必像文档kubeadm 基础教程所述，将从 kubeadm init 取得的令牌复制到每个节点，而是简单自动地并行化令牌分发。要实现自动化，您必须知道控制平面节点启动后将拥有的 IP 地址，或使用 DNS 名称或负载均衡器的地址。\nkubeadm 可以为你生成一个令牌：\nkubeadm token generate kubeadm alpha certs certificate-key 一旦集群启动起来，你就可以从控制平面节点的 /etc/kubernetes/admin.conf 文件获取管理凭证，然后你就可以使用这个凭证同集群通信。\n注意这种搭建集群的方式在安全保证上会有一些宽松，因为这种方式不允许使用 --discovery-token-ca-cert-hash 来验证根 CA 的哈希值（因为当配置节点的时候，它还没有被生成）。获取需更多信息请参阅kubeadm join文档。\n. heading \u0026ldquo;whatsnext\u0026rdquo; %}}  了解更多有关kubeadm init phase kubeadm join启动一个 Kubernetes 工作节点并且将其加入到集群 kubeadm upgrade将 Kubernetes 集群升级到新版本 kubeadm reset使用 kubeadm init 或 kubeadm join 来恢复对节点的变更  "
},
{
	"uri": "https://lijun.in/reference/kubectl/overview/",
	"title": "kubectl 概述",
	"tags": [],
	"description": "",
	"content": "Kubectl 是一个命令行接口，用于对 Kubernetes 集群运行命令。kubectl 在 $HOME/.kube 目录中寻找一个名为 config 的文件。您可以通过设置环境变量 KUBECONFIG 或设置 --kubeconfig 参数指定其它 kubeconfig 文件。\n本文概述了 kubectl 语法和命令操作描述，并提供了常见的示例。有关每个命令的详细信息，包括所有受支持的参数和子命令，请参阅 kubectl 参考文档。有关安装说明，请参见 安装 kubectl 。\n语法 使用以下语法 kubectl 从终端窗口运行命令：\nkubectl [command] [TYPE] [NAME] [flags] 其中 command、TYPE、NAME 和 flags 分别是：\n  command：指定要对一个或多个资源执行的操作，例如 create、get、describe、delete。\n  TYPE：指定资源类型。资源类型不区分大小写，可以指定单数、复数或缩写形式。例如，以下命令输出相同的结果:\n```shell kubectl get pod pod1 kubectl get pods pod1 kubectl get po pod1 ```      NAME：指定资源的名称。名称区分大小写。如果省略名称，则显示所有资源的详细信息 kubectl get pods。\n在对多个资源执行操作时，您可以按类型和名称指定每个资源，或指定一个或多个文件：\n    要按类型和名称指定资源：\n  要对所有类型相同的资源进行分组，请执行以下操作：TYPE1 name1 name2 name\u0026lt;#\u0026gt;。例子：kubectl get pod example-pod1 example-pod2\n  分别指定多个资源类型：TYPE1/name1 TYPE1/name2 TYPE2/name3 TYPE\u0026lt;#\u0026gt;/name\u0026lt;#\u0026gt;。例子：kubectl get pod/example-pod1 replicationcontroller/example-rc1\n    用一个或多个文件指定资源：-f file1 -f file2 -f file\u0026lt;#\u0026gt;\n 使用 YAML 而不是 JSON 因为 YAML 更容易使用，特别是用于配置文件时。例子：kubectl get pod -f ./pod.yaml    flags: 指定可选的参数。例如，可以使用 -s 或 -server 参数指定 Kubernetes API 服务器的地址和端口。  . caution \u0026gt;}}\n从命令行指定的参数会覆盖默认值和任何相应的环境变量。 . /caution \u0026gt;}}\n如果您需要帮助，只需从终端窗口运行 kubectl help 即可。\n操作 下表包含所有 kubectl 操作的简短描述和普通语法：\n   操作 语法 描述     annotate kubectl annotate (-f FILENAME | TYPE NAME | TYPE/NAME) KEY_1=VAL_1 \u0026hellip; KEY_N=VAL_N [\u0026ndash;overwrite] [\u0026ndash;all] [\u0026ndash;resource-version=version] [flags] 添加或更新一个或多个资源的注解。   api-versions kubectl api-versions [flags] 列出可用的 API 版本。   apply kubectl apply -f FILENAME [flags] 从文件或 stdin 对资源应用配置更改。   attach kubectl attach POD -c CONTAINER [-i] [-t] [flags] 附加到正在运行的容器，查看输出流或与容器（stdin）交互。   autoscale kubectl autoscale (-f FILENAME | TYPE NAME | TYPE/NAME) [\u0026ndash;min=MINPODS] \u0026ndash;max=MAXPODS [\u0026ndash;cpu-percent=CPU] [flags] 自动伸缩由副本控制器管理的一组 pod。   cluster-info kubectl cluster-info [flags] 显示有关集群中主服务器和服务的端口信息。   config kubectl config SUBCOMMAND [flags] 修改 kubeconfig 文件。有关详细信息，请参阅各个子命令。   create kubectl create -f FILENAME [flags] 从文件或 stdin 创建一个或多个资源。   delete kubectl delete (-f FILENAME | TYPE [NAME | /NAME | -l label | \u0026ndash;all]) [flags] 从文件、标准输入或指定标签选择器、名称、资源选择器或资源中删除资源。   describe kubectl describe (-f FILENAME | TYPE [NAME_PREFIX | /NAME | -l label]) [flags] 显示一个或多个资源的详细状态。   diff kubectl diff -f FILENAME [flags] 将 live 配置和文件或标准输入做对比 (BETA)   edit kubectl edit (-f FILENAME | TYPE NAME | TYPE/NAME) [flags] 使用默认编辑器编辑和更新服务器上一个或多个资源的定义。   exec kubectl exec POD [-c CONTAINER] [-i] [-t] [flags] [-- COMMAND [args...]] 对 pod 中的容器执行命令。   explain kubectl explain [--recursive=false] [flags] 获取多种资源的文档。例如 pod, node, service 等。   expose kubectl expose (-f FILENAME | TYPE NAME | TYPE/NAME) [\u0026ndash;port=port] [\u0026ndash;protocol=TCP|UDP] [\u0026ndash;target-port=number-or-name] [\u0026ndash;name=name] [\u0026ndash;external-ip=external-ip-of-service] [\u0026ndash;type=type] [flags] 将副本控制器、服务或 pod 作为新的 Kubernetes 服务暴露。   get kubectl get (-f FILENAME | TYPE [NAME | /NAME | -l label]) [\u0026ndash;watch] [\u0026ndash;sort-by=FIELD] [[-o | \u0026ndash;output]=OUTPUT_FORMAT] [flags] 列出一个或多个资源。   label kubectl label (-f FILENAME | TYPE NAME | TYPE/NAME) KEY_1=VAL_1 \u0026hellip; KEY_N=VAL_N [\u0026ndash;overwrite] [\u0026ndash;all] [\u0026ndash;resource-version=version] [flags] 添加或更新一个或多个资源的标签。   logs kubectl logs POD [-c CONTAINER] [--follow] [flags] 在 pod 中打印容器的日志。   patch kubectl patch (-f FILENAME | TYPE NAME | TYPE/NAME) \u0026ndash;patch PATCH [flags] 使用策略合并 patch 程序更新资源的一个或多个字段。   port-forward kubectl port-forward POD [LOCAL_PORT:]REMOTE_PORT [...[LOCAL_PORT_N:]REMOTE_PORT_N] [flags] 将一个或多个本地端口转发到一个 pod。   proxy kubectl proxy [--port=PORT] [--www=static-dir] [--www-prefix=prefix] [--api-prefix=prefix] [flags] 运行 Kubernetes API 服务器的代理。   replace kubectl replace -f FILENAME 从文件或标准输入中替换资源。   rolling-update kubectl rolling-update OLD_CONTROLLER_NAME ([NEW_CONTROLLER_NAME] \u0026ndash;image=NEW_CONTAINER_IMAGE | -f NEW_CONTROLLER_SPEC) [flags] 通过逐步替换指定的副本控制器及其 pod 来执行滚动更新。   run kubectl run NAME \u0026ndash;image=image [\u0026ndash;env=\u0026quot;key=value\u0026rdquo;] [\u0026ndash;port=port] [\u0026ndash;dry-run=server | client | none] [\u0026ndash;overrides=inline-json] [flags] 在集群上运行指定的镜像。   scale kubectl scale (-f FILENAME | TYPE NAME | TYPE/NAME) \u0026ndash;replicas=COUNT [\u0026ndash;resource-version=version] [\u0026ndash;current-replicas=count] [flags] 更新指定副本控制器的大小。   stop kubectl stop 不推荐：相反，请参阅 kubectl delete。   version kubectl version [--client] [flags] 显示运行在客户端和服务器上的 Kubernetes 版本。    记住：有关命令操作的更多信息，请参阅 kubectl 参考文档。\n资源类型 下表列出所有受支持的资源类型及其缩写别名:\n(以下输出可以通过 kubectl api-resources 获取，内容以 Kubernetes 1.13.3 版本为准。)\n   资源名 缩写名 API 分组 按命名空间 资源类型     componentstatuses cs  false ComponentStatus   configmaps cm  true ConfigMap   endpoints ep  true Endpoints   limitranges limits  true LimitRange   namespaces ns  false Namespace   nodes no  false Node   persistentvolumeclaims pvc  true PersistentVolumeClaim   persistentvolumes pv  false PersistentVolume   pods po  true Pod   podtemplates   true PodTemplate   replicationcontrollers rc  true ReplicationController   resourcequotas quota  true ResourceQuota   secrets   true Secret   serviceaccounts sa  true ServiceAccount   services svc  true Service   mutatingwebhookconfigurations  admissionregistration.k8s.io false MutatingWebhookConfiguration   validatingwebhookconfigurations  admissionregistration.k8s.io false ValidatingWebhookConfiguration   customresourcedefinitions crd, crds apiextensions.k8s.io false CustomResourceDefinition   apiservices  apiregistration.k8s.io false APIService   controllerrevisions  apps true ControllerRevision   daemonsets ds apps true DaemonSet   deployments deploy apps true Deployment   replicasets rs apps true ReplicaSet   statefulsets sts apps true StatefulSet   tokenreviews  authentication.k8s.io false TokenReview   localsubjectaccessreviews  authorization.k8s.io true LocalSubjectAccessReview   selfsubjectaccessreviews  authorization.k8s.io false SelfSubjectAccessReview   selfsubjectrulesreviews  authorization.k8s.io false SelfSubjectRulesReview   subjectaccessreviews  authorization.k8s.io false SubjectAccessReview   horizontalpodautoscalers hpa autoscaling true HorizontalPodAutoscaler   cronjobs cj batch true CronJob   jobs  batch true Job   certificatesigningrequests csr certificates.k8s.io false CertificateSigningRequest   leases  coordination.k8s.io true Lease   events ev events.k8s.io true Event   ingresses ing extensions true Ingress   networkpolicies netpol networking.k8s.io true NetworkPolicy   poddisruptionbudgets pdb policy true PodDisruptionBudget   podsecuritypolicies psp policy false PodSecurityPolicy   clusterrolebindings  rbac.authorization.k8s.io false ClusterRoleBinding   clusterroles  rbac.authorization.k8s.io false ClusterRole   rolebindings  rbac.authorization.k8s.io true RoleBinding   roles  rbac.authorization.k8s.io true Role   priorityclasses pc scheduling.k8s.io false PriorityClass   storageclasses sc storage.k8s.io false StorageClass   volumeattachments  storage.k8s.io false VolumeAttachment    输出选项 有关如何格式化或排序某些命令的输出的信息，请使用以下部分。有关哪些命令支持各种输出选项的详细信息，请参阅kubectl 参考文档。\n格式化输出 所有 kubectl 命令的默认输出格式都是人类可读的纯文本格式。要以特定格式向终端窗口输出详细信息，可以将 -o 或 --output 参数添加到受支持的 kubectl 命令中。\n语法 kubectl [command] [TYPE] [NAME] -o=\u0026lt;output_format\u0026gt; 根据 kubectl 操作，支持以下输出格式：\n   Output format Description     -o custom-columns=\u0026lt;spec\u0026gt; 使用逗号分隔的自定义列列表打印表。   -o custom-columns-file=\u0026lt;filename\u0026gt; 使用 \u0026lt;filename\u0026gt; 文件中的自定义列模板打印表。   -o json 输出 JSON 格式的 API 对象   -o jsonpath=\u0026lt;template\u0026gt; 打印 jsonpath 表达式定义的字段   -o jsonpath-file=\u0026lt;filename\u0026gt; 打印 \u0026lt;filename\u0026gt; 文件中 jsonpath 表达式定义的字段。   -o name 仅打印资源名称而不打印任何其他内容。   -o wide 以纯文本格式输出，包含任何附加信息。对于 pod 包含节点名。   -o yaml 输出 YAML 格式的 API 对象。    示例 在此示例中，以下命令将单个 pod 的详细信息输出为 YAML 格式的对象：\nkubectl get pod web-pod-13je7 -o yaml 请记住：有关每个命令支持哪种输出格式的详细信息，请参阅 kubectl 参考文档。\n自定义列 要定义自定义列并仅将所需的详细信息输出到表中，可以使用该 custom-columns 选项。您可以选择内联定义自定义列或使用模板文件：-o=custom-columns=\u0026lt;spec\u0026gt; 或 -o=custom-columns-file=\u0026lt;filename\u0026gt;。\n示例 内联：\n$ kubectl get pods \u0026lt;pod-name\u0026gt; -o custom-columns=NAME:.metadata.name,RSRC:.metadata.resourceVersion 模板文件：\nkubectl get pods \u0026lt;pod-name\u0026gt; -o custom-columns-file=template.txt 其中，template.txt 文件包含：\nNAME RSRC metadata.name metadata.resourceVersion 运行任何一个命令的结果是:\nNAME RSRC submit-queue 610995 Server-side 列 kubectl 支持从服务器接收关于对象的特定列信息。 这意味着对于任何给定的资源，服务器将返回与该资源相关的列和行，以便客户端打印。 通过让服务器封装打印的细节，这允许在针对同一集群使用的客户端之间提供一致的人类可读输出。\n默认情况下，此功能在 kubectl 1.11 及更高版本中启用。要禁用它，请将该 --server-print=false 参数添加到 kubectl get 命令中。\n例子： 要打印有关 pod 状态的信息，请使用如下命令：\nkubectl get pods \u0026lt;pod-name\u0026gt; --server-print=false 输出如下：\nNAME READY STATUS RESTARTS AGE pod-name 1/1 Running 0 1m 排序列表对象 要将对象排序后输出到终端窗口，可以将 --sort-by 参数添加到支持的 kubectl 命令。通过使用 --sort-by 参数指定任何数字或字符串字段来对对象进行排序。要指定字段，请使用 jsonpath 表达式。\n语法 kubectl [command] [TYPE] [NAME] --sort-by=\u0026lt;jsonpath_exp\u0026gt; 示例 要打印按名称排序的 pod 列表，请运行：\n$ kubectl get pods --sort-by=.metadata.name 示例：常用操作 使用以下示例集来帮助您熟悉运行常用 kubectl 操作：\nkubectl apply - 以文件或标准输入为准应用或更新资源。\n# 使用 example-service.yaml 中的定义创建服务。 kubectl apply -f example-service.yaml # 使用 example-controller.yaml 中的定义创建 replication controller。 kubectl apply -f example-controller.yaml # 使用 \u0026lt;directory\u0026gt; 路径下的任意 .yaml, .yml, 或 .json 文件 创建对象。 kubectl apply -f \u0026lt;directory\u0026gt; kubectl get - 列出一个或多个资源。\n# 以纯文本输出格式列出所有 pod。 kubectl get pods # 以纯文本输出格式列出所有 pod，并包含附加信息(如节点名)。 kubectl get pods -o wide # 以纯文本输出格式列出具有指定名称的副本控制器。提示：您可以使用别名 \u0026#39;rc\u0026#39; 缩短和替换 \u0026#39;replicationcontroller\u0026#39; 资源类型。 kubectl get replicationcontroller \u0026lt;rc-name\u0026gt; # 以纯文本输出格式列出所有副本控制器和服务。 kubectl get rc,services # 以纯文本输出格式列出所有守护程序集，包括未初始化的守护程序集。 kubectl get ds --include-uninitialized # 列出在节点 server01 上运行的所有 pod kubectl get pods --field-selector=spec.nodeName=server01 kubectl describe - 显示一个或多个资源的详细状态，默认情况下包括未初始化的资源。\n# 显示名称为 \u0026lt;node-name\u0026gt; 的节点的详细信息。 kubectl describe nodes \u0026lt;node-name\u0026gt; # 显示名为 \u0026lt;pod-name\u0026gt; 的 pod 的详细信息。 kubectl describe pods/\u0026lt;pod-name\u0026gt; # 显示由名为 \u0026lt;rc-name\u0026gt; 的副本控制器管理的所有 pod 的详细信息。 # 记住：副本控制器创建的任何 pod 都以复制控制器的名称为前缀。 kubectl describe pods \u0026lt;rc-name\u0026gt; # 描述所有的 pod，不包括未初始化的 pod kubectl describe pods --include-uninitialized=false . note \u0026gt;}}\nkubectl get 命令通常用于检索同一资源类型的一个或多个资源。 它具有丰富的参数，允许您使用 -o 或 --output 参数自定义输出格式。您可以指定 -w 或 --watch 参数以开始观察特定对象的更新。 kubectl describe 命令更侧重于描述指定资源的许多相关方面。它可以调用对 API 服务器 的多个 API 调用来为用户构建视图。 例如，该 kubectl describe node 命令不仅检索有关节点的信息，还检索在其上运行的 pod 的摘要，为节点生成的事件等。\n. /note \u0026gt;}}\nkubectl delete - 从文件、stdin 或指定标签选择器、名称、资源选择器或资源中删除资源。\n# 使用 pod.yaml 文件中指定的类型和名称删除 pod。 kubectl delete -f pod.yaml # 删除标签名= \u0026lt;label-name\u0026gt; 的所有 pod 和服务。 kubectl delete pods,services -l name=\u0026lt;label-name\u0026gt; # 删除所有具有标签名称= \u0026lt;label-name\u0026gt; 的 pod 和服务，包括未初始化的那些。 kubectl delete pods,services -l name=\u0026lt;label-name\u0026gt; --include-uninitialized # 删除所有 pod，包括未初始化的 pod。 kubectl delete pods --all kubectl exec - 对 pod 中的容器执行命令。\n# 从 pod \u0026lt;pod-name\u0026gt; 中获取运行 \u0026#39;date\u0026#39; 的输出。默认情况下，输出来自第一个容器。 kubectl exec \u0026lt;pod-name\u0026gt; date # 运行输出 \u0026#39;date\u0026#39; 获取在容器的 \u0026lt;container-name\u0026gt; 中 pod \u0026lt;pod-name\u0026gt; 的输出。 kubectl exec \u0026lt;pod-name\u0026gt; -c \u0026lt;container-name\u0026gt; date # 获取一个交互 TTY 并运行 /bin/bash \u0026lt;pod-name \u0026gt;。默认情况下，输出来自第一个容器。 kubectl exec -ti \u0026lt;pod-name\u0026gt; /bin/bash kubectl logs - 打印 Pod 中容器的日志。\n# 从 pod 返回日志快照。 kubectl logs \u0026lt;pod-name\u0026gt; # 从 pod \u0026lt;pod-name\u0026gt; 开始流式传输日志。这类似于 \u0026#39;tail -f\u0026#39; Linux 命令。 kubectl logs -f \u0026lt;pod-name\u0026gt; 示例：创建和使用插件 使用以下示例来帮助您熟悉编写和使用 kubectl 插件：\n# 用任何语言创建一个简单的插件，并为生成的可执行文件命名 # 以前缀 \u0026#34;kubectl-\u0026#34; 开始 cat ./kubectl-hello #!/bin/bash # 这个插件打印单词 \u0026#34;hello world\u0026#34; echo \u0026#34;hello world\u0026#34; # 我们的插件写好了，让我们把它变成可执行的 sudo chmod +x ./kubectl-hello # 并将其移动到路径中的某个位置 sudo mv ./kubectl-hello /usr/local/bin # 我们现在已经创建并\u0026#34;安装\u0026#34;了一个 kubectl 插件。 # 我们可以开始使用我们的插件，从 kubectl 调用它，就像它是一个常规命令一样 kubectl hello hello world # 我们可以\u0026quot;卸载\u0026quot;一个插件，只需从我们的路径中删除它 sudo rm /usr/local/bin/kubectl-hello 为了查看可用的所有 kubectl 插件，我们可以使用 kubectl plugin list 子命令：\nkubectl plugin list 以下 kubectl-适配 的插件是可用的： /usr/local/bin/kubectl-hello /usr/local/bin/kubectl-foo /usr/local/bin/kubectl-bar # 这个指令也可以警告我们哪些插件 # 被运行，或是被其它插件覆盖了 # 例如 sudo chmod -x /usr/local/bin/kubectl-foo kubectl plugin list 以下 kubectl-适配 的插件是可用的： /usr/local/bin/kubectl-hello /usr/local/bin/kubectl-foo - 警告: /usr/local/bin/kubectl-foo 被识别为一个插件，但是它并不可以执行 /usr/local/bin/kubectl-bar 错误: 发现了一个插件警告 我们可以将插件视为在现有 kubectl 命令之上构建更复杂功能的一种方法：\ncat ./kubectl-whoami #!/bin/bash # 这个插件借用 `kubectl config` 指令来输出 # 当前用户的信息，基于当前指定的 context kubectl config view --template=\u0026#39;{{ range .contexts }}{{ if eq .name \u0026#34;\u0026#39;$(kubectl config current-context)\u0026#39;\u0026#34; }}Current user: {{ .context.user }}{{ end }}{{ end }}\u0026#39; 运行上面的插件为我们提供了一个输出，其中包含我们 KUBECONFIG 文件中当前所选定上下文对应的用户：\n# 使文件成为可执行的 sudo chmod +x ./kubectl-whoami # 然后移动到我们的路径中 sudo mv ./kubectl-whoami /usr/local/bin kubectl whoami Current user: plugins-user 要了解关于插件的更多信息，请查看示例 cli 插件。\n. heading \u0026ldquo;whatsnext\u0026rdquo; %}} 开始使用 kubectl 命令。\n"
},
{
	"uri": "https://lijun.in/reference/issues-security/security/",
	"title": "Kubernetes 安全和信息披露",
	"tags": [],
	"description": "",
	"content": "本页面介绍 Kubernetes 安全和信息披露相关的内容。\n安全公告 加入 kubernets-announce 组，以获取关于安全性和主要 API 公告的电子邮件。\n您也可以使用此链接订阅上述的 RSS 反馈。\n报告一个漏洞 我们非常感谢向 Kubernetes 开源社区报告漏洞的安全研究人员和用户。 所有的报告都由社区志愿者进行彻底调查。\n如需报告，请连同安全细节以及预期的所有 Kubernetes bug 报告详细信息电邮到security@kubernetes.io 列表。\n您还可以通过电子邮件向私有 security@kubernetes.io 列表发送电子邮件，邮件中应该包含所有 Kubernetes 错误报告所需的详细信息。\n您可以使用产品安全团队成员的 GPG 密钥加密您的电子邮件到此列表。 使用 GPG 加密不需要公开。\n我应该在什么时候报告漏洞？  您认为在 Kubernetes 中发现了一个潜在的安全漏洞 您不确定漏洞如何影响 Kubernetes 您认为您在 Kubernetes 依赖的另一个项目中发现了一个漏洞  对于具有漏洞报告和披露流程的项目，请直接在该项目处报告    我什么时候不应该报告漏洞？  您需要帮助调整 Kubernetes 组件的安全性 您需要帮助应用与安全相关的更新 您的问题与安全无关  安全漏洞响应 每个报告在 3 个工作日内由产品安全团队成员确认和分析。这将启动安全发布过程。\n与产品安全团队共享的任何漏洞信息都保留在 Kubernetes 项目中，除非有必要修复该问题，否则不会传播到其他项目。\n随着安全问题从分类、识别修复、发布计划等方面的进展，我们将不断更新报告。\n公开披露时间 公开披露日期由 Kubernetes 产品安全团队和 bug 提交者协商。我们倾向于在用户缓解措施可用时尽快完全披露该 bug。\n当 bug 或其修复还没有被完全理解，解决方案没有经过良好的测试，或者为了处理供应商协调问题时，延迟披露是合理的。\n信息披露的时间范围从即时（尤其是已经公开的）到几周。作为一个基本的约定，我们希望报告日期到披露日期的间隔是 7 天。在设置披露日期时，Kubernetes 产品安全团队拥有最终决定权。\n"
},
{
	"uri": "https://lijun.in/concepts/overview/components/",
	"title": "Kubernetes 组件",
	"tags": [],
	"description": "",
	"content": "当你部署完 Kubernetes, 即拥有了一个完整的集群。 term_id=\u0026quot;cluster\u0026rdquo; length=\u0026quot;all\u0026rdquo; prepend=\u0026quot;一个 Kubernetes 集群包含\u0026quot;\u0026gt;}}\n本文档概述了交付正常运行的 Kubernetes 集群所需的各种组件。\n这张图表展示了包含所有相互关联组件的 Kubernetes 集群。\n控制平面组件（Control Plane Components） 控制平面的组件对集群做出全局决策(比如调度)，以及检测和响应集群事件（例如，当不满足部署的 replicas 字段时，启动新的 text=\u0026quot;pod\u0026rdquo; term_id=\u0026quot;pod\u0026quot;\u0026gt;}}）。\n控制平面组件可以在集群中的任何节点上运行。然而，为了简单起见，设置脚本通常会在同一个计算机上启动所有控制平面组件，并且不会在此计算机上运行用户容器。请参阅构建高可用性集群中对于多主机 VM 的设置示例。\nkube-apiserver term_id=\u0026quot;kube-apiserver\u0026rdquo; length=\u0026quot;all\u0026rdquo; \u0026gt;}}\netcd term_id=\u0026quot;etcd\u0026rdquo; length=\u0026quot;all\u0026rdquo; \u0026gt;}}\nkube-scheduler term_id=\u0026quot;kube-scheduler\u0026rdquo; length=\u0026quot;all\u0026rdquo; \u0026gt;}}\nkube-controller-manager n term_id=\u0026quot;kube-controller-manager\u0026rdquo; length=\u0026quot;all\u0026rdquo; \u0026gt;}}\n这些控制器包括:\n 节点控制器（Node Controller）: 负责在节点出现故障时进行通知和响应。 副本控制器（Replication Controller）: 负责为系统中的每个副本控制器对象维护正确数量的 Pod。 端点控制器（Endpoints Controller）: 填充端点(Endpoints)对象(即加入 Service 与 Pod)。 服务帐户和令牌控制器（Service Account \u0026amp; Token Controllers）: 为新的命名空间创建默认帐户和 API 访问令牌.  云控制器管理器-(cloud-controller-manager) cloud-controller-manager 运行与基础云提供商交互的控制器。cloud-controller-manager 二进制文件是 Kubernetes 1.6 版本中引入的 alpha 功能。\ncloud-controller-manager 仅运行云提供商特定的控制器循环。您必须在 kube-controller-manager 中禁用这些控制器循环，您可以通过在启动 kube-controller-manager 时将 --cloud-provider 参数设置为 external 来禁用控制器循环。\ncloud-controller-manager 允许云供应商的代码和 Kubernetes 代码彼此独立地发展。在以前的版本中，核心的 Kubernetes 代码依赖于特定云提供商的代码来实现功能。在将来的版本中，云供应商专有的代码应由云供应商自己维护，并与运行 Kubernetes 的云控制器管理器相关联。\n以下控制器具有云提供商依赖性:\n 节点控制器（Node Controller）: 用于检查云提供商以确定节点是否在云中停止响应后被删除 路由控制器（Route Controller）: 用于在底层云基础架构中设置路由 服务控制器（Service Controller）: 用于创建、更新和删除云提供商负载均衡器 数据卷控制器（Volume Controller）: 用于创建、附加和装载卷、并与云提供商进行交互以编排卷  Node 组件 节点组件在每个节点上运行，维护运行的 Pod 并提供 Kubernetes 运行环境。\nkubelet term_id=\u0026quot;kubelet\u0026rdquo; length=\u0026quot;all\u0026rdquo; \u0026gt;}}\nkube-proxy term_id=\u0026quot;kube-proxy\u0026rdquo; length=\u0026quot;all\u0026rdquo; \u0026gt;}}\n容器运行环境(Container Runtime) term_id=\u0026quot;container-runtime\u0026rdquo; length=\u0026quot;all\u0026rdquo; \u0026gt;}}\n插件(Addons) 插件使用 Kubernetes 资源 (term_id=\u0026quot;daemonset\u0026rdquo; \u0026gt;}}, term_id=\u0026quot;deployment\u0026rdquo; \u0026gt;}}等) 实现集群功能。因为这些提供集群级别的功能，所以插件的命名空间资源属于 kube-system 命名空间。\n所选的插件如下所述：有关可用插件的扩展列表，请参见插件 (Addons)。\nDNS 尽管并非严格要求其他附加组件，但所有示例都依赖集群 DNS，因此所有 Kubernetes 集群都应具有 DNS。\n除了您环境中的其他 DNS 服务器之外，集群 DNS 还是一个 DNS 服务器，它为 Kubernetes 服务提供 DNS 记录。\nCluster DNS 是一个 DNS 服务器，和您部署环境中的其他 DNS 服务器一起工作，为 Kubernetes 服务提供DNS记录。\nKubernetes 启动的容器自动将 DNS 服务器包含在 DNS 搜索中。\n用户界面(Dashboard) Dashboard 是 Kubernetes 集群的通用基于 Web 的 UI。它使用户可以管理集群中运行的应用程序以及集群本身并进行故障排除。\n容器资源监控 容器资源监控将关于容器的一些常见的时间序列度量值保存到一个集中的数据库中，并提供用于浏览这些数据的界面。\n集群层面日志 集群层面日志 机制负责将容器的日志数据保存到一个集中的日志存储中，该存储能够提供搜索和浏览接口。\n  进一步了解 Nodes 进一步了解 kube-scheduler 阅读 etcd 官方文档  "
},
{
	"uri": "https://lijun.in/concepts/services-networking/dns-pod-service/",
	"title": "Pod 与 Service 的 DNS",
	"tags": [],
	"description": "",
	"content": "该页面概述了Kubernetes对DNS的支持。\n介绍 Kubernetes DNS 在群集上调度 DNS Pod 和服务，并配置 kubelet 以告知各个容器使用 DNS 服务的 IP 来解析 DNS 名称。\n怎样获取 DNS 名字? 在集群中定义的每个 Service（包括 DNS 服务器自身）都会被指派一个 DNS 名称。 默认，一个客户端 Pod 的 DNS 搜索列表将包含该 Pod 自己的 Namespace 和集群默认域。 通过如下示例可以很好地说明：\n假设在 Kubernetes 集群的 Namespace bar 中，定义了一个Service foo。 运行在Namespace bar 中的一个 Pod，可以简单地通过 DNS 查询 foo 来找到该 Service。 运行在 Namespace quux 中的一个 Pod 可以通过 DNS 查询 foo.bar 找到该 Service。\n以下各节详细介绍了受支持的记录类型和支持的布局。 其中代码部分的布局，名称或查询命令均被视为实现细节，如有更改，恕不另行通知。 有关最新规范请查看 Kubernetes 基于 DNS 的服务发现.\n支持的 DNS 模式 下面各段详细说明支持的记录类型和布局。 如果任何其它的布局、名称或查询，碰巧也能够使用，这就需要研究下它们的实现细节，以免后续修改它们又不能使用了。\nService A 记录 “正常” Service（除了 Headless Service）会以 my-svc.my-namespace.svc.cluster-domain.example 这种名字的形式被指派一个 DNS A 记录。 这会解析成该 Service 的 Cluster IP。\n“Headless” Service（没有Cluster IP）也会以 my-svc.my-namespace.svc.cluster-domain.example 这种名字的形式被指派一个 DNS A 记录。 不像正常 Service，它会解析成该 Service 选择的一组 Pod 的 IP。 希望客户端能够使用这一组 IP，否则就使用标准的 round-robin 策略从这一组 IP 中进行选择。\nSRV 记录 命名端口需要创建 SRV 记录，这些端口是正常 Service或 Headless Services 的一部分。 对每个命名端口，SRV 记录具有 _my-port-name._my-port-protocol.my-svc.my-namespace.svc.cluster-domain.example 这种形式。 对普通 Service，这会被解析成端口号和 CNAME：my-svc.my-namespace.svc.cluster-domain.example。 对 Headless Service，这会被解析成多个结果，Service 对应的每个 backend Pod 各一个， 包含 auto-generated-name.my-svc.my-namespace.svc.cluster-domain.example 这种形式 Pod 的端口号和 CNAME。\nPods Pod的 hostname 和 subdomain 字段 当前，创建 Pod 后，它的主机名是该 Pod 的 metadata.name 值。\nPodSpec 有一个可选的 hostname 字段，可以用来指定 Pod 的主机名。当这个字段被设置时，它将优先于 Pod 的名字成为该 Pod 的主机名。举个例子，给定一个 hostname 设置为 \u0026ldquo;my-host\u0026rdquo; 的 Pod，该 Pod 的主机名将被设置为 \u0026ldquo;my-host\u0026rdquo;。\nPodSpec 还有一个可选的 subdomain 字段，可以用来指定 Pod 的子域名。举个例子，一个 Pod 的 hostname 设置为 “foo”，subdomain 设置为 “bar”，在 namespace “my-namespace” 中对应的完全限定域名（FQDN）为 “foo.bar.my-namespace.svc.cluster-domain.example”。\n实例:\napiVersion: v1 kind: Service metadata: name: default-subdomain spec: selector: name: busybox clusterIP: None ports: - name: foo # Actually, no port is needed. port: 1234 targetPort: 1234 --- apiVersion: v1 kind: Pod metadata: name: busybox1 labels: name: busybox spec: hostname: busybox-1 subdomain: default-subdomain containers: - image: busybox:1.28 command: - sleep - \u0026#34;3600\u0026#34; name: busybox --- apiVersion: v1 kind: Pod metadata: name: busybox2 labels: name: busybox spec: hostname: busybox-2 subdomain: default-subdomain containers: - image: busybox:1.28 command: - sleep - \u0026#34;3600\u0026#34; name: busybox 如果 Headless Service 与 Pod 在同一个 Namespace 中，它们具有相同的子域名，集群的 KubeDNS 服务器也会为该 Pod 的完整合法主机名返回 A 记录。 例如，在同一个 Namespace 中，给定一个主机名为 “busybox-1” 的 Pod，子域名设置为 “default-subdomain”，名称为 “default-subdomain” 的 Headless Service ，Pod 将看到自己的 FQDN 为 “busybox-1.default-subdomain.my-namespace.svc.cluster.local”。 DNS 会为那个名字提供一个 A 记录，指向该 Pod 的 IP。 “busybox1” 和 “busybox2” 这两个 Pod 分别具有它们自己的 A 记录。\n端点对象可以为任何端点地址及其 IP 指定 hostname。\n因为没有为 Pod 名称创建A记录，所以要创建 Pod 的 A 记录需要 hostname 。\n没有 hostname 但带有 subdomain 的 Pod 只会为指向Pod的IP地址的 headless 服务创建 A 记录(default-subdomain.my-namespace.svc.cluster-domain.example)。 另外，除非在服务上设置了 publishNotReadyAddresses=True，否则 Pod 需要准备好 A 记录。\n \u0026ldquo;Default\u0026rdquo;: Pod从运行所在的节点继承名称解析配置。 参考 相关讨论 获取更多信息。 \u0026ldquo;ClusterFirst\u0026rdquo;: 与配置的群集域后缀不匹配的任何DNS查询(例如 “www.kubernetes.io” )都将转发到从节点继承的上游名称服务器。 群集管理员可能配置了额外的存根域和上游DNS服务器。 See 相关讨论 获取如何 DNS 的查询和处理信息的相关资料。 \u0026ldquo;ClusterFirstWithHostNet\u0026rdquo;: 对于与 hostNetwork 一起运行的 Pod，应显式设置其DNS策略 \u0026ldquo;ClusterFirstWithHostNet\u0026rdquo;。 \u0026ldquo;None\u0026rdquo;: 它允许 Pod 忽略 Kubernetes 环境中的 DN S设置。 应该使用 Pod Spec 中的 dnsConfig 字段提供所有 DNS 设置。  \u0026ldquo;Default\u0026rdquo; 不是默认的 DNS 策略。 如果未明确指定 dnsPolicy，则使用 “ClusterFirst”。\n下面的示例显示了一个Pod，其DNS策略设置为 \u0026ldquo;ClusterFirstWithHostNet\u0026rdquo;，因为它已将 hostNetwork 设置为 true。\napiVersion: v1 kind: Pod metadata: name: busybox namespace: default spec: containers: - image: busybox:1.28 command: - sleep - \u0026#34;3600\u0026#34; imagePullPolicy: IfNotPresent name: busybox restartPolicy: Always hostNetwork: true dnsPolicy: ClusterFirstWithHostNet Pod 的 DNS 设定 Pod 的 DNS 配置可让用户对 Pod 的 DNS 设置进行更多控制。\ndnsConfig 字段是可选的，它可以与任何 dnsPolicy 设置一起使用。 但是，当 Pod 的 dnsPolicy 设置为 \u0026ldquo;None\u0026rdquo; 时，必须指定 dnsConfig 字段。\n用户可以在 dnsConfig 字段中指定以下属性：\n nameservers: 将用作于 Pod 的 DNS 服务器的 IP 地址列表。最多可以指定3个 IP 地址。 当 Pod 的 dnsPolicy 设置为 \u0026ldquo;None\u0026rdquo; 时，列表必须至少包含一个IP地址，否则此属性是可选的。列出的服务器将合并到从指定的 DNS 策略生成的基本名称服务器，并删除重复的地址。 searches: 用于在 Pod 中查找主机名的 DNS 搜索域的列表。此属性是可选的。指定后，提供的列表将合并到根据所选 DNS 策略生成的基本搜索域名中。 重复的域名将被删除。 Kubernetes最多允许6个搜索域。 options: 对象的可选列表，其中每个对象可能具有 name 属性（必需）和 value 属性（可选）。 此属性中的内容将合并到从指定的 DNS 策略生成的选项。 重复的条目将被删除。  以下是具有自定义DNS设置的Pod示例：\ncodenew file=\u0026quot;service/networking/custom-dns.yaml\u0026rdquo; \u0026gt;}}\n创建上面的Pod后，容器 test 会在其 /etc/resolv.conf 文件中获取以下内容：\nnameserver 1.2.3.4 search ns1.svc.cluster-domain.example my.dns.search.suffix options ndots:2 edns0 对于IPv6设置，搜索路径和名称服务器应按以下方式设置：\nkubectl exec -it dns-example -- cat /etc/resolv.conf 有以下输出：\nnameserver fd00:79:30::a search default.svc.cluster-domain.example svc.cluster-domain.example cluster-domain.example options ndots:5 可用功能 Pod DNS 配置和 DNS 策略 \u0026ldquo;None\u0026rdquo; 的版本对应如下所示。\n   k8s version Feature support     1.14 Stable   1.10 Beta (on by default)   1.9 Alpha     有关管理 DNS 配置的指导，请查看 配置 DNS 服务\n"
},
{
	"uri": "https://lijun.in/concepts/configuration/pod-overhead/",
	"title": "Pod 开销",
	"tags": [],
	"description": "",
	"content": "for_k8s_version=\u0026quot;v1.18\u0026rdquo; state=\u0026quot;beta\u0026rdquo; \u0026gt;}}\n在节点上运行 Pod 时，Pod 本身占用大量系统资源。这些资源是运行 Pod 内容器所需资源的附加资源。 POD 开销 是一个特性，用于计算 Pod 基础设施在容器请求和限制之上消耗的资源。\nPod 开销 在 Kubernetes 中，Pod 的开销是根据与 Pod 的 RuntimeClass 相关联的开销在 准入 时设置的。\n当启用 Pod 开销时，在调度 Pod 时，除了考虑容器资源请求的总和外，还要考虑 Pod 开销。类似地，Kubelet 将在确定 Pod cgroup 的大小和执行 Pod 驱逐排序时包含 Pod 开销。\n启用 Pod 开销 您需要确保在集群中启用了 PodOverhead 特性门（在 1.18 默认是开启的），以及一个用于定义 overhead 字段的 RuntimeClass。\n使用示例 要使用 PodOverhead 特性，需要一个定义 overhead 字段的 RuntimeClass. 作为例子，可以在虚拟机和来宾操作系统中通过一个虚拟化容器运行时来定义 RuntimeClass 如下，其中每个 Pod 大约使用 120MiB:\n--- kind: RuntimeClass apiVersion: node.k8s.io/v1beta1 metadata: name: kata-fc handler: kata-fc overhead: podFixed: memory: \u0026#34;120Mi\u0026#34; cpu: \u0026#34;250m\u0026#34; 通过指定 kata-fc RuntimeClass 处理程序创建的工作负载会将内存和 cpu 开销计入资源配额计算、节点调度以及 Pod cgroup 分级。\n假设我们运行下面给出的工作负载示例 test-pod:\napiVersion: v1 kind: Pod metadata: name: test-pod spec: runtimeClassName: kata-fc containers: - name: busybox-ctr image: busybox stdin: true tty: true resources: limits: cpu: 500m memory: 100Mi - name: nginx-ctr image: nginx resources: limits: cpu: 1500m memory: 100Mi 在准入阶段 RuntimeClass 准入控制器 更新工作负载的 PodSpec 以包含 RuntimeClass 中定义的 overhead. 如果 PodSpec 中该字段已定义，该 Pod 将会被拒绝。在这个例子中，由于只指定了 RuntimeClass 名称，所以准入控制器更新了 Pod, 包含了一个 overhead.\n在 RuntimeClass 准入控制器之后，可以检验一下已更新的 PodSpec:\nkubectl get pod test-pod -o jsonpath=\u0026#39;{.spec.overhead}\u0026#39; 输出：\nmap[cpu:250m memory:120Mi] 如果定义了 ResourceQuata, 则容器请求的总量以及 overhead 字段都将计算在内。\n当 kube-scheduler 决定在哪一个节点调度运行新的 Pod 时，调度器会兼顾该 Pod 的 overhead 以及该 Pod 的容器请求总量。在这个示例中，调度器将资源请求和开销相加，然后寻找具备 2.25 CPU 和 320 MiB 内存可用的节点。\n一旦 Pod 调度到了某个节点， 该节点上的 kubelet 将为该 Pod 新建一个 text=\u0026quot;cgroup\u0026rdquo; term_id=\u0026quot;cgroup\u0026rdquo; \u0026gt;}}. 底层容器运行时将在这个 pod 中创建容器。\n如果该资源对每一个容器都定义了一个限制（定义了受限的 Guaranteed QoS 或者 Bustrable QoS），kubelet 会为与该资源（CPU 的 cpu.cfs_quota_us 以及内存的 memory.limit_in_bytes） 相关的 pod cgroup 设定一个上限。该上限基于容器限制总量与 PodSpec 中定义的 overhead 之和。\n对于 CPU, 如果 Pod 的 QoS 是 Guaranteed 或者 Burstable, kubelet 会基于容器请求总量与 PodSpec 中定义的 overhead 之和设置 cpu.shares.\n请看这个例子，验证工作负载的容器请求：\nkubectl get pod test-pod -o jsonpath=\u0026#39;{.spec.containers[*].resources.limits}\u0026#39; 容器请求总计 2000m CPU 和 200MiB 内存：\nmap[cpu: 500m memory:100Mi] map[cpu:1500m memory:100Mi] 对照从节点观察到的情况来检查一下：\nkubectl describe node | grep test-pod -B2 该输出显示请求了 2250m CPU 以及 320MiB 内存，包含了 PodOverhead 在内：\n Namespace Name CPU Requests CPU Limits Memory Requests Memory Limits AGE --------- ---- ------------ ---------- --------------- ------------- --- default test-pod 2250m (56%) 2250m (56%) 320Mi (1%) 320Mi (1%) 36m 验证 Pod cgroup 限制 在工作负载所运行的节点上检查 Pod 的内存 cgroups. 在接下来的例子中，将在该节点上使用具备 CRI 兼容的容器运行时命令行工具 crictl. 这是一个展示 PodOverhead 行为的进阶示例，用户并不需要直接在该节点上检查 cgroups.\n首先在特定的节点上确定该 Pod 的标识符：ying\n​```bash\n在该 Pod 调度的节点上执行如下命令： POD_ID=\u0026quot;$(sudo crictl pods \u0026ndash;name test-pod -q)\u0026rdquo;\n \u0026lt;!-- From this, you can determine the cgroup path for the Pod: --\u0026gt; 可以依此判断该 Pod 的 cgroup 路径： \u0026lt;!-- ```bash # Run this on the node where the Pod is scheduled --\u0026gt; ​```bash # 在该 Pod 调度的节点上执行如下命令： sudo crictl inspectp -o=json $POD_ID | grep cgroupsPath 执行结果的 cgroup 路径中包含了该 Pod 的 pause 容器。Pod 级别的 cgroup 即上面的一个目录。\n \u0026quot;cgroupsPath\u0026quot;: \u0026quot;/kubepods/podd7f4b509-cf94-4951-9417-d1087c92a5b2/7ccf55aee35dd16aca4189c952d83487297f3cd760f1bbf09620e206e7d0c27a\u0026quot; 在这个例子中，该 pod 的 cgroup 路径是 kubepods/podd7f4b509-cf94-4951-9417-d1087c92a5b2。验证内存的 Pod 级别 cgroup 设置：\n​```bash\n在该 Pod 调度的节点上执行这个命令。 另外，修改 cgroup 的名称以匹配为该 pod 分配的 cgroup。 cat /sys/fs/cgroup/memory/kubepods/podd7f4b509-cf94-4951-9417-d1087c92a5b2/memory.limit_in_bytes\n \u0026lt;!-- This is 320 MiB, as expected: --\u0026gt; 和预期的一样是 320 MiB 335544320\n \u0026lt;!-- ### Observability --\u0026gt; ### 可观察性 \u0026lt;!-- A `kube_pod_overhead` metric is available in [kube-state-metrics](https://github.com/kubernetes/kube-state-metrics) to help identify when PodOverhead is being utilized and to help observe stability of workloads running with a defined Overhead. This functionality is not available in the 1.9 release of kube-state-metrics, but is expected in a following release. Users will need to build kube-state-metrics from source in the meantime. --\u0026gt; 在 [kube-state-metrics](https://github.com/kubernetes/kube-state-metrics) 中可以通过 `kube_pod_overhead` 指标来协助确定何时使用 PodOverhead 以及协助观察以一个既定开销运行的工作负载的稳定性。 该特性在 kube-state-metrics 的 1.9 发行版本中不可用，不过预计将在后续版本中发布。在此之前，用户需要从源代码构建 kube-state-metrics. ## * [RuntimeClass](/docs/concepts/containers/runtime-class/) * [PodOverhead 设计](https://github.com/kubernetes/enhancements/blob/master/keps/sig-node/20190226-pod-overhead.md) "
},
{
	"uri": "https://lijun.in/concepts/workloads/pods/pod/",
	"title": "Pods",
	"tags": [],
	"description": "",
	"content": "Pod 是可以在 Kubernetes 中创建和管理的、最小的可部署的计算单元。\nPod 是什么？ Pod （就像在鲸鱼荚或者豌豆荚中）是一组（一个或多个） glossary_tooltip text=\u0026quot;容器\u0026rdquo; term_id=\u0026quot;container\u0026rdquo; \u0026gt;}}（例如 Docker 容器），这些容器共享存储、网络、以及怎样运行这些容器的声明。Pod 中的内容总是并置（colocated）的并且一同调度，在共享的上下文中运行。 Pod 所建模的是特定于应用的“逻辑主机”，其中包含一个或多个应用容器，这些容器是相对紧密的耦合在一起 — 在容器出现之前，在相同的物理机或虚拟机上运行意味着在相同的逻辑主机上运行。\n虽然 Kubernetes 支持多种容器运行时，但 Docker 是最常见的一种运行时，它有助于使用 Docker 术语来描述 Pod。\nPod 的共享上下文是一组 Linux 命名空间、cgroups、以及其他潜在的资源隔离相关的因素，这些相同的东西也隔离了 Docker 容器。在 Pod 的上下文中，单个应用程序可能还会应用进一步的子隔离。\nPod 中的所有容器共享一个 IP 地址和端口空间，并且可以通过 localhost 互相发现。他们也能通过标准的进程间通信（如 SystemV 信号量或 POSIX 共享内存）方式进行互相通信。不同 Pod 中的容器的 IP 地址互不相同，没有 特殊配置 就不能使用 IPC 进行通信。这些容器之间经常通过 Pod IP 地址进行通信。\nPod 中的应用也能访问共享 glossary_tooltip text=\u0026quot;卷\u0026rdquo; term_id=\u0026quot;volume\u0026rdquo; \u0026gt;}}，共享卷是 Pod 定义的一部分，可被用来挂载到每个应用的文件系统上。\n在 Docker 体系的术语中，Pod 被建模为一组具有共享命名空间和共享文件系统卷 的 Docker 容器。\n与单个应用程序容器一样，Pod 被认为是相对短暂的（而不是持久的）实体。如 Pod 的生命周期 所讨论的那样：Pod 被创建、给它指定一个唯一 ID （UID）、被调度到节点、在节点上存续直到终止（取决于重启策略）或被删除。如果 glossary_tooltip term_id=\u0026quot;node\u0026rdquo; \u0026gt;}} 宕机，调度到该节点上的 Pod 会在一个超时周期后被安排删除。给定 Pod （由 UID 定义）不会重新调度到新节点；相反，它会被一个完全相同的 Pod 替换掉，如果需要甚至连 Pod 名称都可以一样，除了 UID 是新的(更多信息请查阅 副本控制器（replication controller）。\n当某些东西被说成与 Pod（如卷）具有相同的生命周期时，这表明只要 Pod（具有该 UID）存在，它就存在。如果出于任何原因删除了该 Pod，即使创建了相同的 Pod，相关的内容（例如卷）也会被销毁并重新创建。\n  Pod diagram   一个多容器 Pod，其中包含一个文件拉取器和一个 Web 服务器，该 Web 服务器使用持久卷在容器之间共享存储\n设计 Pod 的目的 管理 Pod 是形成内聚服务单元的多个协作过程模式的模型。它们提供了一个比它们的应用组成集合更高级的抽象，从而简化了应用的部署和管理。Pod 可以用作部署、水平扩展和制作副本的最小单元。在 Pod 中，系统自动处理多个容器的在并置运行（协同调度）、生命期共享（例如，终止），协同复制、资源共享和依赖项管理。\n资源共享和通信 Pod 使它的组成容器间能够进行数据共享和通信。\nPod 中的应用都使用相同的网络命名空间（相同 IP 和 端口空间），而且能够互相“发现”并使用 localhost 进行通信。因此，在 Pod 中的应用必须协调它们的端口使用情况。每个 Pod 在扁平的共享网络空间中具有一个 IP 地址，该空间通过网络与其他物理计算机和 Pod 进行全面通信。\nPod 中的容器获取的系统主机名与为 Pod 配置的 name 相同。网络 部分提供了更多有关此内容的信息。\nPod 除了定义了 Pod 中运行的应用程序容器之外，Pod 还指定了一组共享存储卷。该共享存储卷能使数据在容器重新启动后继续保留，并能在 Pod 内的应用程序之间共享。\n使用 Pod Pod 可以用于托管垂直集成的应用程序栈（例如，LAMP），但最主要的目的是支持位于同一位置的、共同管理的工具程序，例如：\n 内容管理系统、文件和数据加载器、本地缓存管理器等。 日志和检查点备份、压缩、旋转、快照等。 数据更改监视器、日志跟踪器、日志和监视适配器、事件发布器等。 代理、桥接器和适配器 控制器、管理器、配置器和更新器  通常，不会用单个 Pod 来运行同一应用程序的多个实例。\n有关详细说明，请参考 分布式系统工具包：组合容器的模式。\n可考虑的备选方案 为什么不在单个（Docker）容器中运行多个程序？\n 透明度。Pod 内的容器对基础设施可见，使得基础设施能够向这些容器提供服务，例如流程管理和资源监控。这为用户提供了许多便利。 解耦软件依赖关系。可以独立地对单个容器进行版本控制、重新构建和重新部署。Kubernetes 有一天甚至可能支持单个容器的实时更新。 易用性。用户不需要运行他们自己的进程管理器、也不用担心信号和退出代码传播等。 效率。因为基础结构承担了更多的责任，所以容器可以变得更加轻量化。  为什么不支持基于亲和性的容器协同调度？\n这种处理方法尽管可以提供同址，但不能提供 Pod 的大部分好处，如资源共享、IPC、有保证的命运共享和简化的管理。\nPod 的持久性（或稀缺性） 不得将 Pod 视为持久实体。它们无法在调度失败、节点故障或其他驱逐策略（例如由于缺乏资源或在节点维护的情况下）中生存。\n一般来说，用户不需要直接创建 Pod。他们几乎都是使用控制器进行创建，即使对于单例的 Pod 创建也一样使用控制器，例如 Deployments。 控制器提供集群范围的自修复以及副本数和滚动管理。 像 StatefulSet 这样的控制器还可以提供支持有状态的 Pod。\n在集群调度系统中，使用 API 合集作为面向用户的主要原语是比较常见的，包括 Borg、Marathon、Aurora、和 Tupperware。\nPod 暴露为原语是为了便于：\n 调度器和控制器可插拔性 支持 Pod 级别的操作，而不需要通过控制器 API \u0026ldquo;代理\u0026rdquo; 它们 Pod 生命与控制器生命的解耦，如自举 控制器和服务的解耦 — 端点控制器只监视 Pod kubelet 级别的功能与集群级别功能的清晰组合 — kubelet 实际上是 \u0026ldquo;Pod 控制器\u0026rdquo; 高可用性应用程序期望在 Pod 终止之前并且肯定要在 Pod 被删除之前替换 Pod，例如在计划驱逐或镜像预先拉取的情况下。  Pod 的终止 因为 Pod 代表在集群中的节点上运行的进程，所以当不再需要这些进程时（与被 KILL 信号粗暴地杀死并且没有机会清理相比），允许这些进程优雅地终止是非常重要的。 用户应该能够请求删除并且知道进程何时终止，但是也能够确保删除最终完成。当用户请求删除 Pod 时，系统会记录在允许强制删除 Pod 之前所期望的宽限期，并向每个容器中的主进程发送 TERM 信号。一旦过了宽限期，KILL 信号就发送到这些进程，然后就从 API 服务器上删除 Pod。如果 Kubelet 或容器管理器在等待进程终止时发生重启，则终止操作将以完整的宽限期进行重试。\n流程示例：\n 用户发送命令删除 Pod，使用的是默认的宽限期（30秒） API 服务器中的 Pod 会随着宽限期规定的时间进行更新，过了这个时间 Pod 就会被认为已 \u0026ldquo;死亡\u0026rdquo;。 当使用客户端命令查询 Pod 状态时，Pod 显示为 \u0026ldquo;Terminating\u0026rdquo;。 （和第 3 步同步进行）当 Kubelet 看到 Pod 由于步骤 2 中设置的时间而被标记为 terminating 状态时，它就开始执行关闭 Pod 流程。  如果 Pod 定义了 preStop 钩子，就在 Pod 内部调用它。如果宽限期结束了，但是 preStop 钩子还在运行，那么就用小的（2 秒）扩展宽限期调用步骤 2。 给 Pod 内的进程发送 TERM 信号。请注意，并不是所有 Pod 中的容器都会同时收到 TERM 信号，如果它们关闭的顺序很重要，则每个容器可能都需要一个 preStop 钩子。   （和第 3 步同步进行）从服务的端点列表中删除 Pod，Pod 也不再被视为副本控制器的运行状态的 Pod 集的一部分。因为负载均衡器（如服务代理）会将其从轮换中删除，所以缓慢关闭的 Pod 无法继续为流量提供服务。 当宽限期到期时，仍在 Pod 中运行的所有进程都会被 SIGKILL 信号杀死。 kubelet 将通过设置宽限期为 0 （立即删除）来完成在 API 服务器上删除 Pod 的操作。该 Pod 从 API 服务器中消失，并且在客户端中不再可见。  默认情况下，所有删除操作宽限期是 30 秒。kubectl delete 命令支持 --grace-period=\u0026lt;seconds\u0026gt; 选项，允许用户覆盖默认值并声明他们自己的宽限期。设置为 0 会强制删除 Pod。您必须指定一个附加标志 --force 和 --grace-period=0 才能执行强制删除操作。\nPod 的强制删除 强制删除 Pod 被定义为从集群状态与 etcd 中立即删除 Pod。当执行强制删除时，API 服务器并不会等待 kubelet 的确认信息，该 Pod 已在所运行的节点上被终止了。强制执行删除操作会从 API 服务器中立即清除 Pod， 因此可以用相同的名称创建一个新的 Pod。在节点上，设置为立即终止的 Pod 还是会在被强制删除前设置一个小的宽限期。\n强制删除对某些 Pod 可能具有潜在危险，因此应该谨慎地执行。对于 StatefulSet 管理的 Pod，请参考 从 StatefulSet 中删除 Pod 的任务文档。\nPod 容器的特权模式 Pod 中的任何容器都可以使用容器规范 security context 上的 privileged 参数启用特权模式。这对于想要使用 Linux 功能（如操纵网络堆栈和访问设备）的容器很有用。容器内的进程几乎可以获得与容器外的进程相同的特权。使用特权模式，将网络和卷插件编写为不需要编译到 kubelet 中的独立的 Pod 应该更容易。\n您的容器运行时必须支持特权容器模式才能使用此设置。\nAPI 对象 Pod 是 Kubernetes REST API 中的顶级资源。 [Pod API 对象](/docs/reference/generated/kubernetes-api/\u0026lt; param \u0026ldquo;version\u0026rdquo; \u0026gt;}}/#pod-v1-core)定义详细描述了该 Pod 对象。\n"
},
{
	"uri": "https://lijun.in/concepts/workloads/controllers/replicationcontroller/",
	"title": "ReplicationController",
	"tags": [],
	"description": "",
	"content": "现在推荐使用配置 ReplicaSet 的 Deployment 来建立副本管理机制。\nReplicationController 确保在任何时候都有特定数量的 pod 副本处于运行状态。 换句话说，ReplicationController 确保一个 pod 或一组同类的 pod 总是可用的。\nReplicationController 如何工作 当 pod 数量过多时，ReplicationController 会终止多余的 pod。当 pod 数量太少时，ReplicationController 将会启动新的 pod。 与手动创建的 pod 不同，由 ReplicationController 创建的 pod 在失败、被删除或被终止时会被自动替换。 例如，在中断性维护（如内核升级）之后，您的 pod 会在节点上重新创建。 因此，即使您的应用程序只需要一个 pod，您也应该使用 ReplicationController 创建 Pod。 ReplicationController 类似于进程管理器，但是 ReplicationController 不是监控单个节点上的单个进程，而是监控跨多个节点的多个 pod。\n在讨论中，ReplicationController 通常缩写为 \u0026ldquo;rc\u0026rdquo;，并作为 kubectl 命令的快捷方式。\n一个简单的示例是创建一个 ReplicationController 对象来可靠地无限期地运行 Pod 的一个实例。 更复杂的用例是运行一个多副本服务（如 web 服务器）的若干相同副本。\n运行一个示例 ReplicationController 这个示例 ReplicationController 配置运行 nginx web 服务器的三个副本。\ncodenew file=\u0026quot;controllers/replication.yaml\u0026rdquo; \u0026gt;}}\n通过下载示例文件并运行以下命令来运行示例任务:\nkubectl apply -f https://k8s.io/examples/controllers/replication.yaml replicationcontroller/nginx created 使用以下命令检查 ReplicationController 的状态:\nkubectl describe replicationcontrollers/nginx Name: nginx Namespace: default Selector: app=nginx Labels: app=nginx Annotations: \u0026lt;none\u0026gt; Replicas: 3 current / 3 desired Pods Status: 0 Running / 3 Waiting / 0 Succeeded / 0 Failed Pod Template: Labels: app=nginx Containers: nginx: Image: nginx Port: 80/TCP Environment: \u0026lt;none\u0026gt; Mounts: \u0026lt;none\u0026gt; Volumes: \u0026lt;none\u0026gt; Events: FirstSeen LastSeen Count From SubobjectPath Type Reason Message --------- -------- ----- ---- ------------- ---- ------ ------- 20s 20s 1 {replication-controller } Normal SuccessfulCreate Created pod: nginx-qrm3m 20s 20s 1 {replication-controller } Normal SuccessfulCreate Created pod: nginx-3ntk0 20s 20s 1 {replication-controller } Normal SuccessfulCreate Created pod: nginx-4ok8v 在这里，创建了三个 Pod，但没有一个 Pod 正在运行，这可能是因为正在拉取镜像。 稍后，相同的命令可能会显示：\nPods Status: 3 Running / 0 Waiting / 0 Succeeded / 0 Failed 要以机器可读的形式列出属于 ReplicationController 的所有 pod，可以使用如下命令：\npods=$(kubectl get pods --selector=app=nginx --output=jsonpath={.items..metadata.name}) echo $pods nginx-3ntk0 nginx-4ok8v nginx-qrm3m 这里，选择器与 ReplicationController 的选择器相同（参见 kubectl describe 输出），并以不同的形式出现在 replication.yaml 中。 --output=jsonpath 选项指定了一个表达式，只从返回列表中的每个 pod 中获取名称。\n编写一个 ReplicationController Spec 与所有其它 Kubernetes 配置一样，ReplicationController 需要 apiVersion、kind 和 metadata 字段。 有关使用配置文件的常规信息，参考对象管理。\nReplicationController 也需要一个 .spec 部分。\nPod 模板 .spec.template 是 .spec 的唯一必需字段。\n.spec.template 是一个 pod 模板。它的模式与 pod 完全相同，只是它是嵌套的，没有 apiVersion 或 kind 属性。\n除了 Pod 所需的字段外，ReplicationController 中的 pod 模板必须指定适当的标签和适当的重新启动策略。 对于标签，请确保不与其他控制器重叠。参考 pod 选择器。\n只允许 .spec.template.spec.restartPolicy 等于 Always，如果没有指定，这是默认值。\n对于本地容器重启，ReplicationController 委托给节点上的代理， 例如 Kubelet 或 Docker。\nReplicationController 上的标签 ReplicationController 本身可以有标签 （.metadata.labels）。 通常，您可以将这些设置为 .spec.template.metadata.labels； 如果没有指定 .metadata.labels 那么它默认为 .spec.template.metadata.labels。\n但是，Kubernetes 允许它们是不同的，.metadata.labels 不会影响 ReplicationController 的行为。\nPod 选择器 .spec.selector 字段是一个标签选择器。 ReplicationController 管理标签与选择器匹配的所有 Pod。 它不区分它创建或删除的 Pod 和其他人或进程创建或删除的 Pod。 这允许在不影响正在运行的 Pod 的情况下替换 ReplicationController。\n如果指定了 .spec.template.metadata.labels，它必须和 .spec.selector 相同，否则它将被 API 拒绝。 如果没有指定 .spec.selector，它将默认为 .spec.template.metadata.labels。\n另外，通常不应直接使用另一个 ReplicationController 或另一个控制器（例如 Job）来创建其标签与该选择器匹配的任何 Pod。如果这样做，ReplicationController 会认为它创建了这些 Pod。 Kubernetes 并没有阻止你这样做。\n如果您的确创建了多个控制器并且其选择器之间存在重叠，那么您将不得不自己管理删除操作（参考后文）。\n多个副本 你可以通过设置 .spec.replicas 来指定应该同时运行多少个 Pod。 在任何时候，处于运行状态的 Pod 个数都可能高于或者低于设定值。例如，副本个数刚刚被增加或减少时，或者一个 pod 处于优雅终止过程中而其替代副本已经提前开始创建时。\n如果你没有指定 .spec.replicas ，那么它默认是 1。\n使用 ReplicationController 删除一个 ReplicationController 以及它的 Pod 要删除一个 ReplicationController 以及它的 Pod，使用 kubectl delete。 kubectl 将 ReplicationController 缩放为 0 并等待以便在删除 ReplicationController 本身之前删除每个 Pod。 如果这个 kubectl 命令被中断，可以重新启动它。\n当使用 REST API 或 go 客户端库时，您需要明确地执行这些步骤（缩放副本为 0、 等待 Pod 删除，之后删除 ReplicationController 资源）。\n只删除 ReplicationController 你可以删除一个 ReplicationController 而不影响它的任何 pod。\n使用 kubectl ，为 kubectl delete 指定 --cascade=false 选项。\n当使用 REST API 或 go 客户端库时， 只需删除 ReplicationController 对象。\n一旦原始对象被删除，你可以创建一个新的 ReplicationController 来替换它。 只要新的和旧的 .spec.selector 相同，那么新的控制器将领养旧的 Pod。 但是，它不会做出任何努力使现有的 Pod 匹配新的、不同的 Pod 模板。 如果希望以受控方式更新 Pod 以使用新的 spec，请执行滚动更新操作。\n从 ReplicationController 中隔离 pod 通过更改 Pod 的标签，可以从 ReplicationController 的目标中删除 pod。 此技术可用于从服务中删除 pod 以进行调试、数据恢复等。以这种方式删除的 pod 将自动替换（假设复制副本的数量也没有更改）。\n常见的使用模式 重新调度 如上所述，无论您想要继续运行 1 个 pod 还是 1000 个 Pod，一个 ReplicationController 都将确保存在指定数量的 pod，即使在节点故障或 pod 终止(例如，由于另一个控制代理的操作)的情况下也是如此。\n扩缩容 通过简单地更新 replicas 字段，ReplicationController 可以方便地横向扩容或缩容副本的数量，或手动或通过自动缩放控制代理。\n滚动更新 ReplicationController 的设计目的是通过逐个替换 pod 以方便滚动更新服务。\n如 #1353 PR 中所述，建议的方法是使用 1 个副本创建一个新的 ReplicationController，逐个缩放新的（+1）和旧的（-1）控制器，然后在旧的控制器达到 0 个副本后将其删除。这一方法能够实现可控的 Pod 集合更新，即使存在意外失效的状况。\n理想情况下，滚动更新控制器将考虑应用程序的就绪情况，并确保在任何给定时间都有足够数量的 Pod 有效地提供服务。\n这两个 ReplicationController 将需要创建至少具有一个不同标签的 pod，比如 pod 主要容器的镜像标签，因为通常是镜像更新触发滚动更新。\n滚动更新是在客户端工具 kubectl rolling-update 中实现的。 访问 kubectl rolling-update 任务以获得更多的具体示例。\n多个版本跟踪 除了在滚动更新过程中运行应用程序的多个版本之外，通常还会使用多个版本跟踪来长时间，甚至持续运行多个版本。这些跟踪将根据标签加以区分。\n例如，一个服务可能把具有 tier in (frontend), environment in (prod) 的所有 pod 作为目标。 现在假设您有 10 个副本的 pod 组成了这个层。但是你希望能够 canary （金丝雀）发布这个组件的新版本。 您可以为大部分副本设置一个 ReplicationController，其中 replicas 设置为 9，标签为 tier=frontend, environment=prod, track=stable 而为 canary 设置另一个 ReplicationController，其中 replicas 设置为 1，标签为 tier=frontend, environment=prod, track=canary。 现在这个服务覆盖了 canary 和非 canary Pod。但您可以单独处理 ReplicationController，以测试、监控结果等。\n和服务一起使用 ReplicationController 多个 ReplicationController 可以位于一个服务的后面，例如，一部分流量流向旧版本，一部分流量流向新版本。\n一个 ReplicationController 永远不会自行终止，但它不会像服务那样长时间存活。 服务可以由多个 ReplicationController 控制的 Pod 组成，并且在服务的生命周期内（例如，为了执行 pod 更新而运行服务），可以创建和销毁许多 ReplicationController。 服务本身和它们的客户端都应该忽略负责维护服务 Pod 的 ReplicationController 的存在。\n编写多副本的应用 由 ReplicationController 创建的 Pod 是可替换的，语义上是相同的，尽管随着时间的推移，它们的配置可能会变得异构。 这显然适合于多副本的无状态服务器，但是 ReplicationController 也可以用于维护主选、分片和工作池应用程序的可用性。 这样的应用程序应该使用动态的工作分配机制，例如 RabbitMQ 工作队列，而不是静态的或者一次性定制每个 pod 的配置，这被认为是一种反模式。 执行的任何 pod 定制，例如资源的垂直自动调整大小(例如，cpu 或内存)，都应该由另一个在线控制器进程执行，这与 ReplicationController 本身没什么不同。\nReplicationController 的职责 ReplicationController 只需确保所需的 pod 数量与其标签选择器匹配，并且是可操作的。 目前，它的计数中只排除终止的 pod。 未来，可能会考虑系统提供的就绪状态和其他信息，我们可能会对替换策略添加更多控制，我们计划发出事件，这些事件可以被外部客户端用来实现任意复杂的替换和/或缩减策略。\nReplicationController 永远被限制在这个狭隘的职责范围内。 它本身既不执行就绪态探测，也不执行活跃性探测。 它不负责执行自动缩放，而是由外部自动缩放器控制（如 #492 中所述），后者负责更改其 replicas 字段值。 我们不会向 ReplicationController 添加调度策略(例如，spreading)。 它也不应该验证所控制的 pod 是否与当前指定的模板匹配，因为这会阻碍自动调整大小和其他自动化过程。 类似地，完成期限、整理依赖关系、配置扩展和其他特性也属于其他地方。 我们甚至计划考虑批量创建 pod 的机制（查阅 #170）。\nReplicationController 旨在成为可组合的构建基元。 我们希望在它和其他补充原语的基础上构建更高级别的 API 或者工具，以便于将来的用户使用。 kubectl 目前支持的 \u0026ldquo;macro\u0026rdquo; 操作（运行、缩放、滚动更新）就是这方面的概念示例。 例如，我们可以想象类似于 Asgard 的东西管理 ReplicationController、自动定标器、服务、调度策略、 canary 等。\nAPI 对象 在 Kubernetes REST API 中 Replication controller 是顶级资源。 更多关于 API 对象的详细信息可以在 [ReplicationController API 对象](/docs/reference/generated/kubernetes-api/\u0026lt; param \u0026ldquo;version\u0026rdquo; \u0026gt;}}/#replicationcontroller-v1-core)找到。\nReplicationController 的替代方案 ReplicaSet ReplicaSet 是下一代 ReplicationController ，支持新的基于集合的标签选择器。 它主要被 Deployment 用来作为一种编排 pod 创建、删除及更新的机制。 请注意，我们推荐使用 Deployment 而不是直接使用 ReplicaSet，除非您需要自定义更新编排或根本不需要更新。\nDeployment （推荐） Deployment 是一种更高级别的 API 对象，它以类似于 kubectl rolling-update 的方式更新其底层 ReplicaSet 及其 Pod。 如果您想要这种滚动更新功能，那么推荐使用 Deployment，因为与 kubectl rolling-update 不同，它们是声明式的、服务端的，并且具有其它特性。\n裸 Pod 与用户直接创建 pod 的情况不同，ReplicationController 能够替换因某些原因被删除或被终止的 pod ，例如在节点故障或中断节点维护的情况下，例如内核升级。 因此，我们建议您使用 ReplicationController，即使您的应用程序只需要一个 pod。 可以将其看作类似于进程管理器，它只管理跨多个节点的多个 pod ，而不是单个节点上的单个进程。 ReplicationController 将本地容器重启委托给节点上的某个代理(例如，Kubelet 或 Docker)。\nJob 对于预期会自行终止的 pod (即批处理任务)，使用 Job 而不是 ReplicationController。\nDaemonSet 对于提供机器级功能（例如机器监控或机器日志记录）的 pod ，使用 DaemonSet 而不是 ReplicationController。 这些 pod 的生命期与机器的生命期绑定：它们需要在其他 pod 启动之前在机器上运行，并且在机器准备重新启动或者关闭时安全地终止。\n更多信息 请阅读运行无状态的 Replication Controller。\n"
},
{
	"uri": "https://lijun.in/tasks/configure-pod-container/configure-runasusername/",
	"title": "为 Windows 的 pod 和容器配置 RunAsUserName",
	"tags": [],
	"description": "",
	"content": ". feature-state for_k8s_version=\u0026quot;v1.17\u0026rdquo; state=\u0026quot;beta\u0026rdquo; \u0026gt;}}\n本页展示如何为运行在 Windows 节点上的 pod 和容器启用并使用 RunAsUserName 功能。此功能旨在成为 Windows 版的 runAsUser（Linux），允许用户使用与默认用户名不同的用户名运行容器 entrypoint。\n. note \u0026gt;}}\n该功能目前处于 beta 状态。 RunAsUserName 的整体功能不会出现变更，但是关于用户名验证的部分可能会有所更改。 . /note \u0026gt;}}\n. heading \u0026ldquo;prerequisites\u0026rdquo; %}} 你必须有一个 Kubernetes 集群，并且 kubectl 必须能和集群通信。集群应该要有 Windows 工作节点，将在其中调度运行 Windows 工作负载的 pod 和容器。\n为 Pod 设置 Username 要指定运行 Pod 容器时所使用的用户名，请在 Pod 声明中包含 securityContext （[PodSecurityContext](/docs/reference/generated/kubernetes-api/. param \u0026ldquo;version\u0026rdquo; \u0026gt;}}/#podsecuritycontext-v1-core)）字段，并在其内部包含 windowsOptions （[WindowsSecurityContextOptions](/docs/reference/generated/kubernetes-api/. param \u0026ldquo;version\u0026rdquo; \u0026gt;}}/#windowssecuritycontextoptions-v1-core)）字段的 runAsUserName 字段。\n您为 Pod 指定的 Windows SecurityContext 选项适用于该 Pod 中（包括 init 容器）的所有容器。\n这儿有一个已经设置了 runAsUserName 字段的 Windows Pod 的配置文件：\n. codenew file=\u0026quot;windows/run-as-username-pod.yaml\u0026rdquo; \u0026gt;}}\n创建 Pod：\nkubectl apply -f https://k8s.io/examples/windows/run-as-username-pod.yaml 验证 Pod 容器是否在运行：\nkubectl get pod run-as-username-pod-demo 获取该容器的 shell：\nkubectl exec -it run-as-username-pod-demo -- powershell 检查运行 shell 的用户的用户名是否正确：\necho $env:USERNAME 输出结果应该是这样：\nContainerUser 为容器设置 Username 要指定运行容器时所使用的用户名，请在容器清单中包含 securityContext （[SecurityContext](/docs/reference/generated/kubernetes-api/. param \u0026ldquo;version\u0026rdquo; \u0026gt;}}/#securitycontext-v1-core)）字段，并在其内部包含 windowsOptions （[WindowsSecurityContextOptions](/docs/reference/generated/kubernetes-api/. param \u0026ldquo;version\u0026rdquo; \u0026gt;}}/#windowssecuritycontextoptions-v1-core)）字段的 runAsUserName 字段。\n您为容器指定的 Windows SecurityContext 选项仅适用于该容器，并且它会覆盖 Pod 级别设置。\n这儿有一个 Pod 的配置文件，其只有一个容器，并且在 Pod 级别和容器级别都设置了 runAsUserName：\n. codenew file=\u0026quot;windows/run-as-username-container.yaml\u0026rdquo; \u0026gt;}}\n创建 Pod：\nkubectl apply -f https://k8s.io/examples/windows/run-as-username-container.yaml 验证 Pod 容器是否在运行：\nkubectl get pod run-as-username-container-demo 获取该容器的 shell：\nkubectl exec -it run-as-username-container-demo -- powershell 检查运行 shell 的用户的用户名是否正确（应该是容器级别设置的那个）：\necho $env:USERNAME 输出结果应该是这样：\nContainerAdministrator Windows Username 的局限性 想要使用此功能，在 runAsUserName 字段中设置的值必须是有效的用户名。它必须是 DOMAIN\\USER 这种格式，其中 DOMAIN\\ 是可选的。Windows 用户名不区分大小写。此外，关于 DOMAIN 和 USER 还有一些限制：\n runAsUserName 字段不能为空，并且不能包含控制字符（ASCII 值：0x00-0x1F、0x7F） DOMAIN 必须是 NetBios 名称或 DNS 名称，每种名称都有各自的局限性：  NetBios 名称：最多 15 个字符，不能以 .（点）开头，并且不能包含以下字符：\\ / : * ? \u0026quot; \u0026lt; \u0026gt; | DNS 名称：最多 255 个字符，只能包含字母、数字、点和中划线，并且不能以 .（点）或 -（中划线）开头和结尾。   USER 最多不超过 20 个字符，不能 只 包含点或空格，并且不能包含以下字符：\u0026quot; / \\ [ ] : ; | = , + * ? \u0026lt; \u0026gt; @  runAsUserName 字段接受的值的一些示例：ContainerAdministrator、ContainerUser、NT AUTHORITY\\NETWORK SERVICE、NT AUTHORITY\\LOCAL SERVICE。\n关于这些限制的更多信息，可以查看这里和这里。\n. heading \u0026ldquo;whatsnext\u0026rdquo; %}}  Kubernetes 中调度 Windows 容器的指南 使用组托管服务帐户（GMSA）管理工作负载身份 Windows 下 pod 和容器的 GMSA 配置  "
},
{
	"uri": "https://lijun.in/tasks/administer-cluster/manage-resources/cpu-default-namespace/",
	"title": "为命名空间配置默认的CPU请求和限制",
	"tags": [],
	"description": "",
	"content": "本章介绍怎样为命名空间配置默认的 CPU 请求和限制。 一个 Kubernetes 集群可被划分为多个命名空间。如果在配置了 CPU 限制的命名空间创建容器，并且该容器没有声明自己的 CPU 限制，那么这个容器会被指定默认的 CPU 限制。Kubernetes 在一些特定情况还会指定 CPU 请求，本文后续章节将会对其进行解释。\n. heading \u0026ldquo;prerequisites\u0026rdquo; %}} . include \u0026ldquo;task-tutorial-prereqs.md\u0026rdquo; \u0026gt;}} . version-check \u0026gt;}}\n创建命名空间 创建一个命名空间，以便本练习中创建的资源和集群的其余部分相隔离。\nkubectl create namespace default-cpu-example 创建 LimitRange 和 Pod\n这里给出了 LimitRange 对象的配置文件。该配置声明了一个默认的 CPU 请求和一个默认的 CPU 限制。\n. codenew file=\u0026quot;admin/resource/cpu-defaults.yaml\u0026rdquo; \u0026gt;}}\n在命名空间 default-cpu-example 中创建 LimitRange 对象：\nkubectl create -f https://k8s.io/examples/admin/resource/cpu-defaults.yaml --namespace=default-cpu-example 现在如果在 default-cpu-example 命名空间创建一个容器，该容器没有声明自己的 CPU 请求和限制时，将会给它指定默认的 CPU 请求0.5和默认的 CPU 限制值1.\n这里给出了包含一个容器的 Pod 的配置文件。该容器没有声明 CPU 请求和限制。\n. codenew file=\u0026quot;admin/resource/cpu-defaults-pod.yaml\u0026rdquo; \u0026gt;}}\n创建 Pod。\nkubectl create -f https://k8s.io/examples/admin/resource/cpu-defaults-pod.yaml --namespace=default-cpu-example 查看该 Pod 的声明：\nkubectl get pod default-cpu-demo --output=yaml --namespace=default-cpu-example 输出显示该 Pod 的容器有一个500 millicpus的 CPU 请求和一个1 cpu的 CPU 限制。这些是 LimitRange 声明的默认值。\ncontainers: - image: nginx imagePullPolicy: Always name: default-cpu-demo-ctr resources: limits: cpu: \u0026#34;1\u0026#34; requests: cpu: 500m 你只声明容器的限制，而不声明请求会怎么样？ 这是包含一个容器的 Pod 的配置文件。该容器声明了 CPU 限制，而没有声明 CPU 请求。\n. codenew file=\u0026quot;admin/resource/cpu-defaults-pod-2.yaml\u0026rdquo; \u0026gt;}}\n创建 Pod\nkubectl create -f https://k8s.io/examples/admin/resource/cpu-defaults-pod-2.yaml --namespace=default-cpu-example 查看 Pod 的声明：\nkubectl get pod default-cpu-demo-2 --output=yaml --namespace=default-cpu-example 输出显示该容器的 CPU 请求和 CPU 限制设置相同。注意该容器没有被指定默认的 CPU 请求值0.5 cpu。\nresources: limits: cpu: \u0026quot;1\u0026quot; requests: cpu: \u0026quot;1\u0026quot; 你只声明容器的请求，而不声明它的限制会怎么样？ 这里给出了包含一个容器的 Pod 的配置文件。该容器声明了 CPU 请求，而没有声明 CPU 限制。\n. codenew file=\u0026quot;admin/resource/cpu-defaults-pod-3.yaml\u0026rdquo; \u0026gt;}}\n创建 Pod：\nkubectl create -f https://k8s.io/examples/admin/resource/cpu-defaults-pod-3.yaml --namespace=default-cpu-example 查看 Pod 的声明：\nkubectl get pod default-cpu-demo-3 --output=yaml --namespace=default-cpu-example 结果显示该容器的 CPU 请求被设置为容器配置文件中声明的数值。容器的CPU限制被设置为1 cpu，即该命名空间的默认 CPU 限制值。\nresources: limits: cpu: \u0026quot;1\u0026quot; requests: cpu: 750m 默认 CPU 限制和请求的动机 如果你的命名空间有一个资源配额,那么有一个默认的 CPU 限制是有帮助的。这里有两条资源配额强加给命名空间的限制：\n 命名空间中运行的每个容器必须有自己的 CPU 限制。 命名空间中所有容器使用的 CPU 总和不能超过一个声明值。  如果容器没有声明自己的 CPU 限制，将会给它一个默认限制，这样它就能被允许运行在一个有配额限制的命名空间中。\n. heading \u0026ldquo;whatsnext\u0026rdquo; %}} 集群管理员参考   为命名空间配置默认内存请求和限制\n  为命名空间配置内存限制的最小值和最大值\n  为命名空间配置 CPU 限制的最小值和最大值\n  为命名空间配置内存和 CPU 配额\n  为命名空间配置 Pod 配额\n  为 API 对象配置配额\n  应用开发者参考   为容器和 Pod 分配内存资源\n  为容器和 Pod 分配 CPU 资源\n  为 Pod 配置 Service 数量\n  "
},
{
	"uri": "https://lijun.in/tasks/configure-pod-container/assign-cpu-resource/",
	"title": "为容器和 Pods 分配 CPU 资源",
	"tags": [],
	"description": "",
	"content": "此页面显示如何将 CPU request 和 CPU limit 分配给一个容器。容器使用的 CPU 不能超过配额限制。 如果系统有空闲的 CPU 时间，则可以保证根据请求给容器分配尽可能多的 CPU 资源。\n. heading \u0026ldquo;prerequisites\u0026rdquo; %}} . include \u0026ldquo;task-tutorial-prereqs.md\u0026rdquo; \u0026gt;}} . version-check \u0026gt;}}\n集群中的每个节点必须至少具有 1 个 CPU。\n此页面上的一些步骤要求您在集群中运行metrics-server 服务。如果您的集群中已经有正在运行的 metrics-server 服务，那么您可以跳过这些步骤。\n如果您正在运行. glossary_tooltip term_id=\u0026quot;minikube\u0026rdquo; \u0026gt;}}，请运行以下命令启用 metrics-server：\nminikube addons enable metrics-server 查看是 metrics-server（或者其他资源度量 API 服务提供者，metrics.k8s.io ）是否正在运行，请键入以下命令：\nkubectl get apiservices 如果资源指标 API 可用，则会输出将包含一个参考信息 metrics.k8s.io。\nNAME v1beta1.metrics.k8s.io 创建一个命名空间 创建一个命名空间 . glossary_tooltip term_id=\u0026quot;namespace\u0026rdquo; \u0026gt;}}，以便在本练习中创建的资源与集群的其余部分资源隔离。\nkubectl create namespace cpu-example 指定一个 CPU 请求和 CPU 限制 要为容器指定 CPU 请求，请包含 resources：requests 字段 在容器资源清单中。要指定 CPU 限制，请包含 resources：limits。\n在本练习中，您将创建一个具有一个容器的 Pod。容器将会请求 0.5 个 CPU，而且最多限制使用 1 个 CPU。 这是 Pod 的配置文件：\n. codenew file=\u0026quot;pods/resource/cpu-request-limit.yaml\u0026rdquo; \u0026gt;}}\n配置文件的 args 部分提供了容器启动时的参数。 -cpus \u0026ldquo;2\u0026quot;参数告诉容器尝试使用 2 个 CPU。\n创建 Pod 命令如下：\nkubectl apply -f https://k8s.io/examples/pods/resource/cpu-request-limit.yaml --namespace=cpu-example 验证上述创建的 Pod 处于 Running 状态\nkubectl get pod cpu-demo --namespace=cpu-example 查看显示关于 Pod 的详细信息\nkubectl get pod cpu-demo --output=yaml --namespace=cpu-example 输出显示 Pod 中的一个容器的 CPU 请求为 500 milli CPU，并且 CPU 限制为 1 个 CPU。\nresources: limits: cpu: \u0026#34;1\u0026#34; requests: cpu: 500m 使用 kubectl top 命令来获取该 Pod 的指标数据：\nkubectl top pod cpu-demo --namespace=cpu-example 此示例的输出，显示 Pod 使用的是974 milliCPU，即仅略低于 Pod 配置中指定的 1 个 CPU 的限制。\nNAME CPU(cores) MEMORY(bytes) cpu-demo 974m \u0026lt;something\u0026gt; 回想一下，通过设置 - CPU \u0026quot;2\u0026quot;，您将容器配置为尝试使用 2 个 CPU，但是只允许容器使用大约 1 个 CPU。容器的 CPU 使用量受到限制，因为该容器正尝试使用超出其限制的 CPU 资源。\n. note \u0026gt;}} CPU 使用率低于1.0的另一种可能的解释是，节点可能没有足够的 CPU 资源可用。回想一下，此练习的先决条件需要 您的节点至少具有 1 个 CPU。如果您的容器在只有 1 个 CPU 的节点上运行，则容器无论为容器指定的 CPU 限制如何，都不能使用超过 1 个 CPU。 . /note \u0026gt;}}\nCPU 单元 CPU 资源以 CPU 单位度量。Kubernetes中的一个 CPU 等同于：\n 1 个 AWS vCPU 1 个 GCP核心 1 个 Azure vCore 1 个具有超线程功能的裸机英特尔处理器上的超线程  允许使用小数值。要求 0.5 CPU 的容器保证一半 CPU 作为请求 1 个 CPU 的容器。 您可以使用后缀 m 表示毫。例如 100m CPU，100 milliCPU 和 0.1 CPU 都相同。 精度不能超过 1m。\n始终要求 CPU 是绝对数量，而不是相对数量。0.1 在单核，双核或 48 核计算机上的 CPU 数量值是一样的。\nkubectl delete pod cpu-demo --namespace=cpu-example 对您的节点而言，设置一个 CPU 过大的请求 CPU 请求和限制与容器相关联，但是我们可以考虑一下 CPU 对应 Pod 的请求和限制这样的场景：Pod 对 CPU 使用量的请求等于 Pod 中所有容器的请求数量。 同样，CPU 对 Pod 请求资源的限制等于 Pod 中所有容器的请求的 CPU 资源限制数。\nPod 调度基于请求。仅在以下情况下，Pod 将会在节点上运行：节点具有足够的 CPU 资源可用于满足 Pod CPU 请求。\n在本练习中，您将创建一个 Pod，该 Pod 的 CPU 请求对于集群中任何节点的容量而言都会过大。\n这是 Pod 的配置文件，Pod 中有一个容器。容器请求 100 个 CPU，这可能会超出集群中任何节点的容量。\n. codenew file=\u0026quot;pods/resource/cpu-request-limit-2.yaml\u0026rdquo; \u0026gt;}}\n使用如下命令创建该 Pod\nkubectl apply -f https://k8s.io/examples/pods/resource/cpu-request-limit-2.yaml --namespace=cpu-example 查看该 Pod 的状态\nkubectl get pod cpu-demo-2 --namespace=cpu-example 输出显示 Pod 状态为Pending。也就是说，尚未将 Pod 调度到任何节点上运行， 并且 Pod 将无限期地处于Pending状态：\nkubectl get pod cpu-demo-2 --namespace=cpu-example NAME READY STATUS RESTARTS AGE cpu-demo-2 0/1 Pending 0 7m 查看有关 Pod 的详细信息，包括事件如下：\nkubectl describe pod cpu-demo-2 --namespace=cpu-example 输出显示由于节点上的 CPU 资源不足，无法调度容器\nEvents: Reason Message ------ ------- FailedScheduling No nodes are available that match all of the following predicates:: Insufficient cpu (3). 删除您的 Pod\nkubectl delete pod cpu-demo-2 --namespace=cpu-example 如果没有指定 CPU 限制 如果您没有为容器指定 CPU 限制，则适用以下情况之一：\n  容器在可以使用的 CPU 资源上没有上限。容器可以使用运行该节点的所有可用 CPU 资源。\n  容器在具有默认 CPU 限制的命名空间中运行，并且系统会自动为容器分配默认限制。集群管理员可以使用 [LimitRange](/docs/reference/generated/kubernetes-api/. param \u0026ldquo;version\u0026rdquo; \u0026gt;}}/#limitrange-v1-core/) 指定 CPU 限制的默认值。\n  CPU 请求和限制的初衷 通过配置 CPU 请求和在您的容器中运行的容器的限制 集群，您可以有效利用集群上可用的 CPU 资源 节点。通过将 Pod CPU 请求保持在较低水平，可以使 Pod 成为 预定的。通过使 CPU 限制大于 CPU 请求，您可以完成两件事：\n Pod 可能会有大量活动，它利用恰好可用的 CPU 资源。 Pod 在突发期间可以使用的 CPU 资源数量被限制为合理的数量。  清理 删除名称空间：\nkubectl delete namespace cpu-example . heading \u0026ldquo;whatsnext\u0026rdquo; %}} 针对应用开发者   将内存资源分配给容器和 Pod\n  配置 Pod 服务质量\n  针对集群管理员   配置名称空间的默认内存请求和限制\n  为命名空间配置默认的 CPU 请求和限制\n  为命名空间配置最小和最大内存限制\n  为命名空间配置最小和最大 CPU 约束\n  为命名空间配置内存和 CPU 配额\n  为命名空间配置 Pod 配额\n  配置 API 对象的配额\n  "
},
{
	"uri": "https://lijun.in/concepts/configuration/manage-compute-resources-container/",
	"title": "为容器管理计算资源",
	"tags": [],
	"description": "",
	"content": "当您定义 Pod 的时候可以选择为每个容器指定需要的 CPU 和内存（RAM）大小。当为容器指定了资源请求后，调度器就能够更好的判断出将容器调度到哪个节点上。如果您还为容器指定了资源限制，Kubernetes 就可以按照指定的方式来处理节点上的资源竞争。关于资源请求和限制的不同点和更多资料请参考 Resource QoS。\n资源类型 CPU 和内存都是资源类型。资源类型具有基本单位。CPU 的单位是核心数，内存的单位是字节。\n如果您使用的是 Kubernetes v1.14 或更高版本，则可以指定巨页资源。巨页是 Linux 特有的功能，节点内核在其中分配的内存块比默认页大小大得多。\n例如，在默认页面大小为 4KiB 的系统上，您可以指定一个限制，hugepages-2Mi: 80Mi。如果容器尝试分配 40 个 2MiB 大页面（总共 80 MiB ），则分配失败。\n您不能过量使用hugepages- *资源。 这与memory和cpu资源不同。\nCPU和内存统称为计算资源，也可以称为资源。计算资源的数量是可以被请求、分配、消耗和可测量的。它们与 API 资源 不同。 API 资源（如 Pod 和 Service）是可通过 Kubernetes API server 读取和修改的对象。\nPod 和 容器的资源请求和限制 Pod 中的每个容器都可以指定以下的一个或者多个值：\n spec.containers[].resources.limits.cpu spec.containers[].resources.limits.memory spec.containers[].resources.requests.cpu spec.containers[].resources.requests.memory  尽管只能在个别容器上指定请求和限制，但是我们可以方便地计算出 Pod 资源请求和限制。特定资源类型的Pod 资源请求/限制是 Pod 中每个容器的该类型的资源请求/限制的总和。\nCPU 的含义 CPU 资源的限制和请求以 cpu 为单位。\nKubernetes 中的一个 cpu 等于：\n 1 AWS vCPU 1 GCP Core 1 Azure vCore 1 Hyperthread 在带有超线程的裸机 Intel 处理器上  允许浮点数请求。具有 spec.containers[].resources.requests.cpu 为 0.5 的容器保证了一半 CPU 要求 1 CPU的一半。表达式 0.1 等价于表达式 100m，可以看作 “100 millicpu”。有些人说成是“一百毫 cpu”，其实说的是同样的事情。具有小数点（如 0.1）的请求由 API 转换为100m，精度不超过 1m。因此，可能会优先选择 100m 的形式。\nCPU 总是要用绝对数量，不可以使用相对数量；0.1 的 CPU 在单核、双核、48核的机器中的意义是一样的。\n内存的含义 内存的限制和请求以字节为单位。您可以使用以下后缀之一作为平均整数或定点整数表示内存：E，P，T，G，M，K。您还可以使用两个字母的等效的幂数：Ei，Pi，Ti ，Gi，Mi，Ki。例如，以下代表大致相同的值：\n128974848, 129e6, 129M, 123Mi 下面是个例子。\n以下 Pod 有两个容器。每个容器的请求为 0.25 cpu 和 64MiB（226字节）内存，每个容器的限制为 0.5 cpu 和 128MiB 内存。您可以说该 Pod 请求 0.5 cpu 和 128 MiB 的内存，限制为 1 cpu 和 256MiB 的内存。\napiVersion: v1 kind: Pod metadata: name: frontend spec: containers: - name: db image: mysql env: - name: MYSQL_ROOT_PASSWORD value: \u0026#34;password\u0026#34; resources: requests: memory: \u0026#34;64Mi\u0026#34; cpu: \u0026#34;250m\u0026#34; limits: memory: \u0026#34;128Mi\u0026#34; cpu: \u0026#34;500m\u0026#34; - name: wp image: wordpress resources: requests: memory: \u0026#34;64Mi\u0026#34; cpu: \u0026#34;250m\u0026#34; limits: memory: \u0026#34;128Mi\u0026#34; cpu: \u0026#34;500m\u0026#34; 具有资源请求的 Pod 如何调度 当您创建一个 Pod 时，Kubernetes 调度程序将为 Pod 选择一个节点。每个节点具有每种资源类型的最大容量：可为 Pod 提供的 CPU 和内存量。调度程序确保对于每种资源类型，调度的容器的资源请求的总和小于节点的容量。请注意，尽管节点上的实际内存或 CPU 资源使用量非常低，但如果容量检查失败，则调度程序仍然拒绝在该节点上放置 Pod。当资源使用量稍后增加时，例如在请求率的每日峰值期间，这可以防止节点上的资源短缺。\n具有资源限制的 Pod 如何运行 当 kubelet 启动一个 Pod 的容器时，它会将 CPU 和内存限制传递到容器运行时。\n当使用 Docker 时：\n  spec.containers[].resources.requests.cpu 先被转换为可能是小数的 core 值，再乘以 1024，这个数字和 2 的较大者用作 docker run 命令中的--cpu-shares 标志的值。\n  spec.containers[].resources.limits.cpu 先被转换为 millicore 值，再乘以 100，结果就是每 100ms 内 container 可以使用的 CPU 总时间。在此时间间隔（100ms）内，一个 container 使用的 CPU 时间不会超过它被分配的时间。\n  默认的配额（quota）周期为 100 毫秒。 CPU配额的最小精度为 1 毫秒。\n spec.containers[].resources.limits.memory 被转换为整型，作为 docker run 命令中的 --memory 标志的值。  如果容器超过其内存限制，则可能会被终止。如果可重新启动，则与所有其他类型的运行时故障一样，kubelet 将重新启动它。\n如果一个容器超过其内存请求，那么当节点内存不足时，它的 Pod 可能被逐出。\n容器可能被允许也可能不被允许超过其 CPU 限制时间。但是，由于 CPU 使用率过高，不会被杀死。\n要确定容器是否由于资源限制而无法安排或被杀死，请参阅疑难解答 部分。\n监控计算资源使用 Pod 的资源使用情况被报告为 Pod 状态的一部分。\n如果为集群配置了可选 监控工具，则可以直接从 指标 API 或者监控工具检索 Pod 资源的使用情况。\n疑难解答 我的 Pod 处于 pending 状态且事件信息显示 failedScheduling 如果调度器找不到任何该 Pod 可以匹配的节点，则该 Pod 将保持不可调度状态，直到找到一个可以被调度到的位置。每当调度器找不到 Pod 可以调度的地方时，会产生一个事件，如下所示：\nkubectl describe pod frontend | grep -A 3 Events Events: FirstSeen LastSeen Count From Subobject PathReason Message 36s 5s 6 {scheduler } FailedScheduling Failed for reason PodExceedsFreeCPU and possibly others 在上述示例中，由于节点上的 CPU 资源不足，名为 “frontend” 的 Pod 将无法调度。由于内存不足（PodExceedsFreeMemory），类似的错误消息也可能会导致失败。一般来说，如果有这种类型的消息而处于 pending 状态，您可以尝试如下几件事情：\n 向集群添加更多节点。 终止不需要的 Pod，为待处理的 Pod 腾出空间。 检查 Pod 所需的资源是否大于所有节点的资源。 例如，如果全部节点的容量为cpu：1，那么一个请求为 cpu：1.1的 Pod 永远不会被调度。  您可以使用 kubectl describe nodes 命令检查节点容量和分配的数量。 例如：\nkubectl describe nodes e2e-test-node-pool-4lw4 Name: e2e-test-node-pool-4lw4 [ ... lines removed for clarity ...] Capacity: cpu: 2 memory: 7679792Ki pods: 110 Allocatable: cpu: 1800m memory: 7474992Ki pods: 110 [ ... lines removed for clarity ...] Non-terminated Pods: (5 in total) Namespace Name CPU Requests CPU Limits Memory Requests Memory Limits --------- ---- ------------ ---------- --------------- ------------- kube-system fluentd-gcp-v1.38-28bv1 100m (5%) 0 (0%) 200Mi (2%) 200Mi (2%) kube-system kube-dns-3297075139-61lj3 260m (13%) 0 (0%) 100Mi (1%) 170Mi (2%) kube-system kube-proxy-e2e-test-... 100m (5%) 0 (0%) 0 (0%) 0 (0%) kube-system monitoring-influxdb-grafana-v4-z1m12 200m (10%) 200m (10%) 600Mi (8%) 600Mi (8%) kube-system node-problem-detector-v0.1-fj7m3 20m (1%) 200m (10%) 20Mi (0%) 100Mi (1%) Allocated resources: (Total limits may be over 100 percent, i.e., overcommitted.) CPU Requests CPU Limits Memory Requests Memory Limits ------------ ---------- --------------- ------------- 680m (34%) 400m (20%) 920Mi (12%) 1070Mi (14%) 在上面的输出中，您可以看到如果 Pod 请求超过 1120m CPU 或者 6.23Gi 内存，节点将无法满足。\n通过查看 Pods 部分，您将看到哪些 Pod 占用的节点上的资源。\nPod 可用的资源量小于节点容量，因为系统守护程序使用一部分可用资源。 [NodeStatus](/docs/resources-reference/\u0026lt; param \u0026ldquo;version\u0026rdquo; \u0026gt;}}/#nodestatus-v1-core) 的 allocatable 字段给出了可用于 Pod 的资源量。 有关更多信息，请参阅 节点可分配资源。\n可以将 资源配额 功能配置为限制可以使用的资源总量。如果与 namespace 配合一起使用，就可以防止一个团队占用所有资源。\n我的容器被终止了 您的容器可能因为资源枯竭而被终止了。要查看容器是否因为遇到资源限制而被杀死，请在相关的 Pod 上调用 kubectl describe pod：\nkubectl describe pod simmemleak-hra99 Name: simmemleak-hra99 Namespace: default Image(s): saadali/simmemleak Node: kubernetes-node-tf0f/10.240.216.66 Labels: name=simmemleak Status: Running Reason: Message: IP: 10.244.2.75 Replication Controllers: simmemleak (1/1 replicas created) Containers: simmemleak: Image: saadali/simmemleak Limits: cpu: 100m memory: 50Mi State: Running Started: Tue, 07 Jul 2015 12:54:41 -0700 Last Termination State: Terminated Exit Code: 1 Started: Fri, 07 Jul 2015 12:54:30 -0700 Finished: Fri, 07 Jul 2015 12:54:33 -0700 Ready: False Restart Count: 5 Conditions: Type Status Ready False Events: FirstSeen LastSeen Count From SubobjectPath Reason Message Tue, 07 Jul 2015 12:53:51 -0700 Tue, 07 Jul 2015 12:53:51 -0700 1 {scheduler } scheduled Successfully assigned simmemleak-hra99 to kubernetes-node-tf0f Tue, 07 Jul 2015 12:53:51 -0700 Tue, 07 Jul 2015 12:53:51 -0700 1 {kubelet kubernetes-node-tf0f} implicitly required container POD pulled Pod container image \u0026quot;k8s.gcr.io/pause:0.8.0\u0026quot; already present on machine Tue, 07 Jul 2015 12:53:51 -0700 Tue, 07 Jul 2015 12:53:51 -0700 1 {kubelet kubernetes-node-tf0f} implicitly required container POD created Created with docker id 6a41280f516d Tue, 07 Jul 2015 12:53:51 -0700 Tue, 07 Jul 2015 12:53:51 -0700 1 {kubelet kubernetes-node-tf0f} implicitly required container POD started Started with docker id 6a41280f516d Tue, 07 Jul 2015 12:53:51 -0700 Tue, 07 Jul 2015 12:53:51 -0700 1 {kubelet kubernetes-node-tf0f} spec.containers{simmemleak} created Created with docker id 87348f12526a 在上面的例子中，Restart Count: 5 意味着 Pod 中的 simmemleak 容器被终止并重启了五次。\n您可以使用 kubectl get pod 命令加上 -o go-template=... 选项来获取之前终止容器的状态。\nkubectl get pod -o go-template=\u0026#39;{{range.status.containerStatuses}}{{\u0026#34;Container Name: \u0026#34;}}{{.name}}{{\u0026#34;\\r\\nLastState: \u0026#34;}}{{.lastState}}{{end}}\u0026#39; simmemleak-hra99 Container Name: simmemleak LastState: map[terminated:map[exitCode:137 reason:OOM Killed startedAt:2015-07-07T20:58:43Z finishedAt:2015-07-07T20:58:43Z containerID:docker://0e4095bba1feccdfe7ef9fb6ebffe972b4b14285d5acdec6f0d3ae8a22fad8b2]] 您可以看到容器因为 reason:OOM killed 被终止，OOM 表示 Out Of Memory。\n本地临时存储 Kubernetes版本1.8引入了新资源_ephemeral-storage_，用于管理本地临时存储。 在每个Kubernetes节点中，kubelet的根目录（默认为 /var/lib/kubelet）和日志目录（ /var/log ）存储在节点的根分区上。 Pods还通过emptyDir卷，容器日志，镜像层和容器可写层共享和使用此分区。\n该分区是“临时”分区，应用程序无法从该分区获得任何性能SLA（例如磁盘IOPS）。 本地临时存储管理仅适用于根分区。 图像层和可写层的可选分区超出范围。\n如果使用可选的运行时分区，则根分区将不保存任何镜像层或可写层。\n本地临时存储的请求和限制设置 Pod 的每个容器可以指定以下一项或多项：\n spec.containers[].resources.limits.ephemeral-storage spec.containers[].resources.requests.ephemeral-storage  对“临时存储”的限制和请求以字节为单位。您可以使用以下后缀之一将存储表示为纯整数或小数形式：E，P，T，G，M，K。您还可以使用2的幂次方：Ei，Pi，Ti，Gi，Mi，Ki。例如，以下内容表示的值其实大致相同：\n128974848, 129e6, 129M, 123Mi 例如，以下Pod具有两个容器。每个容器都有一个2GiB的本地临时存储请求。每个容器的本地临时存储限制为4GiB。因此，该Pod要求本地临时存储空间为4GiB，存储空间限制为8GiB。\napiVersion: v1 kind: Pod metadata: name: frontend spec: containers: - name: db image: mysql env: - name: MYSQL_ROOT_PASSWORD value: \u0026#34;password\u0026#34; resources: requests: ephemeral-storage: \u0026#34;2Gi\u0026#34; limits: ephemeral-storage: \u0026#34;4Gi\u0026#34; - name: wp image: wordpress resources: requests: ephemeral-storage: \u0026#34;2Gi\u0026#34; limits: ephemeral-storage: \u0026#34;4Gi\u0026#34; 如何调度临时存储请求的 Pod 创建Pod时，Kubernetes调度程序会选择一个节点来运行Pod。每个节点都可以为Pod提供最大数量的本地临时存储。 有关更多信息，请参见节点可分配。\n调度程序会确保调度的容器的资源请求的总和小于节点的容量。\n具有临时存储限制的 Pod 如何运行 对于容器级隔离，如果容器的可写层和日志使用量超出其存储限制，则将驱逐Pod。对于 pod 级别的隔离，如果来自所有容器的本地临时存储使用量以及 Pod 的 emptyDir 卷的总和超过限制，则将驱逐Pod。\n监控临时存储消耗 使用本地临时存储时，kubelet 会持续对本地临时存储时进行监视。 通过定期扫描，来监视每个 emptyDir 卷，日志目录和可写层。 从Kubernetes 1.15开始，作为集群操作员的一个选项，可以通过项目配额 来管理 emptyDir 卷（但是不包括日志目录或可写层）。 项目配额最初是在XFS中实现的，最近又被移植到ext4fs中。 项目配额可用于监视和执行； 从Kubernetes 1.15开始，它们可用作Alpha功能仅用于监视。\n配额比目录扫描更快，更准确。 将目录分配给项目时，在该目录下创建的所有文件都将在该项目中创建，内核仅需跟踪该项目中的文件正在使用多少块。 如果创建并删除了文件，但是文件描述符已打开，它将继续占用空间。 该空间将由配额跟踪，但目录扫描不会检查。\nKubernetes使用从1048576开始的项目ID。正在使用的ID注册于 /etc/projects 和 /etc/projid。 如果此范围内的项目ID用于系统上的其他目的，则这些项目ID必须在 /etc/projects 和 /etc/projid 中注册，以防止Kubernetes使用它们。\n要启用项目配额，集群操作员必须执行以下操作：\n  在kubelet配置中启用 LocalStorageCapacityIsolationFSQuotaMonitoring = true 功能。 在Kubernetes 1.15中默认为 false，因此必须显式设置为 true。\n  确保根分区（或可选的运行时分区）是在启用项目配额的情况下构建的。 所有 XFS 文件系统都支持项目配额，但是 ext4 文件系统必须专门构建。\n  确保在启用了项目配额的情况下挂载了根分区（或可选的运行时分区）。\n  在启用项目配额的情况下构建和挂载文件系统 XFS文件系统在构建时不需要任何特殊操作; 它们是在启用项目配额的情况下自动构建的。\nExt4fs文件系统必须在启用了配额的情况下构建，然后必须在文件系统中启用它们：\n% sudo mkfs.ext4 other_ext4fs_args... -E quotatype=prjquota /dev/block_device % sudo tune2fs -O project -Q prjquota /dev/block_device 要挂载文件系统，ext4fs 和 XFS 都需要在 /etc/fstab 中设置 prjquota 选项：\n/dev/block_device\t/var/kubernetes_data\tdefaults,prjquota\t0\t0 拓展资源 拓展资源是 kubernetes.io 域名之外的标准资源名称。它们允许集群管理员做分发，而且用户可以使用非Kubernetes内置资源。 使用扩展资源需要两个步骤。 首先，集群管理员必须分发拓展资源。 其次，用户必须在 Pod 中请求拓展资源。\ncurl --header \u0026#34;Content-Type: application/json-patch+json\u0026#34; \\ --request PATCH \\ --data \u0026#39;[{\u0026#34;op\u0026#34;: \u0026#34;add\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/status/capacity/example.com~1foo\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;5\u0026#34;}]\u0026#39; \\ http://k8s-master:8080/api/v1/nodes/k8s-node-1/status 管理拓展资源 节点级拓展资源 节点级拓展资源绑定到节点。\n设备插件托管资源 有关如何在每个节点上分发设备插件托管资源的信息，请参阅设备插件。\n其他资源 为了发布新的节点级拓展资源，集群操作员可以向API服务器提交 PATCH HTTP 请求， 以在 status.capacity 中为集群中的节点指定可用数量。 完成此操作后，节点的 status.capacity 将包含新资源。 由kubelet异步使用新资源自动更新 status.allocatable 字段。 请注意，由于调度程序在评估Pod适合性时使用节点的状态 status.allocatable 值， 因此在用新资源修补节点容量和请求在该节点上调度资源的第一个Pod之间可能会有短暂的延迟。\n示例:\n这是一个示例，显示了如何使用 curl 进行HTTP请求，该请求在主节点为 k8s-master 的子节点 k8s-node-1 上通告五个 example.com/foo 资源。\ncurl --header \u0026#34;Content-Type: application/json-patch+json\u0026#34; \\ --request PATCH \\ --data \u0026#39;[{\u0026#34;op\u0026#34;: \u0026#34;add\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/status/capacity/example.com~1foo\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;5\u0026#34;}]\u0026#39; \\ http://k8s-master:8080/api/v1/nodes/k8s-node-1/status 在前面的请求中，~1 是 Patch 路径中字符 / 的编码。 JSON-Patch中的操作路径值被解释为JSON-Pointer。 有关更多详细信息，请参见 IETF RFC 6901, section 3.\n集群级扩展资源 群集级扩展资源不绑定到节点。 它们通常由调度程序扩展程序管理，这些程序处理资源消耗和资源配额。\n您可以在调度程序策略配置中指定由调度程序扩展程序处理的扩展资源。\n示例:\n通过调度程序策略的以下配置，指示群集级扩展资源 \u0026ldquo;example.com/foo\u0026rdquo; 由调度程序扩展程序处理。\n 仅当Pod请求 \u0026ldquo;example.com/foo\u0026rdquo; 时，调度程序才会将 Pod 发送到调度程序扩展程序。 ignoredByScheduler 字段指定调度程序不在其 PodFitsResources 字段中检查 \u0026ldquo;example.com/foo\u0026rdquo; 资源。  { \u0026#34;kind\u0026#34;: \u0026#34;Policy\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;v1\u0026#34;, \u0026#34;extenders\u0026#34;: [ { \u0026#34;urlPrefix\u0026#34;:\u0026#34;\u0026lt;extender-endpoint\u0026gt;\u0026#34;, \u0026#34;bindVerb\u0026#34;: \u0026#34;bind\u0026#34;, \u0026#34;managedResources\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;example.com/foo\u0026#34;, \u0026#34;ignoredByScheduler\u0026#34;: true } ] } ] } 消耗扩展资源 就像 CPU 和内存一样，用户可以使用 Pod 的扩展资源。 调度程序负责核算资源，因此不会同时将过多的可用资源分配给 Pod。\n扩展资源取代了 Opaque 整数资源。 用户可以使用保留字 kubernetes.io 以外的任何域名前缀。\n要在Pod中使用扩展资源，请在容器规范的 spec.containers[].resources.limits 映射中包含资源名称作为键。\n扩展资源不能过量使用，因此如果容器规范中存在请求和限制，则它们必须一致。\n仅当满足所有资源请求(包括 CPU ，内存和任何扩展资源)时，才能调度 Pod。 只要资源请求无法满足，则 Pod 保持在 PENDING 状态。\n示例:\n下面的 Pod 请求2个 CPU 和1个\u0026quot;example.com/foo\u0026rdquo;(扩展资源)。\napiVersion: v1 kind: Pod metadata: name: my-pod spec: containers: - name: my-container image: myimage resources: requests: cpu: 2 example.com/foo: 1 limits: example.com/foo: 1    获取将 分配内存资源给容器和 Pod  的实践经验\n  获取将 分配 CPU 资源给容器和 Pod  的实践经验\n  [容器](/docs/reference/generated/kubernetes-api/\u0026lt; param \u0026ldquo;version\u0026rdquo; \u0026gt;}}/#container-v1-core)\n  [资源需求](/docs/resources-reference/\u0026lt; param \u0026ldquo;version\u0026rdquo; \u0026gt;}}/#resourcerequirements-v1-core)\n  "
},
{
	"uri": "https://lijun.in/tutorials/kubernetes-basics/create-cluster/cluster-interactive/",
	"title": "交互式教程 - 创建集群",
	"tags": [],
	"description": "",
	"content": "   要与终端交互，请使用桌面/平板    Continue to Module 2› -- 继续阅读第二单元›       "
},
{
	"uri": "https://lijun.in/tutorials/kubernetes-basics/expose/expose-interactive/",
	"title": "交互式教程 - 发布您的应用程序",
	"tags": [],
	"description": "",
	"content": "   要与终端交互，请使用台式机/平板电脑    Continue to Module 5›       "
},
{
	"uri": "https://lijun.in/tutorials/kubernetes-basics/update/update-interactive/",
	"title": "交互式教程 - 更新应用",
	"tags": [],
	"description": "",
	"content": "  要与终端交互，请使用桌面/平板电脑版本    回到 Kubernetes 的基础›       "
},
{
	"uri": "https://lijun.in/tutorials/kubernetes-basics/deploy-app/deploy-interactive/",
	"title": "交互式教程 - 部署应用程序",
	"tags": [],
	"description": "",
	"content": "   To interact with the Terminal, please use the desktop/tablet version  -- 要与终端进行交互，请使用桌面/平板电脑版本    Continue to Module 3› -- 继续阅读第3单元›      "
},
{
	"uri": "https://lijun.in/tutorials/kubernetes-basics/explore/explore-interactive/",
	"title": "交互式教程-探索您的应用程序",
	"tags": [],
	"description": "",
	"content": "   To interact with the Terminal, please use the desktop/tablet version  -- 要与终端交互，请使用桌面/平板 版本    Continue to Module 4›       "
},
{
	"uri": "https://lijun.in/tutorials/kubernetes-basics/scale/scale-interactive/",
	"title": "交互教程 - 缩放你的应用程序",
	"tags": [],
	"description": "",
	"content": "  与终端交互，请使用桌面/平板电脑版本    继续参阅第6单元›        "
},
{
	"uri": "https://lijun.in/tasks/administer-cluster/network-policy-provider/cilium-network-policy/",
	"title": "使用 Cilium 作为 NetworkPolicy",
	"tags": [],
	"description": "",
	"content": "本页展示了如何使用 Cilium 作为 NetworkPolicy。\n关于 Cilium 的背景知识，请阅读 Cilium 介绍。\n. heading \u0026ldquo;prerequisites\u0026rdquo; %}} . include \u0026ldquo;task-tutorial-prereqs.md\u0026rdquo; \u0026gt;}} . version-check \u0026gt;}}\n在 Minikube 上部署 Cilium 用于基本测试 为了轻松熟悉 Cilium 您可以根据Cilium Kubernetes 入门指南在 minikube 中执行一个 cilium 的基本的 DaemonSet 安装。\n在 minikube 中的安装配置使用一个简单的“一体化” YAML 文件，包括了 Cilium 的 DaemonSet 配置，连接 minikube 的 etcd 实例，以及适当的 RBAC 设置。\n$ kubectl create -f https://raw.githubusercontent.com/cilium/cilium/master/examples/kubernetes/cilium.yaml configmap \u0026#34;cilium-config\u0026#34; created secret \u0026#34;cilium-etcd-secrets\u0026#34; created serviceaccount \u0026#34;cilium\u0026#34; created clusterrolebinding \u0026#34;cilium\u0026#34; created daemonset \u0026#34;cilium\u0026#34; created clusterrole \u0026#34;cilium\u0026#34; created 入门指南其余的部分用一个示例应用说明了如何强制执行L3/L4（即 IP 地址+端口）的安全策略以及L7 （如 HTTP）的安全策略。\n部署 Cilium 用于生产用途 关于部署 Cilium 用于生产的详细说明，请见Cilium Kubernetes 安装指南 ，此文档包括详细的需求、说明和生产用途 DaemonSet 文件示例。\n了解 Cilium 组件 部署使用 Cilium 的集群会添加 Pods 到kube-system命名空间。 要查看此Pod列表，运行：\nkubectl get pods --namespace=kube-system 您将看到像这样的 Pods 列表：\nNAME DESIRED CURRENT READY NODE-SELECTOR AGE cilium 1 1 1 \u0026lt;none\u0026gt; 2m ... 有两个主要组件需要注意：\n 在集群中的每个节点上都会运行一个 cilium Pod，并利用Linux BPF执行网络策略管理该节点上进出 Pod 的流量。 对于生产部署，Cilium 应该复用 Kubernetes 所使用的键值存储集群（如 etcd），其通常在Kubernetes 的 master 节点上运行。 Cilium Kubernetes安装指南 包括了一个示例 DaemonSet，可以自定义指定此键值存储集群。 简单的 minikube 的“一体化” DaemonSet 不需要这样的配置，因为它会自动连接到 minikube 的 etcd 实例。  . heading \u0026ldquo;whatsnext\u0026rdquo; %}} 群集运行后，您可以按照声明网络策略 用 Cilium 试用 Kubernetes NetworkPolicy。 玩得开心，如果您有任何疑问，请联系我们 Cilium Slack Channel。\n"
},
{
	"uri": "https://lijun.in/setup/production-environment/tools/kops/",
	"title": "使用 Kops 安装 Kubernetes",
	"tags": [],
	"description": "",
	"content": "本篇快速入门介绍了如何在 AWS 上轻松安装 Kubernetes 集群。 本篇使用了一个名为 kops 的工具。\nkops 是一个自用的供应系统：\n 全自动安装流程 使用 DNS 识别集群 自我修复：一切都在自动扩展组中运行 支持多种操作系统（如 Debian、Ubuntu 16.04、CentOS、RHEL、Amazon Linux 和 CoreOS） - 参考 images.md 支持高可用 - 参考 high_availability.md 可以直接提供或者生成 terraform 清单 - 参考 terraform.md  如果您有不同的观点，您可能更喜欢使用 kubeadm 作为构建工具来构建自己的集群。kops 建立在 kubeadm 工作的基础上。\n创建集群 (1/5) 安装 kops 前提条件 您必须安装 kubectl 才能使 kops 工作。\n安装 从下载页面下载 kops（从源代码构建也很容易）：\n在 macOS 上：\ncurl -OL https://github.com/kubernetes/kops/releases/download/1.10.0/kops-darwin-amd64 chmod +x kops-darwin-amd64 mv kops-darwin-amd64 /usr/local/bin/kops # 您也可以使用 Homebrew 安装 kops brew update \u0026amp;\u0026amp; brew install kops 在 Linux 上：\nwget https://github.com/kubernetes/kops/releases/download/1.10.0/kops-linux-amd64 chmod +x kops-linux-amd64 mv kops-linux-amd64 /usr/local/bin/kops (2/5) 为您的集群创建一个 route53 域名 kops 在集群内部都使用 DNS 进行发现操作，因此您可以从客户端访问 kubernetes API 服务器。\nkops 对集群名称有明显的要求：它应该是有效的 DNS 名称。这样一来，您就不会再使集群混乱， 可以与同事明确共享集群，并且无需依赖记住 IP 地址即可访问群集。\n您应该使用子域名来划分集群。作为示例，我们将使用域名 useast1.dev.example.com。 然后，API 服务器端点域名将为 api.useast1.dev.example.com。\nRoute53 托管区域可以服务子域名。您的托管区域可能是 useast1.dev.example.com，还有 dev.example.com 甚至 example.com。 kops 可以与以上任何一种配合使用，因此通常您出于组织原因选择不同的托管区域。 例如，允许您在 dev.example.com 下创建记录，但不能在 example.com 下创建记录。\n假设您使用 dev.example.com 作为托管区域。您可以使用正常流程 或者使用诸如 aws route53 create-hosted-zone --name dev.example.com --caller-reference 1 之类的命令来创建该托管区域。\n然后，您必须在父域名中设置您的 DNS 记录，以便该域名中的记录可以被解析。在这里，您将在 example.com 中为 dev 创建 DNS 记录。 如果它是根域名，则可以在域名注册机构配置 DNS 记录。例如，您需要在购买 example.com 的地方配置 example.com。\n这一步很容易搞砸（这是问题的第一大原因！） 如果您安装了 dig 工具，则可以通过运行以下步骤再次检查集群是否配置正确：\ndig DNS dev.example.com\n您应该看到 Route53 分配了您的托管区域的 4 条 DNS 记录。\n(3/5) 创建一个 S3 存储桶来存储集群状态 kops 使您即使在安装后也可以管理集群。为此，它必须跟踪已创建的集群及其配置、所使用的密钥等。 此信息存储在 S3 存储桶中。S3 权限用于控制对存储桶的访问。\n多个集群可以使用同一 S3 存储桶，并且您可以在管理同一集群的同事之间共享一个 S3 存储桶 - 这比传递 kubecfg 文件容易得多。 但是有权访问 S3 存储桶的任何人都将拥有对所有集群的管理访问权限，因此您不想在运营团队之外共享它。\n因此，通常每个运维团队都有一个 S3 存储桶（而且名称通常对应于上面托管区域的名称！）\n在我们的示例中，我们选择 dev.example.com 作为托管区域，因此让我们选择 clusters.dev.example.com 作为 S3 存储桶名称。\n 导出 AWS_PROFILE 文件（如果您需要选择一个配置文件用来使 AWS CLI 正常工作）   使用 aws s3 mb s3://clusters.dev.example.com 创建 S3 存储桶   您可以进行 export KOPS_STATE_STORE=s3://clusters.dev.example.com 操作，然后 kops 将默认使用此位置。 我们建议将其放入您的 bash profile 文件或类似文件中。  (4/5) 建立您的集群配置 运行 \u0026ldquo;kops create cluster\u0026rdquo; 以创建您的集群配置：\nkops create cluster --zones=us-east-1c useast1.dev.example.com\nkops 将为您的集群创建配置。请注意，它_仅_创建配置，实际上并没有创建云资源 - 您将在下一步中使用 kops update cluster 进行配置。 这使您有机会查看配置或进行更改。\n它打印出可用于进一步探索的命令：\n 使用以下命令列出集群：kops get cluster 使用以下命令编辑该集群：kops edit cluster useast1.dev.example.com 使用以下命令编辑您的节点实例组：kops edit ig --name = useast1.dev.example.com nodes 使用以下命令编辑您的主实例组：kops edit ig --name = useast1.dev.example.com master-us-east-1c  如果这是您第一次使用 kops，请花几分钟尝试一下！ 实例组是一组实例，将被注册为 kubernetes 节点。 在 AWS 上，这是通过 auto-scaling-groups 实现的。您可以有多个实例组，例如，如果您想要的是混合实例和按需实例的节点，或者 GPU 和非 GPU 实例。\n(5/5) 在 AWS 中创建集群 运行 \u0026ldquo;kops update cluster\u0026rdquo; 以在 AWS 中创建集群：\nkops update cluster useast1.dev.example.com --yes\n这需要几秒钟的时间才能运行，但实际上集群可能需要几分钟才能准备就绪。每当更改集群配置时，都会使用 kops update cluster 工具。 它将对配置进行的更改应用于您的集群 - 根据需要重新配置 AWS 或者 kubernetes。\n例如，在您运行 kops edit ig nodes 之后，然后运行 kops update cluster --yes 应用您的配置，有时您还必须运行 kops rolling-update cluster 立即回滚更新配置。\n如果没有 --yes 参数，kops update cluster 操作将向您显示其操作的预览效果。这对于生产集群很方便！\n探索其他附加组件 请参阅附加组件列表探索其他附加组件，包括用于 Kubernetes 集群的日志记录、监视、网络策略、可视化和控制的工具。\n清理  删除集群：kops delete cluster useast1.dev.example.com --yes  反馈  Slack Channel: #kops-users GitHub Issues  . heading \u0026ldquo;whatsnext\u0026rdquo; %}}  了解有关 Kubernetes 的 concepts 和 kubectl 的更多信息。 了解 kops 高级用法。 请参阅 kops 文档 获取教程、最佳做法和高级配置选项。  "
},
{
	"uri": "https://lijun.in/tasks/job/parallel-processing-expansion/",
	"title": "使用扩展进行并行处理",
	"tags": [],
	"description": "",
	"content": "在这个示例中，我们将运行从一个公共模板创建的多个 Kubernetes Job。您可能需要先熟悉 Jobs 的基本概念、非并行以及如何使用它。\n基本模板扩展 首先，将以下作业模板下载到名为 job-tmpl.yaml 的文件中。\n. codenew file=\u0026quot;application/job/job-tmpl.yaml\u0026rdquo; \u0026gt;}}\n与 pod 模板不同，我们的 job 模板不是 Kubernetes API 类型。它只是 Job 对象的 yaml 表示， YAML 文件有一些占位符，在使用它之前需要填充这些占位符。$ITEM 语法对 Kubernetes 没有意义。\n在这个例子中，容器所做的唯一处理是 echo 一个字符串并睡眠一段时间。 在真实的用例中，处理将是一些重要的计算，例如渲染电影的一帧，或者处理数据库中的若干行。这时，$ITEM 参数将指定帧号或行范围。\n这个 Job 及其 Pod 模板有一个标签: jobgroup=jobexample。这个标签在系统中没有什么特别之处。 这个标签使得我们可以方便地同时操作组中的所有作业。 我们还将相同的标签放在 pod 模板上，这样我们就可以用一个命令检查这些 Job 的所有 pod。 创建作业之后，系统将添加更多的标签来区分一个 Job 的 pod 和另一个 Job 的 pod。 注意，标签键 jobgroup 对 Kubernetes 并无特殊含义。您可以选择自己的标签方案。\n下一步，将模板展开到多个文件中，每个文件对应要处理的项。\n# 下载 job-templ.yaml curl -L -s -O https://k8s.io/examples/application/job/job-tmpl.yaml # 创建临时目录，并且在目录中创建 job yaml 文件 mkdir ./jobs for i in apple banana cherry do cat job-tmpl.yaml | sed \u0026#34;s/\\$ITEM/$i/\u0026#34; \u0026gt; ./jobs/job-$i.yaml done 检查是否工作正常：\nls jobs/ 输出类似以下内容：\njob-apple.yaml\rjob-banana.yaml\rjob-cherry.yaml\r在这里，我们使用 sed 将字符串 $ITEM 替换为循环变量。 您可以使用任何类型的模板语言(jinja2, erb) 或编写程序来生成 Job 对象。\n接下来，使用 kubectl 命令创建所有作业：\nkubectl create -f ./jobs 输出类似以下内容：\njob.batch/process-item-apple created\rjob.batch/process-item-banana created\rjob.batch/process-item-cherry created\r现在，检查这些作业：\nkubectl get jobs -l jobgroup=jobexample 输出类似以下内容：\nNAME COMPLETIONS DURATION AGE\rprocess-item-apple 1/1 14s 20s\rprocess-item-banana 1/1 12s 20s\rprocess-item-cherry 1/1 12s 20s\r在这里，我们使用 -l 选项选择属于这组作业的所有作业。(系统中可能还有其他不相关的工作，我们不想看到。)\n使用同样的标签选择器，我们还可以检查 pods：\nkubectl get pods -l jobgroup=jobexample 输出类似以下内容：\nNAME READY STATUS RESTARTS AGE\rprocess-item-apple-kixwv 0/1 Completed 0 4m\rprocess-item-banana-wrsf7 0/1 Completed 0 4m\rprocess-item-cherry-dnfu9 0/1 Completed 0 4m\r我们可以使用以下操作命令一次性地检查所有作业的输出：\nkubectl logs -f -l jobgroup=jobexample 输出内容为：\nProcessing item apple\rProcessing item banana\rProcessing item cherry\r多个模板参数 在第一个示例中，模板的每个实例都有一个参数，该参数也用作标签。 但是标签的键名在可包含的字符方面有一定的约束。\n这个稍微复杂一点的示例使用 jinja2 模板语言来生成我们的对象。 我们将使用一行 python 脚本将模板转换为文件。\n首先，粘贴 Job 对象的以下模板到一个名为 job.yaml.jinja2 的文件中：\n{%- set params = [{ \u0026quot;name\u0026quot;: \u0026quot;apple\u0026quot;, \u0026quot;url\u0026quot;: \u0026quot;https://www.orangepippin.com/varieties/apples\u0026quot;, },\r{ \u0026quot;name\u0026quot;: \u0026quot;banana\u0026quot;, \u0026quot;url\u0026quot;: \u0026quot;https://en.wikipedia.org/wiki/Banana\u0026quot;, },\r{ \u0026quot;name\u0026quot;: \u0026quot;raspberry\u0026quot;, \u0026quot;url\u0026quot;: \u0026quot;https://www.raspberrypi.org/\u0026quot; }]\r%}\r{%- for p in params %}\r{%- set name = p[\u0026quot;name\u0026quot;] %}\r{%- set url = p[\u0026quot;url\u0026quot;] %}\rapiVersion: batch/v1\rkind: Job\rmetadata:\rname: jobexample-{{ name }}\rlabels:\rjobgroup: jobexample\rspec:\rtemplate:\rmetadata:\rname: jobexample\rlabels:\rjobgroup: jobexample\rspec:\rcontainers:\r- name: c\rimage: busybox\rcommand: [\u0026quot;sh\u0026quot;, \u0026quot;-c\u0026quot;, \u0026quot;echo Processing URL {{ url }} \u0026amp;\u0026amp; sleep 5\u0026quot;]\rrestartPolicy: Never\r---\r{%- endfor %}\r上面的模板使用 python 字典列表（第 1-4 行）定义每个作业对象的参数。 然后使用 for 循环为每组参数（剩余行）生成一个作业 yaml 对象。 我们利用了多个 yaml 文档可以与 --- 分隔符连接的事实（倒数第二行）。 我们可以将输出直接传递给 kubectl 来创建对象。\n如果您还没有 jinja2 包则需要安装它: pip install --user jinja2。 现在，使用这个一行 python 程序来展开模板:\nalias render_template=\u0026#39;python -c \u0026#34;from jinja2 import Template; import sys; print(Template(sys.stdin.read()).render());\u0026#34;\u0026#39; 输出可以保存到一个文件，像这样：\ncat job.yaml.jinja2 | render_template \u0026gt; jobs.yaml 或直接发送到 kubectl，如下所示：\ncat job.yaml.jinja2 | render_template | kubectl apply -f - 替代方案 如果您有大量作业对象，您可能会发现：\n 即使使用标签，管理这么多 Job 对象也很麻烦。 在一次创建所有作业时，您超过了资源配额，可是您也不希望以递增方式创建 Job 并等待其完成。 同时创建大量作业会使 Kubernetes apiserver、控制器或者调度器负压过大。  在这种情况下，您可以考虑其他的作业模式。\n"
},
{
	"uri": "https://lijun.in/setup/best-practices/cluster-large/",
	"title": "创建大型集群",
	"tags": [],
	"description": "",
	"content": "支持 在 . param \u0026ldquo;version\u0026rdquo; \u0026gt;}} 版本中， Kubernetes 支持的最大节点数为 5000。更具体地说，我们支持满足以下所有条件的配置：\n 节点数不超过 5000 Pod 总数不超过 150000 容器总数不超过 300000 每个节点的 pod 数量不超过 100  . toc \u0026gt;}}\n设定 集群是一组运行着 Kubernetes 代理的节点（物理机或者虚拟机），这些节点由主控节点（集群级控制面）控制。\n通常，集群中的节点数由特定于云平台的配置文件 config-default.sh（可以参考 [GCE 平台的 config-default.sh](http://releases.k8s.io/. param \u0026ldquo;githubbranch\u0026rdquo; \u0026gt;}}/cluster/gce/config-default.sh)）中的 NUM_NODES 参数控制。\n但是，在许多云供应商的平台上，仅将该值更改为非常大的值，可能会导致安装脚本运行失败。例如，在 GCE，由于配额问题，集群会启动失败。\n因此，在创建大型 Kubernetes 集群时，必须考虑以下问题。\n配额问题 为了避免遇到云供应商配额问题，在创建具有大规模节点的集群时，请考虑：\n 增加诸如 CPU，IP 等资源的配额。  例如，在 GCE，您需要增加以下资源的配额：  CPUs VM 实例 永久磁盘总量 使用中的 IP 地址 防火墙规则 转发规则 路由 目标池     由于某些云供应商会对虚拟机的创建进行流控，因此需要对设置脚本进行更改，使其以较小的批次启动新的节点，并且之间有等待时间。  Etcd 存储 为了提高大规模集群的性能，我们将事件存储在专用的 etcd 实例中。\n在创建集群时，现有 salt 脚本可以：\n 启动并配置其它 etcd 实例 配置 API 服务器以使用 etcd 存储事件  主控节点大小和主控组件 在 GCE/Google Kubernetes Engine 和 AWS 上，kube-up 会根据节点数量自动为您集群中的 master 节点配置适当的虚拟机大小。在其它云供应商的平台上，您将需要手动配置它。作为参考，我们在 GCE 上使用的规格为：\n 1-5 个节点：n1-standard-1 6-10 个节点：n1-standard-2 11-100 个节点：n1-standard-4 101-250 个节点：n1-standard-8 251-500 个节点：n1-standard-16 超过 500 节点：n1-standard-32  在 AWS 上使用的规格为\n 1-5 个节点：m3.medium 6-10 个节点：m3.large 11-100 个节点：m3.xlarge 101-250 个节点：m3.2xlarge 251-500 个节点：c4.4xlarge 超过 500 节点：c4.8xlarge  . note \u0026gt;}}\n在 Google Kubernetes Engine 上，主控节点的大小会根据集群的大小自动调整。更多有关信息，请参阅 此博客文章。\n在 AWS 上，主控节点的规格是在集群启动时设置的，并且，即使以后通过手动删除或添加节点的方式使集群缩容或扩容，主控节点的大小也不会更改。 . /note \u0026gt;}}\n插件资源 为了防止内存泄漏或 [集群插件](https://releases.k8s.io/.param \u0026ldquo;githubbranch\u0026rdquo; \u0026gt;}}/cluster/addons) 中的其它资源问题导致节点上所有可用资源被消耗，Kubernetes 限制了插件容器可以消耗的 CPU 和内存资源（请参阅 PR #10653 和 #10778）。\n例如：\ncontainers: - name: fluentd-cloud-logging image: k8s.gcr.io/fluentd-gcp:1.16 resources: limits: cpu: 100m memory: 200Mi 除了 Heapster 之外，这些限制都是静态的，并且限制是基于 4 节点集群上运行的插件数据得出的（请参阅 #10335）。在大规模集群上运行时，插件会消耗大量资源（请参阅 #5880）。因此，如果在不调整这些值的情况下部署了大规模集群，插件容器可能会由于达到限制而不断被杀死。\n为避免遇到集群插件资源问题，在创建大规模集群时，请考虑以下事项：\n 根据集群的规模，如果使用了以下插件，提高其内存和 CPU 上限（每个插件都有一个副本处理整个群集，因此内存和 CPU 使用率往往与集群的规模/负载成比例增长） ：  [InfluxDB 和 Grafana](http://releases.k8s.io/. param \u0026ldquo;githubbranch\u0026rdquo; \u0026gt;}}/cluster/addons/cluster-monitoring/influxdb/influxdb-grafana-controller.yaml) [kubedns、dnsmasq 和 sidecar](http://releases.k8s.io/. param \u0026ldquo;githubbranch\u0026rdquo; \u0026gt;}}/cluster/addons/dns/kube-dns/kube-dns.yaml.in) [Kibana](http://releases.k8s.io/. param \u0026ldquo;githubbranch\u0026rdquo; \u0026gt;}}/cluster/addons/fluentd-elasticsearch/kibana-deployment.yaml)   根据集群的规模，如果使用了以下插件，调整其副本数量（每个插件都有多个副本，增加副本数量有助于处理增加的负载，但是，由于每个副本的负载也略有增加，因此也请考虑增加 CPU/内存限制）：  [elasticsearch](http://releases.k8s.io/. param \u0026ldquo;githubbranch\u0026rdquo; \u0026gt;}}/cluster/addons/fluentd-elasticsearch/es-statefulset.yaml)   根据集群的规模，如果使用了以下插件，限制其内存和 CPU 上限（这些插件在每个节点上都有一个副本，但是 CPU/内存使用量也会随集群负载/规模而略有增加）：  [FluentD 和 ElasticSearch 插件](http://releases.k8s.io/. param \u0026ldquo;githubbranch\u0026rdquo; \u0026gt;}}/cluster/addons/fluentd-elasticsearch/fluentd-es-ds.yaml) [FluentD 和 GCP 插件](http://releases.k8s.io/. param \u0026ldquo;githubbranch\u0026rdquo; \u0026gt;}}/cluster/addons/fluentd-gcp/fluentd-gcp-ds.yaml)    Heapster 的资源限制与您集群的初始大小有关（请参阅 #16185 和 #22940）。如果您发现 Heapster 资源不足，您应该调整堆内存请求的计算公式（有关详细信息，请参阅相关 PR）。\n关于如何检测插件容器是否达到资源限制，参见 计算资源的故障排除 部分。\n未来，我们期望根据集群规模大小来设置所有群集附加资源限制，并在集群扩缩容时动态调整它们。 我们欢迎您来实现这些功能。\n允许启动时次要节点失败 出于各种原因（更多详细信息，请参见 #18969）， 在 kube-up.sh 中设置很大的 NUM_NODES 时，可能会由于少数节点无法正常启动而失败。 此时，您有两个选择：重新启动集群（运行 kube-down.sh，然后再运行 kube-up.sh），或者在运行 kube-up.sh 之前将环境变量 ALLOWED_NOTREADY_NODES 设置为您认为合适的任何值。采取后者时，即使运行成功的节点数量少于 NUM_NODES，kube-up.sh 仍可以运行成功。根据失败的原因，这些节点可能会稍后加入集群，又或者群集的大小保持在 NUM_NODES-ALLOWED_NOTREADY_NODES。\n"
},
{
	"uri": "https://lijun.in/concepts/storage/volume-snapshots/",
	"title": "卷快照",
	"tags": [],
	"description": "",
	"content": "feature-state for_k8s_version=\u0026quot;1.17\u0026rdquo; state=\u0026quot;beta\u0026rdquo; \u0026gt;}}\n在 Kubernetes 中，卷快照是一个存储系统上卷的快照，本文假设你已经熟悉了 Kubernetes 的 持久卷。\n介绍 与 PersistentVolume 和 PersistentVolumeClaim 两个 API 资源用于给用户和管理员提供卷类似，VolumeSnapshotContent 和 VolumeSnapshot 两个 API 资源用于给用户和管理员创建卷快照。\nVolumeSnapshotContent 是一种快照，从管理员已提供的集群中的卷获取。就像持久卷是集群的资源一样，它也是集群中的资源。\nVolumeSnapshot 是用户对于卷的快照的请求。它类似于持久卷声明。\nVolumeSnapshotClass 允许指定属于 VolumeSnapshot 的不同属性。在从存储系统的相同卷上获取的快照之间，这些属性可能有所不同，因此不能通过使用与 PersistentVolumeClaim 相同的 StorageClass 来表示。\n当使用该功能时，用户需要注意以下几点：\n API 对象 VolumeSnapshot，VolumeSnapshotContent 和 VolumeSnapshotClass 是 glossary_tooltip term_id=\u0026quot;CustomResourceDefinition\u0026rdquo; text=\u0026quot;CRDs\u0026rdquo; \u0026gt;}}，不是核心 API 的部分。 VolumeSnapshot 支持仅可用于 CSI 驱动。 作为 beta 版本 VolumeSnapshot 部署过程的一部分，Kubernetes 团队提供了一个部署于控制平面的快照控制器，并且提供了一个叫做 csi-snapshotter 的 sidecar 帮助容器，它和 CSI 驱动程序部署在一起。快照控制器监视 VolumeSnapshot 和 VolumeSnapshotContent 对象，并且负责动态的创建和删除 VolumeSnapshotContent 对象。sidecar csi-snapshotter 监视 VolumeSnapshotContent 对象，并且触发针对 CSI 端点的 CreateSnapshot 和 DeleteSnapshot 的操作。 CSI 驱动可能实现，也可能没有实现卷快照功能。CSI 驱动可能会使用 csi-snapshotter 来提供对卷快照的支持。详见 CSI 驱动程序文档 Kubernetes 负责 CRDs 和快照控制器的安装。  卷快照和卷快照内容的生命周期 VolumeSnapshotContents 是集群中的资源。VolumeSnapshots 是对于这些资源的请求。VolumeSnapshotContents 和 VolumeSnapshots 之间的交互遵循以下生命周期：\n供应卷快照 快照可以通过两种方式进行配置：预配置或动态配置。\n预配置 集群管理员创建多个 VolumeSnapshotContents。它们带有存储系统上实际卷快照的详细信息，可以供集群用户使用。它们存在于 Kubernetes API 中，并且能够被使用。\n动态的 可以从 PersistentVolumeClaim 中动态获取快照，而不用使用已经存在的快照。在获取快照时，卷快照类指定要用的特定于存储提供程序的参数。\n绑定 在预配置和动态配置场景下，快照控制器处理绑定 VolumeSnapshot 对象和其合适的 VolumeSnapshotContent 对象。绑定关系是一对一的。\n在预配置快照绑定场景下，VolumeSnapshotContent 对象创建之后，才会和 VolumeSnapshot 进行绑定。\n快照源的持久性卷声明保护 这种保护的目的是确保在从系统中获取快照时，不会将正在使用的 PersistentVolumeClaim API 对象从系统中删除（因为这可能会导致数据丢失）。\n如果一个 PVC 正在被快照用来作为源进行快照创建，则该 PVC 是使用中的。如果用户删除正作为快照源的 PVC API 对象，则 PVC 对象不会立即被删除掉。相反，PVC 对象的删除将推迟到任何快照不在主动使用它为止。当快照的 Status 中的 ReadyToUse值为 true 时，PVC 将不再用作快照源。\n当从 PersistentVolumeClaim 中生成快照时，PersistentVolumeClaim 就在被使用了。如果删除一个作为快照源的 PersistentVolumeClaim 对象，这个 PersistentVolumeClaim 对象不会立即被删除的。相反，删除 PersistentVolumeClaim 对象的动作会被放弃，或者推迟到快照的 Status 为 ReadyToUse时再执行。\n删除 删除 VolumeSnapshot 对象触发删除 VolumeSnapshotContent 操作，并且 DeletionPolicy 会紧跟着执行。如果 DeletionPolicy 是 Delete，那么底层存储快照会和 VolumeSnapshotContent 一起被删除。如果 DeletionPolicy 是 Retain，那么底层快照和 VolumeSnapshotContent 都会被保留。\n卷快照 每个 VolumeSnapshot 包含一个 spec 和一个状态。\napiVersion: snapshot.storage.k8s.io/v1beta1 kind: VolumeSnapshot metadata: name: new-snapshot-test spec: volumeSnapshotClassName: csi-hostpath-snapclass source: persistentVolumeClaimName: pvc-test persistentVolumeClaimName 是 PersistentVolumeClaim 数据源对快照的名称。这个字段是动态配置快照中的必填字段。\n卷快照可以通过指定 VolumeSnapshotClass 使用 volumeSnapshotClassName 属性来请求特定类。如果没有设置，那么使用默认类（如果有）。\n如下面例子所示，对于预配置的快照，需要给快照指定 volumeSnapshotContentName 来作为源。对于预配置的快照 source 中的volumeSnapshotContentName 字段是必填的。\napiVersion: snapshot.storage.k8s.io/v1beta1 kind: VolumeSnapshot metadata: name: test-snapshot spec: source: volumeSnapshotContentName: test-content 每个 VolumeSnapshotContent 对象包含 spec 和 status。在动态配置时，快照通用控制器创建 VolumeSnapshotContent 对象。下面是例子：\napiVersion: snapshot.storage.k8s.io/v1beta1 kind: VolumeSnapshotContent metadata: name: snapcontent-72d9a349-aacd-42d2-a240-d775650d2455 spec: deletionPolicy: Delete driver: hostpath.csi.k8s.io source: volumeHandle: ee0cfb94-f8d4-11e9-b2d8-0242ac110002 volumeSnapshotClassName: csi-hostpath-snapclass volumeSnapshotRef: name: new-snapshot-test namespace: default uid: 72d9a349-aacd-42d2-a240-d775650d2455 volumeHandle 是存储后端创建卷的唯一标识符，在卷创建期间由 CSI 驱动程序返回。动态设置快照需要此字段。它指出了快照的卷源。\n对于预配置快照，你（作为集群管理员）要按如下命令来创建 VolumeSnapshotContent 对象。\napiVersion: snapshot.storage.k8s.io/v1beta1 kind: VolumeSnapshotContent metadata: name: new-snapshot-content-test spec: deletionPolicy: Delete driver: hostpath.csi.k8s.io source: snapshotHandle: 7bdd0de3-aaeb-11e8-9aae-0242ac110002 volumeSnapshotRef: name: new-snapshot-test namespace: default snapshotHandle 是存储后端创建卷的唯一标识符。对于预设置快照，这个字段是必须的。它指定此 VolumeSnapshotContent 表示的存储系统上的 CSI 快照 id。\n从快照供应卷 你可以配置一个新卷，该卷预填充了快照中的数据，在 持久卷声明 对象中使用 dataSource 字段。\n更多详细信息，请参阅 卷快照和从快照还原卷。\n"
},
{
	"uri": "https://lijun.in/tasks/tools/install-minikube/",
	"title": "安装 Minikube",
	"tags": [],
	"description": "",
	"content": "本页面讲述如何安装 Minikube，该工具用于在您电脑中的虚拟机上运行一个单节点的 Kubernetes 集群。\n. heading \u0026ldquo;prerequisites\u0026rdquo; %}} . tabs name=\u0026quot;minikube_before_you_begin\u0026rdquo; \u0026gt;}} . tab name=\u0026quot;Linux\u0026rdquo; %}}\n若要检查您的 Linux 是否支持虚拟化技术，请运行下面的命令并验证输出结果是否不为空：\ngrep -E --color 'vmx|svm' /proc/cpuinfo . /tab %}}\n. tab name=\u0026quot;macOS\u0026rdquo; %}}\n若要检查您的 macOS 是否支持虚拟化技术，请运行下面的命令：\nsysctl -a | grep -E --color 'machdep.cpu.features|VMX' 如果你在输出结果中看到了 VMX （应该会高亮显示）的字眼，说明您的电脑已启用 VT-x 特性。\n. /tab %}}\n. tab name=\u0026quot;Windows\u0026rdquo; %}}\n若要检查您的 Windows8 及以上的系统是否支持虚拟化技术，请终端或者 cmd 中运行以下命令：\nsysteminfo 如果您看到下面的输出，则表示该 Windows 支持虚拟化技术。\nHyper-V Requirements: VM Monitor Mode Extensions: Yes Virtualization Enabled In Firmware: Yes Second Level Address Translation: Yes Data Execution Prevention Available: Yes 如果您看到下面的输出，则表示您的操作系统已经安装了 Hypervisor，您可以跳过安装 Hypervisor 的步骤。\nHyper-V Requirements: A hypervisor has been detected. Features required for Hyper-V will not be displayed. . /tab %}} . /tabs \u0026gt;}}\n安装 minikube . tabs name=\u0026quot;tab_with_md\u0026rdquo; \u0026gt;}} . tab name=\u0026quot;Linux\u0026rdquo; %}}\n安装 kubectl 请确保你已正确安装 kubectl。您可以根据安装并设置 kubectl 的说明来安装 kubectl。\n安装 Hypervisor 如果还没有装过 hypervisor，请选择以下方式之一进行安装：\n• KVM，也使用了 QEMU\n• VirtualBox\nMinikube 还支持使用一个 --vm-driver=none 选项，让 Kubernetes 组件运行在主机中，而不是在 VM 中。 使用这种驱动方式需要 Docker 和 Linux 环境，但不需要 hypervisor。\n如果你在 Debian 系的 OS 中使用了 none 这种驱动方式，请使用 .deb 包安装 Docker，不要使用 snap 包的方式，Minikube 不支持这种方式。 你可以从 Docker 下载 .deb 包。\n. caution \u0026gt;}}\nnone VM 驱动方式存在导致安全和数据丢失的问题。 使用 --vm-driver=none 之前，请参考这个文档获取详细信息。 . /caution \u0026gt;}}\nMinikube 还支持另外一个类似于 Docker 驱动的方式 vm-driver=podman。 使用超级用户权限（root 用户）运行 Podman 可以最好的确保容器具有足够的权限使用你操作系统上的所有特性。\n. caution \u0026gt;}}\nPodman 驱动方式需要以 root 用户身份运行容器，因为普通用户帐户没有足够的权限使用容器运行可能需要的操作系统上的所有特性。 . /caution \u0026gt;}}\n使用包安装 Minikube Minikube 有 实验性 的安装包。你可以在 Minikube 在 GitHub 上的 releases 找到 Linux (AMD64) 的包。\n根据您的 Linux 发行版选择安装合适的包。\n直接下载并安装 Minikube 如果你不想通过包安装，你也可以下载并使用一个单节点二进制文件。\ncurl -Lo minikube https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64 \\  \u0026amp;\u0026amp; chmod +x minikube 将 Minikube 可执行文件添加至 path：\nsudo mkdir -p /usr/local/bin/ sudo install minikube /usr/local/bin/ 使用 Homebrew 安装 Minikube 你还可以使用 Linux Homebrew 安装 Minikube：\nbrew install minikube . /tab %}} . tab name=\u0026quot;macOS\u0026rdquo; %}}\n安装 kubectl 请确保你已正确安装 kubectl。您可以根据安装并设置 kubectl 的说明来安装 kubectl。\n安装 Hypervisor 如果你还没有安装 hypervisor，请选择以下方式之一进行安装：\n• HyperKit\n• VirtualBox\n• VMware Fusion\n安装 Minikube macOS 安装 Minikube 最简单的方法是使用 Homebrew：\nbrew install minikube 你也可以通过下载单节点二进制文件进行安装：\ncurl -Lo minikube https://storage.googleapis.com/minikube/releases/latest/minikube-darwin-amd64 \\  \u0026amp;\u0026amp; chmod +x minikube 这是一个简单的将 Minikube 可执行文件添加至 path 的方法：\nsudo mv minikube /usr/local/bin . /tab %}} . tab name=\u0026quot;Windows\u0026rdquo; %}}\n安装 kubectl 请确保你已正确安装 kubectl。您可以根据安装并设置 kubectl 的说明来安装 kubectl。\n安装 Hypervisor 如果你还没有安装 hypervisor，请选择以下方式之一进行安装：\n• Hyper-V\n• VirtualBox\n. note \u0026gt;}}\nHyper-V 可以运行在三个版本的 Windows 10 上：企业版、专业版和教育版（Enterprise, Professional, Education）。 . /note \u0026gt;}}\n使用 Chocolatey 安装 Minikube Windows 安装 Minikube 最简单的方法是使用 Chocolatey （以管理员身份运行）：\nchoco install minikube 完成 Minikube 的安装后，关闭当前 CLI 界面再重新打开。 Minikube 应该已经自动添加至 path 中。\n使用安装程序安装 Minikube 在 Windows 上使用 Windows Installer 手动安装 Minikube，下载并运行 minikube-installer.exe 即可。\n直接下载并安装 Minikube 想在 Windows 上手动安装 Minikube，下载 minikube-windows-amd64 并将其重命名为 minikube.exe，然后将其添加至 path 即可。\n. /tab %}} . /tabs \u0026gt;}}\n安装确认 要确认 hypervisor 和 Minikube 均已成功安装，可以运行以下命令来启动本地 Kubernetes 集群：\n. note \u0026gt;}}\n通过 minikube start 设置 --vm-driver。在下面提到 \u0026lt;driver_name\u0026gt; 的地方，用小写字母，输入你安装的 hypervisor 的名称。 指定 VM 驱动程序 列举了 --vm-driver 值的完整列表\n. /note \u0026gt;}}\nminikube start --vm-driver=\u0026lt;driver_name\u0026gt; 一旦 minikube start 完成，你可以运行下面的命令来检查集群的状态：\nminikube status 如果你的集群正在运行，minikube status 的输出结果应该类似于这样：\nhost: Running kubelet: Running apiserver: Running kubeconfig: Configured 在确认 Minikube 与 hypervisor 均正常工作后，您可以继续使用 Minikube 或停止集群。要停止集群，请运行：\nminikube stop 清理本地状态 如果您之前安装过 Minikube，并运行了：\nminikube start 并且 minikube start 返回了一个错误：\nmachine does not exist 那么，你需要清理 minikube 的本地状态：\nminikube delete . heading \u0026ldquo;whatsnext\u0026rdquo; %}}  使用 Minikube 在本地运行 Kubernetes  "
},
{
	"uri": "https://lijun.in/concepts/containers/container-environment/",
	"title": "容器环境",
	"tags": [],
	"description": "",
	"content": "本页描述了在容器环境里容器可用的资源。\n容器环境 Kubernetes 的容器环境给容器提供了几个重要的资源：\n 文件系统，其中包含一个镜像 和一个或多个的卷。 容器自身的信息。 集群中其他对象的信息。  容器信息 容器的 hostname 是它所运行在的 pod 的名称。它可以通过 hostname 命令或者调用 libc 中的 gethostname 函数来获取。\nPod 名称和命名空间可以通过 downward API 使用环境变量。\nPod 定义中的用户所定义的环境变量也可在容器中使用，就像在 Docker 镜像中静态指定的任何环境变量一样。\n集群信息 创建容器时正在运行的所有服务的列表都可用作该容器的环境变量。这些环境变量与 Docker 链接的语法匹配。\n对于名为 foo 的服务，当映射到名为 bar 的容器时，以下变量是被定义了的：\nFOO_SERVICE_HOST=\u0026lt;the host the service is running on\u0026gt; FOO_SERVICE_PORT=\u0026lt;the port the service is running on\u0026gt; Service 具有专用的 IP 地址。如果启用了 [DNS插件](http://releases.k8s.io/\u0026lt; param \u0026ldquo;githubbranch\u0026rdquo; \u0026gt;}}/cluster/addons/dns/)，就可以在容器中通过 DNS 来访问。\n  学习更多有关容器生命周期钩子的知识。 动手获得经验将处理程序附加到容器生命周期事件。  "
},
{
	"uri": "https://lijun.in/concepts/containers/runtime-class/",
	"title": "容器运行时类(Runtime Class)",
	"tags": [],
	"description": "",
	"content": "for_k8s_version=\u0026quot;v1.14\u0026rdquo; state=\u0026quot;beta\u0026rdquo; \u0026gt;}}\n本页面描述了 RuntimeClass 资源和运行时的选择机制。\nRuntimeClass 是一个用于选择容器运行时配置的特性，容器运行时配置用于运行 Pod 中的容器。\n动机 您可以在不同的 pod 之间设置不同的 RuntimeClass，以提供性能与安全性之间的平衡。 例如，如果您的部分工作负载需要高级别的信息安全保证，那么您可以选择性地调度这些 pod， 使它们在使用硬件虚拟化的容器运行时中运行。 然后，您将从可选运行时的额外隔离中获益，代价是一些额外的开销。\n您还可以使用 RuntimeClass 运行具有相同容器运行时但具有不同设置的pod。\n设置 确保 RuntimeClass 特性开关处于开启状态（默认为开启状态）。 关于特性开关的详细介绍，请查阅 Feature Gates。 RuntimeClass 特性开关必须在 apiserver 和 kubelet 同时开启。\n 在节点上配置 CRI 的实现（取决于所选用的运行时） 创建相应的 RuntimeClass 资源  1. 在节点上配置 CRI 实现 RuntimeClass 的配置依赖于 运行时接口（CRI）的实现。 根据你使用的 CRI 实现，查阅相关的文档（下方）来了解如何配置。\nRuntimeClass 假设集群中的节点配置是同构的（换言之，所有的节点在容器运行时方面的配置是相同的）。 如果需要支持异构节点，配置方法请参阅下面的 调度。\n所有这些配置都具有相应的 handler 名，并被 RuntimeClass 引用。 handler 必须符合 DNS-1123 命名规范（字母、数字、或 -）。\n2. 创建相应的 RuntimeClass 资源 在上面步骤 1 中，每个配置都需要有一个用于标识配置的 handler。 针对每个 handler 需要创建一个 RuntimeClass 对象。\nRuntimeClass 资源当前只有两个重要的字段：RuntimeClass 名 (metadata.name) 和 handler (handler)。 对象定义如下所示：\napiVersion: node.k8s.io/v1beta1 # RuntimeClass is defined in the node.k8s.io API group kind: RuntimeClass metadata: name: myclass # The name the RuntimeClass will be referenced by # RuntimeClass is a non-namespaced resource handler: myconfiguration # The name of the corresponding CRI configuration 通常这是默认配置。参阅授权概述了解更多信息。\n使用说明 一旦完成集群中 RuntimeClasses 的配置，使用起来非常方便。 在 Pod spec 中指定 runtimeClassName 即可。例如:\napiVersion: v1 kind: Pod metadata: name: mypod spec: runtimeClassName: myclass # ... 这一设置会告诉 Kubelet 使用所指的 RuntimeClass 来运行该 pod。 如果所指的 RuntimeClass 不存在或者 CRI 无法运行相应的 handler，那么 pod 将会进入 Failed 终止阶段。 你可以查看相应的事件，获取出错信息。\n如果未指定 runtimeClassName ，则将使用默认的 RuntimeHandler，相当于禁用 RuntimeClass 功能特性。\nCRI 配置 关于如何安装 CRI 运行时，请查阅 CRI 安装。\ndockershim Kubernetes 内置 dockershim CRI 不支持配置运行时 handler。\ncontainerd 通过 containerd 的 /etc/containerd/config.toml 配置文件来配置运行时 handler。 handler 需要配置在 runtimes 块中：\n[plugins.cri.containerd.runtimes.${HANDLER_NAME}] 更详细信息，请查阅 containerd 配置文档： https://github.com/containerd/cri/blob/master/docs/config.md\ncri-o 通过 cri-o 的 /etc/crio/crio.conf 配置文件来配置运行时 handler。 handler 需要配置在 crio.runtime 表 下方：\n[crio.runtime.runtimes.${HANDLER_NAME}] runtime_path = \u0026quot;${PATH_TO_BINARY}\u0026quot; 更详细信息，请查阅 containerd 配置文档： https://github.com/kubernetes-sigs/cri-o/blob/master/cmd/crio/config.go\n调度 for_k8s_version=\u0026quot;v1.16\u0026rdquo; state=\u0026quot;beta\u0026rdquo; \u0026gt;}}\n在 Kubernetes v1.16 版本里，RuntimeClass 特性引入了 scheduling 字段来支持异构集群。 通过该字段，可以确保 pod 被调度到支持指定运行时的节点上。 该调度支持，需要确保 RuntimeClass admission controller 处于开启状态（1.16 版本默认开启）。\n为了确保 pod 会被调度到支持指定运行时的 node 上，每个 node 需要设置一个通用的 label 用于被 runtimeclass.scheduling.nodeSelector 挑选。在 admission 阶段，RuntimeClass 的 nodeSelector 将会于 pod 的 nodeSelector 合并，取二者的交集。如果有冲突，pod 将会被拒绝。\n如果 node 需要阻止某些需要特定 RuntimeClass 的 pod，可以在 tolerations 中指定。 与 nodeSelector 一样，tolerations 也在 admission 阶段与 pod 的 tolerations 合并，取二者的并集。\n更多有关 node selector 和 tolerations 的配置信息，请查阅 Assigning Pods to Nodes。\nPod 开销 for_k8s_version=\u0026quot;v1.18\u0026rdquo; state=\u0026quot;beta\u0026rdquo; \u0026gt;}}\n你可以指定与运行 Pod 相关的 开销 资源。声明开销即允许集群（包括调度器）在决策 Pod 和资源时将其考虑在内。 若要使用 Pod 开销特性，你必须确保 PodOverhead 特性开关 处于开启状态（默认为启用状态）。\nPod 开销通过 RuntimeClass 的 overhead 字段定义。通过使用这些字段，你可以指定使用该 RuntimeClass 运行 Pod 时的开销并确保 Kubernetes 将这些开销计算在内。\n  RuntimeClass 设计 RuntimeClass 调度设计 阅读关于 Pod 开销 的概念 PodOverhead 特性设计  "
},
{
	"uri": "https://lijun.in/setup/production-environment/tools/kubeadm/troubleshooting-kubeadm/",
	"title": "对 kubeadm 进行故障排查",
	"tags": [],
	"description": "",
	"content": "与任何程序一样，您可能会在安装或者运行 kubeadm 时遇到错误。 本文列举了一些常见的故障场景，并提供可帮助您理解和解决这些问题的步骤。\n如果您的问题未在下面列出，请执行以下步骤：\n  如果您认为问题是 kubeadm 的错误：\n 转到 github.com/kubernetes/kubeadm 并搜索存在的问题。 如果没有问题，请 打开 并遵循问题模板。    如果您对 kubeadm 的工作方式有疑问，可以在 Slack 上的 #kubeadm 频道提问， 或者在 StackOverflow 上提问。 请加入相关标签，例如 #kubernetes 和 #kubeadm，这样其他人可以帮助您。\n  在安装过程中没有找到 ebtables 或者其他类似的可执行文件 如果在运行 kubeadm init 命令时，遇到以下的警告\n[preflight] WARNING: ebtables not found in system path [preflight] WARNING: ethtool not found in system path 那么或许在您的节点上缺失 ebtables、ethtool 或者类似的可执行文件。 您可以使用以下命令安装它们：\n 对于 Ubuntu/Debian 用户，运行 apt install ebtables ethtool 命令。 对于 CentOS/Fedora 用户，运行 yum install ebtables ethtool 命令。  在安装过程中，kubeadm 一直等待控制平面就绪 如果您注意到 kubeadm init 在打印以下行后挂起：\n[apiclient] Created API client, waiting for the control plane to become ready 这可能是由许多问题引起的。最常见的是：\n  网络连接问题。在继续之前，请检查您的计算机是否具有全部联通的网络连接。\n  kubelet 的默认 cgroup 驱动程序配置不同于 Docker 使用的配置。 检查系统日志文件 (例如 /var/log/message) 或检查 journalctl -u kubelet 的输出。 如果您看见以下内容：\nerror: failed to run Kubelet: failed to create kubelet: misconfiguration: kubelet cgroup driver: \u0026#34;systemd\u0026#34; is different from docker cgroup driver: \u0026#34;cgroupfs\u0026#34; 有两种常见方法可解决 cgroup 驱动程序问题：\n    按照 此处 的说明再次安装 Docker。\n  更改 kubelet 配置以手动匹配 Docker cgroup 驱动程序，您可以参考 在主节点上配置 kubelet 要使用的 cgroup 驱动程序\n   控制平面上的 Docker 容器持续进入崩溃状态或（因其他原因）挂起。您可以运行 docker ps 命令来检查以及 docker logs 命令来检视每个容器的运行日志。  当删除托管容器时 kubeadm 阻塞 如果 Docker 停止并且不删除 Kubernetes 所管理的所有容器，可能发生以下情况：\nsudo kubeadm reset [preflight] Running pre-flight checks [reset] Stopping the kubelet service [reset] Unmounting mounted directories in \u0026#34;/var/lib/kubelet\u0026#34; [reset] Removing kubernetes-managed containers (block) 一个可行的解决方案是重新启动 Docker 服务，然后重新运行 kubeadm reset：\nsudo systemctl restart docker.service sudo kubeadm reset 检查 docker 的日志也可能有用：\njournalctl -ul docker Pods 处于 RunContainerError、CrashLoopBackOff 或者 Error 状态 在 kubeadm init 命令运行后，系统中不应该有 pods 处于这类状态。\n 在 kubeadm init 命令执行完后，如果有 pods 处于这些状态之一，请在 kubeadm 仓库提起一个 issue。coredns (或者 kube-dns) 应该处于 Pending 状态， 直到您部署了网络解决方案为止。 如果在部署完网络解决方案之后，有 Pods 处于 RunContainerError、CrashLoopBackOff 或 Error 状态之一，并且coredns （或者 kube-dns）仍处于 Pending 状态， 那很可能是您安装的网络解决方案由于某种原因无法工作。您或许需要授予它更多的 RBAC 特权或使用较新的版本。请在 Pod Network 提供商的问题跟踪器中提交问题， 然后在此处分类问题。 如果您安装的 Docker 版本早于 1.12.1，请在使用 systemd 来启动 dockerd 和重启 docker 时， 删除 MountFlags=slave 选项。 您可以在 /usr/lib/systemd/system/docker.service 中看到 MountFlags。 MountFlags 可能会干扰 Kubernetes 挂载的卷， 并使 Pods 处于 CrashLoopBackOff 状态。 当 Kubernetes 不能找到 var/run/secrets/kubernetes.io/serviceaccount 文件时会发生错误。  coredns （或 kube-dns）停滞在 Pending 状态 这一行为是 预期之中 的，因为系统就是这么设计的。 kubeadm 的网络供应商是中立的，因此管理员应该选择 安装 pod 的网络解决方案。 您必须完成 Pod 的网络配置，然后才能完全部署 CoreDNS。 在网络被配置好之前，DNS 组件会一直处于 Pending 状态。\nHostPort 服务无法工作 此 HostPort 和 HostIP 功能是否可用取决于您的 Pod 网络配置。请联系 Pod 解决方案的作者， 以确认 HostPort 和 HostIP 功能是否可用。\n已验证 Calico、Canal 和 Flannel CNI 驱动程序支持 HostPort。\n有关更多信息，请参考 CNI portmap 文档.\n如果您的网络提供商不支持 portmap CNI 插件，您或许需要使用 NodePort 服务的功能 或者使用 HostNetwork=true。\n无法通过其服务 IP 访问 Pod   许多网络附加组件尚未启用 hairpin 模式 该模式允许 Pod 通过其服务 IP 进行访问。这是与 CNI 有关的问题。请与网络附加组件提供商联系，以获取他们所提供的 hairpin 模式的最新状态。\n  如果您正在使用 VirtualBox (直接使用或者通过 Vagrant 使用)，您需要 确保 hostname -i 返回一个可路由的 IP 地址。默认情况下，第一个接口连接不能路由的仅主机网络。 解决方法是修改 /etc/hosts，请参考示例 Vagrantfile。\n  TLS 证书错误 以下错误指出证书可能不匹配。\n# kubectl get pods\rUnable to connect to the server: x509: certificate signed by unknown authority (possibly because of \u0026quot;crypto/rsa: verification error\u0026quot; while trying to verify candidate authority certificate \u0026quot;kubernetes\u0026quot;)\r  验证 $HOME/.kube/config 文件是否包含有效证书，并 在必要时重新生成证书。在 kubeconfig 文件中的证书是 base64 编码的。 该 base64 -d 命令可以用来解码证书，openssl x509 -text -noout 命令 可以用于查看证书信息。\n  使用如下方法取消设置 KUBECONFIG 环境变量的值：\nunset KUBECONFIG 或者将其设置为默认的 KUBECONFIG 位置：\nexport KUBECONFIG=/etc/kubernetes/admin.conf   另一个方法是覆盖 kubeconfig 的现有用户 \u0026ldquo;管理员\u0026rdquo; ：\nmv $HOME/.kube $HOME/.kube.bak mkdir $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config   在 Vagrant 中使用 flannel 作为 pod 网络时的默认 NIC 以下错误可能表明 Pod 网络中出现问题：\nError from server (NotFound): the server could not find the requested resource   如果你正在 Vagrant 中使用 flannel 作为 pod 网络，则必须指定 flannel 的默认接口名称。\nVagrant 通常为所有 VM 分配两个接口。第一个为所有主机分配了 IP 地址 10.0.2.15，用于获得 NATed 的外部流量。\n这可能会导致 flannel 出现问题，它默认为主机上的第一个接口。这导致所有主机认为它们具有 相同的公共 IP 地址。为防止这种情况，传递 --iface eth1 标志给 flannel 以便选择第二个接口。\n  容器使用的非公共 IP 在某些情况下 kubectl logs 和 kubectl run 命令或许会返回以下错误，即便除此之外集群一切功能正常：\nError from server: Get https://10.19.0.41:10250/containerLogs/default/mysql-ddc65b868-glc5m/mysql: dial tcp 10.19.0.41:10250: getsockopt: no route to host   这或许是由于 Kubernetes 使用的 IP 无法与看似相同的子网上的其他 IP 进行通信的缘故， 可能是由机器提供商的政策所导致的。\n  Digital Ocean 既分配一个共有 IP 给 eth0，也分配一个私有 IP 在内部用作其浮动 IP 功能的锚点， 然而 kubelet 将选择后者作为节点的 InternalIP 而不是公共 IP\n使用 ip addr show 命令代替 ifconfig 命令去检查这种情况，因为 ifconfig 命令 不会显示有问题的别名 IP 地址。或者指定的 Digital Ocean 的 API 端口允许从 droplet 中 查询 anchor IP：\ncurl http://169.254.169.254/metadata/v1/interfaces/public/0/anchor_ipv4/address 解决方法是通知 kubelet 使用哪个 --node-ip。当使用 Digital Ocean 时，可以是公网IP（分配给 eth0的）， 或者是私网IP（分配给 eth1 的）。私网 IP 是可选的。 这个 KubeletExtraArgs section of the kubeadm NodeRegistrationOptions structure 被用来处理这种情况。\n然后重启 kubelet：\nsystemctl daemon-reload systemctl restart kubelet   coredns pods 有 CrashLoopBackOff 或者 Error 状态 如果有些节点运行的是旧版本的 Docker，同时启用了 SELinux，您或许会遇到 coredns pods 无法启动的情况。 要解决此问题，您可以尝试以下选项之一：\n  升级到 Docker 的较新版本。\n  禁用 SELinux.\n  修改 coredns 部署以设置 allowPrivilegeEscalation 为 true：\n  kubectl -n kube-system get deployment coredns -o yaml | \\  sed \u0026#39;s/allowPrivilegeEscalation: false/allowPrivilegeEscalation: true/g\u0026#39; | \\  kubectl apply -f - CoreDNS 处于 CrashLoopBackOff 时的另一个原因是当 Kubernetes 中部署的 CoreDNS Pod 检测 到环路时。有许多解决方法 可以避免在每次 CoreDNS 监测到循环并退出时，Kubernetes 尝试重启 CoreDNS Pod 的情况。\n. warning \u0026gt;}}\n警告：禁用 SELinux 或设置 allowPrivilegeEscalation 为 true 可能会损害集群的安全性。 . /warning \u0026gt;}}\netcd pods 持续重启 如果您遇到以下错误：\nrpc error: code = 2 desc = oci runtime error: exec failed: container_linux.go:247: starting container process caused \u0026quot;process_linux.go:110: decoding init error from pipe caused \\\u0026quot;read parent: connection reset by peer\\\u0026quot;\u0026quot;\r如果您使用 Docker 1.13.1.84 运行 CentOS 7 就会出现这种问题。 此版本的 Docker 会阻止 kubelet 在 etcd 容器中执行。\n为解决此问题，请选择以下选项之一：\n 回滚到早期版本的 Docker，例如 1.13.1-75  yum downgrade docker-1.13.1-75.git8633870.el7.centos.x86_64 docker-client-1.13.1-75.git8633870.el7.centos.x86_64 docker-common-1.13.1-75.git8633870.el7.centos.x86_64\r 安装较新的推荐版本之一，例如 18.06:  sudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo yum install docker-ce-18.06.1.ce-3.el7.x86_64 无法将以逗号分隔的值列表传递给 --component-extra-args 标志内的参数 kubeadm init 标志例如 --component-extra-args 允许您将自定义参数传递给像 kube-apiserver 这样的控制平面组件。然而，由于解析 (mapStringString) 的基础类型值，此机制将受到限制。\n如果您决定传递一个支持多个逗号分隔值（例如 --apiserver-extra-args \u0026quot;enable-admission-plugins=LimitRanger,NamespaceExists\u0026quot;）参数，将出现 flag: malformed pair, expect string=string 错误。 发生这种问题是因为参数列表 --apiserver-extra-args 预期的是 key=value 形式，而这里的 NamespacesExists 被误认为是缺少取值的键名。\n一种解决方法是尝试分离 key=value 对，像这样： --apiserver-extra-args \u0026quot;enable-admission-plugins=LimitRanger,enable-admission-plugins=NamespaceExists\u0026quot; 但这将导致键 enable-admission-plugins 仅有值 NamespaceExists。\n已知的解决方法是使用 kubeadm 配置文件。\n在节点被云控制管理器初始化之前，kube-proxy 就被调度了 在云环境场景中，可能出现在云控制管理器完成节点地址初始化之前，kube-proxy 就被调度到新节点了。 这会导致 kube-proxy 无法正确获取节点的 IP 地址，并对管理负载平衡器的代理功能产生连锁反应。\n在 kube-proxy Pod 中可以看到以下错误：\nserver.go:610] Failed to retrieve node IP: host IP unknown; known addresses: []\rproxier.go:340] invalid nodeIP, initializing kube-proxy with 127.0.0.1 as nodeIP\r一种已知的解决方案是修补 kube-proxy DaemonSet，以允许在控制平面节点上调度它，而不管它们的条件如何，将其与其他节点保持隔离，直到它们的初始保护条件消除：\nkubectl -n kube-system patch ds kube-proxy -p='{ \u0026quot;spec\u0026quot;: { \u0026quot;template\u0026quot;: { \u0026quot;spec\u0026quot;: { \u0026quot;tolerations\u0026quot;: [ { \u0026quot;key\u0026quot;: \u0026quot;CriticalAddonsOnly\u0026quot;, \u0026quot;operator\u0026quot;: \u0026quot;Exists\u0026quot; }, { \u0026quot;effect\u0026quot;: \u0026quot;NoSchedule\u0026quot;, \u0026quot;key\u0026quot;: \u0026quot;node-role.kubernetes.io/master\u0026quot; } ] } } } }'\r此问题的跟踪 在这里。\nNodeRegistration.Taints 字段在编组 kubeadm 配置时丢失 注意：这个 问题 仅适用于操控 kubeadm 数据类型的工具（例如，YAML 配置文件）。它将在 kubeadm API v1beta2 修复。\n默认情况下，kubeadm 将 node-role.kubernetes.io/master:NoSchedule 污点应用于控制平面节点。 如果您希望 kubeadm 不污染控制平面节点，并将 InitConfiguration.NodeRegistration.Taints 设置成空切片，则应在编组时省略该字段。 如果省略该字段，则 kubeadm 将应用默认污点。\n至少有两种解决方法：\n  使用 node-role.kubernetes.io/master:PreferNoSchedule 污点代替空切片。 除非其他节点具有容量，否则将在主节点上调度 Pods。\n  在 kubeadm init 退出后删除污点：\n  kubectl taint nodes NODE_NAME node-role.kubernetes.io/master:NoSchedule- "
},
{
	"uri": "https://lijun.in/concepts/overview/working-with-objects/names/",
	"title": "对象名称和IDs",
	"tags": [],
	"description": "",
	"content": "集群中的每一个对象都一个名称 来标识在同类资源中的唯一性。\n每个 Kubernetes 对象也有一个UID 来标识在整个集群中的唯一性。\n比如，在同一个namespace中只能命名一个名为 myapp-1234 的 Pod, 但是可以命名一个 Pod 和一个 Deployment 同为 myapp-1234.\n对于非唯一的用户提供的属性，Kubernetes 提供了标签和注释。\n有关名称和 UID 的精确语法规则，请参见标识符设计文档。\n名称 term_id=\u0026quot;name\u0026rdquo; length=\u0026quot;all\u0026rdquo; \u0026gt;}}\n以下是比较常见的三种资源命名约束。\nDNS 子域名 某些资源类型需要一个 name 来作为一个 DNS 子域名，见定义 RFC 1123。也就是命名必须满足如下规则：\n 不能超过253个字符 只能包含字母数字，以及\u0026rsquo;-\u0026rsquo; 和 \u0026lsquo;.\u0026rsquo; 须以字母数字开头 须以字母数字结尾  DNS 标签名称 某些资源类型需要其名称遵循 DNS 标签的标准，见RFC 1123。也就是命名必须满足如下规则：\n 最多63个字符 只能包含字母数字，以及\u0026rsquo;-\u0026rsquo; 须以字母数字开头 须以字母数字结尾  Path 部分名称 一些用与 Path 部分的资源类型要求名称能被安全的 encode。换句话说，其名称不能含有这些字符 \u0026ldquo;.\u0026quot;、\u0026rdquo;..\u0026quot;、\u0026quot;/\u0026ldquo;或\u0026rdquo;%\u0026quot;。\n下面是一个名为nginx-demo的 Pod 的配置清单：\napiVersion: v1 kind: Pod metadata: name: nginx-demo spec: containers: - name: nginx image: nginx:1.14.2 ports: - containerPort: 80 某些资源类型可能有其相应的附加命名约束。\nUIDs term_id=\u0026quot;uid\u0026rdquo; length=\u0026quot;all\u0026rdquo; \u0026gt;}}\nKubernetes UIDs 是通用的唯一标识符 (也叫 UUIDs).\nUUIDs 是标准化的，见 ISO/IEC 9834-8 和 ITU-T X.667.\n  阅读关于 Kubernetes labels。 更多参见 Kubernetes 标识符和名称设计文档.  "
},
{
	"uri": "https://lijun.in/concepts/extend-kubernetes/api-extension/",
	"title": "扩展 Kubernetes API",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lijun.in/tutorials/stateful-application/mysql-wordpress-persistent-volume/",
	"title": "示例：使用 Persistent Volumes 部署 WordPress 和 MySQL",
	"tags": [],
	"description": "",
	"content": "本示例描述了如何通过 Minikube 在 Kubernetes 上安装 WordPress 和 MySQL。这两个应用都使用 PersistentVolumes 和 PersistentVolumeClaims 保存数据。\nPersistentVolume（PV）是一块集群里由管理员手动提供，或 kubernetes 通过 StorageClass 动态创建的存储。 PersistentVolumeClaim（PVC）是一个满足对 PV 存储需要的请求。PersistentVolumes 和 PersistentVolumeClaims 是独立于 Pod 生命周期而在 Pod 重启，重新调度甚至删除过程中保存数据。\n. warning \u0026gt;}}\ndeployment 在生产场景中并不适合，它使用单实例 WordPress 和 MySQL Pods。考虑使用 WordPress Helm Chart 在生产场景中部署 WordPress。 . /warning \u0026gt;}}\n. note \u0026gt;}}\n本教程中提供的文件使用 GA Deployment API，并且特定于 kubernetes 1.9 或更高版本。如果您希望将本教程与 Kubernetes 的早期版本一起使用，请相应地更新 API 版本，或参考本教程的早期版本。 . /note \u0026gt;}}\n. heading \u0026ldquo;objectives\u0026rdquo; %}}  创建 PersistentVolumeClaims 和 PersistentVolumes 创建 kustomization.yaml 使用  Secret 生成器 MySQL 资源配置 WordPress 资源配置   应用整个 kustomization 目录 kubectl apply -k ./ 清理  . heading \u0026ldquo;prerequisites\u0026rdquo; %}} . include \u0026ldquo;task-tutorial-prereqs.md\u0026rdquo; \u0026gt;}} . version-check \u0026gt;}}\n此例在kubectl 1.14 或者更高版本有效。\n下载下面的配置文件：\n  mysql-deployment.yaml\n  wordpress-deployment.yaml\n  创建 PersistentVolumeClaims 和 PersistentVolumes MySQL 和 Wordpress 都需要一个 PersistentVolume 来存储数据。他们的 PersistentVolumeClaims 将在部署步骤中创建。\n许多群集环境都安装了默认的 StorageClass。如果在 PersistentVolumeClaim 中未指定 StorageClass，则使用群集的默认 StorageClass。\n创建 PersistentVolumeClaim 时，将根据 StorageClass 配置动态设置 PersistentVolume。\n. warning \u0026gt;}}\n在本地群集中，默认的 StorageClass 使用hostPath供应器。 hostPath卷仅适用于开发和测试。使用 hostPath 卷，您的数据位于 Pod 调度到的节点上的/tmp中，并且不会在节点之间移动。如果 Pod 死亡并被调度到群集中的另一个节点，或者该节点重新启动，则数据将丢失。 . /warning \u0026gt;}}\n. note \u0026gt;}}\n如果要建立需要使用hostPath设置程序的集群，则必须在 controller-manager 组件中设置--enable-hostpath-provisioner标志。 . /note \u0026gt;}}\n. note \u0026gt;}}\n如果你已经有运行在 Google Kubernetes Engine 的集群，请参考 this guide。 . /note \u0026gt;}}\n创建 kustomization.yaml 创建 Secret 生成器 A Secret 是存储诸如密码或密钥之类的敏感数据的对象。从 1.14 开始，kubectl支持使用 kustomization 文件管理 Kubernetes 对象。您可以通过kustomization.yaml中的生成器创建一个 Secret。\n通过以下命令在kustomization.yaml中添加一个 Secret 生成器。您需要用您要使用的密码替换YOUR_PASSWORD。\ncat \u0026lt;\u0026lt;EOF \u0026gt;./kustomization.yaml secretGenerator: - name: mysql-pass literals: - password=YOUR_PASSWORD EOF 补充 MySQL 和 WordPress 的资源配置 以下 manifest 文件描述了单实例 MySQL 部署。MySQL 容器将 PersistentVolume 挂载在/var/lib/mysql。 MYSQL_ROOT_PASSWORD环境变量设置来自 Secret 的数据库密码。\n. codenew file=\u0026quot;application/wordpress/mysql-deployment.yaml\u0026rdquo; \u0026gt;}}\n以下 manifest 文件描述了单实例 WordPress 部署。WordPress 容器将网站数据文件位于/var/www/html的 PersistentVolume。WORDPRESS_DB_HOST环境变量集上面定义的 MySQL Service 的名称，WordPress 将通过 Service 访问数据库。WORDPRESS_DB_PASSWORD环境变量设置从 Secret kustomize 生成的数据库密码。 . codenew file=\u0026quot;application/wordpress/wordpress-deployment.yaml\u0026rdquo; \u0026gt;}}\n  下载 MySQL deployment 配置文件。\ncurl -LO https://k8s.io/examples/application/wordpress/mysql-deployment.yaml   下载 WordPress 配置文件。\ncurl -LO https://k8s.io/examples/application/wordpress/wordpress-deployment.yaml   补充到 kustomization.yaml 文件。\ncat \u0026lt;\u0026lt;EOF \u0026gt;\u0026gt;./kustomization.yaml resources: - mysql-deployment.yaml - wordpress-deployment.yaml EOF   应用和验证 kustomization.yaml包含用于部署 WordPress 网站的所有资源以及 MySQL 数据库。您可以通过以下方式应用目录\nkubectl apply -k ./ 现在，您可以验证所有对象是否存在。\n  通过运行以下命令验证 Secret 是否存在：\nkubectl get secrets 响应应如下所示：\nNAME TYPE DATA AGE mysql-pass-c57bb4t7mf Opaque 1 9s   验证是否已动态配置 PersistentVolume：\nkubectl get pvc . note \u0026gt;}} 设置和绑定 PV 可能要花费几分钟。 . /note \u0026gt;}}\n响应应如下所示：\nNAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE mysql-pv-claim Bound pvc-8cbd7b2e-4044-11e9-b2bb-42010a800002 20Gi RWO standard 77s wp-pv-claim Bound pvc-8cd0df54-4044-11e9-b2bb-42010a800002 20Gi RWO standard 77s   通过运行以下命令来验证 Pod 是否正在运行：\nkubectl get pods . note \u0026gt;}} 等待 Pod 状态变成RUNNING可能会花费几分钟。 . /note \u0026gt;}}\n响应应如下所示：\nNAME READY STATUS RESTARTS AGE wordpress-mysql-1894417608-x5dzt 1/1 Running 0 40s   通过运行以下命令来验证 Service 是否正在运行：\nkubectl get services wordpress 响应应如下所示：\nNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE wordpress ClusterIP 10.0.0.89 \u0026lt;pending\u0026gt; 80:32406/TCP 4m . note \u0026gt;}} Minikube 只能通过 NodePort 公开服务。EXTERNAL-IP 始终处于挂起状态 . /note \u0026gt;}}\n  运行以下命令以获取 WordPress 服务的 IP 地址：\nminikube service wordpress --url 响应应如下所示：\nhttp://1.2.3.4:32406   复制 IP 地址，然后将页面加载到浏览器中来查看您的站点。\n您应该看到类似于以下屏幕截图的 WordPress 设置页面。\n  . warning \u0026gt;}}\n不要在此页面上保留 WordPress 安装。如果其他用户找到了它，他们可以在您的实例上建立一个网站并使用它来提供恶意内容。通过创建用户名和密码来安装 WordPress 或删除您的实例。\n. /warning \u0026gt;}}\n. heading \u0026ldquo;cleanup\u0026rdquo; %}}   运行一下命令删除您的 Secret，Deployments，Services and PersistentVolumeClaims：\nkubectl delete -k ./   . heading \u0026ldquo;whatsnext\u0026rdquo; %}}  了解更多关于 Introspection and Debugging 了解更多关于 Jobs 了解更多关于 Port Forwarding 了解如何 Get a Shell to a Container  "
},
{
	"uri": "https://lijun.in/tutorials/stateless-application/guestbook/",
	"title": "示例：使用 Redis 部署 PHP 留言板应用程序",
	"tags": [],
	"description": "",
	"content": "本教程向您展示如何使用 Kubernetes 和 Docker 构建和部署 一个简单的多层 web 应用程序。本例由以下组件组成：\n 单实例 Redis 主节点保存留言板条目 多个从 Redis 节点用来读取数据 多个 web 前端实例  . heading \u0026ldquo;objectives\u0026rdquo; %}}  启动 Redis 主节点。 启动 Redis 从节点。 启动留言板前端。 公开并查看前端服务。 清理。  . heading \u0026ldquo;prerequisites\u0026rdquo; %}} . include \u0026ldquo;task-tutorial-prereqs.md\u0026rdquo; \u0026gt;}}\n. version-check \u0026gt;}}\n启动 Redis 主节点 留言板应用程序使用 Redis 存储数据。它将数据写入一个 Redis 主实例，并从多个 Redis 读取数据。\n创建 Redis 主节点的 Deployment 下面包含的清单文件指定了一个 Deployment 控制器，该控制器运行一个 Redis 主节点 Pod 副本。\n. codenew file=\u0026quot;application/guestbook/redis-master-deployment.yaml\u0026rdquo; \u0026gt;}}\n  在下载清单文件的目录中启动终端窗口。\n  从 redis-master-deployment.yaml 文件中应用 Redis 主 Deployment：\nkubectl apply -f https://k8s.io/examples/application/guestbook/redis-master-deployment.yaml    查询 Pod 列表以验证 Redis 主节点 Pod 是否正在运行：\nkubectl get pods    响应应该与此类似： ```shell NAME READY STATUS RESTARTS AGE redis-master-1068406935-3lswp 1/1 Running 0 28s ```   运行以下命令查看 Redis 主节点 Pod 中的日志：\nkubectl logs -f POD-NAME   . note \u0026gt;}}\n将 POD-NAME 替换为您的 Pod 名称。\n. /note \u0026gt;}}\n创建 Redis 主节点的服务 留言板应用程序需要往 Redis 主节点中写数据。因此，需要创建 Service 来代理 Redis 主节点 Pod 的流量。Service 定义了访问 Pod 的策略。\n. codenew file=\u0026quot;application/guestbook/redis-master-service.yaml\u0026rdquo; \u0026gt;}}\n  使用下面的 redis-master-service.yaml 文件创建 Redis 主节点的服务：\nkubectl apply -f https://k8s.io/examples/application/guestbook/redis-master-service.yaml    查询服务列表验证 Redis 主节点服务是否正在运行：\nkubectl get service    响应应该与此类似： ```shell NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.0.0.1 \u0026lt;none\u0026gt; 443/TCP 1m redis-master ClusterIP 10.0.0.151 \u0026lt;none\u0026gt; 6379/TCP 8s ```  . note \u0026gt;}}\n这个清单文件创建了一个名为 Redis-master 的 Service，其中包含一组与前面定义的标签匹配的标签，因此服务将网络流量路由到 Redis 主节点 Pod 上。\n. /note \u0026gt;}}\n启动 Redis 从节点 尽管 Redis 主节点是一个单独的 pod，但是您可以通过添加 Redis 从节点的方式来使其高可用性，以满足流量需求。\n创建 Redis 从节点 Deployment Deployments 根据清单文件中设置的配置进行伸缩。在这种情况下，Deployment 对象指定两个副本。\n如果没有任何副本正在运行，则此 Deployment 将启动容器集群上的两个副本。相反， 如果有两个以上的副本在运行，那么它的规模就会缩小，直到运行两个副本为止。\n. codenew file=\u0026quot;application/guestbook/redis-slave-deployment.yaml\u0026rdquo; \u0026gt;}}\n  从 redis-slave-deployment.yaml 文件中应用 Redis Slave Deployment：\nkubectl apply -f https://k8s.io/examples/application/guestbook/redis-slave-deployment.yaml    查询 Pod 列表以验证 Redis Slave Pod 正在运行：\nkubectl get pods    响应应该与此类似： ```shell NAME READY STATUS RESTARTS AGE redis-master-1068406935-3lswp 1/1 Running 0 1m redis-slave-2005841000-fpvqc 0/1 ContainerCreating 0 6s redis-slave-2005841000-phfv9 0/1 ContainerCreating 0 6s ```  创建 Redis 从节点的 Service 留言板应用程序需要从 Redis 从节点中读取数据。 为了便于 Redis 从节点可发现， 您需要设置一个 Service。Service 为一组 Pod 提供负载均衡。\n. codenew file=\u0026quot;application/guestbook/redis-slave-service.yaml\u0026rdquo; \u0026gt;}}\n  从以下 redis-slave-service.yaml 文件应用 Redis Slave 服务：\nkubectl apply -f https://k8s.io/examples/application/guestbook/redis-slave-service.yaml    查询服务列表以验证 Redis 在服务是否正在运行：\nkubectl get services    响应应该与此类似： ``` NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.0.0.1 \u0026lt;none\u0026gt; 443/TCP 2m redis-master ClusterIP 10.0.0.151 \u0026lt;none\u0026gt; 6379/TCP 1m redis-slave ClusterIP 10.0.0.223 \u0026lt;none\u0026gt; 6379/TCP 6s ```  设置并公开留言板前端 留言板应用程序有一个 web 前端，服务于用 PHP 编写的 HTTP 请求。 它被配置为连接到写请求的 redis-master 服务和读请求的 redis-slave 服务。\n创建留言板前端 Deployment . codenew file=\u0026quot;application/guestbook/frontend-deployment.yaml\u0026rdquo; \u0026gt;}}\n  从 frontend-deployment.yaml 应用前端 Deployment 文件：\nkubectl apply -f https://k8s.io/examples/application/guestbook/frontend-deployment.yaml    查询 Pod 列表，验证三个前端副本是否正在运行：\nkubectl get pods -l app=guestbook -l tier=frontend    响应应该与此类似： ``` NAME READY STATUS RESTARTS AGE frontend-3823415956-dsvc5 1/1 Running 0 54s frontend-3823415956-k22zn 1/1 Running 0 54s frontend-3823415956-w9gbt 1/1 Running 0 54s ```  创建前端服务 应用的 redis-slave 和 redis-master 服务只能在容器集群中访问，因为服务的默认类型是 ClusterIP。ClusterIP 为服务指向的 Pod 集提供一个 IP 地址。这个 IP 地址只能在集群中访问。\n如果您希望客人能够访问您的留言板，您必须将前端服务配置为外部可见的，以便客户机可以从容器集群之外请求服务。Minikube 只能通过 NodePort 公开服务。\n. note \u0026gt;}}\n一些云提供商，如 Google Compute Engine 或 Google Kubernetes Engine，支持外部负载均衡器。如果您的云提供商支持负载均衡器，并且您希望使用它， 只需删除或注释掉 type: NodePort，并取消注释 type: LoadBalancer 即可。\n. /note \u0026gt;}}\n. codenew file=\u0026quot;application/guestbook/frontend-service.yaml\u0026rdquo; \u0026gt;}}\n  从 frontend-service.yaml 文件中应用前端服务：\nkubectl apply -f https://k8s.io/examples/application/guestbook/frontend-service.yaml    查询服务列表以验证前端服务正在运行:\nkubectl get services    响应应该与此类似： ``` NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE frontend ClusterIP 10.0.0.112 \u0026lt;none\u0026gt; 80:31323/TCP 6s kubernetes ClusterIP 10.0.0.1 \u0026lt;none\u0026gt; 443/TCP 4m redis-master ClusterIP 10.0.0.151 \u0026lt;none\u0026gt; 6379/TCP 2m redis-slave ClusterIP 10.0.0.223 \u0026lt;none\u0026gt; 6379/TCP 1m ```  通过 NodePort 查看前端服务 如果您将此应用程序部署到 Minikube 或本地集群，您需要找到 IP 地址来查看您的留言板。\n  运行以下命令获取前端服务的 IP 地址。\nminikube service frontend --url    响应应该与此类似： ``` http://192.168.99.100:31323 ```  复制 IP 地址，然后在浏览器中加载页面以查看留言板。  通过 LoadBalancer 查看前端服务 如果您部署了 frontend-service.yaml。你需要找到 IP 地址来查看你的留言板。\n  运行以下命令以获取前端服务的 IP 地址。\nkubectl get service frontend    响应应该与此类似： ``` NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE frontend ClusterIP 10.51.242.136 109.197.92.229 80:32372/TCP 1m ```  复制外部 IP 地址，然后在浏览器中加载页面以查看留言板。  扩展 Web 前端 伸缩很容易是因为服务器本身被定义为使用一个 Deployment 控制器的 Service。\n  运行以下命令扩展前端 Pod 的数量：\nkubectl scale deployment frontend --replicas=5    查询 Pod 列表验证正在运行的前端 Pod 的数量：\nkubectl get pods    响应应该类似于这样： ``` NAME READY STATUS RESTARTS AGE frontend-3823415956-70qj5 1/1 Running 0 5s frontend-3823415956-dsvc5 1/1 Running 0 54m frontend-3823415956-k22zn 1/1 Running 0 54m frontend-3823415956-w9gbt 1/1 Running 0 54m frontend-3823415956-x2pld 1/1 Running 0 5s redis-master-1068406935-3lswp 1/1 Running 0 56m redis-slave-2005841000-fpvqc 1/1 Running 0 55m redis-slave-2005841000-phfv9 1/1 Running 0 55m ```   运行以下命令缩小前端 Pod 的数量：\nkubectl scale deployment frontend --replicas=2    查询 Pod 列表验证正在运行的前端 Pod 的数量：\nkubectl get pods    响应应该类似于这样： ``` NAME READY STATUS RESTARTS AGE frontend-3823415956-k22zn 1/1 Running 0 1h frontend-3823415956-w9gbt 1/1 Running 0 1h redis-master-1068406935-3lswp 1/1 Running 0 1h redis-slave-2005841000-fpvqc 1/1 Running 0 1h redis-slave-2005841000-phfv9 1/1 Running 0 1h ```  . heading \u0026ldquo;cleanup\u0026rdquo; %}} 删除 Deployments 和服务还会删除正在运行的 Pod。使用标签用一个命令删除多个资源。\n 运行以下命令以删除所有 Pod，Deployments 和 Services。\nkubectl delete deployment -l app=redis kubectl delete service -l app=redis kubectl delete deployment -l app=guestbook kubectl delete service -l app=guestbook    响应应该是： ``` deployment.apps \u0026quot;redis-master\u0026quot; deleted deployment.apps \u0026quot;redis-slave\u0026quot; deleted service \u0026quot;redis-master\u0026quot; deleted service \u0026quot;redis-slave\u0026quot; deleted deployment.apps \u0026quot;frontend\u0026quot; deleted service \u0026quot;frontend\u0026quot; deleted ```   查询 Pod 列表，确认没有 Pod 在运行：\nkubectl get pods    响应应该是： ``` No resources found. ```  . heading \u0026ldquo;whatsnext\u0026rdquo; %}}  完成 Kubernetes Basics 交互式教程 使用 Kubernetes 创建一个博客，使用 MySQL 和 Wordpress 的持久卷 阅读更多关于连接应用程序 阅读更多关于管理资源  "
},
{
	"uri": "https://lijun.in/tasks/administer-cluster/manage-resources/",
	"title": "管理内存，CPU 和 API 资源",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lijun.in/concepts/extend-kubernetes/compute-storage-net/device-plugins/",
	"title": "设备插件",
	"tags": [],
	"description": "使用 Kubernetes 设备插件框架来实现适用于 GPU、NIC、FPGA、InfiniBand 以及类似的需要特定于供应商设置的资源的插件。",
	"content": "for_k8s_version=\u0026quot;v1.10\u0026rdquo; state=\u0026quot;beta\u0026rdquo; \u0026gt;}}\nKubernetes 提供了一个设备插件框架，您可以用来将系统硬件资源发布到 term_id=\u0026quot;kubelet\u0026rdquo; \u0026gt;}}。\n供应商可以实现设备插件，由您手动部署或作为 term_id=\u0026quot;daemonset\u0026rdquo; \u0026gt;}} 来部署，而不必定制 Kubernetes 本身的代码。目标设备包括 GPU、高性能 NIC、FPGA、InfiniBand 适配器以及其他类似的、可能需要特定于供应商的初始化和设置的计算资源。\n注册设备插件 kubelet 输出了一个 Registration 的 gRPC 服务：\nservice Registration { rpc Register(RegisterRequest) returns (Empty) {} } 设备插件可以通过此 gRPC 服务在 kubelet 进行注册。在注册期间，设备插件需要发送下面几样内容：\n 设备插件的 Unix 套接字。 设备插件的 API 版本。 ResourceName 是需要公布的。这里 ResourceName 需要遵循扩展资源命名方案，类似于 vendor-domain/resourcetype。（比如 NVIDIA GPU 就被公布为 nvidia.com/gpu。）  成功注册后，设备插件就向 kubelet 发送他所管理的设备列表，然后 kubelet 负责将这些资源发布到 API 服务器，作为 kubelet 节点状态更新的一部分。\n比如，设备插件在 kubelet 中注册了 hardware-vendor.example/foo 并报告了节点上的两个运行状况良好的设备后，节点状态将更新以通告该节点已安装2个 Foo 设备并且是可用的。\n然后用户需要去请求其他类型的资源的时候，就可以在[Container](/docs/reference/generated/kubernetes-api/\u0026lt; param \u0026ldquo;version\u0026rdquo; \u0026gt;}}/#container-v1-core)规范请求这类设备，但是有以下的限制：\n 扩展资源仅可作为整数资源使用，并且不能被过量使用 设备不能在容器之间共享  假设 Kubernetes 集群正在运行一个设备插件，该插件在一些节点上公布的资源为 hardware-vendor.example/foo。 下面就是一个 Pod 示例，请求此资源以运行某演示负载：\n--- apiVersion: v1 kind: Pod metadata: name: demo-pod spec: containers: - name: demo-container-1 image: k8s.gcr.io/pause:2.0 resources: limits: hardware-vendor.example/foo: 2 # # 这个 pod 需要两个 hardware-vendor.example/foo 设备 # 而且只能够调度到满足需求的 node 上 # # 如果该节点中有2个以上的设备可用，其余的可供其他 pod 使用 设备插件的实现 设备插件的常规工作流程包括以下几个步骤：\n 初始化。在这个阶段，设备插件将执行供应商特定的初始化和设置，以确保设备处于就绪状态。 插件使用主机路径 /var/lib/kubelet/device-plugins/ 下的 Unix socket 启动一个 gRPC 服务，该服务实现以下接口：  service DevicePlugin { // ListAndWatch returns a stream of List of Devices // Whenever a Device state change or a Device disappears, ListAndWatch // returns the new list rpc ListAndWatch(Empty) returns (stream ListAndWatchResponse) {} // Allocate is called during container creation so that the Device // Plugin can run device specific operations and instruct Kubelet // of the steps to make the Device available in the container rpc Allocate(AllocateRequest) returns (AllocateResponse) {} }  插件通过 Unix socket 在主机路径 /var/lib/kubelet/device-plugins/kubelet.sock 处向 kubelet 注册自身。 成功注册自身后，设备插件将以服务模式运行，在此期间，它将持续监控设备运行状况，并在设备状态发生任何变化时向 kubelet 报告。它还负责响应 Allocate gRPC 请求。在Allocate期间，设备插件可能还会做一些设备特定的准备；例如 GPU 清理或 QRNG 初始化。如果操作成功，则设备插件将返回 AllocateResponse，其中包含用于访问被分配的设备容器运行时的配置。kubelet 将此信息传递到容器运行时。  处理 kubelet 重启 设备插件应能监测到 kubelet 重启，并且向新的 kubelet 实例来重新注册自己。在当前实现中，当 kubelet 重启的时候，新的 kubelet 实例会删除 /var/lib/kubelet/device-plugins 下所有已经存在的 Unix sockets。设备插件需要能够监控到它的 Unix socket 被删除，并且当发生此类事件时重新注册自己。\n设备插件部署 你可以将你的设备插件作为节点操作系统的软件包来部署、作为 DaemonSet 来部署或者手动部署。\n规范目录 /var/lib/kubelet/device-plugins 是需要特权访问的，所以设备插件必须要在被授权的安全的上下文中运行。如果你将设备插件部署为 DaemonSet，/var/lib/kubelet/device-plugins 目录必须要在插件的 [PodSpec](/docs/reference/generated/kubernetes-api/\u0026lt; param \u0026ldquo;version\u0026rdquo; \u0026gt;}}/#podspec-v1-core) 中声明作为 term_id=\u0026quot;volume\u0026rdquo; \u0026gt;}} 被 mount 到插件中。\n如果你选择 DaemonSet 方法，你可以通过 Kubernetes 进行以下操作：将设备插件的 Pod 放置在节点上，在出现故障后重新启动 daemon Pod，来进行自动进行升级。\nAPI 兼容性 Kubernetes 设备插件支持还处于 beta 版本。所以在稳定版本出来之前 API 会以不兼容的方式进行更改。作为一个项目，Kubernetes 建议设备插件开发者：\n 注意未来版本的更改 支持多个版本的设备插件 API，以实现向后/向前兼容性。  如果你启用 DevicePlugins 功能，并在需要升级到 Kubernetes 版本来获得较新的设备插件 API 版本的节点上运行设备插件，请在升级这些节点之前先升级设备插件以支持这两个版本。采用该方法将确保升级期间设备分配的连续运行。\n监控设备插件资源 for_k8s_version=\u0026quot;v1.15\u0026rdquo; state=\u0026quot;beta\u0026rdquo; \u0026gt;}}\n为了监控设备插件提供的资源，监控代理程序需要能够发现节点上正在使用的设备，并获取元数据来描述哪个指标与容器相关联。设备监控代理暴露给 Prometheus 的指标应该遵循 Kubernetes Instrumentation Guidelines，使用 pod、namespace 和 container 标签来标识容器。\nkubelet 提供了 gRPC 服务来使得正在使用中的设备被发现，并且还未这些设备提供了元数据：\n// PodResourcesLister is a service provided by the kubelet that provides information about the // node resources consumed by pods and containers on the node service PodResourcesLister { rpc List(ListPodResourcesRequest) returns (ListPodResourcesResponse) {} } gRPC 服务通过 /var/lib/kubelet/pod-resources/kubelet.sock 的 UNIX 套接字来提供服务。设备插件资源的监控代理程序可以部署为守护进程或者 DaemonSet。规范的路径 /var/lib/kubelet/pod-resources 需要特权来进入，所以监控代理程序必须要在获得授权的安全的上下文中运行。如果设备监控代理以 DaemonSet 形式运行，必须要在插件的 [PodSpec](/docs/reference/generated/kubernetes-api/\u0026lt; param \u0026ldquo;version\u0026rdquo; \u0026gt;}}/#podspec-v1-core) 中声明将 /var/lib/kubelet/pod-resources 目录以 term_id=\u0026quot;volume\u0026rdquo; \u0026gt;}} 形式被 mount 到容器中。\n对“PodResources 服务”的支持要求启用 KubeletPodResources 特性门控。从 Kubernetes 1.15 开始默认启用。\n设备插件与拓扑管理器的集成 for_k8s_version=\u0026quot;v1.17\u0026rdquo; state=\u0026quot;alpha\u0026rdquo; \u0026gt;}}\n拓扑管理器是 Kubelet 的一个组件，它允许以拓扑对齐方式来调度资源。为了做到这一点，设备插件 API 进行了扩展来包括一个 TopologyInfo 结构体。\nmessage TopologyInfo { repeated NUMANode nodes = 1; } message NUMANode { int64 ID = 1; } 设备插件希望拓扑管理器可以将填充的 TopologyInfo 结构体作为设备注册的一部分以及设备 ID 和设备的运行状况发送回去。然后设备管理器将使用此信息来咨询拓扑管理器并做出资源分配决策。\nTopologyInfo 支持定义 nodes 字段，允许为 nil（默认）或者是一个 NUMA nodes 的列表。这样就可以使设备插件可以跨越 NUMA nodes 去发布。\n下面是一个由设备插件为设备填充 TopologyInfo 结构体的示例：\npluginapi.Device{ID: \u0026quot;25102017\u0026quot;, Health: pluginapi.Healthy, Topology:\u0026amp;pluginapi.TopologyInfo{Nodes: []*pluginapi.NUMANode{\u0026amp;pluginapi.NUMANode{ID: 0,},}}} 设备插件示例 下面是一些设备插件实现的示例：\n AMD GPU device plugin Intel device plugins 支持 Intel GPU、FPGA 和 QuickAssist 设备 KubeVirt device plugins 用于硬件辅助的虚拟化 The NVIDIA GPU device plugin  需要 nvidia-docker 2.0，允许运行 Docker 容器的时候开启 GPU。   NVIDIA GPU device plugin for Container-Optimized OS RDMA device plugin Solarflare device plugin SR-IOV Network device plugin Xilinx FPGA device plugins    查看 调度 GPU 资源 来学习使用设备插件 查看在 node 上如何广告扩展资源 阅读如何在 Kubernetes 中如何使用 TLS 入口的硬件加速 学习 [Topology Manager] (/docs/tasks/adminster-cluster/topology-manager/)  "
},
{
	"uri": "https://lijun.in/tasks/access-application-cluster/access-cluster/",
	"title": "访问集群",
	"tags": [],
	"description": "",
	"content": "本文阐述多种与集群交互的方法。\n. toc \u0026gt;}}\n使用 kubectl 完成集群的第一次访问 当您第一次访问 Kubernetes API 的时候，我们建议您使用 Kubernetes CLI，kubectl。\n访问集群时，您需要知道集群的地址并且拥有访问的凭证。通常，这些在您通过 Getting started guide 安装集群时都是自动安装好的，或者其他人安装时也应该提供了凭证和集群地址。\n通过以下命令检查 kubectl 是否知道集群地址及凭证：\n$ kubectl config view 有许多 例子 介绍了如何使用 kubectl，可以在 kubectl手册 中找到更完整的文档。\n直接访问 REST API Kubectl 处理 apiserver 的定位和身份验证。 如果要使用 curl 或 wget 等 http 客户端或浏览器直接访问 REST API，可以通过多种方式查找和验证：\n 以代理模式运行 kubectl。  推荐此方式。 使用已存储的 apiserver 地址。 使用自签名的证书来验证 apiserver 的身份。杜绝 MITM 攻击。 对 apiserver 进行身份验证。 未来可能会实现智能化的客户端负载均衡和故障恢复。   直接向 http 客户端提供位置和凭据。  可选的方案。 适用于代理可能引起混淆的某些客户端类型。 需要引入根证书到您的浏览器以防止 MITM 攻击。    使用 kubectl 代理 以下命令以反向代理的模式运行kubectl。它处理 apiserver 的定位和验证。 像这样运行：\n$ kubectl proxy --port=8080 \u0026amp; 参阅 kubectl proxy 获取更多详细信息。\n然后，您可以使用 curl、wget 或浏览器访问 API，如果是 IPv6 则用 [::1] 替换 localhost，如下所示：\n$ curl http://localhost:8080/api/ { \u0026#34;kind\u0026#34;: \u0026#34;APIVersions\u0026#34;, \u0026#34;versions\u0026#34;: [ \u0026#34;v1\u0026#34; ], \u0026#34;serverAddressByClientCIDRs\u0026#34;: [ { \u0026#34;clientCIDR\u0026#34;: \u0026#34;0.0.0.0/0\u0026#34;, \u0026#34;serverAddress\u0026#34;: \u0026#34;10.0.1.149:443\u0026#34; } ] } 不使用 kubectl 代理 在 Kubernetes 1.3 或更高版本中，kubectl config view 不再显示 token。使用 kubectl describe secret ... 来获取默认服务帐户的 token，如下所示：\ngrep/cut 方法实现：\n$ APISERVER=$(kubectl config view | grep server | cut -f 2- -d \u0026#34;:\u0026#34; | tr -d \u0026#34; \u0026#34;) $ TOKEN=$(kubectl describe secret $(kubectl get secrets | grep default | cut -f1 -d \u0026#39; \u0026#39;) | grep -E \u0026#39;^token\u0026#39; | cut -f2 -d\u0026#39;:\u0026#39; | tr -d \u0026#39;\\t\u0026#39;) $ curl $APISERVER/api --header \u0026#34;Authorization: Bearer $TOKEN\u0026#34; --insecure { \u0026#34;kind\u0026#34;: \u0026#34;APIVersions\u0026#34;, \u0026#34;versions\u0026#34;: [ \u0026#34;v1\u0026#34; ], \u0026#34;serverAddressByClientCIDRs\u0026#34;: [ { \u0026#34;clientCIDR\u0026#34;: \u0026#34;0.0.0.0/0\u0026#34;, \u0026#34;serverAddress\u0026#34;: \u0026#34;10.0.1.149:443\u0026#34; } ] } jsonpath 方法实现：\n$ APISERVER=$(kubectl config view --minify -o jsonpath=\u0026#39;{.clusters[0].cluster.server}\u0026#39;) $ TOKEN=$(kubectl get secret $(kubectl get serviceaccount default -o jsonpath=\u0026#39;{.secrets[0].name}\u0026#39;) -o jsonpath=\u0026#39;{.data.token}\u0026#39; | base64 --decode ) $ curl $APISERVER/api --header \u0026#34;Authorization: Bearer $TOKEN\u0026#34; --insecure { \u0026#34;kind\u0026#34;: \u0026#34;APIVersions\u0026#34;, \u0026#34;versions\u0026#34;: [ \u0026#34;v1\u0026#34; ], \u0026#34;serverAddressByClientCIDRs\u0026#34;: [ { \u0026#34;clientCIDR\u0026#34;: \u0026#34;0.0.0.0/0\u0026#34;, \u0026#34;serverAddress\u0026#34;: \u0026#34;10.0.1.149:443\u0026#34; } ] } 上面的例子使用了 --insecure 参数，这使得它很容易受到 MITM 攻击。当 kubectl 访问集群时，它使用存储的根证书和客户端证书来访问服务器（这些安装在 ~/.kube 目录中）。由于集群证书通常是自签名的，因此可能需要特殊配置才能让您的 http 客户端使用根证书。\n在一些集群中，apiserver 不需要身份验证；它可能只服务于 localhost，或者被防火墙保护，这个没有一定的标准。 配置对 API 的访问 描述了集群管理员如何进行配置。此类方法可能与未来的高可用性支持相冲突。\n以编程方式访问 API Kubernetes 官方提供对 Go 和 Python 的客户端库支持。\nGo 客户端  想要获得这个库，请运行命令：go get k8s.io/client-go/\u0026lt;version number\u0026gt;/kubernetes。参阅 https://github.com/kubernetes/client-go 来查看目前支持哪些版本。 基于这个 client-go 客户端库编写应用程序。 请注意，client-go 定义了自己的 API 对象，因此如果需要，请从 client-go 而不是从主存储库导入 API 定义，例如，import \u0026quot;k8s.io/client-go/1.4/pkg/api/v1\u0026quot; 才是对的。  Go 客户端可以像 kubectl CLI 一样使用相同的 kubeconfig 文件 来定位和验证 apiserver。可参阅 示例。\n如果应用程序以 Pod 的形式部署在集群中，那么请参阅 下一章。\nPython 客户端 如果想要使用 Python 客户端，请运行命令：pip install kubernetes。参阅 Python Client Library page 以获得更详细的安装参数。\nPython 客户端可以像 kubectl CLI 一样使用相同的 kubeconfig 文件 来定位和验证 apiserver，可参阅 示例。\n其它语言 目前有多个 客户端库 为其它语言提供访问 API 的方法。 参阅其它库的相关文档以获取他们是如何验证的。\n从 Pod 中访问 API 当你从 Pod 中访问 API 时，定位和验证 apiserver 会有些许不同。\n在 Pod 中定位 apiserver 的推荐方式是通过 kubernetes.default.svc 这个 DNS 名称，该名称将会解析为服务 IP，然后服务 IP 将会路由到 apiserver。\n向 apiserver 进行身份验证的推荐方法是使用 服务帐户 凭据。 通过 kube-system，pod 与服务帐户相关联，并且该服务帐户的凭证（token）被放置在该 pod 中每个容器的文件系统中，位于 /var/run/secrets/kubernetes.io/serviceaccount/token。\n如果可用，则将证书放入每个容器的文件系统中的 /var/run/secrets/kubernetes.io/serviceaccount/ca.crt，并且应该用于验证 apiserver 的服务证书。\n最后，命名空间化的 API 操作所使用的默认命名空间将被放置在每个容器的 /var/run/secrets/kubernetes.io/serviceaccount/namespace 文件中。\n在 pod 中，建议连接 API 的方法是：\n 在 pod 的 sidecar 容器中运行 kubectl proxy，或者以后台进程的形式运行。 这将把 Kubernetes API 代理到当前 pod 的 localhost interface，所以 pod 中的所有容器中的进程都能访问它。 使用 Go 客户端库，并使用 rest.InClusterConfig() 和 kubernetes.NewForConfig() 函数创建一个客户端。 他们处理 apiserver 的定位和身份验证。示例  在每种情况下，pod 的凭证都是为了与 apiserver 安全地通信。\n访问集群中正在运行的服务 上一节介绍了如何连接 Kubernetes API 服务。本节介绍如何连接到 Kubernetes 集群上运行的其他服务。 在 Kubernetes 中，节点，pods 和 服务 都有自己的 IP。 在许多情况下，集群上的节点 IP，pod IP 和某些服务 IP 将无法路由，因此无法从集群外部的计算机（例如桌面计算机）访问它们。\n连接的方法 有多种方式可以从集群外部连接节点、pod 和服务：\n 通过公共 IP 访问服务。  类型为 NodePort 或 LoadBalancer 的服务，集群外部可以访问。 请参阅 服务 和 kubectl expose 文档。 取决于您的集群环境，该服务可能仅暴露给您的公司网络，或者也可能暴露给整个互联网。 请考虑公开该服务是否安全。它是否进行自己的身份验证？ 在服务后端放置 pod。要从一组副本中访问一个特定的 pod，例如进行调试，请在 pod 上放置一个唯一的标签，然后创建一个选择此标签的新服务。 在大多数情况下，应用程序开发人员不应该通过其 nodeIP 直接访问节点。   使用 Proxy Verb 访问服务、node 或者 pod。  在访问远程服务之前进行 apiserver 身份验证和授权。 如果服务不能够安全地暴露到互联网，或者服务不能获得节点 IP 端口的访问权限，或者是为了 debug，那么请使用此选项。 代理可能会给一些 web 应用带来问题。 只适用于 HTTP/HTTPS。 更多详细信息在 [这里]。   从集群中的 node 或者 pod 中访问。  运行一个 pod，然后使用 kubectl exec 来连接 pod 里的 shell。 然后从 shell 中连接其它的节点、pod 和服务。 有些集群可能允许您通过 ssh 连接到 node，从那您可能可以访问集群的服务。 这是一个非正式的方式，可能可以运行在个别的集群上。 浏览器和其它一些工具可能没有被安装。集群的 DNS 可能无法使用。    发现内建服务 通常来说，集群中会有 kube-system 创建的一些运行的服务。\n通过 kubectl cluster-info 命令获得这些服务列表：\n$ kubectl cluster-info Kubernetes master is running at https://104.197.5.247 elasticsearch-logging is running at https://104.197.5.247/api/v1/namespaces/kube-system/services/elasticsearch-logging/proxy kibana-logging is running at https://104.197.5.247/api/v1/namespaces/kube-system/services/kibana-logging/proxy kube-dns is running at https://104.197.5.247/api/v1/namespaces/kube-system/services/kube-dns/proxy grafana is running at https://104.197.5.247/api/v1/namespaces/kube-system/services/monitoring-grafana/proxy heapster is running at https://104.197.5.247/api/v1/namespaces/kube-system/services/monitoring-heapster/proxy 这展示了访问每个服务的 proxy-verb URL。 例如，如果集群启动了集群级别的日志（使用 Elasticsearch），并且传递合适的凭证，那么可以通过 https://104.197.5.247/api/v1/namespaces/kube-system/services/elasticsearch-logging/proxy/ 进行访问。日志也能通过 kubectl 代理获取，例如： http://localhost:8080/api/v1/namespaces/kube-system/services/elasticsearch-logging/proxy/。 （参阅 上面的内容 来获取如何使用 kubectl 代理来传递凭证）\n手动构建 apiserver 代理 URL 如上所述，您可以使用 kubectl cluster-info 命令来获得服务的代理 URL。要创建包含服务端点、后缀和参数的代理 URL，只需添加到服务的代理 URL： http://kubernetes_master_address/api/v1/namespaces/namespace_name/services/service_name[:port_name]/proxy\n如果尚未为端口指定名称，则不必在 URL 中指定 port_name。\n默认情况下，API server 使用 http 代理您的服务。要使用 https，请在服务名称前加上 https:： http://kubernetes_master_address/api/v1/namespaces/namespace_name/services/https:service_name:[port_name]/proxy\nURL 名称段支持的格式为：\n \u0026lt;service_name\u0026gt; - 使用 http 代理到默认或未命名的端口 \u0026lt;service_name\u0026gt;:\u0026lt;port_name\u0026gt; - 使用 http 代理到指定的端口 https:\u0026lt;service_name\u0026gt;: - 使用 https 代理到默认或未命名的端口（注意后面的冒号） https:\u0026lt;service_name\u0026gt;:\u0026lt;port_name\u0026gt; - 使用 https 代理到指定的端口  示例  要访问 Elasticsearch 服务端点 _search?q=user:kimchy，您需要使用：http://104.197.5.247/api/v1/namespaces/kube-system/services/elasticsearch-logging/proxy/_search?q=user:kimchy 要访问 Elasticsearch 集群健康信息 _cluster/health?pretty=true，您需要使用：https://104.197.5.247/api/v1/namespaces/kube-system/services/elasticsearch-logging/proxy/_cluster/health?pretty=true  { \u0026#34;cluster_name\u0026#34; : \u0026#34;kubernetes_logging\u0026#34;, \u0026#34;status\u0026#34; : \u0026#34;yellow\u0026#34;, \u0026#34;timed_out\u0026#34; : false, \u0026#34;number_of_nodes\u0026#34; : 1, \u0026#34;number_of_data_nodes\u0026#34; : 1, \u0026#34;active_primary_shards\u0026#34; : 5, \u0026#34;active_shards\u0026#34; : 5, \u0026#34;relocating_shards\u0026#34; : 0, \u0026#34;initializing_shards\u0026#34; : 0, \u0026#34;unassigned_shards\u0026#34; : 5 } 使用 web 浏览器访问运行在集群上的服务 您可以在浏览器地址栏中输入 apiserver 代理 URL。但是：\n Web 浏览器通常不能传递 token，因此您可能需要使用基本（密码）身份验证。Apiserver 可以配置为接受基本身份验证，但您的集群可能未进行配置。 某些 Web 应用程序可能无法运行，尤其是那些使用客户端 javascript 以不知道代理路径前缀的方式构建 URL 的应用程序。  请求重定向 已弃用并删除了重定向功能。请改用代理（见下文）。\n多种代理 使用 Kubernetes 时可能会遇到几种不同的代理：\n  kubectl 代理：\n 在用户的桌面或 pod 中运行 代理从本地主机地址到 Kubernetes apiserver 客户端到代理将使用 HTTP 代理到 apiserver 使用 HTTPS 定位 apiserver 添加身份验证 header      apiserver 代理：\n 内置于 apiserver 中 将集群外部的用户连接到集群 IP，否则这些 IP 可能无法访问 运行在 apiserver 进程中 客户端代理使用 HTTPS（也可配置为 http） 代理将根据可用的信息决定使用 HTTP 或者 HTTPS 代理到目标 可用于访问节点、Pod 或服务 在访问服务时进行负载平衡      kube proxy：\n 运行在每个节点上 代理 UDP 和 TCP 不能代理 HTTP 提供负载均衡 只能用来访问服务      位于 apiserver 之前的 Proxy/Load-balancer：\n 存在和实现因集群而异（例如 nginx） 位于所有客户和一个或多个 apiserver 之间 如果有多个 apiserver，则充当负载均衡器      外部服务上的云负载均衡器：\n 由一些云提供商提供（例如 AWS ELB，Google Cloud Load Balancer） 当 Kubernetes 服务类型为 LoadBalancer 时自动创建 只使用 UDP/TCP 具体实现因云提供商而异。    除了前两种类型之外，Kubernetes 用户通常不需要担心任何其他问题。集群管理员通常会确保后者的正确配置。\n"
},
{
	"uri": "https://lijun.in/concepts/cluster-administration/certificates/",
	"title": "证书",
	"tags": [],
	"description": "",
	"content": "当使用客户端证书进行认证时，用户可以使用现有部署脚本，或者通过 easyrsa、openssl 或 cfssl 手动生成证书。\neasyrsa 使用 easyrsa 能够手动地为集群生成证书。\n  下载、解压并初始化 easyrsa3 的补丁版本。\ncurl -LO https://storage.googleapis.com/kubernetes-release/easy-rsa/easy-rsa.tar.gz tar xzf easy-rsa.tar.gz cd easy-rsa-master/easyrsa3 ./easyrsa init-pki    生成 CA（通过 --batch 参数设置自动模式。 通过 --req-cn 设置默认使用的 CN）\n./easyrsa --batch \u0026quot;--req-cn=${MASTER_IP}@`date +%s`\u0026quot; build-ca nopass    生成服务器证书和密钥。 参数 --subject-alt-name 设置了访问 API 服务器时可能使用的 IP 和 DNS 名称。 MASTER_CLUSTER_IP 通常为 --service-cluster-ip-range 参数中指定的服务 CIDR 的 首个 IP 地址，--service-cluster-ip-range 同时用于 API 服务器和控制器管理器组件。 --days 参数用于设置证书的有效期限。 下面的示例还假设用户使用 cluster.local 作为默认的 DNS 域名。\n./easyrsa --subject-alt-name=\u0026quot;IP:${MASTER_IP},\u0026quot;\\ \u0026quot;IP:${MASTER_CLUSTER_IP},\u0026quot;\\ \u0026quot;DNS:kubernetes,\u0026quot;\\ \u0026quot;DNS:kubernetes.default,\u0026quot;\\ \u0026quot;DNS:kubernetes.default.svc,\u0026quot;\\ \u0026quot;DNS:kubernetes.default.svc.cluster,\u0026quot;\\ \u0026quot;DNS:kubernetes.default.svc.cluster.local\u0026quot; \\ --days=10000 \\ build-server-full server nopass    拷贝 pki/ca.crt、 pki/issued/server.crt 和 pki/private/server.key 至您的目录。\n  填充并在 API 服务器的启动参数中添加以下参数：\n--client-ca-file=/yourdirectory/ca.crt --tls-cert-file=/yourdirectory/server.crt --tls-private-key-file=/yourdirectory/server.key    openssl 使用 openssl 能够手动地为集群生成证书。\n  生成密钥位数为 2048 的 ca.key：\nopenssl genrsa -out ca.key 2048    依据 ca.key 生成 ca.crt （使用 -days 参数来设置证书有效时间）：\nopenssl req -x509 -new -nodes -key ca.key -subj \u0026quot;/CN=${MASTER_IP}\u0026quot; -days 10000 -out ca.crt    生成密钥位数为 2048 的 server.key：\nopenssl genrsa -out server.key 2048    创建用于生成证书签名请求（CSR）的配置文件。 确保在将其保存至文件（如 csr.conf）之前将尖括号标记的值（如 \u0026lt;MASTER_IP\u0026gt;） 替换为你想使用的真实值。 注意：MASTER_CLUSTER_IP 是前面小节中描述的 API 服务器的服务集群 IP (service cluster IP)。 下面的示例也假设用户使用 cluster.local 作为默认的 DNS 域名。\n[ req ] default_bits = 2048 prompt = no default_md = sha256 req_extensions = req_ext distinguished_name = dn [ dn ] C = \u0026lt;country\u0026gt; ST = \u0026lt;state\u0026gt; L = \u0026lt;city\u0026gt; O = \u0026lt;organization\u0026gt; OU = \u0026lt;organization unit\u0026gt; CN = \u0026lt;MASTER_IP\u0026gt; [ req_ext ] subjectAltName = @alt_names [ alt_names ] DNS.1 = kubernetes DNS.2 = kubernetes.default DNS.3 = kubernetes.default.svc DNS.4 = kubernetes.default.svc.cluster DNS.5 = kubernetes.default.svc.cluster.local IP.1 = \u0026lt;MASTER_IP\u0026gt; IP.2 = \u0026lt;MASTER_CLUSTER_IP\u0026gt; [ v3_ext ] authorityKeyIdentifier=keyid,issuer:always basicConstraints=CA:FALSE keyUsage=keyEncipherment,dataEncipherment extendedKeyUsage=serverAuth,clientAuth subjectAltName=@alt_names    基于配置文件生成证书签名请求：\nopenssl req -new -key server.key -out server.csr -config csr.conf    使用 ca.key、ca.crt 和 server.csr 生成服务器证书：\nopenssl x509 -req -in server.csr -CA ca.crt -CAkey ca.key \\ -CAcreateserial -out server.crt -days 10000 \\ -extensions v3_ext -extfile csr.conf    查看证书：\nopenssl x509 -noout -text -in ./server.crt    最后，添加同样的参数到 API 服务器的启动参数中。\ncfssl cfssl 是另一种用来生成证书的工具。\n  按如下所示的方式下载、解压并准备命令行工具。 注意：你可能需要基于硬件架构和你所使用的 cfssl 版本对示例命令进行修改。\ncurl -L https://pkg.cfssl.org/R1.2/cfssl_linux-amd64 -o cfssl chmod +x cfssl curl -L https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64 -o cfssljson chmod +x cfssljson curl -L https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64 -o cfssl-certinfo chmod +x cfssl-certinfo    创建目录来存放物料，并初始化 cfssl：\nmkdir cert cd cert ../cfssl print-defaults config \u0026gt; config.json ../cfssl print-defaults csr \u0026gt; csr.json    创建用来生成 CA 文件的 JSON 配置文件，例如 ca-config.json：\n{ \u0026quot;signing\u0026quot;: { \u0026quot;default\u0026quot;: { \u0026quot;expiry\u0026quot;: \u0026quot;8760h\u0026quot; }, \u0026quot;profiles\u0026quot;: { \u0026quot;kubernetes\u0026quot;: { \u0026quot;usages\u0026quot;: [ \u0026quot;signing\u0026quot;, \u0026quot;key encipherment\u0026quot;, \u0026quot;server auth\u0026quot;, \u0026quot;client auth\u0026quot; ], \u0026quot;expiry\u0026quot;: \u0026quot;8760h\u0026quot; } } } }    创建用来生成 CA 证书签名请求（CSR）的 JSON 配置文件，例如 ca-csr.json。 确保将尖括号标记的值替换为你想使用的真实值。\n{ \u0026quot;CN\u0026quot;: \u0026quot;kubernetes\u0026quot;, \u0026quot;key\u0026quot;: { \u0026quot;algo\u0026quot;: \u0026quot;rsa\u0026quot;, \u0026quot;size\u0026quot;: 2048 }, \u0026quot;names\u0026quot;:[{ \u0026quot;C\u0026quot;: \u0026quot;\u0026lt;country\u0026gt;\u0026quot;, \u0026quot;ST\u0026quot;: \u0026quot;\u0026lt;state\u0026gt;\u0026quot;, \u0026quot;L\u0026quot;: \u0026quot;\u0026lt;city\u0026gt;\u0026quot;, \u0026quot;O\u0026quot;: \u0026quot;\u0026lt;organization\u0026gt;\u0026quot;, \u0026quot;OU\u0026quot;: \u0026quot;\u0026lt;organization unit\u0026gt;\u0026quot; }] }    生成 CA 密钥（ca-key.pem）和证书（ca.pem）：\n../cfssl gencert -initca ca-csr.json | ../cfssljson -bare ca    按如下所示的方式创建用来为 API 服务器生成密钥和证书的 JSON 配置文件。 确保将尖括号标记的值替换为你想使用的真实值。 MASTER_CLUSTER_IP 是前面小节中描述的 API 服务器的服务集群 IP。 下面的示例也假设用户使用 cluster.local 作为默认的 DNS 域名。\n{ \u0026quot;CN\u0026quot;: \u0026quot;kubernetes\u0026quot;, \u0026quot;hosts\u0026quot;: [ \u0026quot;127.0.0.1\u0026quot;, \u0026quot;\u0026lt;MASTER_IP\u0026gt;\u0026quot;, \u0026quot;\u0026lt;MASTER_CLUSTER_IP\u0026gt;\u0026quot;, \u0026quot;kubernetes\u0026quot;, \u0026quot;kubernetes.default\u0026quot;, \u0026quot;kubernetes.default.svc\u0026quot;, \u0026quot;kubernetes.default.svc.cluster\u0026quot;, \u0026quot;kubernetes.default.svc.cluster.local\u0026quot; ], \u0026quot;key\u0026quot;: { \u0026quot;algo\u0026quot;: \u0026quot;rsa\u0026quot;, \u0026quot;size\u0026quot;: 2048 }, \u0026quot;names\u0026quot;: [{ \u0026quot;C\u0026quot;: \u0026quot;\u0026lt;country\u0026gt;\u0026quot;, \u0026quot;ST\u0026quot;: \u0026quot;\u0026lt;state\u0026gt;\u0026quot;, \u0026quot;L\u0026quot;: \u0026quot;\u0026lt;city\u0026gt;\u0026quot;, \u0026quot;O\u0026quot;: \u0026quot;\u0026lt;organization\u0026gt;\u0026quot;, \u0026quot;OU\u0026quot;: \u0026quot;\u0026lt;organization unit\u0026gt;\u0026quot; }] }    为 API 服务器生成密钥和证书，生成的秘钥和证书分别默认保存在文件 server-key.pem 和 server.pem 中：\n../cfssl gencert -ca=ca.pem -ca-key=ca-key.pem \\ --config=ca-config.json -profile=kubernetes \\ server-csr.json | ../cfssljson -bare server    分发自签名 CA 证书 客户端节点可能拒绝承认自签名 CA 证书有效。 对于非生产环境的部署，或运行在企业防火墙后的部署，用户可以向所有客户端分发自签名 CA 证书， 并刷新本地的有效证书列表。\n在每个客户端上执行以下操作：\nsudo cp ca.crt /usr/local/share/ca-certificates/kubernetes.crt sudo update-ca-certificates Updating certificates in /etc/ssl/certs... 1 added, 0 removed; done. Running hooks in /etc/ca-certificates/update.d.... done. 证书 API 您可以按照这里记录的方式， 使用 certificates.k8s.io API 来准备 x509 证书，用于认证。\n"
},
{
	"uri": "https://lijun.in/tutorials/kubernetes-basics/deploy-app/",
	"title": "部署应用",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lijun.in/setup/",
	"title": "💖 - 入门",
	"tags": [],
	"description": "",
	"content": "本节介绍了设置和运行 Kubernetes 环境的不同选项。\n不同的 Kubernetes 解决方案满足不同的要求：易于维护、安全性、可控制性、可用资源以及操作和管理 Kubernetes 集群所需的专业知识。\n可以在本地机器、云、本地数据中心上部署 Kubernetes 集群，或选择一个托管的 Kubernetes 集群。还可以跨各种云提供商或裸机环境创建自定义解决方案。\n更简单地说，可以在学习和生产环境中创建一个 Kubernetes 集群。\n学习环境 如果正打算学习 Kubernetes，请使用基于 Docker 的解决方案：Docker 是 Kubernetes 社区支持或生态系统中用来在本地计算机上设置 Kubernetes 集群的一种工具。\ntable caption=\u0026quot;本地机器解决方案表，其中列出了社区和生态系统支持的用于部署 Kubernetes 的工具。\u0026rdquo; \u0026gt;}}\n   社区 生态系统     Minikube Docker Desktop   kind (Kubernetes IN Docker) Minishift    MicroK8s    生产环境 在评估生产环境的解决方案时，请考虑要管理自己 Kubernetes 集群（抽象层面）的哪些方面或将其转移给提供商。\nKubernetes 合作伙伴 包括一个 已认证的 Kubernetes 提供商列表。\n"
},
{
	"uri": "https://lijun.in/setup/learning-environment/",
	"title": "💖 - 学习环境",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lijun.in/concepts/overview/",
	"title": "😊 - 概念",
	"tags": [],
	"description": "",
	"content": "Kubernetes概念与术语 组件   etcd\n  Master\n kube-apiserver kube-controller-manager kube-scheduler    Node\n kubelet kube-proxy docker    附加组件\n  DNS\n kube-dns coredns    Ingress Controller\n traefik NGINX Ingress Controller 官方维护 Nginx Plus HAProxy Ingress AppsCode Voyager contour Cloudflare Warp Ingress F5 Big IP Controller Gloo gRPC Load balancing Kong nghttpx Ingress Controller Ambassador Skipper    Heapster\n  Dashboard\n  Kubernator\n  Federation\n  eventrouter\n    资源对象   Pod\n Init Container Pod Security Policy Pod Lifecycle Pod Hook Pod Preset Disruption Resource Quota liveness和readiness    集群配置\n Node Namespace Label Annotation Taint 和 Toleration 亲和性（Affinity）和反亲和性（anti-affinity） Garbage Collection    控制器 * Deployment * DaemonSet * StatefulSet * ReplicaSet * Job * CronJob * Horizontal Pod Autoscaling * cron-hpa-controller * Escalator\n  服务发现\n Service  服务暴露  ClusterIP NodePort LoadBalance     Ingress    身份与权限控制\n Service Account Network Policy  kubernetes-network-policy-recipes   Security Context RBAC    存储配置\n Secret ConfigMap Volume Persistent Volume Local Volume Storage Class Stork    API 扩展\n CustomResourceDefinition Operator  awesome-operators Prometheus  ops-kube-alerting-rules-operator   Confluent Operator Kong API Kubernetes Operators K8s Operator Workshop Cert Operator Cert manager Operator Kit Container Linux Update Operator DB Operator etcd Elasticsearch Memcached MongoDB MySQL Operator PostgreSQL Another PostgreSQL Kafka Envoy Operator rbac-manager Akrobateo   Aggregated API Server custom-metrics-apiserver-ingress-nginx metacontroller  部署配置   单机部署\n minikube kubeasz Sealos    集群部署\n Sealos Breeze kubeadm Kubespray LinuxKit kubeasz    部署 Windows 节点\n  Kubernetes on Azure\n  插件扩展\n CNI  Flannel Weave Net Contiv Calico OVN SR-IOV Romana OpenContrail Canal kuryr-kubernetes Cilium CNI-Genie Kube-router Nuage Multus-cni Virtlet   CSI  Ceph   CRI  Docker HyperContainer Runc  cri-containerd cri-o   gVisor Rkt Mirantis Infranetes   Scheduler 扩展  Sticky Node Scheduler ksched kube-node-index-prioritizing-scheduler   Device 插件 keepalived-vip External DNS kubevirt    服务治理\n Helm  Helmfile   Service mesh  Istio  Envoy  文档  LearnEnvoy Envoy 官方文档中文版 Envoy 官方文档   大牛   工具  Fortio outlier-istio     Linkerd Conduit   持续集成  Jenkins Drone Apollo   CI/CD  Skaffold Jenkins X Spinnaker Kubernetes Pipeliner Draft Forge Flux GitKube KubeCI Keel Brigade   Kompose 灰度发布  Shipper      "
},
{
	"uri": "https://lijun.in/reference/access-authn-authz/",
	"title": "😍 - 访问 API",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lijun.in/tutorials/online-training/",
	"title": "😎 - 在线培训课程",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lijun.in/tasks/administer-cluster/",
	"title": "😝 - 管理集群",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lijun.in/tasks/configure-pod-container/",
	"title": "😝 - 配置 Pods 和容器",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lijun.in/reference/kubectl/jsonpath/",
	"title": "JSONPath 支持",
	"tags": [],
	"description": "",
	"content": "Kubectl 支持 JSONPath 模板。\nJSONPath 模板由 {} 包起来的 JSONPath 表达式组成。Kubectl 使用 JSONPath 表达式来过滤 JSON 对象中的特定字段并格式化输出。除了原始的 JSONPath 模板语法，以下函数和语法也是有效的:\n 使用双引号将 JSONPath 表达式内的文本引起来。 使用 range，end 运算符来迭代列表。 使用负片索引后退列表。负索引不会\u0026quot;环绕\u0026quot;列表，并且只要 -index + listLength\u0026gt; = 0 就有效。  note \u0026gt;}}\n  $ 运算符是可选的，因为默认情况下表达式总是从根对象开始。\n  结果对象将作为其 String() 函数输出。\n  /note \u0026gt;}}\n给定 JSON 输入:\n{ \u0026#34;kind\u0026#34;: \u0026#34;List\u0026#34;, \u0026#34;items\u0026#34;:[ { \u0026#34;kind\u0026#34;:\u0026#34;None\u0026#34;, \u0026#34;metadata\u0026#34;:{\u0026#34;name\u0026#34;:\u0026#34;127.0.0.1\u0026#34;}, \u0026#34;status\u0026#34;:{ \u0026#34;capacity\u0026#34;:{\u0026#34;cpu\u0026#34;:\u0026#34;4\u0026#34;}, \u0026#34;addresses\u0026#34;:[{\u0026#34;type\u0026#34;: \u0026#34;LegacyHostIP\u0026#34;, \u0026#34;address\u0026#34;:\u0026#34;127.0.0.1\u0026#34;}] } }, { \u0026#34;kind\u0026#34;:\u0026#34;None\u0026#34;, \u0026#34;metadata\u0026#34;:{\u0026#34;name\u0026#34;:\u0026#34;127.0.0.2\u0026#34;}, \u0026#34;status\u0026#34;:{ \u0026#34;capacity\u0026#34;:{\u0026#34;cpu\u0026#34;:\u0026#34;8\u0026#34;}, \u0026#34;addresses\u0026#34;:[ {\u0026#34;type\u0026#34;: \u0026#34;LegacyHostIP\u0026#34;, \u0026#34;address\u0026#34;:\u0026#34;127.0.0.2\u0026#34;}, {\u0026#34;type\u0026#34;: \u0026#34;another\u0026#34;, \u0026#34;address\u0026#34;:\u0026#34;127.0.0.3\u0026#34;} ] } } ], \u0026#34;users\u0026#34;:[ { \u0026#34;name\u0026#34;: \u0026#34;myself\u0026#34;, \u0026#34;user\u0026#34;: {} }, { \u0026#34;name\u0026#34;: \u0026#34;e2e\u0026#34;, \u0026#34;user\u0026#34;: {\u0026#34;username\u0026#34;: \u0026#34;admin\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;secret\u0026#34;} } ] }    函数 描述 示例 结果     text 纯文本 kind is {.kind} kind is List   @ 当前对象 {@} 与输入相同   . or [] 子运算符 {.kind} or {['kind']} List   .. 递归下降 {..name} 127.0.0.1 127.0.0.2 myself e2e   * 通配符。获取所有对象 {.items[*].metadata.name} [127.0.0.1 127.0.0.2]   [start:end :step] 下标运算符 {.users[0].name} myself   [,] 并集运算符 {.items[*]['metadata.name', 'status.capacity']} 127.0.0.1 127.0.0.2 map[cpu:4] map[cpu:8]   ?() 过滤 {.users[?(@.name==\u0026quot;e2e\u0026quot;)].user.password} secret   range, end 迭代列表 {range .items[*]}[{.metadata.name}, {.status.capacity}] {end} [127.0.0.1, map[cpu:4]] [127.0.0.2, map[cpu:8]]   '' 引用解释执行字符串 {range .items[*]}{.metadata.name}{'\\t'}{end} 127.0.0.1 127.0.0.2    使用 kubectl 和 JSONPath 表达式的示例:\nkubectl get pods -o json kubectl get pods -o=jsonpath=\u0026#39;{@}\u0026#39; kubectl get pods -o=jsonpath=\u0026#39;{.items[0]}\u0026#39; kubectl get pods -o=jsonpath=\u0026#39;{.items[0].metadata.name}\u0026#39; kubectl get pods -o=jsonpath=\u0026#39;{range .items[*]}{.metadata.name}{\u0026#34;\\t\u0026#34;}{.status.startTime}{\u0026#34;\\n\u0026#34;}{end}\u0026#39; 在 Windows 上，您必须 double 引用任何包含空格的 JSONPath 模板(不是上面 bash 所示的单引号)。反过来，这意味着您必须在模板中的所有文字周围使用单引号或转义的双引号。例如:\nC:\\\u0026gt; kubectl get pods -o=jsonpath=\u0026#34;{range .items[*]}{.metadata.name}{\u0026#39;\\t\u0026#39;}{.status.startTime}{\u0026#39;\\n\u0026#39;}{end}\u0026#34; C:\\\u0026gt; kubectl get pods -o=jsonpath=\u0026#34;{range .items[*]}{.metadata.name}{\\\u0026#34;\\t\\\u0026#34;}{.status.startTime}{\\\u0026#34;\\n\\\u0026#34;}{end}\u0026#34; "
},
{
	"uri": "https://lijun.in/tasks/manage-kubernetes-objects/",
	"title": "😝 - 管理 Kubernetes 对象",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lijun.in/reference/command-line-tools-reference/kube-scheduler/",
	"title": "kube-scheduler",
	"tags": [],
	"description": "",
	"content": ". heading \u0026ldquo;synopsis\u0026rdquo; %}} Kubernetes 调度器是一个策略丰富、拓扑感知、工作负载特定的功能，调度器显著影响可用性、性能和容量。调度器需要考虑个人和集体的资源要求、服务质量要求、硬件/软件/政策约束、亲和力和反亲和力规范、数据局部性、负载间干扰、完成期限等。工作负载特定的要求必要时将通过 API 暴露。\nkube-scheduler [flags] . heading \u0026ldquo;options\u0026rdquo; %}} \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--add-dir-header\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt; \u0026lt;!-- If true, adds the file directory to the header --\u0026gt; 如果为 true，则将文件目录添加到标题中 \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt; \u0026lt;!-- --address string\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;Default: \u0026quot;0.0.0.0\u0026quot; --\u0026gt; --address string\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;默认: \u0026quot;0.0.0.0\u0026quot; \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt; \u0026lt;!-- DEPRECATED: the IP address on which to listen for the --port port (set to 0.0.0.0 for all IPv4 interfaces and :: for all IPv6 interfaces). See --bind-address instead. --\u0026gt; 弃用: 要监听 --port 端口的 IP 地址（对于所有 IPv4 接口设置为 0.0.0.0，对于所有 IPv6 接口设置为 ::）。 请参阅 --bind-address。 \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--algorithm-provider string\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt; \u0026lt;!-- DEPRECATED: the scheduling algorithm provider to use, one of: ClusterAutoscalerProvider | DefaultProvider --\u0026gt; 弃用: 要使用的调度算法，可选值：ClusterAutoscalerProvider | DefaultProvider \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--alsologtostderr\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt; \u0026lt;!-- 日志记录到标准错误以及文件 --\u0026gt; \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--authentication-kubeconfig string\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt; \u0026lt;!-- 指向具有足够权限以创建 tokenaccessreviews.authentication.k8s.io 的 'core' kubernetes 服务器的 kubeconfig 文件。这是可选的。如果为空，则所有令牌请求均被视为匿名请求，并且不会在集群中查找任何客户端 CA。 --\u0026gt; \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--authentication-skip-lookup\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt; \u0026lt;!-- If false, the authentication-kubeconfig will be used to lookup missing authentication configuration from the cluster. --\u0026gt; 如果为 false，则 authentication-kubeconfig 将用于从集群中查找缺少的身份验证配置。 \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt; \u0026lt;!-- --authentication-token-webhook-cache-ttl duration\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;Default: 10s --\u0026gt; --authentication-token-webhook-cache-ttl duration\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;默认: 10s \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt; \u0026lt;!-- The duration to cache responses from the webhook token authenticator. --\u0026gt; 缓存来自 Webhook 令牌身份验证器的响应的持续时间。 \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt; \u0026lt;!-- --authentication-tolerate-lookup-failure\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;Default: true --\u0026gt; --authentication-tolerate-lookup-failure\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;默认: true \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt; \u0026lt;!-- If true, failures to look up missing authentication configuration from the cluster are not considered fatal. Note that this can result in authentication that treats all requests as anonymous. --\u0026gt; 如果为 true，则无法从集群中查找缺少的身份验证配置是致命的。请注意，这可能导致身份验证将所有请求视为匿名。 \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt; \u0026lt;!-- --authorization-always-allow-paths stringSlice\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;Default: [/healthz] --\u0026gt; --authorization-always-allow-paths stringSlice\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;默认: [/healthz] \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt; \u0026lt;!-- A list of HTTP paths to skip during authorization, i.e. these are authorized without contacting the 'core' kubernetes server. --\u0026gt; 在授权过程中跳过的 HTTP 路径列表，即在不联系 'core' kubernetes 服务器的情况下被授权的 HTTP 路径。 \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--authorization-kubeconfig string\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt; \u0026lt;!-- kubeconfig file pointing at the 'core' kubernetes server with enough rights to create subjectaccessreviews.authorization.k8s.io. This is optional. If empty, all requests not skipped by authorization are forbidden. --\u0026gt; 指向具有足够权限以创建 subjectaccessreviews.authorization.k8s.io 的 'core' kubernetes 服务器的 kubeconfig 文件。这是可选的。如果为空，则禁止所有未经授权跳过的请求。 \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt; \u0026lt;!-- --authorization-webhook-cache-authorized-ttl duration\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;Default: 10s --\u0026gt; --authorization-webhook-cache-authorized-ttl duration\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;默认: 10s \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt; \u0026lt;!-- The duration to cache 'authorized' responses from the webhook authorizer. --\u0026gt; 缓存来自 Webhook 授权者的 'authorized' 响应的持续时间。 \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt; \u0026lt;!-- --authorization-webhook-cache-unauthorized-ttl duration\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;Default: 10s --\u0026gt; --authorization-webhook-cache-unauthorized-ttl duration\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;默认: 10s \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt; \u0026lt;!-- The duration to cache 'unauthorized' responses from the webhook authorizer. --\u0026gt; 缓存来自 Webhook 授权者的 'unauthorized' 响应的持续时间。 \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--azure-container-registry-config string\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt; \u0026lt;!-- Path to the file containing Azure container registry configuration information. --\u0026gt; 包含 Azure 容器仓库配置信息的文件的路径。 \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt; \u0026lt;!-- --bind-address ip\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;Default: 0.0.0.0 --\u0026gt; --bind-address ip\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;默认: 0.0.0.0 \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt; \u0026lt;!-- The IP address on which to listen for the --secure-port port. The associated interface(s) must be reachable by the rest of the cluster, and by CLI/web clients. If blank, all interfaces will be used (0.0.0.0 for all IPv4 interfaces and :: for all IPv6 interfaces). --\u0026gt; 侦听 --secure-port 端口的 IP 地址。集群的其余部分以及 CLI/ Web 客户端必须可以访问关联的接口。如果为空，将使用所有接口（所有 IPv4 接口使用 0.0.0.0，所有 IPv6 接口使用 ::）。 \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--cert-dir string\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt; \u0026lt;!-- The directory where the TLS certs are located. If --tls-cert-file and --tls-private-key-file are provided, this flag will be ignored. --\u0026gt; TLS 证书所在的目录。如果提供了--tls-cert-file 和 --tls private-key-file，则将忽略此参数。 \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--client-ca-file string\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt; \u0026lt;!-- If set, any request presenting a client certificate signed by one of the authorities in the client-ca-file is authenticated with an identity corresponding to the CommonName of the client certificate. --\u0026gt; 如果已设置，由 client-ca-file 中的授权机构签名的客户端证书的任何请求都将使用与客户端证书的 CommonName 对应的身份进行身份验证。 \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--config string\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt; \u0026lt;!-- The path to the configuration file. Flags override values in this file. --\u0026gt; 配置文件的路径。标志会覆盖此文件中的值。 \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--contention-profiling\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt; \u0026lt;!-- DEPRECATED: enable lock contention profiling, if profiling is enabled --\u0026gt; 弃用: 如果启用了性能分析，则启用锁竞争分析 \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--feature-gates mapStringBool\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt; \u0026lt;!-- A set of key=value pairs that describe feature gates for alpha/experimental features. Options are:\u0026lt;br/\u0026gt;APIListChunking=true|false (BETA - default=true)\u0026lt;br/\u0026gt;APIResponseCompression=true|false (BETA - default=true)\u0026lt;br/\u0026gt;AllAlpha=true|false (ALPHA - default=false)\u0026lt;br/\u0026gt;AppArmor=true|false (BETA - default=true)\u0026lt;br/\u0026gt;AttachVolumeLimit=true|false (BETA - default=true)\u0026lt;br/\u0026gt;BalanceAttachedNodeVolumes=true|false (ALPHA - default=false)\u0026lt;br/\u0026gt;BlockVolume=true|false (BETA - default=true)\u0026lt;br/\u0026gt;BoundServiceAccountTokenVolume=true|false (ALPHA - default=false)\u0026lt;br/\u0026gt;CPUManager=true|false (BETA - default=true)\u0026lt;br/\u0026gt;CRIContainerLogRotation=true|false (BETA - default=true)\u0026lt;br/\u0026gt;CSIBlockVolume=true|false (BETA - default=true)\u0026lt;br/\u0026gt;CSIDriverRegistry=true|false (BETA - default=true)\u0026lt;br/\u0026gt;CSIInlineVolume=true|false (BETA - default=true)\u0026lt;br/\u0026gt;CSIMigration=true|false (ALPHA - default=false)\u0026lt;br/\u0026gt;CSIMigrationAWS=true|false (ALPHA - default=false)\u0026lt;br/\u0026gt;CSIMigrationAzureDisk=true|false (ALPHA - default=false)\u0026lt;br/\u0026gt;CSIMigrationAzureFile=true|false (ALPHA - default=false)\u0026lt;br/\u0026gt;CSIMigrationGCE=true|false (ALPHA - default=false)\u0026lt;br/\u0026gt;CSIMigrationOpenStack=true|false (ALPHA - default=false)\u0026lt;br/\u0026gt;CSINodeInfo=true|false (BETA - default=true)\u0026lt;br/\u0026gt;CustomCPUCFSQuotaPeriod=true|false (ALPHA - default=false)\u0026lt;br/\u0026gt;CustomResourceDefaulting=true|false (BETA - default=true)\u0026lt;br/\u0026gt;DevicePlugins=true|false (BETA - default=true)\u0026lt;br/\u0026gt;DryRun=true|false (BETA - default=true)\u0026lt;br/\u0026gt;DynamicAuditing=true|false (ALPHA - default=false)\u0026lt;br/\u0026gt;DynamicKubeletConfig=true|false (BETA - default=true)\u0026lt;br/\u0026gt;EndpointSlice=true|false (ALPHA - default=false)\u0026lt;br/\u0026gt;EphemeralContainers=true|false (ALPHA - default=false)\u0026lt;br/\u0026gt;EvenPodsSpread=true|false (ALPHA - default=false)\u0026lt;br/\u0026gt;ExpandCSIVolumes=true|false (BETA - default=true)\u0026lt;br/\u0026gt;ExpandInUsePersistentVolumes=true|false (BETA - default=true)\u0026lt;br/\u0026gt;ExpandPersistentVolumes=true|false (BETA - default=true)\u0026lt;br/\u0026gt;ExperimentalHostUserNamespaceDefaulting=true|false (BETA - default=false)\u0026lt;br/\u0026gt;HPAScaleToZero=true|false (ALPHA - default=false)\u0026lt;br/\u0026gt;HyperVContainer=true|false (ALPHA - default=false)\u0026lt;br/\u0026gt;IPv6DualStack=true|false (ALPHA - default=false)\u0026lt;br/\u0026gt;KubeletPodResources=true|false (BETA - default=true)\u0026lt;br/\u0026gt;LegacyNodeRoleBehavior=true|false (ALPHA - default=true)\u0026lt;br/\u0026gt;LocalStorageCapacityIsolation=true|false (BETA - default=true)\u0026lt;br/\u0026gt;LocalStorageCapacityIsolationFSQuotaMonitoring=true|false (ALPHA - default=false)\u0026lt;br/\u0026gt;MountContainers=true|false (ALPHA - default=false)\u0026lt;br/\u0026gt;NodeDisruptionExclusion=true|false (ALPHA - default=false)\u0026lt;br/\u0026gt;NodeLease=true|false (BETA - default=true)\u0026lt;br/\u0026gt;NonPreemptingPriority=true|false (ALPHA - default=false)\u0026lt;br/\u0026gt;PodOverhead=true|false (ALPHA - default=false)\u0026lt;br/\u0026gt;PodShareProcessNamespace=true|false (BETA - default=true)\u0026lt;br/\u0026gt;ProcMountType=true|false (ALPHA - default=false)\u0026lt;br/\u0026gt;QOSReserved=true|false (ALPHA - default=false)\u0026lt;br/\u0026gt;RemainingItemCount=true|false (BETA - default=true)\u0026lt;br/\u0026gt;RemoveSelfLink=true|false (ALPHA - default=false)\u0026lt;br/\u0026gt;RequestManagement=true|false (ALPHA - default=false)\u0026lt;br/\u0026gt;ResourceLimitsPriorityFunction=true|false (ALPHA - default=false)\u0026lt;br/\u0026gt;ResourceQuotaScopeSelectors=true|false (BETA - default=true)\u0026lt;br/\u0026gt;RotateKubeletClientCertificate=true|false (BETA - default=true)\u0026lt;br/\u0026gt;RotateKubeletServerCertificate=true|false (BETA - default=true)\u0026lt;br/\u0026gt;RunAsGroup=true|false (BETA - default=true)\u0026lt;br/\u0026gt;RuntimeClass=true|false (BETA - default=true)\u0026lt;br/\u0026gt;SCTPSupport=true|false (ALPHA - default=false)\u0026lt;br/\u0026gt;ScheduleDaemonSetPods=true|false (BETA - default=true)\u0026lt;br/\u0026gt;ServerSideApply=true|false (BETA - default=true)\u0026lt;br/\u0026gt;ServiceLoadBalancerFinalizer=true|false (BETA - default=true)\u0026lt;br/\u0026gt;ServiceNodeExclusion=true|false (ALPHA - default=false)\u0026lt;br/\u0026gt;StartupProbe=true|false (BETA - default=true)\u0026lt;br/\u0026gt;StorageVersionHash=true|false (BETA - default=true)\u0026lt;br/\u0026gt;StreamingProxyRedirects=true|false (BETA - default=true)\u0026lt;br/\u0026gt;SupportNodePidsLimit=true|false (BETA - default=true)\u0026lt;br/\u0026gt;SupportPodPidsLimit=true|false (BETA - default=true)\u0026lt;br/\u0026gt;Sysctls=true|false (BETA - default=true)\u0026lt;br/\u0026gt;TTLAfterFinished=true|false (ALPHA - default=false)\u0026lt;br/\u0026gt;TaintBasedEvictions=true|false (BETA - default=true)\u0026lt;br/\u0026gt;TaintNodesByCondition=true|false (BETA - default=true)\u0026lt;br/\u0026gt;TokenRequest=true|false (BETA - default=true)\u0026lt;br/\u0026gt;TokenRequestProjection=true|false (BETA - default=true)\u0026lt;br/\u0026gt;TopologyManager=true|false (ALPHA - default=false)\u0026lt;br/\u0026gt;ValidateProxyRedirects=true|false (BETA - default=true)\u0026lt;br/\u0026gt;VolumePVCDataSource=true|false (BETA - default=true)\u0026lt;br/\u0026gt;VolumeSnapshotDataSource=true|false (ALPHA - default=false)\u0026lt;br/\u0026gt;VolumeSubpathEnvExpansion=true|false (BETA - default=true)\u0026lt;br/\u0026gt;WatchBookmark=true|false (BETA - default=true)\u0026lt;br/\u0026gt;WinDSR=true|false (ALPHA - default=false)\u0026lt;br/\u0026gt;WinOverlay=true|false (ALPHA - default=false)\u0026lt;br/\u0026gt;WindowsGMSA=true|false (BETA - default=true)\u0026lt;br/\u0026gt;WindowsRunAsUserName=true|false (ALPHA - default=false) --\u0026gt; 一组 key=value 对，描述了 alpha/experimental 特征开关。选项包括：\u0026lt;br/\u0026gt;APIListChunking=true|false (BETA - 默认值=true)\u0026lt;br/\u0026gt;APIResponseCompression=true|false (BETA - 默认值=true)\u0026lt;br/\u0026gt;AllAlpha=true|false (ALPHA - 默认值=false)\u0026lt;br/\u0026gt;AppArmor=true|false (BETA - 默认值=true)\u0026lt;br/\u0026gt;AttachVolumeLimit=true|false (BETA - 默认值=true)\u0026lt;br/\u0026gt;BalanceAttachedNodeVolumes=true|false (ALPHA - 默认值=false)\u0026lt;br/\u0026gt;BlockVolume=true|false (BETA - 默认值=true)\u0026lt;br/\u0026gt;BoundServiceAccountTokenVolume=true|false (ALPHA - 默认值=false)\u0026lt;br/\u0026gt;CPUManager=true|false (BETA - 默认值=true)\u0026lt;br/\u0026gt;CRIContainerLogRotation=true|false (BETA - 默认值=true)\u0026lt;br/\u0026gt;CSIBlockVolume=true|false (BETA - 默认值=true)\u0026lt;br/\u0026gt;CSIDriverRegistry=true|false (BETA - 默认值=true)\u0026lt;br/\u0026gt;CSIInlineVolume=true|false (BETA - 默认值=true)\u0026lt;br/\u0026gt;CSIMigration=true|false (ALPHA - 默认值=false)\u0026lt;br/\u0026gt;CSIMigrationAWS=true|false (ALPHA - 默认值=false)\u0026lt;br/\u0026gt;CSIMigrationAzureDisk=true|false (ALPHA - 默认值=false)\u0026lt;br/\u0026gt;CSIMigrationAzureFile=true|false (ALPHA - 默认值=false)\u0026lt;br/\u0026gt;CSIMigrationGCE=true|false (ALPHA - 默认值=false)\u0026lt;br/\u0026gt;CSIMigrationOpenStack=true|false (ALPHA - 默认值=false)\u0026lt;br/\u0026gt;CSINodeInfo=true|false (BETA - 默认值=true)\u0026lt;br/\u0026gt;CustomCPUCFSQuotaPeriod=true|false (ALPHA - 默认值=false)\u0026lt;br/\u0026gt;CustomResourceDefaulting=true|false (BETA - 默认值=true)\u0026lt;br/\u0026gt;DevicePlugins=true|false (BETA - 默认值=true)\u0026lt;br/\u0026gt;DryRun=true|false (BETA - 默认值=true)\u0026lt;br/\u0026gt;DynamicAuditing=true|false (ALPHA - 默认值=false)\u0026lt;br/\u0026gt;DynamicKubeletConfig=true|false (BETA - 默认值=true)\u0026lt;br/\u0026gt;EndpointSlice=true|false (ALPHA - 默认值=false)\u0026lt;br/\u0026gt;EphemeralContainers=true|false (ALPHA - 默认值=false)\u0026lt;br/\u0026gt;EvenPodsSpread=true|false (ALPHA - 默认值=false)\u0026lt;br/\u0026gt;ExpandCSIVolumes=true|false (BETA - 默认值=true)\u0026lt;br/\u0026gt;ExpandInUsePersistentVolumes=true|false (BETA - 默认值=true)\u0026lt;br/\u0026gt;ExpandPersistentVolumes=true|false (BETA - 默认值=true)\u0026lt;br/\u0026gt;ExperimentalHostUserNamespaceDefaulting=true|false (BETA - 默认值=false)\u0026lt;br/\u0026gt;HPAScaleToZero=true|false (ALPHA - 默认值=false)\u0026lt;br/\u0026gt;HyperVContainer=true|false (ALPHA - 默认值=false)\u0026lt;br/\u0026gt;IPv6DualStack=true|false (ALPHA - 默认值=false)\u0026lt;br/\u0026gt;KubeletPodResources=true|false (BETA - 默认值=true)\u0026lt;br/\u0026gt;LegacyNodeRoleBehavior=true|false (ALPHA - 默认值=true)\u0026lt;br/\u0026gt;LocalStorageCapacityIsolation=true|false (BETA - 默认值=true)\u0026lt;br/\u0026gt;LocalStorageCapacityIsolationFSQuotaMonitoring=true|false (ALPHA - 默认值=false)\u0026lt;br/\u0026gt;MountContainers=true|false (ALPHA - 默认值=false)\u0026lt;br/\u0026gt;NodeDisruptionExclusion=true|false (ALPHA - 默认值=false)\u0026lt;br/\u0026gt;NodeLease=true|false (BETA - 默认值=true)\u0026lt;br/\u0026gt;NonPreemptingPriority=true|false (ALPHA - 默认值=false)\u0026lt;br/\u0026gt;PodOverhead=true|false (ALPHA - 默认值=false)\u0026lt;br/\u0026gt;PodShareProcessNamespace=true|false (BETA - 默认值=true)\u0026lt;br/\u0026gt;ProcMountType=true|false (ALPHA - 默认值=false)\u0026lt;br/\u0026gt;QOSReserved=true|false (ALPHA - 默认值=false)\u0026lt;br/\u0026gt;RemainingItemCount=true|false (BETA - 默认值=true)\u0026lt;br/\u0026gt;RemoveSelfLink=true|false (ALPHA - 默认值=false)\u0026lt;br/\u0026gt;RequestManagement=true|false (ALPHA - 默认值=false)\u0026lt;br/\u0026gt;ResourceLimitsPriorityFunction=true|false (ALPHA - 默认值=false)\u0026lt;br/\u0026gt;ResourceQuotaScopeSelectors=true|false (BETA - 默认值=true)\u0026lt;br/\u0026gt;RotateKubeletClientCertificate=true|false (BETA - 默认值=true)\u0026lt;br/\u0026gt;RotateKubeletServerCertificate=true|false (BETA - 默认值=true)\u0026lt;br/\u0026gt;RunAsGroup=true|false (BETA - 默认值=true)\u0026lt;br/\u0026gt;RuntimeClass=true|false (BETA - 默认值=true)\u0026lt;br/\u0026gt;SCTPSupport=true|false (ALPHA - 默认值=false)\u0026lt;br/\u0026gt;ScheduleDaemonSetPods=true|false (BETA - 默认值=true)\u0026lt;br/\u0026gt;ServerSideApply=true|false (BETA - 默认值=true)\u0026lt;br/\u0026gt;ServiceLoadBalancerFinalizer=true|false (BETA - 默认值=true)\u0026lt;br/\u0026gt;ServiceNodeExclusion=true|false (ALPHA - 默认值=false)\u0026lt;br/\u0026gt;StartupProbe=true|false (BETA - 默认值=true)\u0026lt;br/\u0026gt;StorageVersionHash=true|false (BETA - 默认值=true)\u0026lt;br/\u0026gt;StreamingProxyRedirects=true|false (BETA - 默认值=true)\u0026lt;br/\u0026gt;SupportNodePidsLimit=true|false (BETA - 默认值=true)\u0026lt;br/\u0026gt;SupportPodPidsLimit=true|false (BETA - 默认值=true)\u0026lt;br/\u0026gt;Sysctls=true|false (BETA - 默认值=true)\u0026lt;br/\u0026gt;TTLAfterFinished=true|false (ALPHA - 默认值=false)\u0026lt;br/\u0026gt;TaintBasedEvictions=true|false (BETA - 默认值=true)\u0026lt;br/\u0026gt;TaintNodesByCondition=true|false (BETA - 默认值=true)\u0026lt;br/\u0026gt;TokenRequest=true|false (BETA - 默认值=true)\u0026lt;br/\u0026gt;TokenRequestProjection=true|false (BETA - 默认值=true)\u0026lt;br/\u0026gt;TopologyManager=true|false (ALPHA - 默认值=false)\u0026lt;br/\u0026gt;ValidateProxyRedirects=true|false (BETA - 默认值=true)\u0026lt;br/\u0026gt;VolumePVCDataSource=true|false (BETA - 默认值=true)\u0026lt;br/\u0026gt;VolumeSnapshotDataSource=true|false (ALPHA - 默认值=false)\u0026lt;br/\u0026gt;VolumeSubpathEnvExpansion=true|false (BETA - 默认值=true)\u0026lt;br/\u0026gt;WatchBookmark=true|false (BETA - 默认值=true)\u0026lt;br/\u0026gt;WinDSR=true|false (ALPHA - 默认值=false)\u0026lt;br/\u0026gt;WinOverlay=true|false (ALPHA - 默认值=false)\u0026lt;br/\u0026gt;WindowsGMSA=true|false (BETA - 默认值=true)\u0026lt;br/\u0026gt;WindowsRunAsUserName=true|false (ALPHA - 默认值=false) \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt; \u0026lt;!-- --hard-pod-affinity-symmetric-weight int32\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;Default: 1 --\u0026gt; --hard-pod-affinity-symmetric-weight int32\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;默认: 1 \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt; \u0026lt;!-- DEPRECATED: RequiredDuringScheduling affinity is not symmetric, but there is an implicit PreferredDuringScheduling affinity rule corresponding to every RequiredDuringScheduling affinity rule. --hard-pod-affinity-symmetric-weight represents the weight of implicit PreferredDuringScheduling affinity rule. Must be in the range 0-100.This option was moved to the policy configuration file --\u0026gt; 弃用: RequiredDuringScheduling 亲和力不是对称的，但是存在与每个 RequiredDuringScheduling 关联性规则相对应的隐式 PreferredDuringScheduling 关联性规则 --hard-pod-affinity-symmetric-weight 代表隐式 PreferredDuringScheduling 关联性规则的权重。权重必须在 0-100 范围内。此选项已移至策略配置文件。 \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;-h, --help\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt; \u0026lt;!-- help for kube-scheduler --\u0026gt; kube-scheduler 帮助命令 \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--http2-max-streams-per-connection int\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt; \u0026lt;!-- The limit that the server gives to clients for the maximum number of streams in an HTTP/2 connection. Zero means to use golang's default. --\u0026gt; 服务器为客户端提供的 HTTP/2 连接最大限制。零表示使用 golang 的默认值。 \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt; \u0026lt;!-- --kube-api-burst int32\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;Default: 100 --\u0026gt; --kube-api-burst int32\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;默认: 100 \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt; \u0026lt;!-- DEPRECATED: burst to use while talking with kubernetes apiserver --\u0026gt; 弃用: 与 kubernetes apiserver 通信时使用 \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt; \u0026lt;!-- --kube-api-content-type string\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;Default: \u0026quot;application/vnd.kubernetes.protobuf\u0026quot; --\u0026gt; --kube-api-content-type string\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;默认: \u0026quot;application/vnd.kubernetes.protobuf\u0026quot; \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt; \u0026lt;!-- DEPRECATED: content type of requests sent to apiserver. --\u0026gt; 弃用: 发送到 apiserver 的请求的内容类型。 \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt; \u0026lt;!-- --kube-api-qps float32\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;Default: 50 --\u0026gt; --kube-api-qps float32\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;默认: 50 \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt; \u0026lt;!-- DEPRECATED: QPS to use while talking with kubernetes apiserver --\u0026gt; 弃用: 与 kubernetes apiserver 通信时要使用的 QPS \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--kubeconfig string\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt; \u0026lt;!-- DEPRECATED: path to kubeconfig file with authorization and master location information. --\u0026gt; 弃用: 具有授权和主节点位置信息的 kubeconfig 文件的路径。 \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt; \u0026lt;!-- --leader-elect\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;Default: true --\u0026gt; --leader-elect\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;默认: true \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt; \u0026lt;!-- Start a leader election client and gain leadership before executing the main loop. Enable this when running replicated components for high availability. --\u0026gt; 在执行主循环之前，开始领导者选举并选出领导者。为实现高可用性，运行多副本的组件并选出领导者。 \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt; \u0026lt;!-- --leader-elect-lease-duration duration\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;Default: 15s --\u0026gt; --leader-elect-lease-duration duration\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;默认: 15s \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt; \u0026lt;!-- The duration that non-leader candidates will wait after observing a leadership renewal until attempting to acquire leadership of a led but unrenewed leader slot. This is effectively the maximum duration that a leader can be stopped before it is replaced by another candidate. This is only applicable if leader election is enabled. --\u0026gt; 非领导者候选人在观察到领导者更新后将等待直到试图获得领导但未更新的领导者职位的等待时间。这实际上是领导者在被另一位候选人替代之前可以停止的最大持续时间。该情况仅在启用了领导者选举的情况下才适用。 \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt; \u0026lt;!-- --leader-elect-renew-deadline duration\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;Default: 10s --\u0026gt; --leader-elect-renew-deadline duration\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;默认: 10s \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt; \u0026lt;!-- The interval between attempts by the acting master to renew a leadership slot before it stops leading. This must be less than or equal to the lease duration. This is only applicable if leader election is enabled. --\u0026gt; \u0026lt;/td\u0026gt; 领导者尝试在停止领导之前更新领导职位的间隔时间。该时间必须小于或等于租赁期限。仅在启用了领导者选举的情况下才适用。 \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt; \u0026lt;!-- --leader-elect-resource-lock endpoints\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;Default: \u0026quot;endpoints\u0026quot; --\u0026gt; --leader-elect-resource-lock endpoints\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;默认: \u0026quot;endpoints\u0026quot; \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt; \u0026lt;!-- The type of resource object that is used for locking during leader election. Supported options are endpoints (default) and `configmaps`. --\u0026gt; 在领导者选举期间用于锁定的资源对象的类型。支持的选项是端点（默认）和 `configmaps` \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt; \u0026lt;!-- --leader-elect-resource-name string\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;Default: \u0026quot;kube-scheduler\u0026quot; --\u0026gt; --leader-elect-resource-name string\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;默认: \u0026quot;kube-scheduler\u0026quot; \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt; \u0026lt;!-- The name of resource object that is used for locking during leader election. --\u0026gt; 在领导者选举期间用于锁定的资源对象的名称。 \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt; \u0026lt;!-- --leader-elect-resource-namespace string\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;Default: \u0026quot;kube-system\u0026quot; --\u0026gt; --leader-elect-resource-namespace string\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;默认: \u0026quot;kube-system\u0026quot; \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt; \u0026lt;!-- The namespace of resource object that is used for locking during leader election. --\u0026gt; 在领导者选举期间用于锁定的资源对象的命名空间。 \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt; \u0026lt;!-- --leader-elect-retry-period duration\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;Default: 2s --\u0026gt; --leader-elect-retry-period duration\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;默认: 2s \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt; \u0026lt;!-- The duration the clients should wait between attempting acquisition and renewal of a leadership. This is only applicable if leader election is enabled. --\u0026gt; 客户应在尝试获取和更新领导之间等待的时间。仅在启用了领导者选举的情况下才适用。 \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt; \u0026lt;!-- --lock-object-name string\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;Default: \u0026quot;kube-scheduler\u0026quot; --\u0026gt; --lock-object-name string\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;默认: \u0026quot;kube-scheduler\u0026quot; \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt; \u0026lt;!-- DEPRECATED: define the name of the lock object. Will be removed in favor of leader-elect-resource-name --\u0026gt; 弃用: 定义锁对象的名称。将被删除以便使用 Leader-elect-resource-name \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt; \u0026lt;!-- --lock-object-namespace string\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;Default: \u0026quot;kube-system\u0026quot; --\u0026gt; --lock-object-namespace string\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;默认: \u0026quot;kube-system\u0026quot; \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt; \u0026lt;!-- DEPRECATED: define the namespace of the lock object. Will be removed in favor of leader-elect-resource-namespace. --\u0026gt; 弃用: 定义锁对象的命名空间。将被删除以便使用 leader-elect-resource-namespace。 \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt; \u0026lt;!-- --log-backtrace-at traceLocation\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;Default: :0 --\u0026gt; --log-backtrace-at traceLocation\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;默认: :0 \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt; \u0026lt;!-- when logging hits line file:N, emit a stack trace --\u0026gt; 当记录命中行文件：N 时发出堆栈跟踪 \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--log-dir string\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt; \u0026lt;!-- If non-empty, write log files in this directory --\u0026gt; 如果为非空，则在此目录中写入日志文件 \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--log-file string\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt; \u0026lt;!-- If non-empty, use this log file --\u0026gt; 如果为非空，请使用此日志文件 \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt; \u0026lt;!-- --log-file-max-size uint\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;Default: 1800 --\u0026gt; --log-file-max-size uint\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;默认: 1800 \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt; \u0026lt;!-- Defines the maximum size a log file can grow to. Unit is megabytes. If the value is 0, the maximum file size is unlimited. --\u0026gt; 定义日志文件可以增长到的最大值。单位为兆字节。如果值为0，则最大文件大小为无限制。 \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt; \u0026lt;!-- --log-flush-frequency duration\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;Default: 5s --\u0026gt; --log-flush-frequency duration\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;默认: 5s \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt; \u0026lt;!-- Maximum number of seconds between log flushes --\u0026gt; 两次日志刷新之间的最大秒数 \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt; \u0026lt;!-- --logtostderr\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;Default: true --\u0026gt; --logtostderr\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;默认: true \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt; \u0026lt;!-- log to standard error instead of files --\u0026gt; 日志记录到标准错误而不是文件 \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--master string\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt; \u0026lt;!-- The address of the Kubernetes API server (overrides any value in kubeconfig) --\u0026gt; Kubernetes API 服务器的地址（覆盖 kubeconfig 中的任何值） \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--policy-config-file string\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt; \u0026lt;!-- DEPRECATED: file with scheduler policy configuration. This file is used if policy ConfigMap is not provided or --use-legacy-policy-config=true --\u0026gt; 弃用：具有调度程序策略配置的文件。如果未提供 policy ConfigMap 或 --use-legacy-policy-config = true，则使用此文件 \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--policy-configmap string\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt; \u0026lt;!-- DEPRECATED: name of the ConfigMap object that contains scheduler's policy configuration. It must exist in the system namespace before scheduler initialization if --use-legacy-policy-config=false. The config must be provided as the value of an element in 'Data' map with the key='policy.cfg' --\u0026gt; 弃用: 包含调度程序策略配置的 ConfigMap 对象的名称。如果 --use-legacy-policy-config = false，则它必须在调度程序初始化之前存在于系统命名空间中。必须将配置作为键为 'policy.cfg' 的 'Data' 映射中元素的值提供 \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt; \u0026lt;!-- --policy-configmap-namespace string\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;Default: \u0026quot;kube-system\u0026quot; --\u0026gt; --policy-configmap-namespace string\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;默认: \u0026quot;kube-system\u0026quot; \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt; \u0026lt;!-- DEPRECATED: the namespace where policy ConfigMap is located. The kube-system namespace will be used if this is not provided or is empty. --\u0026gt; 弃用: 策略 ConfigMap 所在的命名空间。如果未提供或为空，则将使用 kube-system 命名空间。 \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt; \u0026lt;!-- --port int\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;Default: 10251 --\u0026gt; --port int\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;默认: 10251 \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt; \u0026lt;!-- DEPRECATED: the port on which to serve HTTP insecurely without authentication and authorization. If 0, don't serve plain HTTP at all. See --secure-port instead. --\u0026gt; 弃用: 在没有身份验证和授权的情况下不安全地为 HTTP 服务的端口。如果为0，则根本不提供 HTTP。请参见--secure-port。 \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--profiling\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt; \u0026lt;!-- DEPRECATED: enable profiling via web interface host:port/debug/pprof/ --\u0026gt; 弃用: 通过 Web 界面主机启用配置文件：port/debug/pprof/ \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--requestheader-allowed-names stringSlice\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt; \u0026lt;!-- List of client certificate common names to allow to provide usernames in headers specified by --requestheader-username-headers. If empty, any client certificate validated by the authorities in --requestheader-client-ca-file is allowed. --\u0026gt; 客户端证书通用名称列表允许在 --requestheader-username-headers 指定的头部中提供用户名。如果为空，则允许任何由权威机构 --requestheader-client-ca-file 验证的客户端证书。 \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--requestheader-client-ca-file string\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt; \u0026lt;!-- Root certificate bundle to use to verify client certificates on incoming requests before trusting usernames in headers specified by --requestheader-username-headers. WARNING: generally do not depend on authorization being already done for incoming requests. --\u0026gt; 在信任 --requestheader-username-headers 指定的头部中的用户名之前用于验证传入请求上的客户端证书的根证书包。警告：通常不依赖于传入请求已经完成的授权。 \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt; \u0026lt;!-- --requestheader-extra-headers-prefix stringSlice\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;Default: [x-remote-extra-] --\u0026gt; --requestheader-extra-headers-prefix stringSlice\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;默认: [x-remote-extra-] \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt; \u0026lt;!-- List of request header prefixes to inspect. X-Remote-Extra- is suggested. --\u0026gt; 要检查请求头部前缀列表。建议使用 X-Remote-Extra- \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt; \u0026lt;!-- --requestheader-group-headers stringSlice\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;Default: [x-remote-group] --\u0026gt; --requestheader-group-headers stringSlice\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;默认: [x-remote-group] \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt; \u0026lt;!-- List of request headers to inspect for groups. X-Remote-Group is suggested. --\u0026gt; 用于检查组的请求头部列表。建议使用 X-Remote-Group。 \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt; \u0026lt;!-- --requestheader-username-headers stringSlice\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;Default: [x-remote-user] --\u0026gt; --requestheader-username-headers stringSlice\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;默认: [x-remote-user] \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt; \u0026lt;!-- List of request headers to inspect for usernames. X-Remote-User is common. --\u0026gt; 用于检查用户名的请求头部列表。 X-Remote-User 很常见。 \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt; \u0026lt;!-- --scheduler-name string\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;Default: \u0026quot;default-scheduler\u0026quot; --\u0026gt; --scheduler-name string\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;默认: \u0026quot;default-scheduler\u0026quot; \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt; \u0026lt;!-- DEPRECATED: name of the scheduler, used to select which pods will be processed by this scheduler, based on pod's \u0026quot;spec.schedulerName\u0026quot;. --\u0026gt; 弃用: 调度程序名称用于根据 Pod 的 \u0026quot;spec.schedulerName\u0026quot; 选择此调度程序将处理的 Pod。 \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt; \u0026lt;!-- --secure-port int\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;Default: 10259 --\u0026gt; --secure-port int\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;默认: 10259 \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt; \u0026lt;!-- The port on which to serve HTTPS with authentication and authorization.If 0, don't serve HTTPS at all. --\u0026gt; 通过身份验证和授权为 HTTPS 服务的端口。如果为 0，则根本不提供 HTTPS。 \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--skip-headers\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt; \u0026lt;!-- If true, avoid header prefixes in the log messages --\u0026gt; 如果为 true，请在日志消息中避免头部前缀 \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--skip-log-headers\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt; \u0026lt;!-- If true, avoid headers when opening log files --\u0026gt; 如果为true，则在打开日志文件时避免头部 \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt; \u0026lt;!-- --stderrthreshold severity\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;Default: 2 --\u0026gt; --stderrthreshold severity\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;默认: 2 \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt; \u0026lt;!-- logs at or above this threshold go to stderr --\u0026gt; 达到或超过此阈值的日志转到 stderr \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--tls-cert-file string\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt; \u0026lt;!-- File containing the default x509 Certificate for HTTPS. (CA cert, if any, concatenated after server cert). If HTTPS serving is enabled, and --tls-cert-file and --tls-private-key-file are not provided, a self-signed certificate and key are generated for the public address and saved to the directory specified by --cert-dir. --\u0026gt; 包含默认的 HTTPS x509 证书的文件。（CA证书（如果有）在服务器证书之后并置）。如果启用了 HTTPS 服务，并且未提供 --tls-cert-file 和 --tls-private-key-file，则会为公共地址生成一个自签名证书和密钥，并将其保存到 --cert-dir 指定的目录中。 \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--tls-cipher-suites stringSlice\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt; \u0026lt;!-- Comma-separated list of cipher suites for the server. If omitted, the default Go cipher suites will be use. Possible values: TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_RC4_128_SHA,TLS_ECDHE_RSA_WITH_3DES_EDE_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_RC4_128_SHA,TLS_RSA_WITH_3DES_EDE_CBC_SHA,TLS_RSA_WITH_AES_128_CBC_SHA,TLS_RSA_WITH_AES_128_CBC_SHA256,TLS_RSA_WITH_AES_128_GCM_SHA256,TLS_RSA_WITH_AES_256_CBC_SHA,TLS_RSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_RC4_128_SHA --\u0026gt; 服务器的密码套件列表，以逗号分隔。如果省略，将使用默认的 Go 密码套件。可能的值： TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_RC4_128_SHA,TLS_ECDHE_RSA_WITH_3DES_EDE_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_RC4_128_SHA,TLS_RSA_WITH_3DES_EDE_CBC_SHA,TLS_RSA_WITH_AES_128_CBC_SHA,TLS_RSA_WITH_AES_128_CBC_SHA256,TLS_RSA_WITH_AES_128_GCM_SHA256,TLS_RSA_WITH_AES_256_CBC_SHA,TLS_RSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_RC4_128_SHA \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--tls-min-version string\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt; \u0026lt;!-- Minimum TLS version supported. Possible values: VersionTLS10, VersionTLS11, VersionTLS12, VersionTLS13 --\u0026gt; 支持的最低 TLS 版本。可能的值：VersionTLS10, VersionTLS11, VersionTLS12, VersionTLS13 \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--tls-private-key-file string\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt; \u0026lt;!-- File containing the default x509 private key matching --tls-cert-file. --\u0026gt; 包含与 --tls-cert-file 匹配的默认 x509 私钥的文件。 \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt; \u0026lt;!-- --tls-sni-cert-key namedCertKey\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;Default: [] --\u0026gt; --tls-sni-cert-key namedCertKey\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;默认: [] \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt; \u0026lt;!-- A pair of x509 certificate and private key file paths, optionally suffixed with a list of domain patterns which are fully qualified domain names, possibly with prefixed wildcard segments. If no domain patterns are provided, the names of the certificate are extracted. Non-wildcard matches trump over wildcard matches, explicit domain patterns trump over extracted names. For multiple key/certificate pairs, use the --tls-sni-cert-key multiple times. Examples: \u0026quot;example.crt,example.key\u0026quot; or \u0026quot;foo.crt,foo.key:*.foo.com,foo.com\u0026quot;. --\u0026gt; 一对 x509 证书和私钥文件路径，可选地后缀为完全限定域名的域模式列表，并可能带有前缀的通配符段。如果未提供域模式，则获取证书名称。非通配符匹配胜过通配符匹配，显式域模式胜过获取名称。 对于多个密钥/证书对，请多次使用 --tls-sni-cert-key。例如: \u0026quot;example.crt,example.key\u0026quot; 或者 \u0026quot;foo.crt,foo.key:*.foo.com,foo.com\u0026quot;。 \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--use-legacy-policy-config\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt; \u0026lt;!-- DEPRECATED: when set to true, scheduler will ignore policy ConfigMap and uses policy config file --\u0026gt; 弃用: 设置为 true 时，调度程序将忽略策略 ConfigMap 并使用策略配置文件 \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;-v, --v Level\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt; \u0026lt;!-- number for the log level verbosity --\u0026gt; 日志级别详细程度的数字 \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--version version[=true]\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt; \u0026lt;!-- Print version information and quit --\u0026gt; 打印版本信息并退出 \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--vmodule moduleSpec\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt; \u0026lt;!-- comma-separated list of pattern=N settings for file-filtered logging --\u0026gt; 以逗号分隔的 pattern = N 设置列表，用于文件过滤的日志记录 \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--write-config-to string\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt; \u0026lt;!-- If set, write the configuration values to this file and exit. --\u0026gt; 如果已设置，请将配置值写入此文件并退出。 \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt;  "
},
{
	"uri": "https://lijun.in/reference/kubectl/kubectl/",
	"title": "kubectl",
	"tags": [],
	"description": "",
	"content": ". heading \u0026ldquo;synopsis\u0026rdquo; %}} kubectl 管理控制 Kubernetes 集群。\n获取更多信息，请访问 kubectl 概述。\nkubectl [flags] . heading \u0026ldquo;options\u0026rdquo; %}} \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--add-dir-header\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt; \u0026lt;!-- If true, adds the file directory to the header --\u0026gt; 设置为 true 表示添加文件目录到 header 中 \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--alsologtostderr\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt; \u0026lt;!-- log to standard error as well as files --\u0026gt; 表示将日志输出到文件的同时输出到 stderr \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--as string\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt; \u0026lt;!-- Username to impersonate for the operation --\u0026gt; 以指定用户的身份执行操作 \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--as-group stringArray\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt; \u0026lt;!-- Group to impersonate for the operation, this flag can be repeated to specify multiple groups. --\u0026gt; 模拟指定的组来执行操作，可以使用这个标志来指定多个组。 \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--azure-container-registry-config string\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt; \u0026lt;!-- Path to the file containing Azure container registry configuration information. --\u0026gt; 包含 Azure 容器仓库配置信息的文件的路径。 \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--cache-dir string\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;默认值: \u0026quot;~/.kube/http-cache\u0026quot;\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt; \u0026lt;!-- Default HTTP cache directory --\u0026gt; 默认 HTTP 缓存目录 \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--certificate-authority string\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt; \u0026lt;!-- Path to a cert file for the certificate authority --\u0026gt; 指向证书机构的 cert 文件路径 \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--client-certificate string\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt; \u0026lt;!-- Path to a client certificate file for TLS --\u0026gt; TLS 使用的客户端证书路径 \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--client-key string\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt; \u0026lt;!-- Path to a client key file for TLS --\u0026gt; TLS 使用的客户端密钥文件路径 \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--cloud-provider-gce-lb-src-cidrs cidrs\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;默认值: 130.211.0.0/22,209.85.152.0/22,209.85.204.0/22,35.191.0.0/16\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt; \u0026lt;!-- CIDRs opened in GCE firewall for LB traffic proxy \u0026amp; health checks --\u0026gt; 在 GCE 防火墙中打开 CIDR，以进行 LB 流量代理和运行状况检查。 \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--cluster string\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt; \u0026lt;!-- The name of the kubeconfig cluster to use --\u0026gt; 要使用的 kubeconfig 集群的名称 \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--context string\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt; \u0026lt;!-- The name of the kubeconfig context to use --\u0026gt; 要使用的 kubeconfig 上下文的名称 \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--default-not-ready-toleration-seconds int\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;默认值: 300\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt; \u0026lt;!-- Indicates the tolerationSeconds of the toleration for notReady:NoExecute that is added by default to every pod that does not already have such a toleration. --\u0026gt; 表示 `notReady` 状态的容忍度秒数：默认情况下，`NoExecute` 被添加到尚未具有此容忍度的每个 Pod 中。 \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--default-unreachable-toleration-seconds int\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;默认值: 300\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt; \u0026lt;!-- Indicates the tolerationSeconds of the toleration for unreachable:NoExecute that is added by default to every pod that does not already have such a toleration. --\u0026gt; 表示 `unreachable` 状态的容忍度秒数：默认情况下，`NoExecute` 被添加到尚未具有此容忍度的每个 Pod 中。 \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;-h, --help\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt; \u0026lt;!-- help for kubectl --\u0026gt; kubectl 操作的帮助命令 \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--insecure-skip-tls-verify\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt; \u0026lt;!-- If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure --\u0026gt; 设置为 true，则表示不会检查服务器证书的有效性。这样会导致您的 HTTPS 连接不安全。 \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--kubeconfig string\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt; \u0026lt;!-- Path to the kubeconfig file to use for CLI requests. --\u0026gt; CLI 请求使用的 kubeconfig 配置文件的路径。 \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--log-backtrace-at traceLocation\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;默认值: 0\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt; \u0026lt;!-- when logging hits line file:N, emit a stack trace --\u0026gt; 当日志机制运行到指定文件的指定行（file:N）时，打印调用堆栈信息 \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--log-dir string\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt; \u0026lt;!-- If non-empty, write log files in this directory --\u0026gt; 如果不为空，则将日志文件写入此目录 \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--log-file string\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt; \u0026lt;!-- If non-empty, use this log file --\u0026gt; 如果不为空，则将使用此日志文件 \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--log-file-max-size uint\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;默认值: 1800\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt; \u0026lt;!-- Defines the maximum size a log file can grow to. Unit is megabytes. If the value is 0, the maximum file size is unlimited. --\u0026gt; 定义日志文件的最大尺寸。单位为兆字节。如果值设置为 0，则表示日志文件大小不受限制。 \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--log-flush-frequency duration\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;默认值: 5s\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt; \u0026lt;!-- Maximum number of seconds between log flushes --\u0026gt; 两次日志刷新操作之间的最长时间（秒） \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--logtostderr\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;默认值: true\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt; \u0026lt;!-- log to standard error instead of files --\u0026gt; 日志输出到 stderr 而不是文件中 \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--match-server-version\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt; \u0026lt;!-- Require server version to match client version --\u0026gt; 要求客户端版本和服务端版本相匹配 \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;-n, --namespace string\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt; \u0026lt;!-- If present, the namespace scope for this CLI request --\u0026gt; 如果存在，CLI 请求将使用此命名空间 \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--password string\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt; \u0026lt;!-- Password for basic authentication to the API server --\u0026gt; API 服务器进行基本身份验证的密码 \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--profile string\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;默认值: \u0026quot;none\u0026quot;\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt; \u0026lt;!-- Name of profile to capture. One of (none|cpu|heap|goroutine|threadcreate|block|mutex) --\u0026gt; 要记录的性能指标的名称。可取 (none|cpu|heap|goroutine|threadcreate|block|mutex) 其中之一。 \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--profile-output string\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;默认值: \u0026quot;profile.pprof\u0026quot;\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt; \u0026lt;!-- Name of the file to write the profile to --\u0026gt; 用于转储所记录的性能信息的文件名 \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--request-timeout string\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;默认值: \u0026quot;0\u0026quot;\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt; \u0026lt;!-- The length of time to wait before giving up on a single server request. Non-zero values should contain a corresponding time unit (e.g. 1s, 2m, 3h). A value of zero means don't timeout requests. --\u0026gt; 放弃单个服务器请求之前的等待时间，非零值需要包含相应时间单位（例如：1s、2m、3h）。零值则表示不做超时要求。 \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;-s, --server string\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt; \u0026lt;!-- The address and port of the Kubernetes API server --\u0026gt; Kubernetes API 服务器的地址和端口 \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--skip-headers\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt; \u0026lt;!-- If true, avoid header prefixes in the log messages --\u0026gt; 设置为 true 则表示跳过在日志消息中出现 header 前缀信息 \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--skip-log-headers\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt; \u0026lt;!-- If true, avoid headers when opening log files --\u0026gt; 设置为 true 则表示在打开日志文件时跳过 header 信息 \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--stderrthreshold severity\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;默认值: 2\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt; \u0026lt;!-- logs at or above this threshold go to stderr --\u0026gt; 等于或高于此阈值的日志将输出到标准错误输出（stderr） \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--token string\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt; \u0026lt;!-- Bearer token for authentication to the API server --\u0026gt; 用于对 API 服务器进行身份认证的持有者令牌 \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--user string\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt; \u0026lt;!-- The name of the kubeconfig user to use --\u0026gt; 指定使用 kubeconfig 配置文件中的用户名 \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--username string\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt; \u0026lt;!-- Username for basic authentication to the API server --\u0026gt; 用于 API 服务器的基本身份验证的用户名 \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;-v, --v Level\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt; \u0026lt;!-- number for the log level verbosity --\u0026gt; 指定输出日志的日志详细级别 \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--version version[=true]\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt; \u0026lt;!-- Print version information and quit --\u0026gt; 打印 kubectl 版本信息并退出 \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--vmodule moduleSpec\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt; \u0026lt;!-- comma-separated list of pattern=N settings for file-filtered logging --\u0026gt; 以逗号分隔的 pattern=N 设置列表，用于过滤文件的日志记录 \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt;  . heading \u0026ldquo;seealso\u0026rdquo; %}}  kubectl annotate\t- 更新资源所关联的注解 kubectl api-resources\t- 打印服务器上所支持的 API 资源 kubectl api-versions\t- 以“组/版本”的格式输出服务端所支持的 API 版本 kubectl apply\t- 基于文件名或标准输入，将新的配置应用到资源上 kubectl attach\t- 连接到一个正在运行的容器   kubectl auth\t- 检查授权信息 kubectl autoscale\t- 对一个资源对象（Deployment、ReplicaSet 或 ReplicationController ）进行扩缩 kubectl certificate\t- 修改证书资源 kubectl cluster-info\t- 显示集群信息 kubectl completion\t- 根据已经给出的 Shell（bash 或 zsh），输出 Shell 补全后的代码 kubectl config\t- 修改 kubeconfig 配置文件   kubectl convert\t- 在不同的 API 版本之间转换配置文件 kubectl cordon\t- 标记节点为不可调度的 kubectl cp\t- 将文件和目录拷入/拷出容器。 kubectl create\t- 通过文件或标准输入来创建资源 kubectl delete\t- 通过文件名、标准输入、资源和名字删除资源，或者通过资源和标签选择器来删除资源   kubectl describe\t- 显示某个资源或某组资源的详细信息 kubectl diff\t- 显示目前版本与将要应用的版本之间的差异 kubectl drain\t- 腾空节点，准备维护 kubectl edit\t- 修改服务器上的某资源 kubectl exec\t- 在容器中执行相关命令 kubectl explain\t- 显示资源文档说明 kubectl expose\t- 给定副本控制器、服务、Deployment 或 Pod，将其暴露为新的 kubernetes Service   kubectl get\t- 显示一个或者多个资源信息 kubectl kustomize\t- 从目录或远程 URL 中构建 kustomization kubectl label\t- 更新资源的标签 kubectl logs\t- 输出 pod 中某容器的日志 kubectl options\t- 打印所有命令都支持的共有参数列表 kubectl patch\t- 基于策略性合并修补（Stategic Merge Patch）规则更新某资源中的字段   kubectl plugin\t- 运行命令行插件 kubectl port-forward\t- 将一个或者多个本地端口转发到 pod kubectl proxy\t- 运行一个 kubernetes API 服务器代理 kubectl replace\t- 基于文件名或标准输入替换资源 kubectl rollout\t- 管理资源的上线 kubectl run\t- 在集群中使用指定镜像启动容器   kubectl scale\t- 为一个 Deployment、ReplicaSet、ReplicationController 或 Job 设置一个新的规模尺寸值 kubectl set\t- 为对象设置功能特性 kubectl taint\t- 在一个或者多个节点上更新污点配置 kubectl top\t- 显示资源（CPU /内存/存储）使用率 kubectl uncordon\t- 标记节点为可调度的 kubectl version\t- 打印客户端和服务器的版本信息 kubectl wait\t- 实验性：等待一个或多个资源达到某种状态  "
},
{
	"uri": "https://lijun.in/reference/command-line-tools-reference/kubelet/",
	"title": "kubelet",
	"tags": [],
	"description": "",
	"content": ". heading \u0026ldquo;synopsis\u0026rdquo; %}} kubelet 是在每个 Node 节点上运行的主要 “节点代理”。它向 apiserver 注册节点时可以使用主机名（hostname）；可以提供用于覆盖主机名的参数；还可以执行特定于某云服务商的逻辑。\nkubelet 是基于 PodSpec 来工作的。每个 PodSpec 是一个描述 Pod 的 YAML 或 JSON 对象。kubelet 接受通过各种机制（主要是通过 apiserver）提供的一组 PodSpec，并确保这些 PodSpec 中描述的容器处于运行状态且运行状况良好。kubelet 不管理不是由 Kubernetes 创建的容器。\n除了来自 apiserver 的 PodSpec 之外，还可以通过以下三种方式将容器清单（manifest）提供给 kubelet。\nFile（文件）：利用命令行参数给定路径。kubelet 周期性地监视此路径下的文件是否有更新。监视周期默认为 20s，且可通过参数进行配置。\nHTTP endpoint（HTTP 端点）：利用命令行参数指定 HTTP 端点。此端点每 20 秒被检查一次（也可以使用参数进行配置）。\nHTTP server（HTTP 服务器）：kubelet 还可以侦听 HTTP 并响应简单的 API（当前未经过规范）来提交新的清单。\nkubelet [flags] . heading \u0026ldquo;options\u0026rdquo; %}} "
},
{
	"uri": "https://lijun.in/concepts/storage/volume-pvc-datasource/",
	"title": "CSI 卷克隆",
	"tags": [],
	"description": "",
	"content": "本文档介绍 Kubernetes 中克隆现有 CSI 卷的概念。阅读前建议先熟悉卷。\n介绍 glossary_tooltip text=\u0026quot;CSI\u0026rdquo; term_id=\u0026quot;csi\u0026rdquo; \u0026gt;}} 卷克隆功能增加了通过在 dataSource 字段中指定存在的 glossary_tooltip text=\u0026quot;PVC\u0026rdquo; term_id=\u0026quot;persistent-volume-claim\u0026rdquo; \u0026gt;}}s，来表示用户想要克隆的 glossary_tooltip term_id=\u0026quot;volume\u0026rdquo; \u0026gt;}}。\n克隆，意思是为已有的 Kubernetes 卷创建副本，它可以像任何其它标准卷一样被使用。唯一的区别就是配置后，后端设备将创建指定完全相同的副本，而不是创建一个“新的”空卷。\n从 Kubernetes API 的角度看，克隆的实现只是在创建新的 PVC 时，增加了指定一个现有 PVC 作为数据源的能力。源 PVC 必须是 bound 状态且可用的（不在使用中）。\n用户在使用该功能时，需要注意以下事项：\n 克隆支持（VolumePVCDataSource）仅适用于 CSI 驱动。 克隆支持仅适用于 动态供应器。 CSI 驱动可能实现，也可能未实现卷克隆功能。 仅当 PVC 与目标 PVC 存在于同一命名空间（源和目标 PVC 必须在相同的命名空间）时，才可以克隆 PVC。 仅在同一存储类中支持克隆。  目标卷必须和源卷具有相同的存储类 可以使用默认的存储类并且 storageClassName 字段在规格中忽略了   克隆只能在两个使用相同 VolumeMode 设置的卷中进行（如果请求克隆一个块存储模式的卷，源卷必须也是块存储模式）。  供应 克隆卷与其他任何 PVC 一样配置，除了需要增加 dataSource 来引用同一命名空间中现有的 PVC。\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: clone-of-pvc-1 namespace: myns spec: accessModes: - ReadWriteOnce storageClassName: cloning resources: requests: storage: 5Gi dataSource: kind: PersistentVolumeClaim name: pvc-1 你必须为 spec.resources.requests.storage 指定一个值，并且你指定的值必须大于或等于源卷的值。\n结果是一个名称为 clone-of-pvc-1 的新 PVC 与指定的源 pvc-1 拥有相同的内容。\n用法 一旦新的 PVC 可用，被克隆的 PVC 项其他 PVC 一样被使用。可以预期的是，新创建的 PVC 是一个独立的对象。可以独立使用，克隆，快照或删除它，而不需要考虑它的原始数据源 PVC。这也意味着，源没有以任何方式链接到新创建的 PVC，它也可以被修改或删除，而不会影响到新创建的克隆。\n"
},
{
	"uri": "https://lijun.in/concepts/workloads/controllers/deployment/",
	"title": "Deployments",
	"tags": [],
	"description": "",
	"content": "apiVersion: extensions/v1beta1 # 接口版本 kind: Deployment # 接口类型 metadata: name: cango-demo # Deployment名称 namespace: cango-prd # 命名空间 labels: app: cango-demo # 标签 spec: replicas: 3 strategy: # 部署策略 rollingUpdate: # 由于replicas为3,则整个升级,pod个数在2-4个之间 maxSurge: 1 # 滚动升级时会先启动1个pod maxUnavailable: 1 # 滚动升级时允许的最大Unavailable的pod个数 template: metadata: labels: app: cango-demo # 模板名称必填 sepc: # 定义容器模板,该模板可以包含多个容器 containers: - name: cango-demo # 镜像名称 image: swr.cn-east-2.myhuaweicloud.com/cango-prd/cango-demo:0.0.1-SNAPSHOT # 镜像地址 command: [ \u0026#34;/bin/sh\u0026#34;,\u0026#34;-c\u0026#34;,\u0026#34;cat /etc/config/path/to/special-key\u0026#34; ] # 启动命令 args: # 启动参数 - \u0026#39;-storage.local.retention=$(STORAGE_RETENTION)\u0026#39; - \u0026#39;-storage.local.memory-chunks=$(STORAGE_MEMORY_CHUNKS)\u0026#39; - \u0026#39;-config.file=/etc/prometheus/prometheus.yml\u0026#39; - \u0026#39;-alertmanager.url=http://alertmanager:9093/alertmanager\u0026#39; - \u0026#39;-web.external-url=$(EXTERNAL_URL)\u0026#39; # 如果command和args均没有写，那么用Docker默认的配置。 # 如果command写了，但args没有写，那么Docker默认的配置会被忽略而且仅仅执行.yaml文件的command（不带任何参数的）。 # 如果command没写，但args写了，那么Docker默认配置的ENTRYPOINT的命令行会被执行，但是调用的参数是.yaml中的args。 # 如果如果command和args都写了，那么Docker默认的配置被忽略，使用.yaml的配置。 imagePullPolicy: IfNotPresent # 如果不存在则拉取 livenessProbe: # 表示container是否处于live状态。如果LivenessProbe失败，LivenessProbe将会通知kubelet对应的container不健康了。随后kubelet将kill掉container，并根据RestarPolicy进行进一步的操作。默认情况下LivenessProbe在第一次检测之前初始化值为Success，如果container没有提供LivenessProbe，则也认为是Success； httpGet: path: /health # 如果没有心跳检测接口就为/ port: 8080 scheme: HTTP initialDelaySeconds: 60 # 启动后延时多久开始运行检测 timeoutSeconds: 5 successThreshold: 1 failureThreshold: 5 readinessProbe: readinessProbe: httpGet: path: /health # 如果没有心跳检测接口就为/ port: 8080 scheme: HTTP initialDelaySeconds: 30 # 启动后延时多久开始运行检测 timeoutSeconds: 5 successThreshold: 1 failureThreshold: 5 resources: # CPU内存限制 requests: cpu: 2 memory: 2048Mi limits: cpu: 2 memory: 2048Mi env: # 通过环境变量的方式，直接传递pod=自定义Linux OS环境变量 - name: LOCAL_KEY # 本地Key value: value - name: CONFIG_MAP_KEY # 局策略可使用configMap的配置Key， valueFrom: configMapKeyRef: name: special-config # configmap中找到name为special-config key: special.type # 找到name为special-config里data下的key ports: - name: http containerPort: 8080 # 对service暴露端口 volumeMounts: # 挂载volumes中定义的磁盘 - name: log-cache mount: /tmp/log - name: sdb # 普通用法，该卷跟随容器销毁，挂载一个目录 mountPath: /data/media - name: nfs-client-root # 直接挂载硬盘方法，如挂载下面的nfs目录到/mnt/nfs mountPath: /mnt/nfs - name: example-volume-config # 高级用法第1种，将ConfigMap的log-script,backup-script分别挂载到/etc/config目录下的一个相对路径path/to/...下，如果存在同名文件，直接覆盖。 mountPath: /etc/config - name: rbd-pvc # 高级用法第2中，挂载PVC(PresistentVolumeClaim) # 使用volume将ConfigMap作为文件或目录直接挂载，其中每一个key-value键值对都会生成一个文件，key为文件名，value为内容， volumes: # 定义磁盘给上面volumeMounts挂载 - name: log-cache emptyDir: {} - name: sdb # 挂载宿主机上面的目录 hostPath: path: /any/path/it/will/be/replaced - name: example-volume-config # 供ConfigMap文件内容到指定路径使用 configMap: name: example-volume-config # ConfigMap中名称 items: - key: log-script # ConfigMap中的Key path: path/to/log-script # 指定目录下的一个相对路径path/to/log-script - key: backup-script # ConfigMap中的Key path: path/to/backup-script # 指定目录下的一个相对路径path/to/backup-script - name: nfs-client-root # 供挂载NFS存储类型 nfs: server: 10.42.0.55 # NFS服务器地址 path: /opt/public # showmount -e 看一下路径 - name: rbd-pvc # 挂载PVC磁盘 persistentVolumeClaim: claimName: rbd-pvc1 # 挂载已经申请的pvc磁盘 一个 Deployment 控制器为 Pods和 ReplicaSets提供描述性的更新方式。\n描述 Deployment 中的 desired state，并且 Deployment 控制器以受控速率更改实际状态，以达到期望状态。可以定义 Deployments 以创建新的 ReplicaSets ，或删除现有 Deployments ，并通过新的 Deployments 使用其所有资源。\n不要管理 Deployment 拥有的 ReplicaSets 。如果存在下面未介绍的用例，请考虑在主 Kubernetes 仓库中提出 issue。\nYou describe a desired state in a Deployment, and the Deployment controller changes the actual state to the desired state at a controlled rate. You can define Deployments to create new ReplicaSets, or to remove existing Deployments and adopt all their resources with new Deployments.\n\u0026ndash;\u0026gt; 描述 Deployment 中的 desired state，并且 Deployment 控制器以受控速率更改实际状态，以达到期望状态。可以定义 Deployments 以创建新的 ReplicaSets ，或删除现有 Deployments ，并通过新的 Deployments 使用其所有资源。\n用例 以下是典型的 Deployments 用例：\n 创建 Deployment 以展开 ReplicaSet 。 ReplicaSet 在后台创建 Pods。检查 ReplicaSet 展开的状态，查看其是否成功。   声明 Pod 的新状态 通过更新 Deployment 的 PodTemplateSpec。将创建新的 ReplicaSet ，并且 Deployment 管理器以受控速率将 Pod 从旧 ReplicaSet 移动到新 ReplicaSet 。每个新的 ReplicaSet 都会更新 Deployment 的修改历史。   回滚到较早的 Deployment 版本，如果 Deployment 的当前状态不稳定。每次回滚都会更新 Deployment 的修改。   扩展 Deployment 以承担更多负载.   暂停 Deployment  对其 PodTemplateSpec 进行修改，然后恢复它以启动新的展开。   使用 Deployment 状态 作为卡住展开的指示器。   清理较旧的 ReplicaSets  ，那些不在需要的。  创建 Deployment 下面是 Deployment 示例。创建一个 ReplicaSet 展开三个 nginx Pods：\ncodenew file=\u0026quot;controllers/nginx-deployment.yaml\u0026rdquo; \u0026gt;}}\n在该例中：\n 将创建名为 nginx-deployment 的 Deployment ，由 .metadata.name 字段指示。   Deployment 创建三个复制的 Pods，由 replicas 字段指示。   selector 字段定义 Deployment 如何查找要管理的 Pods。 在这种情况下，只需选择在 Pod 模板（app: nginx）中定义的标签。但是，更复杂的选择规则是可能的，只要 Pod 模板本身满足规则。  `matchLabels` 字段是 {key,value} 的映射。单个 {key,value}在 `matchLabels` 映射中的值等效于 `matchExpressions` 的元素，其键字段是“key”，运算符为“In”，值数组仅包含“value”。所有要求，从 `matchLabels` 和 `matchExpressions`，必须满足才能匹配。   template 字段包含以下子字段：   Pod 标记为app: nginx，使用labels字段。   Pod 模板规范或 .template.spec 字段指示 Pods 运行一个容器， nginx，运行 nginx Docker Hub版本1.7.9的镜像 。   创建一个容器并使用name字段将其命名为 nginx。  按照以下步骤创建上述 Deployment ：\n开始之前，请确保的 Kubernetes 集群已启动并运行。\n 通过运行以下命令创建 Deployment ：   可以指定 `--record` 标志来写入在资源注释`kubernetes.io/change-cause`中执行的命令。它对以后的检查是有用的。   例如，查看在每个 Deployment 修改中执行的命令。 ```shell kubectl apply -f https://k8s.io/examples/controllers/nginx-deployment.yaml ```  运行 kubectl get deployments 以检查 Deployment 是否已创建。如果仍在创建 Deployment ，则输出以下内容：  ```shell NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE nginx-deployment 3 0 0 0 1s ```  检查集群中的 Deployments 时，将显示以下字段：   * `NAME` 列出了集群中 Deployments 的名称。 * `DESIRED` 显示应用程序的所需 _副本_ 数，在创建 Deployment 时定义这些副本。这是 _期望状态_。 * `CURRENT`显示当前正在运行的副本数。 * `UP-TO-DATE`显示已更新以实现期望状态的副本数。 * `AVAILABLE`显示应用程序可供用户使用的副本数。 * `AGE` 显示应用程序运行的时间量。  请注意，根据`.spec.replicas`副本字段，所需副本的数量为 3。  要查看 Deployment 展开状态，运行 kubectl rollout status deployment.v1.apps/nginx-deployment。输出：  ```shell Waiting for rollout to finish: 2 out of 3 new replicas have been updated... deployment.apps/nginx-deployment successfully rolled out ```  几秒钟后再次运行 kubectl get deployments。输出：  ```shell NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE nginx-deployment 3 3 3 3 18s ```  请注意， Deployment 已创建所有三个副本，并且所有副本都是最新的（它们包含最新的 Pod 模板）并且可用。  要查看 Deployment 创建的 ReplicaSet （rs），运行 kubectl get rs。输出：  ```shell NAME DESIRED CURRENT READY AGE nginx-deployment-75675f5897 3 3 3 18s ```  请注意， ReplicaSet 的名称始终被格式化为`[DEPLOYMENT-NAME]-[RANDOM-STRING]`。随机字符串是随机生成并使用 pod-template-hash 作为种子。  要查看每个 Pod 自动生成的标签，运行 kubectl get pods --show-labels。返回以下输出：  ```shell NAME READY STATUS RESTARTS AGE LABELS nginx-deployment-75675f5897-7ci7o 1/1 Running 0 18s app=nginx,pod-template-hash=3123191453 nginx-deployment-75675f5897-kzszj 1/1 Running 0 18s app=nginx,pod-template-hash=3123191453 nginx-deployment-75675f5897-qqcnn 1/1 Running 0 18s app=nginx,pod-template-hash=3123191453 ```  创建的复制集可确保有三个 `nginx` Pods。  必须在 Deployment 中指定适当的选择器和 Pod 模板标签（在本例中为app: nginx）。不要与其他控制器（包括其他 Deployments 和状态设置）重叠标签或选择器。Kubernetes 不会阻止重叠，如果多个控制器具有重叠的选择器，这些控制器可能会冲突并运行意外。\nPod-template-hash 标签 不要更改此标签。\nDeployment 控制器将 pod-template-hash 标签添加到 Deployment 创建或使用的每个 ReplicaSet 。\n此标签可确保 Deployment 的子 ReplicaSets 不重叠。它通过对 ReplicaSet 的 PodTemplate 进行哈希处理，并使用生成的哈希值添加到 ReplicaSet 选择器、Pod 模板标签,并在 ReplicaSet 可能具有的任何现有 Pod 中。\n更新 Deployment 仅当 Deployment Pod 模板（即 .spec.template）时，才会触发 Deployment 展开，例如，如果模板的标签或容器镜像已更新，其他更新（如扩展 Deployment ）不会触发展开。\n按照以下步骤更新 Deployment ：\n 让我们更新 nginx Pods，以使用 nginx:1.9.1 镜像 ，而不是 nginx:1.7.9 镜像 。  ```shell kubectl --record deployment.apps/nginx-deployment set image deployment.v1.apps/nginx-deployment nginx=nginx:1.9.1 ```  输出： ```shell deployment.apps/nginx-deployment image updated ```  或者，可以 `edit` Deployment 并将 `.spec.template.spec.containers[0].image` 从 `nginx:1.7.9` 更改至 `nginx:1.9.1`。 ```shell kubectl edit deployment.v1.apps/nginx-deployment ```  输出： ```shell deployment.apps/nginx-deployment edited ```  要查看展开状态，运行：  ```shell kubectl rollout status deployment.v1.apps/nginx-deployment ```  输出： ```shell Waiting for rollout to finish: 2 out of 3 new replicas have been updated... ```  或者 ```shell deployment.apps/nginx-deployment successfully rolled out ```  在更新后的 Deployment 上获取更多信息\n  在展开成功后，可以通过运行 kubectl get deployments来查看 Deployment 。 输出：\nNAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE nginx-deployment 3 3 3 3 36s     运行 kubectl get rs 以查看 Deployment 通过创建新的 ReplicaSet 并缩放它更新了 Pods 最多 3 个副本，以及将旧 ReplicaSet 缩放到 0 个副本。\nkubectl get rs   输出： ```shell NAME DESIRED CURRENT READY AGE nginx-deployment-1564180365 3 3 3 6s nginx-deployment-2035384211 0 0 0 36s ```    运行 get pods 现在应仅显示新的 Pods:\nkubectl get pods   输出：\n```shell NAME READY STATUS RESTARTS AGE nginx-deployment-1564180365-khku8 1/1 Running 0 14s nginx-deployment-1564180365-nacti 1/1 Running 0 14s nginx-deployment-1564180365-z9gth 1/1 Running 0 14s ```  下次要更新这些 Pods 时，只需再次更新 Deployment Pod 模板。   Deployment 可确保在更新时仅关闭一定数量的 Pods。默认情况下，它确保至少 75%所需 Pods 运行（25%最大不可用）。   Deployment 还确保仅创建一定数量的 Pods 高于期望的 Pods 数。默认情况下，它可确保最多增加 25% 期望 Pods 数（25%最大增量）。  例如，如果仔细查看上述 Deployment ，将看到它首先创建了一个新的 Pod，然后删除了一些旧的 Pods，并创建了新的 Pods。它不会杀死老 Pods，直到有足够的数量新的 Pods 已经出现，并没有创造新的 Pods，直到足够数量的旧 Pods 被杀死。它确保至少 2 个 Pods 可用，并且总共最多 4 个 Pods 可用。   获取 Deployment 的更多信息 kubectl describe deployments   输出：\nName: nginx-deployment Namespace: default CreationTimestamp: Thu, 30 Nov 2017 10:56:25 +0000 Labels: app=nginx Annotations: deployment.kubernetes.io/revision=2 Selector: app=nginx Replicas: 3 desired | 3 updated | 3 total | 3 available | 0 unavailable StrategyType: RollingUpdate MinReadySeconds: 0 RollingUpdateStrategy: 25% max unavailable, 25% max surge Pod Template: Labels: app=nginx Containers: nginx: Image: nginx:1.9.1 Port: 80/TCP Environment: \u0026lt;none\u0026gt; Mounts: \u0026lt;none\u0026gt; Volumes: \u0026lt;none\u0026gt; Conditions: Type Status Reason ---- ------ ------ Available True MinimumReplicasAvailable Progressing True NewReplicaSetAvailable OldReplicaSets: \u0026lt;none\u0026gt; NewReplicaSet: nginx-deployment-1564180365 (3/3 replicas created) Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal ScalingReplicaSet 2m deployment-controller Scaled up replica set nginx-deployment-2035384211 to 3 Normal ScalingReplicaSet 24s deployment-controller Scaled up replica set nginx-deployment-1564180365 to 1 Normal ScalingReplicaSet 22s deployment-controller Scaled down replica set nginx-deployment-2035384211 to 2 Normal ScalingReplicaSet 22s deployment-controller Scaled up replica set nginx-deployment-1564180365 to 2 Normal ScalingReplicaSet 19s deployment-controller Scaled down replica set nginx-deployment-2035384211 to 1 Normal ScalingReplicaSet 19s deployment-controller Scaled up replica set nginx-deployment-1564180365 to 3 Normal ScalingReplicaSet 14s deployment-controller Scaled down replica set nginx-deployment-2035384211 to 0 可以看到，当第一次创建 Deployment 时，它创建了一个 ReplicaSet （nginx-deployment-2035384211）并将其直接扩展至 3 个副本。更新 Deployment 时，它创建了一个新的 ReplicaSet （nginx-deployment-1564180365），并将其扩展为 1，然后将旧 ReplicaSet 缩小到 2，以便至少有 2 个 Pod 可用，并且最多创建 4 个 Pod。然后，它继续向上和向下扩展新的和旧的 ReplicaSet ，具有相同的滚动更新策略。最后，将有 3 个可用的副本在新的 ReplicaSet 中，旧 ReplicaSet 将缩小到 0。  翻转（多 Deployment 动态更新） 每次 Deployment 控制器观察新 Deployment 时，都会创建一个 ReplicaSet 以启动所需的 Pods。如果更新了 Deployment ，则控制其标签的 Pods 的现有 ReplicaSet 匹配 .spec.selector，但其模板不匹配 .spec.template 被缩小。最终，新的 ReplicaSet 缩放为 .spec.replicas，所有旧 ReplicaSets 缩放为 0。\n当 Deployment 正在展开时进行更新， Deployment 会为每个更新创建一个新的 ReplicaSet 并开始向上扩展，之前的 ReplicaSet 会被添加到旧 ReplicaSets 队列并开始向下扩展。\n例如，假设创建一个 Deployment 以创建 nginx:1.7.9 的 5 个副本，然后更新 Deployment 以创建 5 个 nginx:1.9.1 的副本，而此时只有 3 个nginx:1.7.9 的副本已创建。在这种情况下， Deployment 会立即开始杀死3个 nginx:1.7.9 Pods，并开始创建 nginx:1.9.1 Pods。它不等待 nginx:1.7.9 的 5 个副本在改变任务之前完成创建。\n使用标签选择器进行更新 通常不鼓励更新标签选择器，建议提前规划选择器。在任何情况下，如果需要执行标签选择器更新，请格外小心，并确保已掌握所有的含义。\n在 API 版本 apps/v1 中， Deployment 标签选择器在创建后是不可变的。\n 选择器添加还需要使用新标签更新 Deployment 规范中的 Pod 模板标签，否则将返回验证错误。此更改是非重叠的，这意味着新的选择器不选择使用旧选择器创建的 ReplicaSets 和 Pod，从而导致弃用所有旧 ReplicaSets 和创建新的 ReplicaSet 。 选择器更新更改选择器键中的现有值 \u0026ndash; 导致发生与添加相同的行为。 选择器删除从 Deployment 选择器中删除现有密钥 \u0026ndash; 不需要在Pod 模板标签做任意更改。现有 ReplicaSets 不会孤立，并且不会创建新的 ReplicaSet ，但请注意，已删除的标签仍然存在于任何现有的 Pods 和 ReplicaSets 中。  回滚 Deployment 有时，可能需要回滚 Deployment ；例如，当 Deployment 不稳定时，例如循环崩溃。默认情况下，所有 Deployment 历史记录都保留在系统中，以便可以随时回滚（可以通过修改修改历史记录限制来更改该限制）。\n触发 Deployment 展开时，将创建 Deployment 修改版。这意味着仅当 Deployment Pod 模板 （.spec.template） 发生更改时，才会创建新修改版本，例如，如果更新模板的标签或容器镜像 。其他更新，如扩展 Deployment 、不要创建 Deployment 修改版，以便方便同时手动或自动缩放。这意味着，当回滚到较早的修改版时，只有 Deployment Pod 模板部分回滚。\n 假设在更新 Deployment 时犯了一个拼写错误，将镜像名称命名为 nginx:1.91 而不是 nginx:1.9.1： kubectl set image deployment.v1.apps/nginx-deployment nginx=nginx:1.91 --record=true   输出： ```shell deployment.apps/nginx-deployment image updated ```    展开遇到问题。可以通过检查展开状态来验证它：\nkubectl rollout status deployment.v1.apps/nginx-deployment   输出： ```shell Waiting for rollout to finish: 1 out of 3 new replicas have been updated... ```   按 Ctrl-C 停止上述展开状态表。有关卡住展开的详细信息，参考这里   查看旧 ReplicaSets ： kubectl get rs   输出： ```shell NAME DESIRED CURRENT READY AGE nginx-deployment-1564180365 3 3 3 25s nginx-deployment-2035384211 0 0 0 36s nginx-deployment-3066724191 1 1 0 6s ```    查看创建的 Pod，看到由新 ReplicaSet 创建的 1 个 Pod 卡在镜像拉取循环中。\nkubectl get pods   输出： ```shell NAME READY STATUS RESTARTS AGE nginx-deployment-1564180365-70iae 1/1 Running 0 25s nginx-deployment-1564180365-jbqqo 1/1 Running 0 25s nginx-deployment-1564180365-hysrc 1/1 Running 0 25s nginx-deployment-3066724191-08mng 0/1 ImagePullBackOff 0 6s ```   Deployment 控制器自动停止不良展开，并停止向上扩展新的 ReplicaSet 。这取决于指定的滚动更新参数（具体为 `maxUnavailable`）。默认情况下，Kubernetes 将值设置为 25%。   获取 Deployment 描述信息： kubectl describe deployment   输出： ```shell Name: nginx-deployment Namespace: default CreationTimestamp: Tue, 15 Mar 2016 14:48:04 -0700 Labels: app=nginx Selector: app=nginx Replicas: 3 desired | 1 updated | 4 total | 3 available | 1 unavailable StrategyType: RollingUpdate MinReadySeconds: 0 RollingUpdateStrategy: 25% max unavailable, 25% max surge Pod Template: Labels: app=nginx Containers: nginx: Image: nginx:1.91 Port: 80/TCP Host Port: 0/TCP Environment: \u0026lt;none\u0026gt; Mounts: \u0026lt;none\u0026gt; Volumes: \u0026lt;none\u0026gt; Conditions: Type Status Reason ---- ------ ------ Available True MinimumReplicasAvailable Progressing True ReplicaSetUpdated OldReplicaSets: nginx-deployment-1564180365 (3/3 replicas created) NewReplicaSet: nginx-deployment-3066724191 (1/1 replicas created) Events: FirstSeen LastSeen Count From SubobjectPath Type Reason Message --------- -------- ----- ---- ------------- -------- ------ ------- 1m 1m 1 {deployment-controller } Normal ScalingReplicaSet Scaled up replica set nginx-deployment-2035384211 to 3 22s 22s 1 {deployment-controller } Normal ScalingReplicaSet Scaled up replica set nginx-deployment-1564180365 to 1 22s 22s 1 {deployment-controller } Normal ScalingReplicaSet Scaled down replica set nginx-deployment-2035384211 to 2 22s 22s 1 {deployment-controller } Normal ScalingReplicaSet Scaled up replica set nginx-deployment-1564180365 to 2 21s 21s 1 {deployment-controller } Normal ScalingReplicaSet Scaled down replica set nginx-deployment-2035384211 to 1 21s 21s 1 {deployment-controller } Normal ScalingReplicaSet Scaled up replica set nginx-deployment-1564180365 to 3 13s 13s 1 {deployment-controller } Normal ScalingReplicaSet Scaled down replica set nginx-deployment-2035384211 to 0 13s 13s 1 {deployment-controller } Normal ScalingReplicaSet Scaled up replica set nginx-deployment-3066724191 to 1 ```  要解决此问题，需要回滚到以前稳定的 Deployment 版本。\n检查 Deployment 展开历史 按照如下步骤检查回滚历史：\n 首先，检查 Deployment 修改历史：  ```shell kubectl rollout history deployment.v1.apps/nginx-deployment ```  输出： ```shell deployments \u0026quot;nginx-deployment\u0026quot; REVISION CHANGE-CAUSE 1 kubectl apply --filename=https://k8s.io/examples/controllers/nginx-deployment.yaml --record=true 2 kubectl set image deployment.v1.apps/nginx-deployment nginx=nginx:1.9.1 --record=true 3 kubectl set image deployment.v1.apps/nginx-deployment nginx=nginx:1.91 --record=true ```  `CHANGE-CAUSE` 从 Deployment 注释 `kubernetes.io/change-cause` 创建时复制到其修改版。可以通过以下条件指定 `CHANGE-CAUSE` 消息：  * 使用 `kubectl annotate deployment.v1.apps/nginx-deployment kubernetes.io/change-cause=\u0026quot;image updated to 1.9.1\u0026quot;` Deployment 对 Deployment 进行分号。 * 追加 `--record` 以保存正在更改资源的 `kubectl` 命令。 * 手动编辑资源的清单。  查看修改历史的详细信息，运行：  ```shell kubectl rollout history deployment.v1.apps/nginx-deployment --revision=2 ```  输出： ```shell deployments \u0026quot;nginx-deployment\u0026quot; revision 2 Labels: app=nginx pod-template-hash=1159050644 Annotations: kubernetes.io/change-cause=kubectl set image deployment.v1.apps/nginx-deployment nginx=nginx:1.9.1 --record=true Containers: nginx: Image: nginx:1.9.1 Port: 80/TCP QoS Tier: cpu: BestEffort memory: BestEffort Environment Variables: \u0026lt;none\u0026gt; No volumes. ```  回滚到上一次修改 按照下面给出的步骤将 Deployment 从当前版本回滚到以前的版本（即版本 2）。\n 现在已决定撤消当前展开并回滚到以前的版本：  ```shell kubectl rollout undo deployment.v1.apps/nginx-deployment ```  输出： ```shell deployment.apps/nginx-deployment ```  或者，可以通过使用 `--to-revision` 来回滚到特定修改版本： ```shell kubectl rollout undo deployment.v1.apps/nginx-deployment --to-revision=2 ```  输出： ```shell deployment.apps/nginx-deployment ```  更多有关回滚相关指令，请参考 [`kubectl rollout`](/docs/reference/generated/kubectl/kubectl-commands#rollout).  现在， Deployment 将回滚到以前的稳定版本。如所见， Deployment 回滚事件回滚到修改版 2 是从 Deployment 控制器生成的。  检查回滚是否成功、 Deployment 是否正在运行，运行：  ```shell kubectl get deployment nginx-deployment ```  输出： ```shell NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE nginx-deployment 3 3 3 3 30m ```  获取 Deployment 描述信息：  ```shell kubectl describe deployment nginx-deployment ```  输出： ```shell Name: nginx-deployment Namespace: default CreationTimestamp: Sun, 02 Sep 2018 18:17:55 -0500 Labels: app=nginx Annotations: deployment.kubernetes.io/revision=4 kubernetes.io/change-cause=kubectl set image deployment.v1.apps/nginx-deployment nginx=nginx:1.9.1 --record=true Selector: app=nginx Replicas: 3 desired | 3 updated | 3 total | 3 available | 0 unavailable StrategyType: RollingUpdate MinReadySeconds: 0 RollingUpdateStrategy: 25% max unavailable, 25% max surge Pod Template: Labels: app=nginx Containers: nginx: Image: nginx:1.9.1 Port: 80/TCP Host Port: 0/TCP Environment: \u0026lt;none\u0026gt; Mounts: \u0026lt;none\u0026gt; Volumes: \u0026lt;none\u0026gt; Conditions: Type Status Reason ---- ------ ------ Available True MinimumReplicasAvailable Progressing True NewReplicaSetAvailable OldReplicaSets: \u0026lt;none\u0026gt; NewReplicaSet: nginx-deployment-c4747d96c (3/3 replicas created) Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal ScalingReplicaSet 12m deployment-controller Scaled up replica set nginx-deployment-75675f5897 to 3 Normal ScalingReplicaSet 11m deployment-controller Scaled up replica set nginx-deployment-c4747d96c to 1 Normal ScalingReplicaSet 11m deployment-controller Scaled down replica set nginx-deployment-75675f5897 to 2 Normal ScalingReplicaSet 11m deployment-controller Scaled up replica set nginx-deployment-c4747d96c to 2 Normal ScalingReplicaSet 11m deployment-controller Scaled down replica set nginx-deployment-75675f5897 to 1 Normal ScalingReplicaSet 11m deployment-controller Scaled up replica set nginx-deployment-c4747d96c to 3 Normal ScalingReplicaSet 11m deployment-controller Scaled down replica set nginx-deployment-75675f5897 to 0 Normal ScalingReplicaSet 11m deployment-controller Scaled up replica set nginx-deployment-595696685f to 1 Normal DeploymentRollback 15s deployment-controller Rolled back deployment \u0026quot;nginx-deployment\u0026quot; to revision 2 Normal ScalingReplicaSet 15s deployment-controller Scaled down replica set nginx-deployment-595696685f to 0 ```  缩放 Deployment 可以使用如下指令缩放 Deployment ：\nkubectl scale deployment.v1.apps/nginx-deployment --replicas=10 输出：\ndeployment.apps/nginx-deployment scaled 假设启用水平自动缩放 Pod在集群中，可以为 Deployment 设置自动缩放器，并选择最小和最大 要基于现有 Pods 的 CPU 利用率运行的 Pods。\nkubectl autoscale deployment.v1.apps/nginx-deployment --min=10 --max=15 --cpu-percent=80 输出：\ndeployment.apps/nginx-deployment scaled 比例缩放 滚动更新 Deployments 支持同时运行应用程序的多个版本。当自动缩放器缩放处于展开中间的滚动更新 Deployment （仍在进行中或暂停）时， Deployment 控制器平衡现有活动中的其他 ReplicaSets （带 Pods 的 ReplicaSets ），以降低风险。这称为比例缩放。\n例如，运行一个10个副本的 Deployment ，最大增加=3， 最大不可用=2.\n 确保这10个副本都在运行。 kubectl get deploy   输出：\nNAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE nginx-deployment 10 10 10 10 50s  更新到新镜像，该镜像恰好无法从集群内部解析。 kubectl set image deployment.v1.apps/nginx-deployment nginx=nginx:sometag   输出： ```shell deployment.apps/nginx-deployment image updated ```   镜像更新使用 ReplicaSet nginx-deployment-1989198191 启动新的展开，但由于上面提到的最大不可用要求。检查展开状态： kubectl get rs    输出： ```shell NAME DESIRED CURRENT READY AGE nginx-deployment-1989198191 5 5 0 9s nginx-deployment-618515232 8 8 8 1m ```   然后，出现了新的 Deployment 扩展请求。自动缩放器增加 Deployment 副本到15。 Deployment 控制器需要决定在何处添加这些新 5 个副本。如果未使用比例缩放，所有 5 个都将添加到新的 ReplicaSet 中。使用比例缩放，可以将其他副本分布到所有 ReplicaSets 。更大的比例转到 ReplicaSets 与大多数副本和较低的比例都转到副本较少的 ReplicaSets 。任何剩余部分都添加到具有最多副本的 ReplicaSet 。具有零副本的 ReplicaSets 不会放大。  在上面的示例中，3 个副本添加到旧 ReplicaSet 中，2 个副本添加到新 ReplicaSet 。展开过程最终应将所有副本移动到新的 ReplicaSet ，假定新的副本变得正常。要确认这一点，请运行：\nkubectl get deploy 输出：\nNAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE nginx-deployment 15 18 7 8 7m 展开状态确认副本如何添加到每个 ReplicaSet 。\nkubectl get rs 输出：\nNAME DESIRED CURRENT READY AGE nginx-deployment-1989198191 7 7 0 7m nginx-deployment-618515232 11 11 11 7m 暂停、恢复 Deployment 可以在触发一个或多个更新之前暂停 Deployment ，然后继续它。这允许在暂停和恢复之间应用多个修补程序，而不会触发不必要的 Deployment 。\n  例如，对于一个刚刚创建的 Deployment ： 获取 Deployment 信息：\nkubectl get deploy   输出：\nNAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE nginx 3 3 3 3 1m 获取 Deployment 状态：\nkubectl get rs 输出：\nNAME DESIRED CURRENT READY AGE nginx-2142116321 3 3 3 1m 使用如下指令中断运行：\n```shell kubectl rollout pause deployment.v1.apps/nginx-deployment ```  输出： ```shell deployment.apps/nginx-deployment paused ```    然后更新 Deployment 镜像：\nkubectl set image deployment.v1.apps/nginx-deployment nginx=nginx:1.9.1   输出： ```shell deployment.apps/nginx-deployment image updated ```    注意没有新的展开：\nkubectl rollout history deployment.v1.apps/nginx-deployment   输出： ```shell deployments \u0026quot;nginx\u0026quot; REVISION CHANGE-CAUSE 1 \u0026lt;none\u0026gt; ```    获取展开状态确保 Deployment 更新已经成功：\nkubectl get rs   输出： ```shell NAME DESIRED CURRENT READY AGE nginx-2142116321 3 3 3 2m ```    更新是很容易的，例如，可以这样更新使用到的资源：\nkubectl set resources deployment.v1.apps/nginx-deployment -c=nginx --limits=cpu=200m,memory=512Mi   输出： ```shell deployment.apps/nginx-deployment resource requirements updated ```  暂停 Deployment 之前的初始状态将继续其功能，但新的更新只要暂停 Deployment ， Deployment 就不会产生任何效果。    最后，恢复 Deployment 并观察新的 ReplicaSet ，并更新所有新的更新：\nkubectl rollout resume deployment.v1.apps/nginx-deployment   输出： ```shell deployment.apps/nginx-deployment resumed ```    观察展开的状态，直到完成。\nkubectl get rs -w   输出： ```shell NAME DESIRED CURRENT READY AGE nginx-2142116321 2 2 2 2m nginx-3926361531 2 2 0 6s nginx-3926361531 2 2 1 18s nginx-2142116321 1 2 2 2m nginx-2142116321 1 2 2 2m nginx-3926361531 3 2 1 18s nginx-3926361531 3 2 1 18s nginx-2142116321 1 1 1 2m nginx-3926361531 3 3 1 18s nginx-3926361531 3 3 2 19s nginx-2142116321 0 1 1 2m nginx-2142116321 0 1 1 2m nginx-2142116321 0 0 0 2m nginx-3926361531 3 3 3 20s ```    获取最近展开的状态：\nkubectl get rs   输出： ```shell NAME DESIRED CURRENT READY AGE nginx-2142116321 0 0 0 2m nginx-3926361531 3 3 3 28s ```  可以在触发一个或多个更新之前暂停 Deployment ，然后继续它。这允许在暂停和恢复之间应用多个修补程序，而不会触发不必要的 Deployment 。\n 例如，对于一个刚刚创建的 Deployment ： 获取 Deployment 信息： kubectl get deploy   输出：\nNAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE nginx 3 3 3 3 1m 获取 Deployment 状态：\nkubectl get rs 输出：\nNAME DESIRED CURRENT READY AGE nginx-2142116321 3 3 3 1m 使用如下指令中断运行： shell kubectl rollout pause deployment.v1.apps/nginx-deployment\n输出： ``` deployment.apps/nginx-deployment paused ```   然后更新 Deployment 镜像： kubectl set image deployment.v1.apps/nginx-deployment nginx=nginx:1.9.1   输出： ``` deployment.apps/nginx-deployment image updated ```   注意没有新的展开： kubectl rollout history deployment.v1.apps/nginx-deployment   输出： ``` deployments \u0026quot;nginx\u0026quot; REVISION CHANGE-CAUSE 1 \u0026lt;none\u0026gt; ```   获取展开状态确保 Deployment 更新已经成功： kubectl get rs   输出： ``` NAME DESIRED CURRENT READY AGE nginx-2142116321 3 3 3 2m ```   更新是很容易的，例如，可以这样更新使用到的资源： kubectl set resources deployment.v1.apps/nginx-deployment -c=nginx --limits=cpu=200m,memory=512Mi   输出： ``` deployment.apps/nginx-deployment resource requirements updated ```  暂停 Deployment 之前的初始状态将继续其功能，但新的更新只要暂停 Deployment ， Deployment 就不会产生任何效果。   最后，恢复 Deployment 并观察新的 ReplicaSet ，并更新所有新的更新： kubectl rollout resume deployment.v1.apps/nginx-deployment   输出： ``` deployment.apps/nginx-deployment resumed ```   观察展开的状态，直到完成。 kubectl get rs -w   输出： ``` NAME DESIRED CURRENT READY AGE nginx-2142116321 2 2 2 2m nginx-3926361531 2 2 0 6s nginx-3926361531 2 2 1 18s nginx-2142116321 1 2 2 2m nginx-2142116321 1 2 2 2m nginx-3926361531 3 2 1 18s nginx-3926361531 3 2 1 18s nginx-2142116321 1 1 1 2m nginx-3926361531 3 3 1 18s nginx-3926361531 3 3 2 19s nginx-2142116321 0 1 1 2m nginx-2142116321 0 1 1 2m nginx-2142116321 0 0 0 2m nginx-3926361531 3 3 3 20s ```   获取最近展开的状态： kubectl get rs   输出： ``` NAME DESIRED CURRENT READY AGE nginx-2142116321 0 0 0 2m nginx-3926361531 3 3 3 28s ```  暂停的 Deployment 不可以回滚，除非恢复它以后。\nDeployment 状态 一个 Deployment 的生命周期中会有许多状态。当正在生产新的 ReplicaSet 时可能是正在运行，可能是已完成，也可能是Deployment 失败。\n正在 Deployment Kubernetes 使用 运行中 来标记一个 Deployment ，当下面的任务被执行时：\n 创建新的 ReplicaSet 。 正在向上扩展最新的 ReplicaSet 。 Deployment 向下扩展旧的 ReplicaSet(s) 。 新的 Pods 已经就绪或者可用（在最小就绪时间内就绪）。  可以使用 kubectl rollout status 监视 Deployment 的进度。\n完成 Deployment Kubernetes 将 Deployment 标记为 完成，当它具有以下特征时：\n 与 Deployment 关联的所有副本都已更新到指定的最新版本，这意味着请求的任何更新都已完成。   与 Deployment 关联的所有副本都可用。   未运行 Deployment 的旧副本。  可以使用 kubectl rollout status 检查 Deployment 是否已完成。如果展开成功完成，kubectl rollout status 返回退出代码 0。\nkubectl rollout status deployment.v1.apps/nginx-deployment 输出：\nWaiting for rollout to finish: 2 of 3 updated replicas are available... deployment.apps/nginx-deployment successfully rolled out $ echo $? 0 Deployment 失败 你的 Deployment 可能会在未完成的情况下尝试 Deployment 其最新的 ReplicaSet 时遇到问题。可能发生此情况由于以下一些因素：\n 配额不足 就绪探测失败 镜像拉取错误 权限不足 限制范围 应用程序运行时配置错误  检测此条件的一种方法是在 Deployment 规范中指定截止时间参数：（[.spec.progressDeadlineSeconds]（#progress-deadline-seconds））。.spec.progressDeadlineSeconds 进度截止时间秒表示 Deployment 控制器在指示（处于 Deployment 状态）之前等待的秒数 Deployment 进度已停止。\n以下 kubectl 命令设置具有进度的规范，使控制器报告 10 分钟后 Deployment 进度不足：\nkubectl patch deployment.v1.apps/nginx-deployment -p \u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;progressDeadlineSeconds\u0026#34;:600}}\u0026#39; 输出：\ndeployment.apps/nginx-deployment patched 超过截止时间后， Deployment 控制器将添加具有以下属性到 Deployment 的 .status.conditions ：\n Type=Progressing Status=False Reason=ProgressDeadlineExceeded  参考 Kubernetes API 约定 获取更多状态条件相关信息。\nKubernetes 对已停止的 Deployment 不执行任何操作，只需使用Reason=ProgressDeadlineExceeded。更高级别的编排器可以利用它并相应地采取行动，例如，将 Deployment 回滚到其以前的版本。\n如果暂停 Deployment ，Kubernetes 不会根据指定的截止时间检查进度。可以在展开栏中间安全地暂停 Deployment ，并在不触发超过最后期限时恢复。\nDeployments 可能会出现短暂的错误，既不是因为设置的超时时间过短，也不是因为任何真正的暂时性错误。例如配额不足。如果描述 Deployment ，将注意到以下部分：\nkubectl describe deployment nginx-deployment 输出：\n\u0026lt;...\u0026gt; Conditions: Type Status Reason ---- ------ ------ Available True MinimumReplicasAvailable Progressing True ReplicaSetUpdated ReplicaFailure True FailedCreate \u0026lt;...\u0026gt; 如果运行 kubectl get deployment nginx-deployment -o yaml， Deployment 状态输出：\nstatus: availableReplicas: 2 conditions: - lastTransitionTime: 2016-10-04T12:25:39Z lastUpdateTime: 2016-10-04T12:25:39Z message: Replica set \u0026#34;nginx-deployment-4262182780\u0026#34; is progressing. reason: ReplicaSetUpdated status: \u0026#34;True\u0026#34; type: Progressing - lastTransitionTime: 2016-10-04T12:25:42Z lastUpdateTime: 2016-10-04T12:25:42Z message: Deployment has minimum availability. reason: MinimumReplicasAvailable status: \u0026#34;True\u0026#34; type: Available - lastTransitionTime: 2016-10-04T12:25:39Z lastUpdateTime: 2016-10-04T12:25:39Z message: \u0026#39;Error creating: pods \u0026#34;nginx-deployment-4262182780-\u0026#34; is forbidden: exceeded quota: object-counts, requested: pods=1, used: pods=3, limited: pods=2\u0026#39; reason: FailedCreate status: \u0026#34;True\u0026#34; type: ReplicaFailure observedGeneration: 3 replicas: 2 unavailableReplicas: 2 最终，一旦超过 Deployment 进度截止时间，Kubernetes 将更新状态和进度状态：\nConditions: Type Status Reason ---- ------ ------ Available True MinimumReplicasAvailable Progressing False ProgressDeadlineExceeded ReplicaFailure True FailedCreate 可以通过缩减 Deployment 来解决配额不足的问题，或者直接在命名空间中增加配额。如果配额条件满足， Deployment 控制器完成了 Deployment 展开， Deployment 状态会更新为成功（Status=True and Reason=NewReplicaSetAvailable）。\nConditions: Type Status Reason ---- ------ ------ Available True MinimumReplicasAvailable Progressing True NewReplicaSetAvailable Type=Available和 Status=True 表示 Deployment 具有最低可用性。最低可用性由 Deployment 策略中的参数指定。Type=Progressing 和 Status=True 表示 Deployment 处于展开中间，并且正在运行，或者已成功完成进度，最小所需新的副本处于可用（请参阅此种状态原因的相关细节，在我们的案例中Reason=NewReplicaSetAvailable 表示 Deployment 已完成）。\n可以使用 kubectl rollout status 检查 Deployment 是否未能取得进展。kubectl rollout status如果 Deployment 已超过进度截止时间，则返回非零退出代码。\nkubectl rollout status deployment.v1.apps/nginx-deployment 输出：\nWaiting for rollout to finish: 2 out of 3 new replicas have been updated... error: deployment \u0026#34;nginx\u0026#34; exceeded its progress deadline $ echo $? 1 对失败 Deployment 的操作 应用于完整 Deployment 的所有操作也适用于失败的 Deployment 。可以向上/向下扩展，回滚到以前的修改版，或者如果需要在 Deployment Pod 模板中应用多个调整，甚至将其暂停。\n清理策略 可以在 Deployment 中设置 .spec.revisionHistoryLimit ，以指定保留多少此 Deployment 的 ReplicaSets。其余的将在后台进行垃圾回收。默认情况下，是10。\n显式将此字段设置为 0 将导致清理 Deployment 的所有历史记录，因此 Deployment 将无法回滚。\n金丝雀 Deployment 如果要使用 Deployment 向用户或服务器子集展开版本，则可以创建多个 Deployments ，每个版本一个，遵循资源管理。\n编写 Deployment 脚本 同其他 Kubernetes 配置， Deployment 需要 apiVersion， kind， 和 metadata 字段。有关配置文件的其他信息，参考 应用 Deployment ，配置容器，和 使用 kubectl 管理资源 相关文档。\nDeployment 还需要 .spec 部分。\nPod 示例 .spec仅需要 .spec.template 和 .spec.selector。\n.spec.template 是一个 Pod 示例。它和 Pod的约束完全相同，除了它是嵌套的，而且没有 apiVersion 或 kind。\n除了 Pod 的必填字段外， Deployment 中的 Pod 模板必须指定适当的标签和适当的重新启动策略。对于标签，请确保不要与其他控制器重叠。请参考选择器)。\n只有 .spec.template.spec.restartPolicy 等于 Always 被允许，这是在没有指定时的默认设置。\n副本 .spec.replicas 是指定所需 Pod 的可选字段。它的默认值是1。\n选择器 .spec.selector 是指定本次 Deployment Pods 标签选择器的必要字段。\n.spec.selector 必须匹配 .spec.template.metadata.labels，否则请求会被 API 拒绝。\n在 API apps/v1版本中，.spec.selector 和 .metadata.labels 不会被默认设置为 .spec.template.metadata.labels，如果没有设置的话。所以需要明确进行设置。同时在 apps/v1版本中， Deployment 创建后 .spec.selector 是可变的。\n当 Pods 的标签和选择器匹配时，此类 Pods 的模板和 .spec.template 不同，或者此类 Pods 的总数超过 .spec.replicas， Deployment 会终结这些 Pods。如果 Pods 总数达不到期望值，会用 .spec.template 创建新的 Pods。\n不应创建其标签与此选择器匹配的 Pods，或者直接创建另一个 Deployment ，或通过创建其他控制器（如 ReplicaSet 或复制控制器）。如果这样做，第一个 Deployment 认为它创建了这些其他 Pods。Kubernetes 不会阻止你这么做。\n如果有多个具有重叠选择器的控制器，则控制器之间会因冲突而故障。\n策略 .spec.strategy 策略指定用于用新 Pods 替换旧 Pods 的策略。.spec.strategy.type 可以是“Recreate”或“RollingUpdate”。“RollingUpdate”是默认值。\n重新创建 Deployment 当 .spec.strategy.type==Recreate,所有现有的 Pods 在创建新 Pods 之前被杀死。\n滚动更新 Deployment Deployment 会在 .spec.strategy.type==RollingUpdate时，采取 滚动更新的方式更新Pods。可以指定 maxUnavailable 和 maxSurge 来控制滚动更新操作。\n最大不可用 .spec.strategy.rollingUpdate.maxUnavailable 是指定最大数量的可选字段，表示在更新过程中不可用的 Pods。该值可以是绝对数字（例如，5）或所需 Pods 的百分比（例如，10%）。绝对数按百分比计算，四舍五入下来。如果 .spec.strategy.rollingUpdate.maxSurge 为 0，则该值不能为 0。默认值为 25%。\n例如，当此值设置为 30% 时，滚动更新开始时会立即将旧 ReplicaSet 向下扩展到期望 Pods 的70%。新 Pods 准备就绪后，可以缩放旧 ReplicaSet 进一步向下，然后向上扩展新的 ReplicaSet ，确保可用的 Pods 总数在更新期间，任何时候都至少为 70% 所需的 Pods。\n最大增量 .spec.strategy.rollingUpdate.maxSurge 是指定最大 Pods 数的可选字段可在所需的 Pods 数上创建。该值可以是绝对数（例如，5）或所需 Pods 的百分比（例如，10%）。如果 MaxUnavailable 0，则值不能为 0。绝对数通过舍入从百分比计算。默认值为 25%。\n例如，当此值设置为 30% 时，启动滚动更新后，会立即展开新的 ReplicaSet ，以便新旧 Pod 的总数不超过所需的 130%。一旦旧 Pods 被杀死，新的 ReplicaSet 可以进一步扩展，确保更新期间任何时间运行的 Pods 总数最多为所需 Pods 总数的130%。\nDeployment 失败等待时间 .spec.progressDeadlineSeconds 是一个可选字段，用于指定等待的秒数而后在系统报告中返回Deployment 失败，同时在资源状态中 Type=Progressing、 Status=False 、 Reason=ProgressDeadlineExceeded 。 Deployment 控制器将保留正在重试 Deployment 。将来，一旦实现自动回滚， Deployment 控制器将回滚 Deployment ，只要它探测到这样的条件。\n如果指定，则此字段需要大于 .spec.minReadySeconds。\n最小就绪时间 .spec.minReadySeconds 是一个可选字段，用于指定新创建的 Pod 在没有任意容器崩溃情况下的最小就绪时间，以便将其视为可用。默认值为 0（Pod 在准备就绪后立即将被视为可用）。了解有关何时Pod 被视为已准备就绪，参考容器探针。\n回滚 .spec.rollbackTo 字段已经在 API 版本 extensions/v1beta1 和 apps/v1beta1中废弃了，并且从 apps/v1beta2版本开始不在支持。相应的，会开始使用已经引入回滚到上一个版本中的 kubectl rollout undo。\n修改历史限制 Deployment 修改历史记录存储在它所控制的 ReplicaSets 中。\n.spec.revisionHistoryLimit 修改历史记录限制是一个可选字段，用于指定要保留的旧 ReplicaSets 的数量以允许回滚。这些旧 ReplicaSets 消耗 etcd 中的资源，并占用 kubectl get rs 的输出。每个 Deployment 修改版的配置都存储在其 ReplicaSets 中；因此，一旦删除了旧的 ReplicaSet ，将失去回滚到 Deployment 版本的能力。默认情况下，将保留 10 个旧 ReplicaSets ，但其理想值取决于新 Deployment 的频率和稳定性。\n更具体地说，将此字段设置为 0 意味着将清理所有具有 0 个副本的旧 ReplicaSets 。在这种情况下，无法撤消新的 Deployment 展开，因为它的修改历史被清除了。\n暂停 .spec.paused 是用于暂停和恢复 Deployment 的可选布尔字段。暂停的 Deployment 和未暂停的 Deployment 唯一的区别，只要暂停 Deployment 处于暂停状态， PodTemplateSpec 的任意修改都不会触发新的展开。默认 Deployment 在创建时是不会被暂停的。\nDeployments 的替代方案 kubectl滚动更新 kubectl rolling update更新 Pods 和副本控制器的方式类似。但是，建议采取 Deployments 的方式来更新，因为它们是声明性的，在服务器端，并且具有其他功能，例如，即使在滚动更新完成后，也会回滚到以前的任何修改版本。\n"
},
{
	"uri": "https://lijun.in/reference/command-line-tools-reference/kube-proxy/",
	"title": "kube-proxy",
	"tags": [],
	"description": "",
	"content": ". heading \u0026ldquo;synopsis\u0026rdquo; %}} Kubernetes 网络代理在每个节点上运行。网络代理反映了每个节点上 Kubernetes API 中定义的服务，并且可以执行简单的 TCP、UDP 和 SCTP 流转发，或者在一组后端进行循环 TCP、UDP 和 SCTP 转发。当前可通过 Docker-links-compatible 环境变量找到服务集群 IP 和端口，这些环境变量指定了服务代理打开的端口。有一个可选的插件，可以为这些集群 IP 提供集群 DNS。用户必须使用 apiserver API 创建服务才能配置代理。\nkube-proxy [flags] . heading \u0026ldquo;options\u0026rdquo; %}} "
},
{
	"uri": "https://lijun.in/reference/setup-tools/kubeadm/kubeadm-join/",
	"title": "kubeadm join",
	"tags": [],
	"description": "",
	"content": "此命令用来初始化 Kubernetes 工作节点并将其加入集群。\n. include \u0026ldquo;generated/kubeadm_join.md\u0026rdquo; \u0026gt;}}\n加入流程 kubeadm join 初始化 Kubernetes 工作节点并将其加入集群。 该操作过程包含下面几个步骤：\n  kubeadm 从 API 服务器下载必要的集群信息。 默认情况下，它使用引导令牌和 CA 密钥哈希来验证数据的真实性。 也可以通过文件或 URL 直接发现根 CA。\n  如果调用 kubeadm 时启用了 --feature-gates=DynamicKubeletConfig，它首先从主机上检索 kubelet 初始化配置并将其写入磁盘。 当 kubelet 启动时，kubeadm 更新节点的 Node.spec.configSource 属性。 进一步了解动态 kubelet 配置 请参考 使用配置文件设置 Kubelet 参数 和 重新配置集群中节点的 Kubelet。\n  一旦知道集群信息，kubelet 就可以开始 TLS 引导过程。 TLS 引导程序使用共享令牌与 Kubernetes API 服务器进行临时的身份验证，以提交证书签名请求 (CSR)； 默认情况下，控制平面自动对该 CSR 请求进行签名。\n  最后，kubeadm 配置本地 kubelet 使用分配给节点的确定标识连接到 API 服务器。\n  发现要信任的集群 CA Kubeadm 的发现有几个选项，每个选项都有安全性上的优缺点。 适合您的环境的正确方法取决于节点是如何准备的以及您对网络的安全性期望和节点的生命周期特点。\n带 CA 锁定模式的基于令牌的发现 这是 Kubernetes 1.8 及以上版本中的默认模式。 在这种模式下，kubeadm 下载集群配置（包括根CA）并使用令牌验证它，并且会验证根 CA 的公钥与所提供的哈希是否匹配，以及 API 服务器证书在根 CA 下是否有效。\nCA 键哈希格式为 sha256:\u0026lt;hex_encoded_hash\u0026gt;。 默认情况下，在 kubeadm init 最后打印的 kubeadm join 命令或者 kubeadm token create --print-join-command 的输出信息中返回哈希值。 它使用标准格式 (请参考 RFC7469) 并且也能通过第三方工具或者驱动系统进行计算。 例如，使用 OpenSSL CLI：\nopenssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2\u0026gt;/dev/null | openssl dgst -sha256 -hex | sed \u0026#39;s/^.* //\u0026#39; kubeadm join 命令示例\nkubeadm join --discovery-token abcdef.1234567890abcdef --discovery-token-ca-cert-hash sha256:1234..cdef 1.2.3.4:6443 优势：\n  允许引导节点安全地发现主节点的信任根，即使其他工作节点或网络受到损害。\n  方便手动执行，因为所需的所有信息都适合于易于复制和粘贴的单个 kubeadm join 命令。\n  劣势：\n CA 哈希通常在主节点被提供之前是不知道的，这使得构建使用 kubeadm 的自动化配置工具更加困难。 通过预先生成CA，您可以解决这个限制。  无 CA 锁定模式的基于令牌的发现 _这是 Kubernetes 1.7 和早期版本_中的默认设置；使用时要注意一些重要的补充说明。 此模式仅依赖于对称令牌来签名(HMAC-SHA256)发现信息，这些发现信息为主节点建立信任根。 在 Kubernetes 1.8 及以上版本中仍然可以使用 --discovery-token-unsafe-skip-ca-verification 参数，但是如果可能的话，您应该考虑使用一种其他模式。\nkubeadm join 命令示例\nkubeadm join --token abcdef.1234567890abcdef --discovery-token-unsafe-skip-ca-verification 1.2.3.4:6443` 优势\n  仍然可以防止许多网络级攻击。\n  可以提前生成令牌并与主节点和工作节点共享，这样主节点和工作节点就可以并行引导而无需协调。 这允许它在许多配置场景中使用。\n  劣势\n 如果攻击者能够通过某些漏洞窃取引导令牌，那么他们可以使用该令牌（连同网络级访问）为其它处于引导过程中的节点提供假冒的主节点。 在您的环境中，这可能是一个适当的折衷方法，也可能不是。  基于 HTTPS 或文件发现 这种方案提供了一种带外方式在主节点和引导节点之间建立信任根。 如果使用 kubeadm 构建自动配置，请考虑使用此模式。\nkubeadm join 命令示例：\n  kubeadm join --discovery-file path/to/file.conf （本地文件）\n  kubeadm join --discovery-file https://url/file.conf (远程 HTTPS URL)\n  优势：\n 允许引导节点安全地发现主节点的信任根，即使网络或其他工作节点受到损害。  劣势：\n 要求您有某种方法将发现信息从主节点传送到引导节点。 例如，这可以通过云提供商或驱动工具实现。 该文件中的信息不是加密的，而是需要 HTTPS 或等效文件来保证其完整性。  确保您的安装更加安全 Kubeadm 的默认值可能不适用于所有人。 本节说明如何以牺牲可用性为代价来加强 kubeadm 安装。\n关闭节点客户端证书的自动批准 默认情况下，Kubernetes 启用了 CSR 自动批准器，如果在身份验证时使用 Bootstrap Token，它会批准对 kubelet 的任何客户端证书的请求。 如果不希望集群自动批准kubelet客户端证书，可以通过执行以下命令关闭它：\n$ kubectl delete clusterrole kubeadm:node-autoapprove-bootstrap 关闭后，kubeadm join 操作将会被阻断，直到管理员已经手动批准了在途中的 CSR 才会继续：\n$ kubectl get csr NAME AGE REQUESTOR CONDITION node-csr-c69HXe7aYcqkS1bKmH4faEnHAWxn6i2bHZ2mD04jZyQ 18s system:bootstrap:878f07 Pending $ kubectl certificate approve node-csr-c69HXe7aYcqkS1bKmH4faEnHAWxn6i2bHZ2mD04jZyQ certificatesigningrequest \u0026quot;node-csr-c69HXe7aYcqkS1bKmH4faEnHAWxn6i2bHZ2mD04jZyQ\u0026quot; approved $ kubectl get csr NAME AGE REQUESTOR CONDITION node-csr-c69HXe7aYcqkS1bKmH4faEnHAWxn6i2bHZ2mD04jZyQ 1m system:bootstrap:878f07 Approved,Issued 只有执行了 kubectl certificate approve 后，kubeadm join 才会继续。\n关闭对集群信息 ConfigMap 的公开访问 为了实现使用令牌作为唯一验证信息的加入工作流，默认情况下会公开带有验证主节点标识所需数据的 ConfigMap。 虽然此 ConfigMap 中没有私有数据，但一些用户可能希望无论如何都关闭它。 这样做需要禁用 kubeadm join 工作流的 --discovery-token 参数。 以下是实现步骤：\n$ kubectl -n kube-public get cm cluster-info -o yaml | grep \u0026quot;kubeconfig:\u0026quot; -A11 | grep \u0026quot;apiVersion\u0026quot; -A10 | sed \u0026quot;s/ //\u0026quot; | tee cluster-info.yaml apiVersion: v1 clusters: - cluster: certificate-authority-data: \u0026lt;ca-cert\u0026gt; server: https://\u0026lt;ip\u0026gt;:\u0026lt;port\u0026gt; name: \u0026quot;\u0026quot; contexts: [] current-context: \u0026quot;\u0026quot; kind: Config preferences: {} users: []   使用 cluster-info.yaml 文件作为 kubeadm join --discovery-file 参数。\n  关闭 cluster-info ConfigMap 的公开访问：\n  $ kubectl -n kube-public delete rolebinding kubeadm:bootstrap-signer-clusterinfo 这些命令应该在执行 kubeadm init 之后、在kubeadm join 之前执行。\n使用带有配置文件的 kubeadm join . caution \u0026gt;}}\n配置文件目前是 alpha 功能，在将来的版本中可能会变动。 . /caution \u0026gt;}}\n可以用配置文件替代命令行参数的方法配置 kubeadm join，一些高级功能也只有在使用配置文件时才可选用。 该文件通过 --config 参数来传递，并且文件中必须包含 JoinConfiguration 结构。\n执行下面的命令可以查看 JoinConfiguration 默认值：\nkubeadm config print-default --api-objects=JoinConfiguration 要了解 JoinConfiguration 中各个字段的详细信息请参考 godoc。\n. heading \u0026ldquo;whatsnext\u0026rdquo; %}}  kubeadm init 初始化 Kubernetes 主节点 kubeadm token 管理 kubeadm join 的令牌 kubeadm reset 将 kubeadm init 或 kubeadm join 对主机的更改恢复到之前状态  "
},
{
	"uri": "https://lijun.in/concepts/overview/kubernetes-api/",
	"title": "Kubernetes API",
	"tags": [],
	"description": "",
	"content": "API协议文档描述了主系统和API概念。\nAPI参考文档描述了API整体规范。\n访问文档讨论了通过远程访问API的相关问题。\nKubernetes API是系统描述性配置的基础。 Kubectl 命令行工具被用于创建、更新、删除、获取API对象。\nKubernetes 通过API资源存储自己序列化状态(现在存储在etcd)。\nKubernetes 被分成多个组件，各部分通过API相互交互。\nAPI 变更 根据经验，任何成功的系统都需要随着新的用例出现或现有用例发生变化的情况下，进行相应的进化与调整。因此，我们希望Kubernetes API也可以保持持续的进化和调整。同时，在较长一段时间内，我们也希望与现有客户端版本保持良好的向下兼容性。一般情况下，增加新的API资源和资源字段不会导致向下兼容性问题发生；但如果是需要删除一个已有的资源或者字段，那么必须通过API废弃流程来进行。\n参考API变更文档，了解兼容性变更的要素以及如何变更API的流程。\nOpenAPI 和 API Swagger 定义 完整的 API 详细文档使用 OpenAPI生成.\n随着 Kubernetes 1.10 版本的正式启用，Kubernetes API 服务通过 /openapi/v2 接口提供 OpenAPI 规范。 通过设置 HTTP 标头的规定了请求的结构。\n   Header Possible Values     Accept application/json, application/com.github.proto-openapi.spec.v2@v1.0+protobuf (the default content-type is application/json for */* or not passing this header)   Accept-Encoding gzip (not passing this header is acceptable)    在1.14版本之前，区分结构的接口通过(/swagger.json, /swagger-2.0.0.json, /swagger-2.0.0.pb-v1, /swagger-2.0.0.pb-v1.gz) 提供不同格式的 OpenAPI 规范。但是这些接口已经被废弃，并且已经在 Kubernetes 1.14 中被删除。\n获取 OpenAPI 规范的例子:\n   1.10 之前 从 1.10 开始     GET /swagger.json GET /openapi/v2 Accept: application/json   GET /swagger-2.0.0.pb-v1 GET /openapi/v2 Accept: application/com.github.proto-openapi.spec.v2@v1.0+protobuf   GET /swagger-2.0.0.pb-v1.gz GET /openapi/v2 Accept: application/com.github.proto-openapi.spec.v2@v1.0+protobuf Accept-Encoding: gzip    Kubernetes实现了另一种基于Protobuf的序列化格式，该格式主要用于集群内通信，并在设计方案中进行了说明，每个模式的IDL文件位于定义API对象的Go软件包中。 在 1.14 版本之前， Kubernetes apiserver 也提供 API 服务用于返回 Swagger v1.2 Kubernetes API 规范通过 /swaggerapi 接口. 但是这个接口已经被废弃，并且在 Kubernetes 1.14 中已经被移除。\nAPI 版本 为了使删除字段或者重构资源表示更加容易，Kubernetes 支持 多个API版本。每一个版本都在不同API路径下，例如 /api/v1 或者 /apis/extensions/v1beta1。\n我们选择在API级别进行版本化，而不是在资源或字段级别进行版本化，以确保API提供清晰，一致的系统资源和行为视图，并控制对已废止的API和/或实验性API的访问。 JSON和Protobuf序列化模式遵循架构更改的相同准则 - 下面的所有描述都同时适用于这两种格式。\n请注意，API版本控制和软件版本控制只有间接相关性。 API和发行版本建议 描述了API版本与软件版本之间的关系。\n不同的API版本名称意味着不同级别的软件稳定性和支持程度。 每个级别的标准在API变更文档中有更详细的描述。 内容主要概括如下：\n Alpha 测试版本：  版本名称包含了 alpha (例如：v1alpha1)。 可能是有缺陷的。启用该功能可能会带来隐含的问题，默认情况是关闭的。 支持的功能可能在没有通知的情况下随时删除。 API的更改可能会带来兼容性问题，但是在后续的软件发布中不会有任何通知。 由于bugs风险的增加和缺乏长期的支持，推荐在短暂的集群测试中使用。   Beta 测试版本：  版本名称包含了 beta (例如: v2beta3)。 代码已经测试过。启用该功能被认为是安全的，功能默认已启用。 所有已支持的功能不会被删除，细节可能会发生变化。 对象的模式和/或语义可能会在后续的beta测试版或稳定版中以不兼容的方式进行更改。 发生这种情况时，我们将提供迁移到下一个版本的说明。 这可能需要删除、编辑和重新创建API对象。执行编辑操作时需要谨慎行事，这可能需要停用依赖该功能的应用程序。 建议仅用于非业务关键型用途，因为后续版本中可能存在不兼容的更改。 如果您有多个可以独立升级的集群，则可以放宽此限制。 请尝试我们的 beta 版本功能并且给出反馈！一旦他们退出 beta 测试版，我们可能不会做出更多的改变。   稳定版本：  版本名称是 vX，其中 X 是整数。 功能的稳定版本将出现在许多后续版本的发行软件中。    API 组 为了更容易地扩展Kubernetes API，我们实现了API组。 API组在REST路径和序列化对象的 apiVersion 字段中指定。\n目前有几个API组正在使用中：\n  核心组（通常被称为遗留组）位于REST路径 /api/v1 并使用 apiVersion：v1。\n  指定的组位于REST路径 /apis/$GROUP_NAME/$VERSION，并使用 apiVersion：$GROUP_NAME/$VERSION （例如 apiVersion：batch/v1）。 在Kubernetes API参考中可以看到支持的API组的完整列表。\n  社区支持使用以下两种方式来提供自定义资源对API进行扩展自定义资源：\n  CustomResourceDefinition 适用于具有非常基本的CRUD需求的用户。\n  需要全套Kubernetes API语义的用户可以实现自己的apiserver， 并使用聚合器 为客户提供无缝的服务。\n  启用 API 组 某些资源和API组默认情况下处于启用状态。 可以通过在apiserver上设置 --runtime-config 来启用或禁用它们。 --runtime-config 接受逗号分隔的值。 例如：要禁用batch/v1，请设置 --runtime-config=batch/v1=false，以启用batch/v2alpha1，请设置--runtime-config=batch/v2alpha1。 该标志接受描述apiserver的运行时配置的逗号分隔的一组键值对。\n启用或禁用组或资源需要重新启动apiserver和控制器管理器来使得 --runtime-config 更改生效。\n启用 extensions/v1beta1 组中资源 在 extensions/v1beta1 API 组中，DaemonSets，Deployments，StatefulSet, NetworkPolicies, PodSecurityPolicies 和 ReplicaSets 是默认禁用的。 例如：要启用 deployments 和 daemonsets，请设置 --runtime-config=extensions/v1beta1/deployments=true,extensions/v1beta1/daemonsets=true。\n出于遗留原因，仅在 extensions / v1beta1 API 组中支持各个资源的启用/禁用。\n"
},
{
	"uri": "https://lijun.in/setup/release/version-skew-policy/",
	"title": "Kubernetes 版本及版本倾斜支持策略",
	"tags": [],
	"description": "",
	"content": "本文描述 Kubernetes 各组件之间版本倾斜支持策略。 特定的集群部署工具可能会有额外的限制。\nSupported versions Kubernetes 版本号格式为 x.y.z，其中 x 为大版本号，y 为小版本号，z 为补丁版本号。 版本号格式遵循 Semantic Versioning 规则。 更多信息，请参阅 Kubernetes Release Versioning。\nKubernetes 项目会维护最近的三个小版本分支。\n一些 bug 修复，包括安全修复，根据其安全性和可用性，有可能会回合到这些分支。 补丁版本会定期或根据需要从这些分支中发布。 最终是否发布是由patch release team 来决定的。Patch release team同时也是release managers. 如需了解更多信息，请查看 Kubernetes Patch releases.\n小版本大约每3个月发布一个，所以每个小版本分支会维护9个月。\nSupported version skew kube-apiserver In highly-available (HA) clusters, the newest and oldest kube-apiserver instances must be within one minor version. 在 高可用（HA）集群 中， 多个 kube-apiserver 实例小版本号最多差1。\n例如：\n 最新的 kube-apiserver 版本号如果是 1.13 其他 kube-apiserver 版本号只能是 1.13 或 1.12  kubelet kubelet 版本号不能高于 kube-apiserver，最多可以比 kube-apiserver 低两个小版本。\n例如：\n kube-apiserver 版本号如果是 1.13 kubelet 只能是 1.13 、 1.12 和 1.11  . note \u0026gt;}}\nHA集群中多个 kube-apiserver 实例版本号不一致，相应的 kubelet 版本号可选范围也要减小。 ./ note \u0026gt;}}\n例如：\n 如果 kube-apiserver 的多个实例同时存在 1.13 和 1.12 kubelet 只能是 1.12 或 1.11（1.13 不再支持，因为它比1.12版本的 kube-apiserver 更新）  kube-controller-manager, kube-scheduler, and cloud-controller-manager kube-controller-manager、kube-scheduler 和 cloud-controller-manager 版本不能高于 kube-apiserver 版本号。 最好它们的版本号与 kube-apiserver 保持一致，但允许比 kube-apiserver 低一个小版本（为了支持在线升级）。\n例如：\n 如果 kube-apiserver 版本号为 1.13 kube-controller-manager、kube-scheduler 和 cloud-controller-manager 版本支持 1.13 和 1.12  . note \u0026gt;}}\n但 kube-controller-manager、kube-scheduler 和 cloud-controller-manager 版本可用范围会相应的减小。 . /note \u0026gt;}}\n例如：\n kube-apiserver 实例同时存在 1.13 和 1.12 版本 kube-controller-manager、kube-scheduler 和 cloud-controller-manager 可以通过 load balancer 与所有的 kube-apiserver 通信 kube-controller-manager、kube-scheduler 和 cloud-controller-manager 可选版本为 1.12（1.13 不再支持，因为它比 1.12 版本的 kube-apiserver 更新）  kubectl kubectl 可以比 kube-apiserver 高一个小版本，也可以低一个小版本。\n例如：\n 如果 kube-apiserver 当前是 1.13 版本 kubectl 则支持 1.14 、1.13 和 1.12  . note \u0026gt;}}\n如果 HA 集群中的多个 kube-apiserver 实例版本号不一致，相应的 kubectl 可用版本范围也会减小。 . /note \u0026gt;}}\n例如：\n kube-apiserver 多个实例同时存在 1.13 和 1.12 kubectl 可选的版本为 1.13 和 1.12（其他版本不再支持，因为它会比其中某个 kube-apiserver 实例高或低一个小版本）  支持的组件升级次序 组件之间支持的版本倾斜会影响组件升级的顺序。 本节描述组件从版本 1.n 到 1.(n+1) 的升级次序。\nkube-apiserver 前提条件：\n 单实例集群时，kube-apiserver 实例版本号须是 1.n HA 集群时，所有的 kube-apiserver 实例版本号必须是 1.n 或 1.(n+1)（确保满足最新和最旧的实例小版本号相差不大于1） kube-controller-manager、kube-scheduler 和 cloud-controller-manager 版本号必须为 1.n（确保不高于 API server 的版本，且版本号相差不大于1） kubelet 实例版本号必须是 1.n 或 1.(n-1)（确保版本号不高于 API server，且版本号相差不大于2） 注册的 admission 插件必须能够处理新的 kube-apiserver 实例发送过来的数据：  ValidatingWebhookConfiguration 和 MutatingWebhookConfiguration 对象必须升级到可以处理 1.(n+1) 版本新加的 REST 资源(或使用1.15版本提供的 matchPolicy: Equivalent 选项) 插件可以处理任何 1.(n+1) 版本新的 REST 资源数据和新加的字段    升级 kube-apiserver 到 1.(n+1)\n. note \u0026gt;}}\nkube-apiserver 不能跨小版本号升级，即使是单实例集群也不可以。\n. /note \u0026gt;}}\nkube-controller-manager, kube-scheduler, and cloud-controller-manager 前提条件：\n kube-apiserver 实例必须为 1.(n+1) （HA 集群中，所有的kube-apiserver 实例必须在组件升级前完成升级）  升级 kube-controller-manager、kube-scheduler 和 cloud-controller-manager 到 1.(n+1)\nkubelet 前提条件：\n kube-apiserver 实例必须为 1.(n+1) 版本  kubelet 可以升级到 1.(n+1)（或者停留在 1.n 或 1.(n-1)）\n. warning \u0026gt;}}\n 他们必须升级到与 kube-apiserver 相差不超过1个小版本，才可以升级其他控制面组件 有可能使用低于3个在维护的小版本 ./ warning \u0026gt;}}  "
},
{
	"uri": "https://lijun.in/concepts/extend-kubernetes/operator/",
	"title": "Operator 模式",
	"tags": [],
	"description": "",
	"content": "Operator 是 Kubernetes 的扩展软件，它利用自定义资源管理应用及其组件。 Operator 遵循 Kubernetes 的理念，特别是在控制回路方面。\n初衷 Operator 模式旨在捕获（正在管理一个或一组服务的）运维人员的关键目标。 负责特定应用和 service 的运维人员，在系统应该如何运行、如何部署以及出现问题时如何处理等方面有深入的了解。\n在 Kubernetes 上运行工作负载的人们都喜欢通过自动化来处理重复的任务。Operator 模式会封装您编写的（Kubernetes 本身提供功能以外的）任务自动化代码。\nKubernetes 上的 Operator Kubernetes 为自动化而生。无需任何修改，您即可以从 Kubernetes 核心中获得许多内置的自动化功能。 您可以使用 Kubernetes 自动化部署和运行工作负载， 甚至 可以自动化 Kubernetes 自身。\nKubernetes text=\u0026quot;控制器\u0026rdquo; term_id=\u0026quot;controller\u0026rdquo; \u0026gt;}} 使您无需修改 Kubernetes 自身的代码，即可以扩展集群的行为。 Operator 是 Kubernetes API 的客户端，充当自定义资源的控制器。\nOperator 示例 使用 Operator 可以自动化的事情包括：\n 按需部署应用 获取/还原应用状态的备份 处理应用代码的升级以及相关改动。例如，数据库 schema 或额外的配置设置 发布一个 service，要求不支持 Kubernetes API 的应用也能发现它 模拟整个或部分集群中的故障以测试其稳定性 在没有内部成员选举程序的情况下，为分布式应用选择首领角色  想要更详细的了解 Operator？这儿有一个详细的示例：\n 有一个名为 SampleDB 的自定义资源，您可以将其配置到集群中。 一个包含 Operator 控制器部分的 Deployment，用来确保 Pod 处于运行状态。 Operator 代码的容器镜像。 控制器代码，负责查询控制平面以找出已配置的 SampleDB 资源。 Operator 的核心是告诉 API 服务器，如何使现实与代码里配置的资源匹配。  如果添加新的 SampleDB，Operator 将设置 PersistentVolumeClaims 以提供持久化的数据库存储，设置 StatefulSet 以运行 SampleDB，并设置 Job 来处理初始配置。 如果您删除它，Operator 将建立快照，然后确保 StatefulSet 和 Volume 已被删除。   Operator 也可以管理常规数据库的备份。对于每个 SampleDB 资源，Operator 会确定何时创建（可以连接到数据库并进行备份的）Pod。这些 Pod 将依赖于 ConfigMap 和/或 具有数据库连接详细信息和凭据的 Secret。 由于 Operator 旨在为其管理的资源提供强大的自动化功能，因此它还需要一些额外的支持性代码。在这个示例中，代码将检查数据库是否正运行在旧版本上，如果是，则创建 Job 对象为您升级数据库。  部署 Operator 部署 Operator 最常见的方法是将自定义资源及其关联的控制器添加到您的集群中。跟运行容器化应用一样，Controller 通常会运行在 text=\u0026quot;控制平面\u0026rdquo; term_id=\u0026quot;control-plane\u0026rdquo; \u0026gt;}} 之外。例如，您可以在集群中将控制器作为 Deployment 运行。\n使用 Operator 部署 Operator 后，您可以对 Operator 所使用的资源执行添加、修改或删除操作。按照上面的示例，您将为 Operator 本身建立一个 Deployment，然后：\nkubectl get SampleDB # 查找所配置的数据库 kubectl edit SampleDB/example-database # 手动修改某些配置 可以了！Operator 会负责应用所作的更改并保持现有服务处于良好的状态\n编写你自己的 Operator 如果生态系统中没可以实现您目标的 Operator，您可以自己编写代码。在接下来一节中，您会找到编写自己的云原生 Operator 需要的库和工具的链接。\n您还可以使用任何支持 Kubernetes API 客户端的语言或运行时来实现 Operator（即控制器）。\n  详细了解自定义资源 在 OperatorHub.io 上找到现成的、适合您的 Operator 借助已有的工具来编写您自己的 Operator，例如：  KUDO (Kubernetes 通用声明式 Operator) kubebuilder Metacontroller，可与 Webhook 结合使用，以实现自己的功能。 Operator 框架   发布您的 Operator，让别人也可以使用 阅读 CoreOS 原文，其介绍了 Operator 介绍 阅读这篇来自谷歌云的关于构建 Operator 最佳实践的文章  "
},
{
	"uri": "https://lijun.in/concepts/storage/storage-classes/",
	"title": "Storage Classes",
	"tags": [],
	"description": "",
	"content": "本文描述了 Kubernetes 中 StorageClass 的概念。建议先熟悉 卷 和 持久卷 的概念。\n介绍 StorageClass 为管理员提供了描述存储 \u0026ldquo;类\u0026rdquo; 的方法。 不同的类型可能会映射到不同的服务质量等级或备份策略，或是由集群管理员制定的任意策略。 Kubernetes 本身并不清楚各种类代表的什么。这个类的概念在其他存储系统中有时被称为 \u0026ldquo;配置文件\u0026rdquo;。\nStorageClass 资源 每个 StorageClass 都包含 provisioner、parameters 和 reclaimPolicy 字段， 这些字段会在 StorageClass 需要动态分配 PersistentVolume 时会使用到。\nStorageClass 对象的命名很重要，用户使用这个命名来请求生成一个特定的类。 当创建 StorageClass 对象时，管理员设置 StorageClass 对象的命名和其他参数，一旦创建了对象就不能再对其更新。\n管理员可以为没有申请绑定到特定 StorageClass 的 PVC 指定一个默认的存储类 ： 更多详情请参阅 PersistentVolumeClaim 章节。\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: standard provisioner: kubernetes.io/aws-ebs parameters: type: gp2 reclaimPolicy: Retain allowVolumeExpansion: true mountOptions: - debug volumeBindingMode: Immediate 存储分配器 每个 StorageClass 都有一个分配器，用来决定使用哪个卷插件分配 PV。该字段必须指定。\n   卷插件 内置分配器 配置例子     AWSElasticBlockStore ✓ AWS EBS   AzureFile ✓ Azure File   AzureDisk ✓ Azure Disk   CephFS - -   Cinder ✓ OpenStack Cinder   FC - -   FlexVolume - -   Flocker ✓ -   GCEPersistentDisk ✓ GCE PD   Glusterfs ✓ Glusterfs   iSCSI - -   Quobyte ✓ Quobyte   NFS - -   RBD ✓ Ceph RBD   VsphereVolume ✓ vSphere   PortworxVolume ✓ Portworx Volume   ScaleIO ✓ ScaleIO   StorageOS ✓ StorageOS   Local - Local    您不限于指定此处列出的 \u0026ldquo;内置\u0026rdquo; 分配器（其名称前缀为 \u0026ldquo;kubernetes.io\u0026rdquo; 并打包在 Kubernetes 中）。 您还可以运行和指定外部分配器，这些独立的程序遵循由 Kubernetes 定义的 规范。 外部供应商的作者完全可以自由决定他们的代码保存于何处、打包方式、运行方式、使用的插件（包括 Flex）等。 代码仓库 kubernetes-sigs/sig-storage-lib-external-provisioner 包含一个用于为外部分配器编写功能实现的类库。可以通过下面的代码仓库，查看外部分配器列表。\nkubernetes-incubator/external-storage.\n例如，NFS 没有内部分配器，但可以使用外部分配器。 也有第三方存储供应商提供自己的外部分配器。\n回收策略 由 StorageClass 动态创建的 PersistentVolume 会在类的 reclaimPolicy 字段中指定回收策略，可以是 Delete 或者 Retain。如果 StorageClass 对象被创建时没有指定 reclaimPolicy，它将默认为 Delete。\n通过 StorageClass 手动创建并管理的 PersistentVolume 会使用它们被创建时指定的回收政策。\n允许卷扩展 feature-state for_k8s_version=\u0026quot;v1.11\u0026rdquo; state=\u0026quot;beta\u0026rdquo; \u0026gt;}}\nPersistentVolume 可以配置为可扩展。将此功能设置为 true 时，允许用户通过编辑相应的 PVC 对象来调整卷大小。\n当基础存储类的 allowVolumeExpansion 字段设置为 true 时，以下类型的卷支持卷扩展。\ntable caption = \u0026ldquo;Table of Volume types and the version of Kubernetes they require\u0026rdquo;\n   卷类型 Kubernetes 版本要求     gcePersistentDisk 1.11   awsElasticBlockStore 1.11   Cinder 1.11   glusterfs 1.11   rbd 1.11   Azure File 1.11   Azure Disk 1.11   Portworx 1.11   FlexVolume 1.13   CSI 1.14 (alpha), 1.16 (beta)    table \u0026gt;}}\n此功能仅可用于扩容卷，不能用于缩小卷。\n挂载选项 由 StorageClass 动态创建的 PersistentVolume 将使用类中 mountOptions 字段指定的挂载选项。\n如果卷插件不支持挂载选项，却指定了该选项，则分配操作会失败。 挂载选项在 StorageClass 和 PV 上都不会做验证，所以如果挂载选项无效，那么这个 PV 就会失败。\n卷绑定模式 volumeBindingMode 字段控制了 卷绑定和动态分配 应该发生在什么时候。\n默认情况下，Immediate 模式表示一旦创建了 PersistentVolumeClaim 也就完成了卷绑定和动态分配。 对于由于拓扑限制而非集群所有节点可达的存储后端，PersistentVolume 会在不知道 Pod 调度要求的情况下绑定或者分配。\n集群管理员可以通过指定 WaitForFirstConsumer 模式来解决此问题。 该模式将延迟 PersistentVolume 的绑定和分配，直到使用该 PersistentVolumeClaim 的 Pod 被创建。 PersistentVolume 会根据 Pod 调度约束指定的拓扑来选择或分配。这些包括但不限于 资源需求， 节点筛选器， pod 亲和性和互斥性, 以及 污点和容忍度.\n以下插件支持动态分配的 WaitForFirstConsumer 模式:\n AWSElasticBlockStore GCEPersistentDisk AzureDisk  以下插件支持预创建绑定 PersistentVolume 的 WaitForFirstConsumer 模式：\n 上述全部 Local  feature-state state=\u0026quot;beta\u0026rdquo; for_k8s_version=\u0026quot;1.17\u0026rdquo; \u0026gt;}}\n动态配置和预先创建的 PV 也支持 CSI卷， 但是您需要查看特定 CSI 驱动程序的文档以查看其支持的拓扑键名和例子。\n允许的拓扑结构 feature-state for_k8s_version=\u0026quot;v1.12\u0026rdquo; state=\u0026quot;beta\u0026rdquo; \u0026gt;}}\n当集群操作人员使用了 WaitForFirstConsumer 的卷绑定模式，在大部分情况下就没有必要将配置限制为特定的拓扑结构。 然而，如果还有需要的话，可以使用 allowedTopologies。\n这个例子描述了如何将分配卷的拓扑限制在特定的区域，在使用时应该根据插件支持情况替换 zone 和 zones 参数。\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: standard provisioner: kubernetes.io/gce-pd parameters: type: pd-standard volumeBindingMode: WaitForFirstConsumer allowedTopologies: - matchLabelExpressions: - key: failure-domain.beta.kubernetes.io/zone values: - us-central1-a - us-central1-b 参数 Storage class 具有描述属于卷的参数。取决于分配器，可以接受不同的参数。 例如，参数 type 的值 io1 和参数 iopsPerGB 特定于 EBS PV。当参数被省略时，会使用默认值。\n一个 StorageClass 最多可以定义 512 个参数。这些参数对象的总长度不能超过 256 KiB, 包括参数的键和值。\nAWS EBS apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: slow provisioner: kubernetes.io/aws-ebs parameters: type: io1 iopsPerGB: \u0026#34;10\u0026#34; fsType: ext4  type：io1，gp2，sc1，st1。详细信息参见 AWS 文档。默认值：gp2。 zone(弃用)：AWS 区域。如果没有指定 zone 和 zones，通常卷会在 Kubernetes 集群节点所在的活动区域中轮询调度分配。zone 和 zones 参数不能同时使用。 zones(弃用)：以逗号分隔的 AWS 区域列表。如果没有指定 zone 和 zones，通常卷会在 Kubernetes 集群节点所在的活动区域中轮询调度分配。zone和zones参数不能同时使用。 iopsPerGB：只适用于 io1 卷。每 GiB 每秒 I/O 操作。AWS 卷插件将其与请求卷的大小相乘以计算 IOPS 的容量，并将其限制在 20 000 IOPS（AWS 支持的最高值，请参阅 AWS 文档。 这里需要输入一个字符串，即 \u0026quot;10\u0026quot;，而不是 10。 fsType：受 Kubernetes 支持的文件类型。默认值：\u0026quot;ext4\u0026quot;。 encrypted：指定 EBS 卷是否应该被加密。合法值为 \u0026quot;true\u0026quot; 或者 \u0026quot;false\u0026quot;。这里需要输入字符串，即 \u0026quot;true\u0026quot;, 而非 true。 kmsKeyId：可选。加密卷时使用密钥的完整 Amazon 资源名称。如果没有提供，但 encrypted 值为 true，AWS 生成一个密钥。关于有效的 ARN 值，请参阅 AWS 文档。  zone 和 zones 已被弃用并被 允许的拓扑结构 取代。\nGCE PD apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: slow provisioner: kubernetes.io/gce-pd parameters: type: pd-standard replication-type: none  type：pd-standard 或者 pd-ssd。默认：pd-standard zone(弃用)：GCE 区域。如果没有指定 zone 和 zones，通常卷会在 Kubernetes 集群节点所在的活动区域中轮询调度分配。zone 和 zones 参数不能同时使用。 zones(弃用)：逗号分隔的 GCE 区域列表。如果没有指定 zone 和 zones，通常卷会在 Kubernetes 集群节点所在的活动区域中轮询调度（round-robin）分配。zone 和 zones 参数不能同时使用。 fstype: ext4 或 xfs。 默认: ext4。宿主机操作系统必须支持所定义的文件系统类型。 replication-type：none 或者 regional-pd。默认值：none。  如果 replication-type 设置为 none，会分配一个常规（当前区域内的）持久化磁盘。\n如果 replication-type 设置为 regional-pd，会分配一个 区域性持久化磁盘（Regional Persistent Disk）。在这种情况下，用户必须使用 zones 而非 zone 来指定期望的复制区域（zone）。如果指定来两个特定的区域，区域性持久化磁盘会在这两个区域里分配。如果指定了多于两个的区域，Kubernetes 会选择其中任意两个区域。如果省略了 zones 参数，Kubernetes 会在集群管理的区域中任意选择。\nzone 和 zones 已被弃用并被 allowedTopologies 取代。\nGlusterfs apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: slow provisioner: kubernetes.io/glusterfs parameters: resturl: \u0026#34;http://127.0.0.1:8081\u0026#34; clusterid: \u0026#34;630372ccdc720a92c681fb928f27b53f\u0026#34; restauthenabled: \u0026#34;true\u0026#34; restuser: \u0026#34;admin\u0026#34; secretNamespace: \u0026#34;default\u0026#34; secretName: \u0026#34;heketi-secret\u0026#34; gidMin: \u0026#34;40000\u0026#34; gidMax: \u0026#34;50000\u0026#34; volumetype: \u0026#34;replicate:3\u0026#34;  resturl：分配 gluster 卷的需求的 Gluster REST 服务/Heketi 服务 url。 通用格式应该是 IPaddress:Port，这是 GlusterFS 动态分配器的必需参数。 如果 Heketi 服务在 openshift/kubernetes 中安装并暴露为可路由服务，则可以使用类似于 http://heketi-storage-project.cloudapps.mystorage.com 的格式，其中 fqdn 是可解析的 heketi 服务网址。 restauthenabled：Gluster REST 服务身份验证布尔值，用于启用对 REST 服务器的身份验证。如果此值为 \u0026lsquo;true\u0026rsquo;，则必须填写 restuser 和 restuserkey 或 secretNamespace + secretName。此选项已弃用，当在指定 restuser，restuserkey，secretName 或 secretNamespace 时，身份验证被启用。 restuser：在 Gluster 可信池中有权创建卷的 Gluster REST服务/Heketi 用户。 restuserkey：Gluster REST 服务/Heketi 用户的密码将被用于对 REST 服务器进行身份验证。此参数已弃用，取而代之的是 secretNamespace + secretName。    secretNamespace，secretName：Secret 实例的标识，包含与 Gluster REST 服务交互时使用的用户密码。 这些参数是可选的，secretNamespace 和 secretName 都省略时使用空密码。所提供的 Secret 必须将类型设置为 \u0026ldquo;kubernetes.io/glusterfs\u0026rdquo;，例如以这种方式创建：\nkubectl create secret generic heketi-secret \\ --type=\u0026quot;kubernetes.io/glusterfs\u0026quot; --from-literal=key='opensesame' \\ --namespace=default secret 的例子可以在 glusterfs-provisioning-secret.yaml 中找到。\n   clusterid：630372ccdc720a92c681fb928f27b53f 是集群的 ID，当分配卷时，Heketi 将会使用这个文件。它也可以是一个 clusterid 列表，例如： \u0026quot;8452344e2becec931ece4e33c4674e4e,42982310de6c63381718ccfa6d8cf397\u0026quot;。这个是可选参数。 gidMin，gidMax：storage class GID 范围的最小值和最大值。在此范围（gidMin-gidMax）内的唯一值（GID）将用于动态分配卷。这些是可选的值。如果不指定，卷将被分配一个 2000-2147483647 之间的值，这是 gidMin 和 gidMax 的默认值。    volumetype：卷的类型及其参数可以用这个可选值进行配置。如果未声明卷类型，则由分配器决定卷的类型。\n例如： \u0026lsquo;Replica volume\u0026rsquo;: volumetype: replicate:3 其中 \u0026lsquo;3\u0026rsquo; 是 replica 数量. \u0026lsquo;Disperse/EC volume\u0026rsquo;: volumetype: disperse:4:2 其中 \u0026lsquo;4\u0026rsquo; 是数据，\u0026lsquo;2\u0026rsquo; 是冗余数量. \u0026lsquo;Distribute volume\u0026rsquo;: volumetype: none\n有关可用的卷类型和管理选项，请参阅 管理指南。\n更多相关的参考信息，请参阅 如何配置 Heketi。\n当动态分配持久卷时，Gluster 插件自动创建名为 gluster-dynamic-\u0026lt;claimname\u0026gt; 的端点和 headless service。在 PVC 被删除时动态端点和 headless service 会自动被删除。\n  OpenStack Cinder apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: gold provisioner: kubernetes.io/cinder parameters: availability: nova  availability：可用区域。如果没有指定，通常卷会在 Kubernetes 集群节点所在的活动区域中轮询调度分配。  feature-state state=\u0026quot;deprecated\u0026rdquo; for_k8s_version=\u0026quot;1.11\u0026rdquo; \u0026gt;}} OpenStack 的内部驱动程序已经被弃用。请使用 OpenStack 的外部驱动程序。\nvSphere   使用用户指定的磁盘格式创建一个 StorageClass。\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: fast provisioner: kubernetes.io/vsphere-volume parameters: diskformat: zeroedthick   `diskformat`: `thin`, `zeroedthick` 和 `eagerzeroedthick`。默认值: `\u0026quot;thin\u0026quot;`。   在用户指定的数据存储上创建磁盘格式的 StorageClass。\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: fast provisioner: kubernetes.io/vsphere-volume parameters: diskformat: zeroedthick datastore: VSANDatastore   `datastore`：用户也可以在 StorageClass 中指定数据存储。卷将在 storage class 中指定的数据存储上创建，在这种情况下是 `VSANDatastore`。该字段是可选的。如果未指定数据存储，则将在用于初始化 vSphere Cloud Provider 的 vSphere 配置文件中指定的数据存储上创建该卷。  Kubernetes 中的存储策略管理  * 使用现有的 vCenter SPBM 策略 vSphere 用于存储管理的最重要特性之一是基于策略的管理。基于存储策略的管理（SPBM）是一个存储策略框架，提供单一的统一控制平面的跨越广泛的数据服务和存储解决方案。 SPBM 使能 vSphere 管理员克服先期的存储配置挑战，如容量规划，差异化服务等级和管理容量空间。 SPBM 策略可以在 StorageClass 中使用 `storagePolicyName` 参数声明。  * Kubernetes 内的 Virtual SAN 策略支持 Vsphere Infrastructure（VI）管理员将能够在动态卷配置期间指定自定义 Virtual SAN 存储功能。您现在可以定义存储需求，例如性能和可用性，当动态卷供分配时会以存储功能的形式提供。存储功能需求会转换为 Virtual SAN 策略，然后当 persistent volume（虚拟磁盘）在创建时，会将其推送到 Virtual SAN 层。虚拟磁盘分布在 Virtual SAN 数据存储中以满足要求。 更多有关 persistent volume 管理的存储策略的详细信息， 您可以参考 [基于存储策略的动态分配卷管理](https://vmware.github.io/vsphere-storage-for-kubernetes/documentation/policy-based-mgmt.html)。  有几个 vSphere 例子 供您在 Kubernetes for vSphere 中尝试进行 persistent volume 管理。\nCeph RBD apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: fast provisioner: kubernetes.io/rbd parameters: monitors: 10.16.153.105:6789 adminId: kube adminSecretName: ceph-secret adminSecretNamespace: kube-system pool: kube userId: kube userSecretName: ceph-secret-user userSecretNamespace: default fsType: ext4 imageFormat: \u0026#34;2\u0026#34; imageFeatures: \u0026#34;layering\u0026#34;  monitors：Ceph monitor，逗号分隔。该参数是必需的。 adminId：Ceph 客户端 ID，用于在池 ceph 池中创建映像。默认是 \u0026ldquo;admin\u0026rdquo;。 adminSecret：adminId 的 Secret 名称。该参数是必需的。 提供的 secret 必须有值为 \u0026ldquo;kubernetes.io/rbd\u0026rdquo; 的 type 参数。 adminSecretNamespace：adminSecret 的命名空间。默认是 \u0026ldquo;default\u0026rdquo;。 pool: Ceph RBD 池. 默认是 \u0026ldquo;rbd\u0026rdquo;。 userId：Ceph 客户端 ID，用于映射 RBD 镜像。默认与 adminId 相同。    userSecretName：用于映射 RBD 镜像的 userId 的 Ceph Secret 的名字。 它必须与 PVC 存在于相同的 namespace 中。该参数是必需的。 提供的 secret 必须具有值为 \u0026ldquo;kubernetes.io/rbd\u0026rdquo; 的 type 参数，例如以这样的方式创建：\nkubectl create secret generic ceph-secret --type=\u0026#34;kubernetes.io/rbd\u0026#34; \\  --from-literal=key=\u0026#39;QVFEQ1pMdFhPUnQrSmhBQUFYaERWNHJsZ3BsMmNjcDR6RFZST0E9PQ==\u0026#39; \\  --namespace=kube-system    userSecretNamespace：userSecretName 的命名空间。 fsType：Kubernetes 支持的 fsType。默认：\u0026quot;ext4\u0026quot;。 imageFormat：Ceph RBD 镜像格式，\u0026ldquo;1\u0026rdquo; 或者 \u0026ldquo;2\u0026rdquo;。默认值是 \u0026ldquo;1\u0026rdquo;。 imageFeatures：这个参数是可选的，只能在你将 imageFormat 设置为 \u0026ldquo;2\u0026rdquo; 才使用。 目前支持的功能只是 layering。默认是 \u0026ldquo;\u0026quot;，没有功能打开。  Quobyte apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: slow provisioner: kubernetes.io/quobyte parameters: quobyteAPIServer: \u0026#34;http://138.68.74.142:7860\u0026#34; registry: \u0026#34;138.68.74.142:7861\u0026#34; adminSecretName: \u0026#34;quobyte-admin-secret\u0026#34; adminSecretNamespace: \u0026#34;kube-system\u0026#34; user: \u0026#34;root\u0026#34; group: \u0026#34;root\u0026#34; quobyteConfig: \u0026#34;BASE\u0026#34; quobyteTenant: \u0026#34;DEFAULT\u0026#34;  quobyteAPIServer：Quobyte API 服务器的格式是 \u0026quot;http(s)://api-server:7860\u0026quot; registry：用于挂载卷的 Quobyte registry。你可以指定 registry 为 \u0026lt;host\u0026gt;:\u0026lt;port\u0026gt; 或者如果你想指定多个 registry，你只需要在他们之间添加逗号，例如 \u0026lt;host1\u0026gt;:\u0026lt;port\u0026gt;,\u0026lt;host2\u0026gt;:\u0026lt;port\u0026gt;,\u0026lt;host3\u0026gt;:\u0026lt;port\u0026gt;。 主机可以是一个 IP 地址，或者如果您有正在运行的 DNS，您也可以提供 DNS 名称。 adminSecretNamespace：adminSecretName的 namespace。 默认值是 \u0026ldquo;default\u0026rdquo;。    adminSecretName：保存关于 Quobyte 用户和密码的 secret，用于对 API 服务器进行身份验证。 提供的 secret 必须有值为 \u0026ldquo;kubernetes.io/quobyte\u0026rdquo; 的 type 参数 和 user 与 password 的键值， 例如以这种方式创建：\nkubectl create secret generic quobyte-admin-secret \\  --type=\u0026#34;kubernetes.io/quobyte\u0026#34; --from-literal=key=\u0026#39;opensesame\u0026#39; \\  --namespace=kube-system    user：对这个用户映射的所有访问权限。默认是 \u0026ldquo;root\u0026rdquo;。 group：对这个组映射的所有访问权限。默认是 \u0026ldquo;nfsnobody\u0026rdquo;。 quobyteConfig：使用指定的配置来创建卷。您可以创建一个新的配置，或者，可以修改 Web console 或 quobyte CLI 中现有的配置。默认是 \u0026ldquo;BASE\u0026rdquo;。 quobyteTenant：使用指定的租户 ID 创建/删除卷。这个 Quobyte 租户必须已经于 Quobyte。 默认是 \u0026ldquo;DEFAULT\u0026rdquo;。  Azure 磁盘 Azure Unmanaged Disk Storage Class（非托管磁盘存储类） kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: slow provisioner: kubernetes.io/azure-disk parameters: skuName: Standard_LRS location: eastus storageAccount: azure_storage_account_name  skuName：Azure 存储帐户 Sku 层。默认为空。 location：Azure 存储帐户位置。默认为空。 storageAccount：Azure 存储帐户名称。如果提供存储帐户，它必须位于与集群相同的资源组中，并且 location 是被忽略的。如果未提供存储帐户，则会在与群集相同的资源组中创建新的存储帐户。  Azure 磁盘 Storage Class（从 v1.7.2 开始） kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: slow provisioner: kubernetes.io/azure-disk parameters: storageaccounttype: Standard_LRS kind: Shared  storageaccounttype：Azure 存储帐户 Sku 层。默认为空。 kind：可能的值是 shared（默认）、dedicated 和 managed。 当 kind 的值是 shared 时，所有非托管磁盘都在集群的同一个资源组中的几个共享存储帐户中创建。 当 kind 的值是 dedicated 时，将为在集群的同一个资源组中新的非托管磁盘创建新的专用存储帐户。 resourceGroup: 指定要创建 Azure 磁盘所属的资源组。必须是已存在的资源组名称。若未指定资源组，磁盘会默认放入与当前 Kubernetes 集群相同的资源组中。   Premium VM 可以同时添加 Standard_LRS 和 Premium_LRS 磁盘，而 Standard 虚拟机只能添加 Standard_LRS 磁盘。 托管虚拟机只能连接托管磁盘，非托管虚拟机只能连接非托管磁盘。  Azure 文件 kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: azurefile provisioner: kubernetes.io/azure-file parameters: skuName: Standard_LRS location: eastus storageAccount: azure_storage_account_name  skuName：Azure 存储帐户 Sku 层。默认为空。 location：Azure 存储帐户位置。默认为空。 storageAccount：Azure 存储帐户名称。默认为空。 如果不提供存储帐户，会搜索所有与资源相关的存储帐户，以找到一个匹配 skuName 和 location 的账号。 如果提供存储帐户，它必须存在于与集群相同的资源组中，skuName 和 location 会被忽略。 secretNamespace：包含 Azure 存储帐户名称和密钥的密钥的名称空间。 默认值与 Pod 相同。 secretName：包含 Azure 存储帐户名称和密钥的密钥的名称。 默认值为 azure-storage-account-\u0026lt;accountName\u0026gt;-secret readOnly：指示是否将存储安装为只读的标志。默认为 false，表示 读/写 挂载。 该设置也会影响VolumeMounts中的 ReadOnly 设置。  在存储分配期间，为挂载凭证创建一个名为 secretName 的 secret。如果集群同时启用了 RBAC 和 Controller Roles， 为 system:controller:persistent-volume-binder 的 clusterrole 添加 secret 资源的 create 权限。\n在多租户上下文中，强烈建议显式设置 secretNamespace 的值，否则其他用户可能会读取存储帐户凭据。\nPortworx 卷 apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: portworx-io-priority-high provisioner: kubernetes.io/portworx-volume parameters: repl: \u0026#34;1\u0026#34; snap_interval: \u0026#34;70\u0026#34; io_priority: \u0026#34;high\u0026#34;  fs：选择的文件系统：none/xfs/ext4（默认：ext4）。 block_size：以 Kbytes 为单位的块大小（默认值：32）。 repl：同步副本数量，以复制因子 1..3（默认值：1）的形式提供。 这里需要填写字符串，即，\u0026quot;1\u0026quot; 而不是 1。 io_priority：决定是否从更高性能或者较低优先级存储创建卷 high/medium/low（默认值：low）。 snap_interval：触发快照的时钟/时间间隔（分钟）。快照是基于与先前快照的增量变化，0 是禁用快照（默认：0）。 这里需要填写字符串，即，是 \u0026quot;70\u0026quot; 而不是 70。 aggregation_level：指定卷分配到的块数量，0 表示一个非聚合卷（默认：0）。 这里需要填写字符串，即，是 \u0026quot;0\u0026quot; 而不是 0。 ephemeral：指定卷在卸载后进行清理还是持久化。 emptyDir 的使用场景可以将这个值设置为 true ， persistent volumes 的使用场景可以将这个值设置为 false（例如 Cassandra 这样的数据库）true/false（默认为 false）。这里需要填写字符串，即，是 \u0026quot;true\u0026quot; 而不是 true。  ScaleIO kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: slow provisioner: kubernetes.io/scaleio parameters: gateway: https://192.168.99.200:443/api system: scaleio protectionDomain: pd0 storagePool: sp1 storageMode: ThinProvisioned secretRef: sio-secret readOnly: false fsType: xfs  provisioner：属性设置为 kubernetes.io/scaleio gateway 到 ScaleIO API 网关的地址（必需） system：ScaleIO 系统的名称（必需） protectionDomain：ScaleIO 保护域的名称（必需） storagePool：卷存储池的名称（必需） storageMode：存储提供模式：ThinProvisioned（默认）或 ThickProvisioned secretRef：对已配置的 Secret 对象的引用（必需） readOnly：指定挂载卷的访问模式（默认为 false） fsType：卷的文件系统（默认是 ext4）  ScaleIO Kubernetes 卷插件需要配置一个 Secret 对象。 secret 必须用 kubernetes.io/scaleio 类型创建，并与引用它的 PVC 所属的名称空间使用相同的值 如下面的命令所示：\nkubectl create secret generic sio-secret --type=\u0026#34;kubernetes.io/scaleio\u0026#34; \\ --from-literal=username=sioadmin --from-literal=password=d2NABDNjMA== \\ --namespace=default StorageOS apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: fast provisioner: kubernetes.io/storageos parameters: pool: default description: Kubernetes volume fsType: ext4 adminSecretNamespace: default adminSecretName: storageos-secret  pool：分配卷的 StorageOS 分布式容量池的名称。如果未指定，则使用通常存在的 default 池。 description：分配给动态创建的卷的描述。所有卷描述对于 storage class 都是相同的， 但不同的 storage class 可以使用不同的描述，以区分不同的使用场景。 默认为 Kubernetas volume。 fsType：请求的默认文件系统类型。请注意，在 StorageOS 中用户定义的规则可以覆盖此值。默认为 ext4 adminSecretNamespace：API 配置 secret 所在的命名空间。如果设置了 adminSecretName，则是必需的。 adminSecretName：用于获取 StorageOS API 凭证的 secret 名称。如果未指定，则将尝试默认值。  StorageOS Kubernetes 卷插件可以使 Secret 对象来指定用于访问 StorageOS API 的端点和凭据。 只有当默认值已被更改时，这才是必须的。 secret 必须使用 kubernetes.io/storageos 类型创建，如以下命令：\nkubectl create secret generic storageos-secret \\ --type=\u0026#34;kubernetes.io/storageos\u0026#34; \\ --from-literal=apiAddress=tcp://localhost:5705 \\ --from-literal=apiUsername=storageos \\ --from-literal=apiPassword=storageos \\ --namespace=default 用于动态分配卷的 Secret 可以在任何名称空间中创建，并通过 adminSecretNamespace 参数引用。 预先配置的卷使用的 Secret 必须在与引用它的 PVC 在相同的名称空间中。\n本地 feature-state for_k8s_version=\u0026quot;v1.14\u0026rdquo; state=\u0026quot;stable\u0026rdquo; \u0026gt;}}\nkind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: local-storage provisioner: kubernetes.io/no-provisioner volumeBindingMode: WaitForFirstConsumer 本地卷还不支持动态分配，然而还是需要创建 StorageClass 以延迟卷绑定，直到完成 pod 的调度。这是由 WaitForFirstConsumer 卷绑定模式指定的。\n延迟卷绑定使得调度器在为 PersistentVolumeClaim 选择一个合适的 PersistentVolume 时能考虑到所有 pod 的调度限制。\n"
},
{
	"uri": "https://lijun.in/setup/production-environment/turnkey/",
	"title": "Turnkey 云解决方案",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lijun.in/tutorials/kubernetes-basics/explore/",
	"title": "了解你的应用",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lijun.in/concepts/architecture/cloud-controller/",
	"title": "云控制器管理器的基础概念",
	"tags": [],
	"description": "",
	"content": "云控制器管理器（cloud controller manager，CCM）这个概念 （不要与二进制文件混淆）创建的初衷是为了让特定的云服务供应商代码和 Kubernetes 核心相互独立演化。云控制器管理器与其他主要组件（如 Kubernetes 控制器管理器，API 服务器和调度程序）一起运行。它也可以作为 Kubernetes 的插件启动，在这种情况下，它会运行在 Kubernetes 之上。\n云控制器管理器基于插件机制设计，允许新的云服务供应商通过插件轻松地与 Kubernetes 集成。目前已经有在 Kubernetes 上加入新的云服务供应商计划，并为云服务供应商提供从原先的旧模式迁移到新 CCM 模式的方案。\n本文讨论了云控制器管理器背后的概念，并提供了相关功能的详细信息。\n这是没有云控制器管理器的 Kubernetes 集群的架构：\n设计 在上图中，Kubernetes 和云服务供应商通过几个不同的组件进行了集成，分别是：\n Kubelet Kubernetes 控制管理器 Kubernetes API 服务器  CCM 整合了前三个组件中的所有依赖于云的逻辑，以创建与云的单一集成点。CCM 的新架构如下所示：\nCCM 的组成部分 CCM 打破了 Kubernetes 控制器管理器（KCM）的一些功能，并将其作为一个单独的进程运行。具体来说，它打破了 KCM 中依赖于云的控制器。KCM 具有以下依赖于云的控制器：\n 节点控制器 卷控制器 路由控制器 服务控制器  在 1.9 版本中，CCM 运行前述列表中的以下控制器：\n 节点控制器 路由控制器 服务控制器  注意卷控制器不属于 CCM，由于其中涉及到的复杂性和对现有供应商特定卷的逻辑抽象，因此决定了卷控制器不会被移动到 CCM 之中。\n使用 CCM 支持 volume 的最初计划是使用 Flex volume 来支持可插拔卷，但是现在正在计划一项名为 CSI 的项目以取代 Flex。\n考虑到这些正在进行中的变化，在 CSI 准备就绪之前，我们决定停止当前的工作。\nCCM 的功能 CCM 从依赖于云提供商的 Kubernetes 组件继承其功能，本节基于这些组件组织。\n1. Kubernetes 控制器管理器 CCM 的大多数功能都来自 KCM，如上一节所述，CCM 运行以下控制器。\n 节点控制器 路由控制器 服务控制器  节点控制器 节点控制器负责通过从云提供商获取有关在集群中运行的节点的信息来初始化节点，节点控制器执行以下功能：\n 使用特定于云的域（zone）/区（region）标签初始化节点； 使用特定于云的实例详细信息初始化节点，例如，类型和大小； 获取节点的网络地址和主机名； 如果节点无响应，请检查云以查看该节点是否已从云中删除。如果已从云中删除该节点，请删除 Kubernetes 节点对象。  路由控制器 Route 控制器负责适当地配置云中的路由，以便 Kubernetes 集群中不同节点上的容器可以相互通信。route 控制器仅适用于 Google Compute Engine 群集。\n服务控制器 2. Kubelet 节点控制器包含 kubelet 中云依赖的功能，在引入 CCM 之前，kubelet 负责使用特定于云平台的功能特性（如 IP 地址，域/区标签和实例类型信息）初始化节点。CCM 的引入已将此初始化操作从 kubelet 转移到 CCM 中。\n在这个新模型中，kubelet 初始化一个没有特定于云平台的功能特性的节点。但是，它会为新创建的节点添加污点，使节点不可调度，直到 CCM 使用云的规格信息初始化节点后，才会清除这种污点，便得该节点可被调度。\n插件机制 云控制器管理器使用 Go 接口允许插入任何云的实现。具体来说，它使用此处定义的 CloudProvider 接口。\n上面强调的四个共享控制器的实现，以及一些辅助设施（scaffolding）和共享的 cloudprovider 接口，将被保留在 Kubernetes 核心中。但特定于云提供商的实现将在核心之外构建，并实现核心中定义的接口。\n有关开发插件的更多信息，请参阅开发云控制器管理器。\n授权 本节分解了 CCM 执行其操作时各种 API 对象所需的访问权限。\n节点控制器 Node 控制器仅适用于 Node 对象，它需要完全访问权限来获取、列出、创建、更新、修补、监视和删除 Node 对象。\nv1/Node:\n Get List Create Update Patch Watch Delete  路由控制器 路由控制器侦听 Node 对象创建并适当地配置路由，它需要访问 Node 对象。\nv1/Node:\n Get  服务控制器 服务控制器侦听 Service 对象创建、更新和删除事件，然后适当地为这些服务配置端点。\n要访问服务，它需要列表和监视访问权限。要更新服务，它需要修补和更新访问权限。\n要为服务设置端点，需要访问 create、list、get、watch 和 update。\nv1/Service:\n List Get Watch Patch Update  其它 CCM 核心的实现需要访问权限以创建事件，并且为了确保安全操作，它需要访问权限以创建服务账户。\nv1/Event:\n Create Patch Update  v1/ServiceAccount:\n Create  针对 CCM 的 RBAC ClusterRole 看起来像这样：\napiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: cloud-controller-manager rules: - apiGroups: - \u0026#34;\u0026#34; resources: - events verbs: - create - patch - update - apiGroups: - \u0026#34;\u0026#34; resources: - nodes verbs: - \u0026#39;*\u0026#39; - apiGroups: - \u0026#34;\u0026#34; resources: - nodes/status verbs: - patch - apiGroups: - \u0026#34;\u0026#34; resources: - services verbs: - list - patch - update - watch - apiGroups: - \u0026#34;\u0026#34; resources: - serviceaccounts verbs: - create - apiGroups: - \u0026#34;\u0026#34; resources: - persistentvolumes verbs: - get - list - update - watch - apiGroups: - \u0026#34;\u0026#34; resources: - endpoints verbs: - create - get - list - watch - update 供应商实施 以下云服务提供商已实现了 CCM：\n Alibaba Cloud AWS Azure BaiduCloud Digital Ocean GCP Linode OpenStack Oracle  群集管理 这里提供了有关配置和运行 CCM 的完整说明。\n"
},
{
	"uri": "https://lijun.in/concepts/cluster-administration/cloud-providers/",
	"title": "云驱动",
	"tags": [],
	"description": "",
	"content": "本文介绍了如何管理运行在特定云驱动上的 Kubernetes 集群。\nkubeadm kubeadm 是创建 kubernetes 集群的一种流行选择。 kubeadm 通过提供配置选项来指定云驱动的配置信息。例如，一个典型的适用于“树内”云驱动的 kubeadm 配置如下：\napiVersion: kubeadm.k8s.io/v1beta2 kind: InitConfiguration nodeRegistration: kubeletExtraArgs: cloud-provider: \u0026#34;openstack\u0026#34; cloud-config: \u0026#34;/etc/kubernetes/cloud.conf\u0026#34; --- apiVersion: kubeadm.k8s.io/v1beta2 kind: ClusterConfiguration kubernetesVersion: v1.13.0 apiServer: extraArgs: cloud-provider: \u0026#34;openstack\u0026#34; cloud-config: \u0026#34;/etc/kubernetes/cloud.conf\u0026#34; extraVolumes: - name: cloud hostPath: \u0026#34;/etc/kubernetes/cloud.conf\u0026#34; mountPath: \u0026#34;/etc/kubernetes/cloud.conf\u0026#34; controllerManager: extraArgs: cloud-provider: \u0026#34;openstack\u0026#34; cloud-config: \u0026#34;/etc/kubernetes/cloud.conf\u0026#34; extraVolumes: - name: cloud hostPath: \u0026#34;/etc/kubernetes/cloud.conf\u0026#34; mountPath: \u0026#34;/etc/kubernetes/cloud.conf\u0026#34; “树内”的云驱动通常需要在命令行中为 kube-apiserver、kube-controller-manager 和 kubelet 指定 --cloud-provider 和 --cloud-config。在 --cloud-config 中为每个供应商指定的文件的内容也同样需要写在下面。 对于所有外部云驱动，请遵循独立云存储库的说明，或浏览所有版本库清单\nAWS 本节介绍在 Amazon Web Services 上运行 Kubernetes 时可以使用的所有配置。 如果希望使用此外部云驱动，其代码库位于 kubernetes/cloud-provider-aws\n节点名称 云驱动 AWS 使用 AWS 实例的私有 DNS 名称作为 Kubernetes 节点对象的名称。\n负载均衡器 用户可以通过配置注解（annotations）来设置 外部负载均衡器，以在 AWS 中使用特定功能，如下所示：\napiVersion: v1 kind: Service metadata: name: example namespace: kube-system labels: run: example annotations: service.beta.kubernetes.io/aws-load-balancer-ssl-cert: arn:aws:acm:xx-xxxx-x:xxxxxxxxx:xxxxxxx/xxxxx-xxxx-xxxx-xxxx-xxxxxxxxx #replace this value service.beta.kubernetes.io/aws-load-balancer-backend-protocol: http spec: type: LoadBalancer ports: - port: 443 targetPort: 5556 protocol: TCP selector: app: example 可以使用 注解 将不同的设置应用于 AWS 中的负载均衡器服务。下面描述了 AWS ELB 所支持的注解：\n service.beta.kubernetes.io/aws-load-balancer-access-log-emit-interval：用于指定访问日志的间隔。 service.beta.kubernetes.io/aws-load-balancer-access-log-enabled：用于在服务中启用或禁用访问日志。 service.beta.kubernetes.io/aws-load-balancer-access-log-s3-bucket-name：用于指定访问日志的 S3 桶名称。 service.beta.kubernetes.io/aws-load-balancer-access-log-s3-bucket-prefix：用于指定访问日志的 S3 桶前缀。 service.beta.kubernetes.io/aws-load-balancer-additional-resource-tags：用于在服务中指定一个逗号分隔的键值对列表，它将作为附加标签被记录在 ELB 中。例如： \u0026quot;Key1=Val1,Key2=Val2,KeyNoVal1=,KeyNoVal2\u0026quot;。 service.beta.kubernetes.io/aws-load-balancer-backend-protocol：用于在服务中指定监听器后端（pod）所使用的协议。如果指定 http（默认）或 https，将创建一个终止连接和解析头的 HTTPS 监听器。 如果设置为 ssl 或 tcp，将会使用 “原生的” SSL 监听器。如果设置为 http且不使用 aws-load-balancer-ssl-cert，将使用 HTTP 监听器。 service.beta.kubernetes.io/aws-load-balancer-ssl-cert：用于在服务中请求安全监听器，其值为合法的证书 ARN（Amazon Resource Name）。更多内容，请参考 ELB 监听器配置。证书 ARN 是 IAM（身份和访问管理）或 CM（证书管理）类型的 ARN，例如 arn:aws:acm:us-east-1:123456789012:certificate/12345678-1234-1234-1234-123456789012。 service.beta.kubernetes.io/aws-load-balancer-connection-draining-enabled：用于在服务中启用或禁用连接耗尽（connection draining）。 service.beta.kubernetes.io/aws-load-balancer-connection-draining-timeout：用于在服务中指定连接耗尽超时时间。 service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout：用于在服务中指定空闲连接超时时间。 service.beta.kubernetes.io/aws-load-balancer-cross-zone-load-balancing-enabled：用于在服务中启用或禁用跨区域负载平衡。 service.beta.kubernetes.io/aws-load-balancer-extra-security-groups：用于在服务中指定要添加到创建的 ELB 中的其他安全组。 service.beta.kubernetes.io/aws-load-balancer-internal：用于在服务中表明需要内部 ELB。 service.beta.kubernetes.io/aws-load-balancer-proxy-protocol：用于在 ELB 上启用代理协议。 当前仅接受 *值，也就是在所有 ELB 后端启用代理协议。将来可能进行调整，只允许特定的后端设置代理协议。 service.beta.kubernetes.io/aws-load-balancer-ssl-ports：用于在服务中指定一个逗号分隔的端口列表，这些端口会使用 SSL/HTTPS 监听器。默认为 *（全部）  AWS 相关的注解信息取自 aws.go 文件的注释。\nAzure 如果希望使用此外部云驱动，其代码库位于 kubernetes/cloud-provider-azure\n节点名称 云驱动 Azure 使用节点的主机名（由 kubelet 决定，或者用 --hostname-override 覆盖）作为 Kubernetes 节点对象的名称。 注意 Kubernetes 节点名必须与 Azure 虚拟机的名称匹配。\nCloudStack 如果希望使用此外部云驱动，其代码库位于 apache/cloudstack-kubernetes-provider。\n节点名称 云驱动 CloudStack 使用节点的主机名（由 kubelet 决定，或者用 --hostname-override 覆盖）作为 Kubernetes 节点对象的名称。 注意 Kubernetes 节点名必须与 CloudStack 虚拟机名匹配。\nGCE 如果希望使用此外部云驱动，其代码库位于 kubernetes/cloud-provider-gcp\n节点名称 GCE 云驱动使用节点的主机名（由 kubelet 确定，或者用 --hostname-override 覆盖）作为 Kubernetes 节点对象的名称。 注意，Kubernetes 节点名的第一个字段必须匹配 GCE 实例名(例如，名为 kubernetes-node-2.c.my-proj.internal 的节点必须对应于一个名为 kubernetes-node-2 的实例)。\nOpenStack 本节介绍了使用 OpenStack 运行 Kubernetes 时所有可用的配置。 如果希望使用此外部云驱动，其代码库位于 kubernetes/cloud-provider-openstack\n节点名称 OpenStack 云驱动使用实例名（由 OpenStack 元数据确定）作为 Kubernetes 节点对象的名称。 请注意，实例名必须是一个有效的 Kubernetes 节点名，以便 kubelet 成功注册其节点对象。\n服务 Kubernetes 的 OpenStack 云驱动实现支持从底层云使用这些 OpenStack 服务：\n   服务 API 版本 必需     块存储 (Cinder) V1†, V2, V3 No   计算 (Nova) V2 No   身份认证 (Keystone) V2‡, V3 Yes   负载均衡器 (Neutron) V1§, V2 No   负载均衡器 (Octavia) V2 No    † Block Storage V1 版本的 API 被弃用，从 Kubernetes 1.9 版本开始加入了 Block Storage V3 版本 API。\n‡ Identity V2 API 支持已被弃用，将在未来的版本中从供应商中移除。从 “Queens” 版本开始，OpenStack 将不再支持 Identity V2 版本的 API。\n§ Kubernetes 1.9 中取消了对 V1 版本 Load Balancing API 的支持。\n服务发现是通过使用供应商配置中提供的 auth-url 所列出 OpenStack 身份认证（Keystone）管理的服务目录来实现的。 当除 Keystone 外的 OpenStack 服务不可用时，供应商将优雅地降低功能，并简单地放弃对受影响特性的支持。 某些功能还可以根据 Neutron 在底层云中发布的扩展列表启用或禁用。\ncloud.conf Kubernetes 知道如何通过 cloud.conf 文件与 OpenStack 交互。该文件将为 Kubernetes 提供 OpenStack 验证端点的凭据和位置。 用户可在创建 cloud.conf 文件时指定以下信息：\n典型配置 下面是一个典型配置的例子，它涉及到最常设置的值。它将供应商指向 OpenStack 云的 Keystone 端点，提供如何使用它进行身份验证的细节，并配置负载均衡器:\n[Global] username=user password=pass auth-url=https://\u0026lt;keystone_ip\u0026gt;/identity/v3 tenant-id=c869168a828847f39f7f06edd7305637 domain-id=2a73b8f597c04551a0fdc8e95544be8a [LoadBalancer] subnet-id=6937f8fa-858d-4bc9-a3a5-18d2c957166a 全局配置 这些配置选项属于 OpenStack 驱动的全局配置，并且应该出现在 cloud.conf 文件中的 [global] 部分:\n auth-url (必需): 用于认证的 keystone API 的 URL。在 OpenStack 控制面板中，这可以在“访问和安全（Access and Security）\u0026gt; API 访问（API Access）\u0026gt; 凭证（Credentials）”中找到。 username (必需): 指 keystone 中一个有效用户的用户名。 password (必需): 指 keystone 中一个有效用户的密码。 tenant-id (必需): 用于指定要创建资源的租户 ID。 tenant-name (可选): 用于指定要在其中创建资源的租户的名称。 trust-id (可选): 用于指定用于授权的信任的标识符。信任表示用户（委托人）将角色委托给另一个用户(受托人)的授权，并可选的允许受托人模仿委托人。可用的信任可以在 Keystone API 的 /v3/OS-TRUST/trusts 端点下找到。 domain-id (可选): 用于指定用户所属域的 ID。 domain-name (可选): 用于指定用户所属域的名称。 region (可选): 用于指定在多区域 OpenStack 云上运行时使用的区域标识符。区域是 OpenStack 部署的一般性划分。虽然区域没有严格的地理含义，但部署可以使用地理名称表示区域标识符，如 us-east。可用区域位于 Keystone API 的 /v3/regions 端点之下。 ca-file (可选): 用于指定自定义 CA 文件的路径。  当使用 Keystone V3 时(它将tenant更改为project)，tenant-id 值会自动映射到 API 中的项目。\n负载均衡器 这些配置选项属于 OpenStack 驱动的全局配置，并且应该出现在 cloud.conf 文件中的 [LoadBalancer] 部分:\n lb-version (可选): 用于覆盖自动版本检测。有效值为 v1 或 v2。如果没有提供值，则自动选择底层 OpenStack 云所支持的最高版本。 use-octavia (可选): 用于确定是否查找和使用 Octavia LBaaS V2 服务目录端点。有效值是 true 或 false。 如果指定了“true”，并且无法找到 Octaiva LBaaS V2 入口，则提供者将退回并尝试寻找一个 Neutron LBaaS V2 端点。默认值是 false。 subnet-id (可选): 用于指定要在其上创建负载均衡器的子网的 ID。 可以在 “Network \u0026gt; Networks” 上找到。 单击相应的网络以获得其子网。 floating-network-id (可选): 如果指定，将为负载均衡器创建一个浮动 IP。 lb-method (可选): 用于指定将负载分配到负载均衡器池成员的算法。值可以是 ROUND_ROBIN、LEAST_CONNECTIONS 或 SOURCE_IP。如果没有指定，默认行为是 ROUND_ROBIN。 lb-provider (可选): 用于指定负载均衡器的提供程序。如果没有指定，将使用在 Neutron 中配置的默认提供者服务。 create-monitor (可选): 指定是否为 Neutron 负载均衡器创建健康监视器。有效值是 true 和 false。 默认为 false。当指定 true 时，还必须设置 monitor-delay、monitor-timeout 和 monitor-max-retries。 monitor-delay (可选): 向负载均衡器的成员发送探测之间的时间间隔。 确保您指定了一个有效的时间单位。 有效时间单位为 ns、us (或 µs)、ms、s、m、h。 monitor-timeout (可选): 在超时之前，监视器等待 ping 响应的最长时间。该值必须小于延迟值。确保您指定了一个有效的时间单位。有效时间单位为 ns、 us (或 µs)、ms、s、m、 h。 monitor-max-retries (可选): 在将负载均衡器成员的状态更改为非活动之前，允许 ping 失败的次数。 必须是 1 到 10 之间的数字。 manage-security-groups (可选): 确定负载均衡器是否应自动管理安全组规则。有效值是 true 和 false。默认为 false。当指定 true 时，还必须提供 node-security-group。 node-security-group (可选): 要管理的安全组的 ID。  块存储 这些配置选项属于 OpenStack 驱动的全局配置，并且应该出现在 cloud.conf 文件中的 [BlockStorage] 部分：\n bs-version (可选): 指所使用的块存储 API 版本。其合法值为 v1、v2、v3和 auto。 auto 为默认值，将使用底层 Openstack 所支持的块存储 API 的最新版本。 trust-device-path (可选): 在大多数情况下，块设备名称由 Cinder 提供（例如：/dev/vda）不可信任。此布尔值切换此行为。将其设置为 true 将导致信任 Cinder 提供的块设备名称。默认值 false 会根据设备序列号和 /dev/disk/by-id 映射发现设备路径，推荐这种方法。 ignore-volume-az (可选): 用于在附加 Cinder 卷时影响可用区使用。 当 Nova 和 Cinder 有不同的可用区域时，应该将其设置为 true。 最常见的情况是，有许多 Nova 可用区，但只有一个 Cinder 可用区。 默认值是 false，以保持在早期版本中使用的行为，但是将来可能会更改。 node-volume-attach-limit (可选): 可连接到节点的最大卷数，对于 Cinder 默认为 256。  如果在 OpenStack 上部署 Kubernetes \u0026lt;= 1.8 的版本，同时使用路径而不是端口来区分端点（Endpoints），那么可能需要显式设置 bs-version 参数。 基于路径的端点形如 http://foo.bar/volume，而基于端口的的端点形如 http://foo.bar:xxx。\n在使用基于路径的端点，并且 Kubernetes 使用较旧的自动检索逻辑的环境中，尝试卷卸载（Detachment）会返回 BS API version autodetection failed. 错误。为了解决这个问题，可以通过添加以下内容到云驱动配置中，来强制使用 Cinder API V2 版本。\n[BlockStorage] bs-version=v2 元数据 这些配置选项属于 OpenStack 提供程序的全局配置，并且应该出现在 cloud.conf 文件中的 [Metadata] 部分：\n search-order (可选): 此配置键影响提供者检索与其运行的实例相关的元数据的方式。 configDrive，metadataService 的默认值导致供应商首先从配置驱动器中检索与实例相关的元数据（如果可用的话），然后检索元数据服务。 他们的替代值：  configDrive - 仅从配置驱动器检索实例元数据。 metadataService - 仅从元数据服务检索实例元数据。 metadataService,configDrive - 如果可用，首先从元数据服务检索实例元数据，然后从配置驱动器检索。    影响这种行为可能是可取的，因为配置驱动器上的元数据可能会随着时间的推移而变得陈旧，而元数据服务总是提供最新的数据视图。并不是所有的 OpenStack 云都同时提供配置驱动和元数据服务，可能只有一个或另一个可用，这就是为什么默认情况下要同时检查两个。\n路由 这些配置选项属于 OpenStack 驱动为 Kubernetes 网络插件 kubenet 提供的设置，并且应该出现在 cloud.conf 文件中的 [Route] 部分:\n router-id (可选)：如果底层云的 Neutron 部署支持 extraroutes 扩展，则使用 router-id 指定要添加路由的路由器。选择的路由器必须跨越包含集群节点的私有网络（通常只有一个节点网络，这个值应该是节点网络的默认路由器）。在 OpenStack 上使用 kubenet 时需要这个值。  OVirt 节点名称 OVirt 云驱动使用节点的主机名（由 kubelet 确定，或者用 --hostname-override 覆盖）作为 Kubernetes 节点对象的名称。 注意 Kubernetes 节点名必须与 VM FQDN 匹配（OVirt 在\u0026lt;vm\u0026gt;\u0026lt;guest_info\u0026gt;\u0026lt;fqdn\u0026gt;...\u0026lt;/fqdn\u0026gt;\u0026lt;/guest_info\u0026gt;\u0026lt;/vm\u0026gt;中说明）。\nPhoton 节点名称 Photon 云驱动使用节点的主机名（由 kubelet 决定，或者用 --hostname-override 覆盖）作为 Kubernetes 节点对象的名称。 注意，Kubernetes 节点名必须与 Photon VM名匹配（或者，如果在 --cloud-config 中将 overrideIP 设置为 true，则 Kubernetes 节点名必须与 Photon VM IP 地址匹配）。\nVSphere 节点名称 VSphere 云驱动使用节点检测到的主机名(由 kubelet 确定)作为 Kubernetes 节点对象的名称。 VSphere 云驱动会忽略 --hostname-override 参数。\nIBM Cloud Kubernetes Service 计算节点 通过使用 IBM Cloud Kubernetes Service 驱动，您可以在单个区域或跨区域的多个区（Region）中创建虚拟和物理（裸金属）节点的集群。 有关更多信息，请参见规划您的集群和工作节点设置。\nKubernetes 节点对象的名称是 IBM Cloud Kubernetes Services 工作节点实例的私有IP地址。\n网络 IBM Cloud Kubernetes Services 驱动提供 VLAN，用于提供高质量的网络性能和节点间的网络隔离。您可以设置自定义防火墙和 Calico 网络策略来为您的集群添加额外的安全层，或者通过 VPN 将您的集群连接到自有数据中心。有关更多信息，请参见规划集群内和私有网络。\n要向公众或集群内部公开应用程序，您可以利用 NodePort、LoadBalancer 或 Ingress 服务。您还可以使用注释自定义 Ingress 应用程序负载均衡器。有关更多信息，请参见计划使用外部网络公开您的应用程序。\n存储 IBM Cloud Kubernetes Services 驱动利用 Kubernetes 原生的持久卷，使用户能够将文件、块和云对象存储装载到他们的应用程序中。还可以使用 database-as-a-service 和第三方附加组件来持久存储数据。有关更多信息，请参见规划高可用性持久存储。\n百度云容器引擎 节点名称 Baidu 云驱动使用节点的私有 IP 地址（由 kubelet 确定，或者用 --hostname-override 覆盖）作为 Kubernetes 节点对象的名称。 注意 Kubernetes 节点名必须匹配百度 VM 的私有 IP。\n"
},
{
	"uri": "https://lijun.in/tasks/administer-cluster/network-policy-provider/kube-router-network-policy/",
	"title": "使用 Kube-router 作为 NetworkPolicy",
	"tags": [],
	"description": "",
	"content": "本页展示了如何使用 Kube-router 作为 NetworkPolicy。\n. heading \u0026ldquo;prerequisites\u0026rdquo; %}} 您需要拥有一个正在运行的 Kubernetes 集群。如果您还没有集群，可以使用任意的集群安装器如 Kops，Bootkube，Kubeadm 等创建一个。\n安装 Kube-router 插件 Kube-router 插件自带一个Network Policy 控制器，监视来自于Kubernetes API server 的 NetworkPolicy 和 pods 的变化，根据策略指示配置 iptables 规则和 ipsets 来允许或阻止流量。请根据 尝试通过集群安装器使用 Kube-router 指南安装 Kube-router 插件。\n. heading \u0026ldquo;whatsnext\u0026rdquo; %}} 在您安装 Kube-router 插件后，可以根据 声明 Network Policy 去尝试使用 Kubernetes NetworkPolicy。\n"
},
{
	"uri": "https://lijun.in/reference/access-authn-authz/admission-controllers/",
	"title": "使用准入控制器",
	"tags": [],
	"description": "",
	"content": "此页面概述了准入控制器。\n什么是准入控制插件？ 准入控制器是一段代码，它会在请求通过认证和授权之后、对象被持久化之前拦截到达 API 服务器的请求。控制器由下面的列表组成，并编译进 kube-apiserver 二进制文件，并且只能由集群管理员配置。在该列表中，有两个特殊的控制器：MutatingAdmissionWebhook 和 ValidatingAdmissionWebhook。它们根据 API 中的配置，分别执行变更和验证准入控制 webhook。\n准入控制器可以执行 “验证” 和/或 “变更” 操作。变更（mutating）控制器可以修改被其接受的对象；验证（validating）控制器则不行。\n准入控制过程分为两个阶段。第一阶段，运行变更准入控制器。第二阶段，运行验证准入控制器。 再次提醒，某些控制器既是变更准入控制器又是验证准入控制器。\n如果任何一个阶段的任何控制器拒绝了该请求，则整个请求将立即被拒绝，并向终端用户返回一个错误。\n最后，除了对对象进行变更外，准入控制器还可以有其它作用：将相关资源作为请求处理的一部分进行变更。 增加使用配额就是一个典型的示例，说明了这样做的必要性。 此类用法都需要相应的回收或回调过程，因为任一准入控制器都无法确定某个请能否通过所有其它准入控制器。\n为什么需要准入控制器？ Kubernetes 的许多高级功能都要求启用一个准入控制器，以便正确地支持该特性。因此，没有正确配置准入控制器的 Kubernetes API 服务器是不完整的，它无法支持您期望的所有特性。\n如何启用一个准入控制器？ Kubernetes API 服务器的 enable-admission-plugins 标志，它指定了一个用于在集群修改对象之前调用的（以逗号分隔的）准入控制插件顺序列表。\n例如，下面的命令就启用了 NamespaceLifecycle 和 LimitRanger 准入控制插件：\nkube-apiserver --enable-admission-plugins=NamespaceLifecycle,LimitRanger ... . note \u0026gt;}}\n根据您 Kubernetes 集群的部署方式以及 API 服务器的启动方式的不同，您可能需要以不同的方式应用设置。 例如，如果将 API 服务器部署为 systemd 服务，你可能需要修改 systemd 单元文件； 如果以自托管方式部署 Kubernetes，你可能需要修改 API 服务器的清单文件。 . /note \u0026gt;}}\n怎么关闭准入控制器？ Kubernetes API 服务器的 disable-admission-plugins 标志，会将传入的（以逗号分隔的）准入控制插件列表禁用，即使是默认启用的插件也会被禁用。\nkube-apiserver --disable-admission-plugins=PodNodeSelector,AlwaysDeny ... 哪些插件是默认启用的？ 下面的命令可以查看哪些插件是默认启用的：\nkube-apiserver -h | grep enable-admission-plugins 在 1.16 中，它们是：\nNamespaceLifecycle, LimitRanger, ServiceAccount, TaintNodesByCondition, Priority, DefaultTolerationSeconds, DefaultStorageClass, StorageObjectInUseProtection, PersistentVolumeClaimResize, MutatingAdmissionWebhook, ValidatingAdmissionWebhook, RuntimeClass, ResourceQuota 每个准入控制器的作用是什么？ AlwaysAdmit {#alwaysadmit} . feature-state for_k8s_version=\u0026quot;v1.13\u0026rdquo; state=\u0026quot;deprecated\u0026rdquo; \u0026gt;}} 该准入控制器会允许所有的 pod 接入集群。已废弃，因为它的行为根本就和没有准入控制器一样。\nAlwaysPullImages 该准入控制器会修改每一个新创建的 Pod 的镜像拉取策略为 Always 。 这在多租户集群中是有用的，这样用户就可以放心，他们的私有镜像只能被那些有凭证的人使用。 如果没有这个准入控制器，一旦镜像被拉取到节点上，任何用户的 pod 都可以通过已了解到的镜像的名称（假设 pod 被调度到正确的节点上）来使用它，而不需要对镜像进行任何授权检查。 当启用这个准入控制器时，总是在启动容器之前拉取镜像，这意味着需要有效的凭证。\nAlwaysDeny {#alwaysdeny} . feature-state for_k8s_version=\u0026quot;v1.13\u0026rdquo; state=\u0026quot;deprecated\u0026rdquo; \u0026gt;}} 拒绝所有的请求。由于没有实际意义，已废弃。\nDefaultStorageClass 该准入控制器监测没有请求任何特定存储类的 PersistentVolumeClaim 对象的创建，并自动向其添加默认存储类。 这样，没有任何特殊存储类需求的用户根本不需要关心它们，它们将获得默认存储类。\n当未配置默认存储类时，此准入控制器不执行任何操作。如果将多个存储类标记为默认存储类，它将拒绝任何创建 PersistentVolumeClaim 的操作，并显示错误。此时准入控制器会忽略任何 PersistentVolumeClaim 更新操作，仅响应创建操作。要修复此错误，管理员必须重新访问其 StorageClass 对象，并仅将其中一个标记为默认。\n关于持久化卷和存储类，以及如何将存储类标记为默认，请参见持久化卷。\nDefaultTolerationSeconds 该准入控制器为 Pod 设置默认的容忍度，在 5 分钟内容忍 notready:NoExecute 和 unreachable:NoExecute 污点。（如果 Pod 尚未容忍 node.kubernetes.io/not-ready：NoExecute 和 node.alpha.kubernetes.io/unreachable：NoExecute 污点的话）\nDenyExecOnPrivileged {#denyexeconprivileged} . feature-state for_k8s_version=\u0026quot;v1.13\u0026rdquo; state=\u0026quot;deprecated\u0026rdquo; \u0026gt;}} 如果一个 pod 拥有一个特权容器，该准入控制器将拦截所有在该 pod 中执行 exec 命令的请求。\n此功能已合并至 DenyEscalatingExec。 而 DenyExecOnPrivileged 准入插件已被废弃，并将在 v1.18 被移除。\n建议使用基于策略的准入插件（例如 PodSecurityPolicy 和自定义准入插件）， 该插件可以针对特定用户或命名空间，还可以防止创建权限过高的 Pod。\nDenyEscalatingExec {#denyescalatingexec} . feature-state for_k8s_version=\u0026quot;v1.13\u0026rdquo; state=\u0026quot;deprecated\u0026rdquo; \u0026gt;}} 该准入控制器将拒绝在由于拥有升级特权，而具备访问宿主机能力的 pod 中执行 exec 和 attach 命令。这包括在特权模式运行的 pod ，可以访问主机 IPC 命名空间的 pod ，和访问主机 PID 命名空间的 pod 。\nDenyExecOnPrivileged 准入插件已被废弃，并将在 v1.18 被移除。\n建议使用基于策略的准入插件（例如 PodSecurityPolicy 和自定义准入插件）， 该插件可以针对特定用户或命名空间，还可以防止创建权限过高的 Pod。\nEventRateLimit {#eventratelimit} . feature-state for_k8s_version=\u0026quot;v1.13\u0026rdquo; state=\u0026quot;alpha\u0026rdquo; \u0026gt;}} 该准入控制器缓解了事件请求淹没 API 服务器的问题。集群管理员可以通过以下方式指定事件速率限制：\n 确保 API 服务器的 --runtime-config 标志中包含了 eventratelimit.admission.k8s.io/v1alpha1=true； 启用 EventRateLimit 准入控制器； 从文件中引用 EventRateLimit 配置文件，并提供给 API 服务器命令的 --admission-control-config-file 标志：  . tabs name=\u0026quot;eventratelimit_example\u0026rdquo; \u0026gt;}} . tab name=\u0026quot;apiserver.config.k8s.io/v1\u0026rdquo; %}}\napiVersion: apiserver.config.k8s.io/v1 kind: AdmissionConfiguration plugins: - name: EventRateLimit path: eventconfig.yaml ... . /tab %}} . tab name=\u0026quot;apiserver.k8s.io/v1alpha1\u0026rdquo; %}}\n# Deprecated in v1.17 in favor of apiserver.config.k8s.io/v1 apiVersion: apiserver.k8s.io/v1alpha1 kind: AdmissionConfiguration plugins: - name: EventRateLimit path: eventconfig.yaml ... . /tab %}} . /tabs \u0026gt;}}\n可以在配置中指定四种类型的限制：\n Server: API 服务器收到的所有事件请求共享一个桶。 Namespace: 每个命名空间都有一个专用的桶。 User: 给每个用户都分配一个桶。 SourceAndObject: 根据事件的源和涉及对象的每种组合分配桶。  下面是一个配置示例 eventconfig.yaml：\napiVersion: eventratelimit.admission.k8s.io/v1alpha1 kind: Configuration limits: - type: Namespace qps: 50 burst: 100 cacheSize: 2000 - type: User qps: 10 burst: 50 详情请参见事件速率限制提案。\nExtendedResourceToleration 该插件有助于创建可扩展资源的专用节点。 如果运营商想创建可扩展资源的专用节点（如 GPU、FPGA 等）， 那他们应该以扩展资源名称作为键名，为节点设置污点。 如果启用了该准入控制器，会将此类污点的容忍自动添加到请求扩展资源的 Pod 中，用户不必再手动添加这些容忍。\nImagePolicyWebhook ImagePolicyWebhook 准入控制器允许使用一个后端的 webhook 做出准入决策。\n配置文件格式 ImagePolicyWebhook 使用配置文件来为后端行为设置配置选项。该文件可以是 json 或 yaml ，并具有以下格式:\nimagePolicy: kubeConfigFile: /path/to/kubeconfig/for/backend # time in s to cache approval allowTTL: 50 # time in s to cache denial denyTTL: 50 # time in ms to wait between retries retryBackoff: 500 # determines behavior if the webhook backend fails defaultAllow: true 从文件中引用 ImagePolicyWebhook 的配置文件，并将其提供给 API 服务器命令 --admission-control-config-file 标志：\n. tabs name=\u0026quot;imagepolicywebhook_example1\u0026rdquo; \u0026gt;}} . tab name=\u0026quot;apiserver.config.k8s.io/v1\u0026rdquo; %}}\napiVersion: apiserver.config.k8s.io/v1 kind: AdmissionConfiguration plugins: - name: ImagePolicyWebhook path: imagepolicyconfig.yaml ... . /tab %}} . tab name=\u0026quot;apiserver.k8s.io/v1alpha1\u0026rdquo; %}}\n# Deprecated in v1.17 in favor of apiserver.config.k8s.io/v1 apiVersion: apiserver.k8s.io/v1alpha1 kind: AdmissionConfiguration plugins: - name: ImagePolicyWebhook path: imagepolicyconfig.yaml ... . /tab %}} . /tabs \u0026gt;}}\n或者，您也可以直接将配置嵌入到文件中：\n. tabs name=\u0026quot;imagepolicywebhook_example2\u0026rdquo; \u0026gt;}} . tab name=\u0026quot;apiserver.config.k8s.io/v1\u0026rdquo; %}}\napiVersion: apiserver.config.k8s.io/v1 kind: AdmissionConfiguration plugins: - name: ImagePolicyWebhook configuration: imagePolicy: kubeConfigFile: \u0026lt;path-to-kubeconfig-file\u0026gt; allowTTL: 50 denyTTL: 50 retryBackoff: 500 defaultAllow: true . /tab %}} . tab name=\u0026quot;apiserver.k8s.io/v1alpha1\u0026rdquo; %}}\n# Deprecated in v1.17 in favor of apiserver.config.k8s.io/v1 apiVersion: apiserver.k8s.io/v1alpha1 kind: AdmissionConfiguration plugins: - name: ImagePolicyWebhook configuration: imagePolicy: kubeConfigFile: \u0026lt;path-to-kubeconfig-file\u0026gt; allowTTL: 50 denyTTL: 50 retryBackoff: 500 defaultAllow: true . /tab %}} . /tabs \u0026gt;}}\nImagePolicyWebhook 的配置文件必须引用 kubeconfig 格式的文件，该文件设置了到后端的连接，要求后端使用 TLS 进行通信。\nkubeconfig 文件的 cluster 字段需要指向远端服务，user 字段需要包含已返回的授权者。\n# clusters 指的是远程服务。 clusters: - name: name-of-remote-imagepolicy-service cluster: certificate-authority: /path/to/ca.pem # CA 用于验证远程服务 server: https://images.example.com/policy # 要查询的远程服务的 URL。必须是 \u0026#39;https\u0026#39; 。 # users 指的是 API 服务器的 Webhook 配置。 users: - name: name-of-api-server user: client-certificate: /path/to/cert.pem # webhook 准入控制器使用的证书 client-key: /path/to/key.pem # 证书匹配的密钥 HTTP 更多的配置，请参阅 kubeconfig 文档。\n请求载荷 当面对一个准入决策时，API server 发送一个描述操作的 JSON 序列化的 imagepolicy.k8s.io/v1alpha1 ImageReview 对象。该对象包含描述被审核容器的字段，以及所有匹配 *.image-policy.k8s.io/* 的 pod 注释。\n注意，webhook API 对象与其他 Kubernetes API 对象一样受制于相同的版本控制兼容性规则。实现者应该知道对 alpha 对象的更宽松的兼容性，并检查请求的 \u0026ldquo;apiVersion\u0026rdquo; 字段，以确保正确的反序列化。此外，API server 必须启用 imagepolicy.k8s.io/v1alpha1 API 扩展组 (--runtime-config=imagepolicy.k8s.io/v1alpha1=true)。\n请求载荷示例：\n{ \u0026#34;apiVersion\u0026#34;:\u0026#34;imagepolicy.k8s.io/v1alpha1\u0026#34;, \u0026#34;kind\u0026#34;:\u0026#34;ImageReview\u0026#34;, \u0026#34;spec\u0026#34;:{ \u0026#34;containers\u0026#34;:[ { \u0026#34;image\u0026#34;:\u0026#34;myrepo/myimage:v1\u0026#34; }, { \u0026#34;image\u0026#34;:\u0026#34;myrepo/myimage@sha256:beb6bd6a68f114c1dc2ea4b28db81bdf91de202a9014972bec5e4d9171d90ed\u0026#34; } ], \u0026#34;annotations\u0026#34;:{ \u0026#34;mycluster.image-policy.k8s.io/ticket-1234\u0026#34;: \u0026#34;break-glass\u0026#34; }, \u0026#34;namespace\u0026#34;:\u0026#34;mynamespace\u0026#34; } } 远程服务将填充请求的 ImageReviewStatus 字段，并返回允许或不允许访问的响应。响应体的 \u0026ldquo;spec\u0026rdquo; 字段会被忽略，并且可以省略。一个允许访问应答会返回：\n{ \u0026#34;apiVersion\u0026#34;: \u0026#34;imagepolicy.k8s.io/v1alpha1\u0026#34;, \u0026#34;kind\u0026#34;: \u0026#34;ImageReview\u0026#34;, \u0026#34;status\u0026#34;: { \u0026#34;allowed\u0026#34;: true } } 不允许访问，服务将返回：\n{ \u0026#34;apiVersion\u0026#34;: \u0026#34;imagepolicy.k8s.io/v1alpha1\u0026#34;, \u0026#34;kind\u0026#34;: \u0026#34;ImageReview\u0026#34;, \u0026#34;status\u0026#34;: { \u0026#34;allowed\u0026#34;: false, \u0026#34;reason\u0026#34;: \u0026#34;image currently blacklisted\u0026#34; } } 更多的文档，请参阅 imagepolicy.v1alpha1 API 对象 和 plugin/pkg/admission/imagepolicy/admission.go。\n使用注解进行扩展 一个 pod 中匹配 *.image-policy.k8s.io/* 的注解都会被发送给 webhook。这允许了解后端镜像策略的用户向它发送额外的信息，并为不同的后端实现接收不同的信息。\n您可以在这里输入的信息有：\n 在紧急情况下，请求 \u0026ldquo;break glass\u0026rdquo; 覆盖一个策略。 从一个记录了 break-glass 的请求的 ticket 系统得到的一个 ticket 号号码。 向策略服务器提供一个提示，用于提供镜像的 imageID，以方便它进行查找。  在任何情况下，注解都是由用户提供的，并不会被 Kubernetes 以任何方式进行验证。在将来，如果一个注解确定将被广泛使用，它可能会被提升为 ImageReviewSpec 的一个命名字段。\nLimitPodHardAntiAffinityTopology 该准入控制器拒绝（定义了 Anti Affinity 拓扑键的）任何 Pod（requiredDuringSchedulingRequiredDuringExecution 中的 kubernetes.io/hostname 除外）\nLimitRanger 该准入控制器会观察传入的请求，并确保它不会违反 Namespace 中 LimitRange 对象枚举的任何约束。如果您在 Kubernetes 部署中使用了 LimitRange 对象，则必须使用此准入控制器来执行这些约束。LimitRanger 还可以用于将默认资源请求应用到没有指定任何内容的 Pod；当前，默认的 LimitRanger 对 default 命名空间中的所有 pod 都应用了 0.1 CPU 的需求。\n请查看 limitRange 设计文档 和 Limit Range 例子了解更多细节。\nMutatingAdmissionWebhook {#mutatingadmissionwebhook} . feature-state for_k8s_version=\u0026quot;v1.13\u0026rdquo; state=\u0026quot;beta\u0026rdquo; \u0026gt;}} 该准入控制器调用任何与请求匹配的变更 webhook。匹配的 webhook 将被串行调用。每一个 webhook 都可以根据需要修改对象。\nMutatingAdmissionWebhook ，顾名思义，仅在变更阶段运行。\n如果由此准入控制器调用的 Webhook 有副作用（如降低配额）， 则它 必须 具有协调系统，因为不能保证后续的 Webhook 和验证准入控制器都会允许完成请求。\n如果你禁用了 MutatingAdmissionWebhook，那么还必须使用 --runtime-config 标志禁止 admissionregistration.k8s.io/v1beta1 组/版本中的 MutatingWebhookConfiguration 对象（版本 \u0026gt;=1.9 时，这两个对象都是默认启用的）。\n谨慎编写和安装变更 webhook  当用户尝试创建的对象与返回的对象不同时，用户可能会感到困惑。 当它们回读的对象与尝试创建的对象不同，内建的控制环可能会出问题。  与覆盖原始请求中设置的字段相比，使用原始请求未设置的字段会引起问题的可能性较小。应尽量避免前面那种方式。   这是一个 beta 特性。Kubernetes 未来的版本可能会限制这些 webhook 可以进行的变更类型。 内建资源和第三方资源的控制环，未来可能会受到破坏性的更改，使现在运行良好的 Webhook 无法再正常运行。即使完成了 webhook API 安装，也不代表会为该 webhook 提供无限期的支持。  NamespaceAutoProvision 该准入控制器会检查命名空间资源上的所有传入请求，并检查所引用的命名空间是否确实存在。如果找不到，它将创建一个命名空间。 此准入控制器对于不想要求命名空间必须先创建后使用的集群部署中很有用。\nNamespaceExists 该准入控制器检查除自身 Namespace 以外的命名空间资源上的所有请求。如果请求引用的命名空间不存在，则拒绝该请求。\nNamespaceLifecycle 该准入控制器禁止在一个正在被终止的 Namespace 中创建新对象，并确保使用不存在的 Namespace 的请求被拒绝。 该准入控制器还会禁止删除三个系统保留的命名空间，即 default、kube-system 和 kube-public。\n删除 Namespace 会触发删除该命名空间中所有对象（pod、services 等）的一系列操作。为了确保这个过程的完整性，我们强烈建议启用这个准入控制器。\nNodeRestriction 该准入控制器限制了 kubelet 可以修改的 Node 和 Pod 对象。 为了受到这个准入控制器的限制，kubelet 必须使用在 system:nodes 组中的凭证，并使用 system:node:\u0026lt;nodeName\u0026gt; 形式的用户名。这样，kubelet 只可修改自己的 Node API 对象，只能修改绑定到节点本身的 Pod 对象。\n在 Kubernetes 1.11+ 的版本中，不允许 kubelet 从 Node API 对象中更新或删除污点。\n在 Kubernetes 1.13+ 的版本中，NodeRestriction 准入插件可防止 kubelet 删除 Node API 对象，并对 kubernetes.io/ 或 k8s.io/ 前缀标签的 kubelet 强制进行如下修改：\n 防止 kubelets 添加/删除/更新带有 node-restriction.kubernetes.io/ 前缀的标签。保留此前缀的标签，供管理员用来标记 Node 对象以隔离工作负载，并且不允许 kubelet 修改带有该前缀的标签。 允许 kubelet 添加/删除/更新这些和这些前缀的标签：  kubernetes.io/hostname kubernetes.io/arch kubernetes.io/os beta.kubernetes.io/instance-type node.kubernetes.io/instance-type failure-domain.beta.kubernetes.io/region failure-domain.beta.kubernetes.io/zone topology.kubernetes.io/region topology.kubernetes.io/zone kubelet.kubernetes.io/-prefixed labels node.kubernetes.io/-prefixed labels    kubelet 保留 kubernetes.io 或 k8s.io 前缀的所有标签，并且将来可能会被 NodeRestriction 准入插件允许或禁止。\n将来的版本可能会增加其他限制，以确保 kubelet 具有正常运行所需的最小权限集。\nOwnerReferencesPermissionEnforcement 该准入控制器保护对 metadata.ownerReferences 对象的访问，以便只有对该对象具有 “删除” 权限的用户才能对其进行更改。该准入控制器还保护对 metadata.ownerReferences[x].blockOwnerDeletion 对象的访问，以便只有对所引用的 属主（owner） 的 finalizers 子资源具有 “更新” 权限的用户才能对其进行更改。\nPersistentVolumeLabel {#persistentvolumelabel} . feature-state for_k8s_version=\u0026quot;v1.13\u0026rdquo; state=\u0026quot;deprecated\u0026rdquo; \u0026gt;}} 该准入控制器会自动将区（region）或区域（zone）标签附加到由云提供商（如 GCE、AWS）定义的 PersistentVolumes 中。 这有助于确保 Pod 和 PersistentVolume 位于相同的区或区域。 如果准入控制器不支持为 PersistentVolumes 自动添加标签，那你可能需要手动添加标签，以防止 Pod 挂载其他区域的卷。 PersistentVolumeLabel 已被废弃，标记持久卷已由云管理控制器接管。 从 1.11 开始，默认情况下禁用此准入控制器。\nPodNodeSelector 该准入控制器通过读取命名空间注解和全局配置，来为命名空间中可以可以使用的节点选择器设置默认值并实施限制。\n配置文件格式 PodNodeSelector 使用配置文件来设置后端行为的选项。 请注意，配置文件格式将在将来某个版本中迁移为版本化文件。 该文件可以是 json 或 yaml，格式如下：\npodNodeSelectorPluginConfig: clusterDefaultNodeSelector: name-of-node-selector namespace1: name-of-node-selector namespace2: name-of-node-selector 从文件中引用 PodNodeSelector 配置文件，提供给 API 服务器命令行标志 --admission-control-config-file：\n. tabs name=\u0026quot;podnodeselector_example1\u0026rdquo; \u0026gt;}} . tab name=\u0026quot;apiserver.config.k8s.io/v1\u0026rdquo; %}}\napiVersion: apiserver.config.k8s.io/v1 kind: AdmissionConfiguration plugins: - name: PodNodeSelector path: podnodeselector.yaml ... . /tab %}} . tab name=\u0026quot;apiserver.k8s.io/v1alpha1\u0026rdquo; %}}\n# Deprecated in v1.17 in favor of apiserver.config.k8s.io/v1 apiVersion: apiserver.k8s.io/v1alpha1 kind: AdmissionConfiguration plugins: - name: PodNodeSelector path: podnodeselector.yaml ... . /tab %}} . /tabs \u0026gt;}}\n配置注解格式 PodNodeSelector 使用键为 scheduler.alpha.kubernetes.io/node-selector 的注解将节点选择器分配给命名空间。\napiVersion: v1 kind: Namespace metadata: annotations: scheduler.alpha.kubernetes.io/node-selector: name-of-node-selector name: namespace3 内部行为 该准入控制器行为如下：\n 如果 Namespace 的注解带有键 scheduler.alpha.kubernetes.io/node-selector ，则将其值用作节点选择器。 如果命名空间缺少此类注解，则使用 PodNodeSelector 插件配置文件中定义的 clusterDefaultNodeSelector 作为节点选择器。 评估 pod 节点选择器和命名空间节点选择器是否存在冲突。存在冲突将导致拒绝。 评估 pod 节点选择器和命名空间的白名单定义的插件配置文件是否存在冲突。存在冲突将导致拒绝。  . note \u0026gt;}}\nPodNodeSelector 允许 Pod 强制在特定标签的节点上运行。另请参阅 PodTolerationRestriction 准入插件，该插件可防止 Pod 在特定污点的节点上运行。 . /note \u0026gt;}}\nPersistentVolumeClaimResize 该准入控制器检查传入的 PersistentVolumeClaim 调整大小请求，对其执行额外的验证操作。\n. note \u0026gt;}}\n对调整卷大小的支持是一种 Alpha 特性。管理员必须将特性门控 ExpandPersistentVolumes 设置为 true 才能启用调整大小。 . /note \u0026gt;}}\n启用 ExpandPersistentVolumes 特性门控之后，建议将 PersistentVolumeClaimResize 准入控制器也启用。除非 PVC 的 StorageClass 明确地将 allowVolumeExpansion 设置为 true 来显式启用调整大小。否则，默认情况下该准入控制器会阻止所有对 PVC 大小的调整。\n例如：由以下 StorageClass 创建的所有 PersistentVolumeClaim 都支持卷容量扩充：\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: gluster-vol-default provisioner: kubernetes.io/glusterfs parameters: resturl: \u0026#34;http://192.168.10.100:8080\u0026#34; restuser: \u0026#34;\u0026#34; secretNamespace: \u0026#34;\u0026#34; secretName: \u0026#34;\u0026#34; allowVolumeExpansion: true 关于持久化卷申领的更多信息，请参见 PersistentVolumeClaims。\nPodPreset 该准入控制器根据与 PodPreset 中条件的匹配情况，将指定字段注入一个 pod。 另请参见 PodPreset 概念和使用 PodPreset 将信息注入 Pod 获取详情。\nPodSecurityPolicy 此准入控制器负责在创建和修改 pod 时根据请求的安全上下文和可用的 pod 安全策略确定是否可以执行请求。\n对于 Kubernetes \u0026lt; 1.6.0 的版本，API Server 必须启用 extensions/v1beta1/podsecuritypolicy API 扩展组 (--runtime-config=extensions/v1beta1/podsecuritypolicy=true)。\n查看 Pod 安全策略文档了解更多细节。\nPodTolerationRestriction 该准入控制器首先验证 Pod 的容忍度与其命名空间的容忍度之间的冲突。如果存在冲突，则拒绝 Pod 请求。 然后，它将命名空间的容忍度合并到 pod 的容忍度中，之后根据命名空间的容忍度白名单检查所得到的容忍度结果。 如果检查成功，则将接受 pod 请求，否则拒绝该请求。\n如果 pod 的命名空间没有任何关联的默认容忍度或容忍度白名单，则使用集群级别的默认容忍度或容忍度白名单（如果有的话）。\n命名空间的容忍度通过注解健 scheduler.alpha.kubernetes.io/defaultTolerations 和 scheduler.alpha.kubernetes.io/tolerationsWhitelist 设置。\n优先级 优先级准入控制器使用 priorityClassName 字段并用整型值填充优先级。如果找不到优先级，则拒绝 Pod。\nResourceQuota 该准入控制器会监测传入的请求，并确保它不违反任何一个 Namespace 中的 ResourceQuota 对象中枚举出来的约束。 如果您在 Kubernetes 部署中使用了 ResourceQuota ，您必须使用这个准入控制器来强制执行配额限制。\n请查看 resourceQuota 设计文档和 Resource Quota 例子了解更多细节。\n容器运行时类 {#runtimeclass} . feature-state for_k8s_version=\u0026quot;v1.16\u0026rdquo; state=\u0026quot;alpha\u0026rdquo; \u0026gt;}} 容器运行时类定义描述了与运行 Pod 相关的开销。此准入控制器将相应地设置 pod.Spec.Overhead 字段。\n详情请参见 Pod 开销。\nSecurityContextDeny 该准入控制器将拒绝任何试图设置特定提升 SecurityContext 字段的 pod。 如果集群没有使用 pod 安全策略来限制安全上下文所能获取的值集，那么应该启用这个功能。\nServiceAccount 该准入控制器实现了 serviceAccounts 的自动化。 如果您打算使用 Kubernetes 的 ServiceAccount 对象，我们强烈建议您使用这个准入控制器。\nStorageObjectInUseProtection StorageObjectInUseProtection 插件将 kubernetes.io/pvc-protection 或 kubernetes.io/pv-protection finalizers 添加到新创建的持久化卷声明（PVC）或持久化卷（PV）中。 如果用户尝试删除 PVC/PV，除非 PVC/PV 的保护控制器移除 finalizers，否则 PVC/PV 不会被删除。有关更多详细信息，请参考保护使用中的存储对象。\nTaintNodesByCondition {#taintnodesbycondition} . feature-state for_k8s_version=\u0026quot;v1.12\u0026rdquo; state=\u0026quot;beta\u0026rdquo; \u0026gt;}} 该准入控制器 . glossary_tooltip text=\u0026quot;污点\u0026rdquo; term_id=\u0026quot;taint\u0026rdquo; \u0026gt;}} 新创建的 NotReady 和 NoSchedule 节点。 避免了可能导致 Pod 在更新其污点以准确反映其所报告状况之前，就安排了在新节点上的竞争条件的情况。\nValidatingAdmissionWebhook {#validatingadmissionwebhook} . feature-state for_k8s_version=\u0026quot;v1.13\u0026rdquo; state=\u0026quot;beta\u0026rdquo; \u0026gt;}} 该准入控制器调用与请求匹配的所有验证 webhook。匹配的 webhook 将被并行调用。如果其中任何一个拒绝请求，则整个请求将失败。 该准入控制器仅在验证阶段运行；与 MutatingAdmissionWebhook 准入控制器所调用的 webhook 相反，它调用的 webhook 应该不会使对象出现变更。\n如果以此方式调用的 webhook 有其它作用（如，配额递减），则它必须具有协调系统，因为不能保证后续的 webhook 或其他有效的准入控制器都允许请求完成。\n\u0026lt;!- If you disable the ValidatingAdmissionWebhook, you must also disable the ValidatingWebhookConfiguration object in the admissionregistration.k8s.io/v1beta1 group/version via the --runtime-config flag (both are on by default in versions 1.9 and later). \u0026ndash;\u0026gt; 如果您禁用了 ValidatingAdmissionWebhook，还必须在 admissionregistration.k8s.io/v1beta1 组/版本中使用 --runtime-config 标志来禁用 ValidatingWebhookConfiguration 对象（默认情况下在 1.9 版和更高版本中均处于启用状态）。\n有推荐的准入控制器吗？ 有，对于 Kubernetes 1.10 以上的版本，推荐使用的准入控制器默认情况下都处于启用状态（查看这里）。 因此您无需显式指定它们。您可以使用 --enable-admission-plugins 标志（ 顺序不重要 ）来启用默认设置以外的其他准入控制器。\n. note \u0026gt;}}\n--admission-control 在 1.10 中已废弃，已由 --enable-admission-plugins 取代。 . /note \u0026gt;}}\n对于 Kubernetes 1.9 及更早版本，我们建议使用 --admission-control 标志（顺序很重要）运行下面的一组准入控制器。\n  v1.9\n--admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota    需要重申的是，在 1.9 中，它们都发生在变更阶段和验证阶段，例如 ResourceQuota 在验证阶段运行，因此是最后一个运行的准入控制器。 MutatingAdmissionWebhook 出现在此列表的前面，因为它在变更阶段运行。  对于更早期版本，没有验证和变更的概念，并且准入控制器按照指定的确切顺序运行。  "
},
{
	"uri": "https://lijun.in/tasks/job/coarse-parallel-processing-work-queue/",
	"title": "使用工作队列进行粗粒度并行处理",
	"tags": [],
	"description": "",
	"content": "本例中，我们会运行包含多个并行工作进程的 Kubernetes Job。\n本例中，每个 Pod 一旦被创建，会立即从任务队列中取走一个工作单元并完成它，然后将工作单元从队列中删除后再退出。\n下面是本次示例的主要步骤：\n  启动一个消息队列服务 本例中，我们使用 RabbitMQ，你也可以用其他的消息队列服务。在实际工作环境中，你可以创建一次消息队列服务然后在多个任务中重复使用。\n  创建一个队列，放上消息数据 每个消息表示一个要执行的任务。本例中，每个消息是一个整数值。我们将基于这个整数值执行很长的计算操作。\n  启动一个在队列中执行这些任务的 Job。该 Job 启动多个 Pod。每个 Pod 从消息队列中取走一个任务，处理它，然后重复执行，直到队列的队尾。\n  . heading \u0026ldquo;prerequisites\u0026rdquo; %}} 要熟悉 Job 基本用法（非并行的），请参考 Job。\n. include \u0026ldquo;task-tutorial-prereqs.md\u0026rdquo; \u0026gt;}} . version-check \u0026gt;}}\n启动消息队列服务 本例使用了 RabbitMQ，使用其他 AMQP 类型的消息服务应该比较容易。\n在实际工作中，在集群中一次性部署某个消息队列服务，之后在很多 Job 中复用，包括需要长期运行的服务。\n按下面的方法启动 RabbitMQ：\n$ kubectl create -f https://raw.githubusercontent.com/kubernetes/kubernetes/release-1.3/examples/celery-rabbitmq/rabbitmq-service.yaml service \u0026#34;rabbitmq-service\u0026#34; created $ kubectl create -f https://raw.githubusercontent.com/kubernetes/kubernetes/release-1.3/examples/celery-rabbitmq/rabbitmq-controller.yaml replicationcontroller \u0026#34;rabbitmq-controller\u0026#34; created 我们仅用到 celery-rabbitmq 示例 中描述的部分功能。\n测试消息队列服务 现在，我们可以试着访问消息队列。我们将会创建一个临时的可交互的 Pod，在它上面安装一些工具，然后用队列做实验。\n首先创建一个临时的可交互的 Pod：\n# 创建一个临时的可交互的 Pod $ kubectl run -i --tty temp --image ubuntu:14.04 Waiting for pod default/temp-loe07 to be running, status is Pending, pod ready: false ... [ previous line repeats several times .. hit return when it stops ] ... 请注意你的 Pod 名称和命令提示符将会不同。\n接下来安装 amqp-tools ，这样我们就能用消息队列了。\n# 安装一些工具 root@temp-loe07:/# apt-get update .... [ lots of output ] .... root@temp-loe07:/# apt-get install -y curl ca-certificates amqp-tools python dnsutils .... [ lots of output ] .... 后续，我们将制作一个包含这些包的 Docker 镜像。\n接着，我们将要验证我们发现 RabbitMQ 服务：\n# 请注意 rabbitmq-service 有Kubernetes 提供的 DNS 名称， root@temp-loe07:/# nslookup rabbitmq-service Server: 10.0.0.10 Address: 10.0.0.10#53 Name: rabbitmq-service.default.svc.cluster.local Address: 10.0.147.152 # 你的 IP 地址将会发生变化。 如果 Kube-DNS 没有正确安装，上一步可能会出错。 你也可以在环境变量中找到服务 IP。\n# env | grep RABBIT | grep HOST RABBITMQ_SERVICE_SERVICE_HOST=10.0.147.152 # 你的 IP 地址将会发生变化。 接着我们将要确认可以创建队列，并能发布消息和消费消息。\n# 下一行，rabbitmq-service 是访问 rabbitmq-service 的主机名。5672是 rabbitmq 的标准端口。 root@temp-loe07:/# export BROKER_URL=amqp://guest:guest@rabbitmq-service:5672 # 如果上一步中你不能解析 \u0026#34;rabbitmq-service\u0026#34;，可以用下面的命令替换： # root@temp-loe07:/# BROKER_URL=amqp://guest:guest@$RABBITMQ_SERVICE_SERVICE_HOST:5672 # 现在创建队列： root@temp-loe07:/# /usr/bin/amqp-declare-queue --url=$BROKER_URL -q foo -d foo # 向它推送一条消息: root@temp-loe07:/# /usr/bin/amqp-publish --url=$BROKER_URL -r foo -p -b Hello # 然后取回它. root@temp-loe07:/# /usr/bin/amqp-consume --url=$BROKER_URL -q foo -c 1 cat \u0026amp;\u0026amp; echo Hello root@temp-loe07:/# 最后一个命令中， amqp-consume 工具从队列中取走了一个消息，并把该消息传递给了随机命令的标准输出。在这种情况下，cat 只会打印它从标准输入或得的内容，echo 只会添加回车符以便示例可读。\n为队列增加任务 现在让我们给队列增加一些任务。在我们的示例中，任务是多个待打印的字符串。\n实践中，消息的内容可以是：\n 待处理的文件名 程序额外的参数 数据库表的关键字范围 模拟任务的配置参数 待渲染的场景的帧序列号  本例中，如果有大量的数据需要被 Job 的所有 Pod 读取，典型的做法是把它们放在一个共享文件系统中，如NFS，并以只读的方式挂载到所有 Pod，或者 Pod 中的程序从类似 HDFS 的集群文件系统中读取。\n例如，我们创建队列并使用 amqp 命令行工具向队列中填充消息。实践中，你可以写个程序来利用 amqp 客户端库来填充这些队列。\n$ /usr/bin/amqp-declare-queue --url=$BROKER_URL -q job1 -d job1 $ for f in apple banana cherry date fig grape lemon melon do /usr/bin/amqp-publish --url=$BROKER_URL -r job1 -p -b $f done 这样，我们给队列中填充了8个消息。\n创建镜像 现在我们可以创建一个做为 Job 来运行的镜像。\n我们将用 amqp-consume 来从队列中读取消息并实际运行我们的程序。这里给出一个非常简单的示例程序：\n. codenew language=\u0026quot;python\u0026rdquo; file=\u0026quot;application/job/rabbitmq/worker.py\u0026rdquo; \u0026gt;}}\n现在，编译镜像。如果你在用源代码树，那么切换到目录 examples/job/work-queue-1。否则的话，创建一个临时目录，切换到这个目录。下载 Dockerfile，和 worker.py。无论哪种情况，都可以用下面的命令编译镜像\n$ docker build -t job-wq-1 . 对于 Docker Hub, 给你的应用镜像打上标签，标签为你的用户名，然后用下面的命令推送到 Hub。用你的 Hub 用户名替换 \u0026lt;username\u0026gt;。\ndocker tag job-wq-1 \u0026lt;username\u0026gt;/job-wq-1 docker push \u0026lt;username\u0026gt;/job-wq-1 如果你在用谷歌容器仓库，用你的项目 ID 作为标签打到你的应用镜像上，然后推送到 GCR。用你的项目 ID 替换 \u0026lt;project\u0026gt;。\ndocker tag job-wq-1 gcr.io/\u0026lt;project\u0026gt;/job-wq-1 gcloud docker -- push gcr.io/\u0026lt;project\u0026gt;/job-wq-1 定义 Job 这里给出一个 Job 定义 yaml文件。你需要拷贝一份并编辑镜像以匹配你使用的名称，保存为 ./job.yaml。\n. codenew file=\u0026quot;application/job/rabbitmq/job.yaml\u0026rdquo; \u0026gt;}}\n本例中，每个 Pod 使用队列中的一个消息然后退出。这样，Job 的完成计数就代表了完成的工作项的数量。本例中我们设置 .spec.completions: 8，因为我们放了8项内容在队列中。\n运行 Job 现在我们运行 Job：\nkubectl create -f ./job.yaml 稍等片刻，然后检查 Job。\n$ kubectl describe jobs/job-wq-1 Name: job-wq-1 Namespace: default Selector: controller-uid=41d75705-92df-11e7-b85e-fa163ee3c11f Labels: controller-uid=41d75705-92df-11e7-b85e-fa163ee3c11f job-name=job-wq-1 Annotations: \u0026lt;none\u0026gt; Parallelism: 2 Completions: 8 Start Time: Wed, 06 Sep 2017 16:42:02 +0800 Pods Statuses: 0 Running / 8 Succeeded / 0 Failed Pod Template: Labels: controller-uid=41d75705-92df-11e7-b85e-fa163ee3c11f job-name=job-wq-1 Containers: c: Image: gcr.io/causal-jigsaw-637/job-wq-1 Port: Environment: BROKER_URL: amqp://guest:guest@rabbitmq-service:5672 QUEUE: job1 Mounts: \u0026lt;none\u0026gt; Volumes: \u0026lt;none\u0026gt; Events: FirstSeen LastSeen Count From SubobjectPath Type Reason Message ───────── ──────── ───── ──── ───────────── ────── ────── ─────── 27s 27s 1 {job } Normal SuccessfulCreate Created pod: job-wq-1-hcobb 27s 27s 1 {job } Normal SuccessfulCreate Created pod: job-wq-1-weytj 27s 27s 1 {job } Normal SuccessfulCreate Created pod: job-wq-1-qaam5 27s 27s 1 {job } Normal SuccessfulCreate Created pod: job-wq-1-b67sr 26s 26s 1 {job } Normal SuccessfulCreate Created pod: job-wq-1-xe5hj 15s 15s 1 {job } Normal SuccessfulCreate Created pod: job-wq-1-w2zqe 14s 14s 1 {job } Normal SuccessfulCreate Created pod: job-wq-1-d6ppa 14s 14s 1 {job } Normal SuccessfulCreate Created pod: job-wq-1-p17e0 我们所有的 Pod 都成功了。耶！\n替代方案 本文所讲述的处理方法的好处是你不需要修改你的 \u0026ldquo;worker\u0026rdquo; 程序使其知道工作队列的存在。\n本文所描述的方法需要你运行一个消息队列服务。如果不方便运行消息队列服务，你也许会考虑另外一种任务模式。\n本文所述的方法为每个工作项创建了一个 Pod。如果你的工作项仅需数秒钟，为每个工作项创建 Pod会增加很多的常规消耗。可以考虑另外的方案请参考示例，这种方案可以实现每个 Pod 执行多个工作项。\n示例中，我们使用 amqp-consume 从消息队列读取消息并执行我们真正的程序。这样的好处是你不需要修改你的程序使其知道队列的存在。要了解怎样使用客户端库和工作队列通信，请参考不同的示例。\n友情提醒 如果设置的完成数量小于队列中的消息数量，会导致一部分消息项不会被执行。\n如果设置的完成数量大于队列中的消息数量，当队列中所有的消息都处理完成后，Job 也会显示为未完成。Job 将创建 Pod 并阻塞等待消息输入。\n当发生下面两种情况时，即使队列中所有的消息都处理完了，Job 也不会显示为完成状态：\n 在 amqp-consume 命令拿到消息和容器成功退出之间的时间段内，执行杀死容器操作； 在 kubelet 向 api-server 传回 Pod 成功运行之前，发生节点崩溃。  "
},
{
	"uri": "https://lijun.in/setup/production-environment/tools/",
	"title": "使用部署工具安装 Kubernetes",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lijun.in/concepts/storage/volume-snapshot-classes/",
	"title": "卷快照类",
	"tags": [],
	"description": "",
	"content": "本文档描述了 Kubernetes 中 VolumeSnapshotClass 的概念。 建议熟悉卷快照（Volume Snapshots）和存储类（Storage Class）。\n介绍 就像 StorageClass 为管理员提供了一种在配置卷时描述存储“类”的方法，VolumeSnapshotClass 提供了一种在配置卷快照时描述存储“类”的方法。\nVolumeSnapshotClass 资源 每个 VolumeSnapshotClass 都包含 driver 、deletionPolicy 和 parameters 字段，当需要动态配置属于该类的 VolumeSnapshot 时使用。\nVolumeSnapshotClass 对象的名称很重要，是用户可以请求特定类的方式。 管理员在首次创建 VolumeSnapshotClass 对象时设置类的名称和其他参数，对象一旦创建就无法更新。\n管理员可以为不请求任何特定类绑定的 VolumeSnapshots 指定默认的 VolumeSnapshotClass。\napiVersion: snapshot.storage.k8s.io/v1beta1 kind: VolumeSnapshotClass metadata: name: csi-hostpath-snapclass driver: hostpath.csi.k8s.io deletionPolicy: Delete parameters: 驱动程序 卷快照类有一个驱动程序，用于确定配置 VolumeSnapshot 的 CSI 卷插件。 必须指定此字段。\n删除策略 卷快照类具有 deletionPolicy 属性。用户可以配置当所绑定的 VolumeSnapshot 对象将被删除时，如何处理 VolumeSnapshotContent 对象。卷快照的这个策略可以是 Retain 或者 Delete。这个策略字段必须指定。\n如果删除策略是 Delete，那么底层的存储快照会和 VolumeSnapshotContent 对象一起删除。如果删除策略是 Retain，那么底层快照和 VolumeSnapshotContent 对象都会被保留。\n参数 卷快照类具有描述属于该卷快照类的卷快照的参数。 可根据 driver 接受不同的参数。\n"
},
{
	"uri": "https://lijun.in/concepts/overview/working-with-objects/namespaces/",
	"title": "命名空间",
	"tags": [],
	"description": "",
	"content": "Kubernetes 支持多个虚拟集群，它们底层依赖于同一个物理集群。 这些虚拟集群被称为命名空间。\n何时使用多个命名空间 命名空间适用于存在很多跨多个团队或项目的用户的场景。对于只有几到几十个用户的集群，根本不需要创建或考虑命名空间。当需要名称空间提供的功能时，请开始使用它们。\n命名空间为名称提供了一个范围。资源的名称需要在命名空间内是唯一的，但不能跨命名空间。命名空间不能相互嵌套，每个 Kubernetes 资源只能在一个命名空间中。\n命名空间是在多个用户之间划分集群资源的一种方法（通过资源配额）。\n在 Kubernetes 未来版本中，相同命名空间中的对象默认将具有相同的访问控制策略。\n不需要使用多个命名空间来分隔轻微不同的资源，例如同一软件的不同版本：使用 labels 来区分同一命名空间中的不同资源。\n使用命名空间 命名空间的创建和删除已在命名空间的管理指南文档中进行了描述。\n查看命名空间 您可以使用以下命令列出集群中现存的命名空间：\nkubectl get namespace NAME STATUS AGE default Active 1d kube-system Active 1d kube-public Active 1d Kubernetes 会创建三个初始命名空间：\n default 没有指明使用其它命名空间的对象所使用的默认命名空间  kube-system The namespace for objects created by the Kubernetes system \u0026ndash;\u0026gt; kube-system Kubernetes 系统创建对象所使用的命名空间  kube-public This namespace is created automatically and is readable by all users (including those not authenticated). This namespace is mostly reserved for cluster usage, in case that some resources should be visible and readable publicly throughout the whole cluster. The public aspect of this namespace is only a convention, not a requirement. \u0026ndash;\u0026gt; kube-public 这个命名空间是自动创建的，所有用户（包括未经过身份验证的用户）都可以读取它。这个命名空间主要用于集群使用，以防某些资源在整个集群中应该是可见和可读的。这个命名空间的公共方面只是一种约定，而不是要求。  为请求设置命名空间 要为当前请求设置命名空间，请使用 --namespace 参数。\n例如：\nkubectl run nginx --image=nginx --namespace=\u0026lt;insert-namespace-name-here\u0026gt; kubectl get pods --namespace=\u0026lt;insert-namespace-name-here\u0026gt; 设置命名空间首选项 您可以永久保存该上下文中所有后续 kubectl 命令使用的命名空间。\nkubectl config set-context --current --namespace=\u0026lt;insert-namespace-name-here\u0026gt; # Validate it kubectl config view | grep namespace: 命名空间和 DNS 当您创建一个 Service 时，Kubernetes 会创建一个相应的 DNS 条目。\n该条目的形式是 \u0026lt;service-name\u0026gt;.\u0026lt;namespace-name\u0026gt;.svc.cluster.local，这意味着如果容器只使用 \u0026lt;service-name\u0026gt;，它将被解析到本地命名空间的服务。这对于跨多个命名空间（如开发、分级和生产）使用相同的配置非常有用。如果您希望跨命名空间访问，则需要使用完全限定域名（FQDN）。\n并非所有对象都在命名空间中 大多数 kubernetes 资源（例如 Pod、Service、副本控制器等）都位于某些命名空间中。但是命名空间资源本身并不在命名空间中。而且底层资源，例如 nodes 和持久化卷不属于任何命名空间。\n查看哪些 Kubernetes 资源在命名空间中，哪些不在命名空间中：\n# In a namespace kubectl api-resources --namespaced=true # Not in a namespace kubectl api-resources --namespaced=false   进一步了解建立新的命名空间。 进一步了解删除命名空间。  "
},
{
	"uri": "https://lijun.in/tasks/administer-cluster/network-policy-provider/",
	"title": "安装网络规则驱动",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lijun.in/reference/using-api/client-libraries/",
	"title": "客户端库",
	"tags": [],
	"description": "",
	"content": "本页面包含基于各种编程语言使用 Kubernetes API 的客户端库概述。\n在使用 Kubernetes REST API 编写应用程序时， 您并不需要自己实现 API 调用和 “请求/响应” 类型。 您可以根据自己的编程语言需要选择使用合适的客户端库。\n客户端库通常为您处理诸如身份验证之类的常见任务。 如果 API 客户端在 Kubernetes 集群中运行，大多数客户端库可以发现并使用 Kubernetes 服务帐户进行身份验证， 或者能够理解 kubeconfig 文件 格式来读取凭据和 API 服务器地址。\n官方支持的 Kubernetes 客户端库 以下客户端库由 Kubernetes SIG API Machinery 正式维护。\n   语言 客户端库 样例程序     Go github.com/kubernetes/client-go/ 浏览   Python github.com/kubernetes-client/python/ 浏览   Java github.com/kubernetes-client/java 浏览   dotnet github.com/kubernetes-client/csharp 浏览   JavaScript github.com/kubernetes-client/javascript 浏览   Haskell github.com/kubernetes-client/haskell 浏览    社区维护的客户端库 以下 Kubernetes API 客户端库是由社区，而非 Kubernetes 团队支持、维护的。\n   语言 客户端库     Clojure github.com/yanatan16/clj-kubernetes-api   Go github.com/ericchiang/k8s   Java (OSGi) bitbucket.org/amdatulabs/amdatu-kubernetes   Java (Fabric8, OSGi) github.com/fabric8io/kubernetes-client   Lisp github.com/brendandburns/cl-k8s   Lisp github.com/xh4/cube   Node.js (TypeScript) github.com/Goyoo/node-k8s-client   Node.js github.com/tenxcloud/node-kubernetes-client   Node.js github.com/godaddy/kubernetes-client   Node.js github.com/ajpauwels/easy-k8s   Perl metacpan.org/pod/Net::Kubernetes   PHP github.com/maclof/kubernetes-client   PHP github.com/allansun/kubernetes-php-client   PHP github.com/travisghansen/kubernetes-client-php   Python github.com/eldarion-gondor/pykube   Python github.com/mnubo/kubernetes-py   Ruby github.com/Ch00k/kuber   Ruby github.com/abonas/kubeclient   Ruby github.com/kontena/k8s-client   Rust github.com/clux/kube-rs   Rust github.com/ynqa/kubernetes-rust   Scala github.com/doriordan/skuber   dotNet github.com/tonnyeremin/kubernetes_gen   DotNet (RestSharp) github.com/masroorhasan/Kubernetes.DotNet   Elixir github.com/obmarg/kazan   Haskell github.com/soundcloud/haskell-kubernetes    "
},
{
	"uri": "https://lijun.in/concepts/containers/container-lifecycle-hooks/",
	"title": "容器生命周期钩子",
	"tags": [],
	"description": "",
	"content": "这个页面描述了 kubelet 管理的容器如何使用容器生命周期钩子框架来运行在其管理生命周期中由事件触发的代码。\n概述 类似于许多具有生命周期钩子组件的编程语言框架，例如 Angular、Kubernetes 为容器提供了生命周期钩子。 钩子使容器能够了解其管理生命周期中的事件，并在执行相应的生命周期钩子时运行在处理程序中实现的代码。\n容器钩子 有两个钩子暴露在容器中:\nPostStart\n这个钩子在创建容器之后立即执行。 但是，不能保证钩子会在容器入口点之前执行。 没有参数传递给处理程序。\nPreStop\n在容器终止之前是否立即调用此钩子，取决于 API 的请求或者管理事件，类似活动探针故障、资源抢占、资源竞争等等。 如果容器已经完全处于终止或者完成状态，则对 preStop 钩子的调用将失败。 它是阻塞的，同时也是同步的，因此它必须在删除容器的调用之前完成。 没有参数传递给处理程序。\n有关终止行为的更详细描述，请参见终止 Pod。\n钩子处理程序的实现 容器可以通过实现和注册该钩子的处理程序来访问该钩子。 针对容器，有两种类型的钩子处理程序可供实现：\n Exec - 执行一个特定的命令，例如 pre-stop.sh，在容器的 cgroups 和名称空间中。 命令所消耗的资源根据容器进行计算。 HTTP - 对容器上的特定端点执行 HTTP 请求。  钩子处理程序执行 当调用容器生命周期管理钩子时，Kubernetes 管理系统在为该钩子注册的容器中执行处理程序。\n钩子处理程序调用在包含容器的 Pod 上下文中是同步的。 这意味着对于 PostStart 钩子，容器入口点和钩子异步触发。 但是，如果钩子运行或挂起的时间太长，则容器无法达到 running 状态。\n行为与 PreStop 钩子的行为类似。 如果钩子在执行过程中挂起，Pod 阶段将保持在 Terminating 状态，并在 Pod 结束的 terminationGracePeriodSeconds 之后被杀死。 如果 PostStart 或 PreStop 钩子失败，它会杀死容器。\n用户应该使他们的钩子处理程序尽可能的轻量级。 但也需要考虑长时间运行的命令也很有用的情况，比如在停止容器之前保存状态。\n钩子寄送保证 钩子的寄送应该是 至少一次，这意味着对于任何给定的事件，例如 PostStart 或 PreStop，钩子可以被调用多次。 如何正确处理，是钩子实现所要考虑的问题。\n通常情况下，只会进行单次寄送。 例如，如果 HTTP 钩子接收器宕机，无法接收流量，则不会尝试重新发送。 然而，偶尔也会发生重复寄送的可能。 例如，如果 kubelet 在发送钩子的过程中重新启动，钩子可能会在 kubelet 恢复后重新发送。\n调试钩子处理程序 钩子处理程序的日志不会在 Pod 事件中公开。 如果处理程序由于某种原因失败，它将播放一个事件。 对于 PostStart，这是 FailedPostStartHook 事件，对于 PreStop，这是 FailedPreStopHook 事件。 您可以通过运行 kubectl describe pod \u0026lt;pod_name\u0026gt; 命令来查看这些事件。 下面是运行这个命令的一些事件输出示例:\nEvents: FirstSeen LastSeen Count From SubobjectPath Type Reason Message --------- -------- ----- ---- ------------- -------- ------ ------- 1m 1m 1 {default-scheduler } Normal Scheduled Successfully assigned test-1730497541-cq1d2 to gke-test-cluster-default-pool-a07e5d30-siqd 1m 1m 1 {kubelet gke-test-cluster-default-pool-a07e5d30-siqd} spec.containers{main} Normal Pulling pulling image \u0026quot;test:1.0\u0026quot; 1m 1m 1 {kubelet gke-test-cluster-default-pool-a07e5d30-siqd} spec.containers{main} Normal Created Created container with docker id 5c6a256a2567; Security:[seccomp=unconfined] 1m 1m 1 {kubelet gke-test-cluster-default-pool-a07e5d30-siqd} spec.containers{main} Normal Pulled Successfully pulled image \u0026quot;test:1.0\u0026quot; 1m 1m 1 {kubelet gke-test-cluster-default-pool-a07e5d30-siqd} spec.containers{main} Normal Started Started container with docker id 5c6a256a2567 38s 38s 1 {kubelet gke-test-cluster-default-pool-a07e5d30-siqd} spec.containers{main} Normal Killing Killing container with docker id 5c6a256a2567: PostStart handler: Error executing in Docker Container: 1 37s 37s 1 {kubelet gke-test-cluster-default-pool-a07e5d30-siqd} spec.containers{main} Normal Killing Killing container with docker id 8df9fdfd7054: PostStart handler: Error executing in Docker Container: 1 38s 37s 2 {kubelet gke-test-cluster-default-pool-a07e5d30-siqd} Warning FailedSync Error syncing pod, skipping: failed to \u0026quot;StartContainer\u0026quot; for \u0026quot;main\u0026quot; with RunContainerError: \u0026quot;PostStart handler: Error executing in Docker Container: 1\u0026quot; 1m 22s 2 {kubelet gke-test-cluster-default-pool-a07e5d30-siqd} spec.containers{main} Warning FailedPostStartHook   了解更多关于容器环境。 获取实践经验将处理程序附加到容器生命周期事件。  "
},
{
	"uri": "https://lijun.in/concepts/services-networking/connect-applications-service/",
	"title": "应用连接到 Service",
	"tags": [],
	"description": "",
	"content": "Kubernetes 连接容器模型 既然有了一个持续运行、可复制的应用，我们就能够将它暴露到网络上。 在讨论 Kubernetes 网络连接的方式之前，非常值得与 Docker 中 “正常” 方式的网络进行对比。\n默认情况下，Docker 使用私有主机网络连接，只能与同在一台机器上的容器进行通信。 为了实现容器的跨节点通信，必须在机器自己的 IP 上为这些容器分配端口，为容器进行端口转发或者代理。\n多个开发人员之间协调端口的使用很难做到规模化，那些难以控制的集群级别的问题，都会交由用户自己去处理。 Kubernetes 假设 Pod 可与其它 Pod 通信，不管它们在哪个主机上。 我们给 Pod 分配属于自己的集群私有 IP 地址，所以没必要在 Pod 或映射到的容器的端口和主机端口之间显式地创建连接。 这表明了在 Pod 内的容器都能够连接到本地的每个端口，集群中的所有 Pod 不需要通过 NAT 转换就能够互相看到。 文档的剩余部分将详述如何在一个网络模型之上运行可靠的服务。\n该指南使用一个简单的 Nginx server 来演示并证明谈到的概念。同样的原则也体现在一个更加完整的 Jenkins CI 应用 中。\n在集群中暴露 Pod 我们在之前的示例中已经做过，然而再让我重试一次，这次聚焦在网络连接的视角。 创建一个 Nginx Pod，指示它具有一个容器端口的说明：\ncodenew file=\u0026quot;service/networking/run-my-nginx.yaml\u0026rdquo; \u0026gt;}}\n这使得可以从集群中任何一个节点来访问它。检查节点，该 Pod 正在运行：\nkubectl apply -f ./run-my-nginx.yaml kubectl get pods -l run=my-nginx -o wide NAME READY STATUS RESTARTS AGE IP NODE my-nginx-3800858182-jr4a2 1/1 Running 0 13s 10.244.3.4 kubernetes-minion-905m my-nginx-3800858182-kna2y 1/1 Running 0 13s 10.244.2.5 kubernetes-minion-ljyd 检查 Pod 的 IP 地址：\nkubectl get pods -l run=my-nginx -o yaml | grep podIP podIP: 10.244.3.4 podIP: 10.244.2.5 应该能够通过 ssh 登录到集群中的任何一个节点上，使用 curl 也能调通所有 IP 地址。 需要注意的是，容器不会使用该节点上的 80 端口，也不会使用任何特定的 NAT 规则去路由流量到 Pod 上。 这意味着可以在同一个节点上运行多个 Pod，使用相同的容器端口，并且可以从集群中任何其他的 Pod 或节点上使用 IP 的方式访问到它们。 像 Docker 一样，端口能够被发布到主机节点的接口上，但是出于网络模型的原因应该从根本上减少这种用法。\n如果对此好奇，可以获取更多关于 如何实现网络模型 的内容。\n创建 Service 我们有 Pod 在一个扁平的、集群范围的地址空间中运行 Nginx 服务，可以直接连接到这些 Pod，但如果某个节点死掉了会发生什么呢？ Pod 会终止，Deployment 将创建新的 Pod，且使用不同的 IP。这正是 Service 要解决的问题。\nKubernetes Service 从逻辑上定义了运行在集群中的一组 Pod，这些 Pod 提供了相同的功能。 当每个 Service 创建时，会被分配一个唯一的 IP 地址（也称为 clusterIP）。 这个 IP 地址与一个 Service 的生命周期绑定在一起，当 Service 存在的时候它也不会改变。 可以配置 Pod 使它与 Service 进行通信，Pod 知道与 Service 通信将被自动地负载均衡到该 Service 中的某些 Pod 上。\n可以使用 kubectl expose 命令为 2个 Nginx 副本创建一个 Service：\nkubectl expose deployment/my-nginx service/my-nginx exposed 这等价于使用 kubectl create -f 命令创建，对应如下的 yaml 文件：\ncodenew file=\u0026quot;service/networking/nginx-svc.yaml\u0026rdquo; \u0026gt;}}\n上述规约将创建一个 Service，对应具有标签 run: my-nginx 的 Pod，目标 TCP 端口 80，并且在一个抽象的 Service 端口（targetPort：容器接收流量的端口；port：抽象的 Service 端口，可以使任何其它 Pod 访问该 Service 的端口）上暴露。 查看 [Service API 对象](/docs/api-reference/\u0026lt; param \u0026ldquo;version\u0026rdquo; \u0026gt;}}/#service-v1-core) 了解 Service 定义支持的字段列表。 查看你的 Service 资源:\nkubectl get svc my-nginx NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE my-nginx ClusterIP 10.0.162.149 \u0026lt;none\u0026gt; 80/TCP 21s 正如前面所提到的，一个 Service 由一组 backend Pod 组成。这些 Pod 通过 endpoints 暴露出来。 Service Selector 将持续评估，结果被 POST 到一个名称为 my-nginx 的 Endpoint 对象上。 当 Pod 终止后，它会自动从 Endpoint 中移除，新的能够匹配上 Service Selector 的 Pod 将自动地被添加到 Endpoint 中。 检查该 Endpoint，注意到 IP 地址与在第一步创建的 Pod 是相同的。\nkubectl describe svc my-nginx Name: my-nginx Namespace: default Labels: run=my-nginx Annotations: \u0026lt;none\u0026gt; Selector: run=my-nginx Type: ClusterIP IP: 10.0.162.149 Port: \u0026lt;unset\u0026gt; 80/TCP Endpoints: 10.244.2.5:80,10.244.3.4:80 Session Affinity: None Events: \u0026lt;none\u0026gt; kubectl get ep my-nginx NAME ENDPOINTS AGE my-nginx 10.244.2.5:80,10.244.3.4:80 1m 现在，能够从集群中任意节点上使用 curl 命令请求 Nginx Service \u0026lt;CLUSTER-IP\u0026gt;:\u0026lt;PORT\u0026gt; 。 注意 Service IP 完全是虚拟的，它从来没有走过网络，如果对它如何工作的原理感到好奇， 可以阅读更多关于 服务代理 的内容。\n访问 Service Kubernetes支持两种查找服务的主要模式: 环境变量和DNS。 前者开箱即用，而后者则需要[CoreDNS集群插件] [CoreDNS 集群插件](http://releases.k8s.io/\u0026lt; param \u0026ldquo;githubbranch\u0026rdquo; \u0026gt;}}/cluster/addons/dns/coredns).\n如果不需要服务环境变量（因为可能与预期的程序冲突，可能要处理的变量太多，或者仅使用DNS等），则可以通过在 [pod spec](/docs/reference/generated/kubernetes-api/ param \u0026ldquo;version\u0026rdquo; \u0026gt;}}/#pod-v1-core)上将 enableServiceLinks 标志设置为 false 来禁用此模式。\n环境变量 当 Pod 在 Node 上运行时，kubelet 会为每个活跃的 Service 添加一组环境变量。 这会有一个顺序的问题。想了解为何，检查正在运行的 Nginx Pod 的环境变量（Pod 名称将不会相同）：\nkubectl exec my-nginx-3800858182-jr4a2 -- printenv | grep SERVICE KUBERNETES_SERVICE_HOST=10.0.0.1 KUBERNETES_SERVICE_PORT=443 KUBERNETES_SERVICE_PORT_HTTPS=443 注意，还没有谈及到 Service。这是因为创建副本先于 Service。 这样做的另一个缺点是，调度器可能在同一个机器上放置所有 Pod，如果该机器宕机则所有的 Service 都会挂掉。 正确的做法是，我们杀掉 2 个 Pod，等待 Deployment 去创建它们。 这次 Service 会 先于 副本存在。这将实现调度器级别的 Service，能够使 Pod 分散创建（假定所有的 Node 都具有同样的容量），以及正确的环境变量：\nkubectl scale deployment my-nginx --replicas=0; kubectl scale deployment my-nginx --replicas=2; kubectl get pods -l run=my-nginx -o wide NAME READY STATUS RESTARTS AGE IP NODE my-nginx-3800858182-e9ihh 1/1 Running 0 5s 10.244.2.7 kubernetes-minion-ljyd my-nginx-3800858182-j4rm4 1/1 Running 0 5s 10.244.3.8 kubernetes-minion-905m 可能注意到，Pod 具有不同的名称，因为它们被杀掉后并被重新创建。\nkubectl exec my-nginx-3800858182-e9ihh -- printenv | grep SERVICE KUBERNETES_SERVICE_PORT=443 MY_NGINX_SERVICE_HOST=10.0.162.149 KUBERNETES_SERVICE_HOST=10.0.0.1 MY_NGINX_SERVICE_PORT=80 KUBERNETES_SERVICE_PORT_HTTPS=443 DNS Kubernetes 提供了一个 DNS 插件 Service，它使用 skydns 自动为其它 Service 指派 DNS 名字。 如果它在集群中处于运行状态，可以通过如下命令来检查：\nkubectl get services kube-dns --namespace=kube-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kube-dns ClusterIP 10.0.0.10 \u0026lt;none\u0026gt; 53/UDP,53/TCP 8m 如果没有在运行，可以 [启用它](http://releases.k8s.io/\u0026lt; param \u0026ldquo;githubbranch\u0026rdquo; \u0026gt;}}/cluster/addons/dns/kube-dns/README.md#how-do-i-configure-it)。 本段剩余的内容，将假设已经有一个 Service，它具有一个长久存在的 IP（my-nginx），一个为该 IP 指派名称的 DNS 服务器（kube-dns 集群插件），所以可以通过标准做法，使在集群中的任何 Pod 都能与该 Service 通信（例如：gethostbyname）。 让我们运行另一个 curl 应用来进行测试：\nkubectl run curl --image=radial/busyboxplus:curl -i --tty Waiting for pod default/curl-131556218-9fnch to be running, status is Pending, pod ready: false Hit enter for command prompt 然后，按回车并执行命令 nslookup my-nginx：\n[ root@curl-131556218-9fnch:/ ]$ nslookup my-nginx Server: 10.0.0.10 Address 1: 10.0.0.10 Name: my-nginx Address 1: 10.0.162.149 Service 安全 到现在为止，我们只在集群内部访问了 Nginx server。在将 Service 暴露到 Internet 之前，我们希望确保通信信道是安全的。对于这可能需要：\n https 自签名证书（除非已经有了一个识别身份的证书） 使用证书配置的 Nginx server 使证书可以访问 Pod 的秘钥  可以从 [Nginx https 示例](https://github.com/kubernetes/kubernetes/tree/\u0026lt; param \u0026ldquo;githubbranch\u0026rdquo; \u0026gt;}}/examples/https-nginx/) 获取所有上述内容，简明示例如下：\nmake keys KEY=/tmp/nginx.key CERT=/tmp/nginx.crt kubectl create secret tls nginxsecret --key /tmp/nginx.key --cert /tmp/nginx.crt secret/nginxsecret created kubectl get secrets NAME TYPE DATA AGE default-token-il9rc kubernetes.io/service-account-token 1 1d nginxsecret Opaque 2 1m 以下是您在运行make时遇到问题时要遵循的手动步骤（例如，在Windows上）：\n#create a public private key pair openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout /d/tmp/nginx.key -out /d/tmp/nginx.crt -subj \u0026#34;/CN=my-nginx/O=my-nginx\u0026#34; #convert the keys to base64 encoding cat /d/tmp/nginx.crt | base64 cat /d/tmp/nginx.key | base64 使用前面命令的输出来创建yaml文件，如下所示。 base64编码的值应全部放在一行上。\napiVersion: \u0026#34;v1\u0026#34; kind: \u0026#34;Secret\u0026#34; metadata: name: \u0026#34;nginxsecret\u0026#34; namespace: \u0026#34;default\u0026#34; data: nginx.crt: \u0026#34;LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURIekNDQWdlZ0F3SUJBZ0lKQUp5M3lQK0pzMlpJTUEwR0NTcUdTSWIzRFFFQkJRVUFNQ1l4RVRBUEJnTlYKQkFNVENHNW5hVzU0YzNaak1SRXdEd1lEVlFRS0V3aHVaMmx1ZUhOMll6QWVGdzB4TnpFd01qWXdOekEzTVRKYQpGdzB4T0RFd01qWXdOekEzTVRKYU1DWXhFVEFQQmdOVkJBTVRDRzVuYVc1NGMzWmpNUkV3RHdZRFZRUUtFd2h1CloybHVlSE4yWXpDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBSjFxSU1SOVdWM0IKMlZIQlRMRmtobDRONXljMEJxYUhIQktMSnJMcy8vdzZhU3hRS29GbHlJSU94NGUrMlN5ajBFcndCLzlYTnBwbQppeW1CL3JkRldkOXg5UWhBQUxCZkVaTmNiV3NsTVFVcnhBZW50VWt1dk1vLzgvMHRpbGhjc3paenJEYVJ4NEo5Ci82UVRtVVI3a0ZTWUpOWTVQZkR3cGc3dlVvaDZmZ1Voam92VG42eHNVR0M2QURVODBpNXFlZWhNeVI1N2lmU2YKNHZpaXdIY3hnL3lZR1JBRS9mRTRqakxCdmdONjc2SU90S01rZXV3R0ljNDFhd05tNnNTSzRqYUNGeGpYSnZaZQp2by9kTlEybHhHWCtKT2l3SEhXbXNhdGp4WTRaNVk3R1ZoK0QrWnYvcW1mMFgvbVY0Rmo1NzV3ajFMWVBocWtsCmdhSXZYRyt4U1FVQ0F3RUFBYU5RTUU0d0hRWURWUjBPQkJZRUZPNG9OWkI3YXc1OUlsYkROMzhIYkduYnhFVjcKTUI4R0ExVWRJd1FZTUJhQUZPNG9OWkI3YXc1OUlsYkROMzhIYkduYnhFVjdNQXdHQTFVZEV3UUZNQU1CQWY4dwpEUVlKS29aSWh2Y05BUUVGQlFBRGdnRUJBRVhTMW9FU0lFaXdyMDhWcVA0K2NwTHI3TW5FMTducDBvMm14alFvCjRGb0RvRjdRZnZqeE04Tzd2TjB0clcxb2pGSW0vWDE4ZnZaL3k4ZzVaWG40Vm8zc3hKVmRBcStNZC9jTStzUGEKNmJjTkNUekZqeFpUV0UrKzE5NS9zb2dmOUZ3VDVDK3U2Q3B5N0M3MTZvUXRUakViV05VdEt4cXI0Nk1OZWNCMApwRFhWZmdWQTRadkR4NFo3S2RiZDY5eXM3OVFHYmg5ZW1PZ05NZFlsSUswSGt0ejF5WU4vbVpmK3FqTkJqbWZjCkNnMnlwbGQ0Wi8rUUNQZjl3SkoybFIrY2FnT0R4elBWcGxNSEcybzgvTHFDdnh6elZPUDUxeXdLZEtxaUMwSVEKQ0I5T2wwWW5scE9UNEh1b2hSUzBPOStlMm9KdFZsNUIyczRpbDlhZ3RTVXFxUlU9Ci0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K\u0026#34; nginx.key: \u0026#34;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\u0026#34; 现在使用文件创建 secrets：\nkubectl apply -f nginxsecrets.yaml kubectl get secrets NAME TYPE DATA AGE default-token-il9rc kubernetes.io/service-account-token 1 1d nginxsecret Opaque 2 1m 现在修改 Nginx 副本，启动一个使用在秘钥中的证书的 https 服务器和 Servcie，都暴露端口（80 和 443）：\ncodenew file=\u0026quot;service/networking/nginx-secure-app.yaml\u0026rdquo; \u0026gt;}}\n关于 nginx-secure-app manifest 值得注意的点如下：\n 它在相同的文件中包含了 Deployment 和 Service 的规格 [Nginx server](https://github.com/kubernetes/kubernetes/tree/param \u0026ldquo;githubbranch\u0026rdquo; \u0026gt;}}/examples/https-nginx/default.conf) 处理 80 端口上的 http 流量，以及 443 端口上的 https 流量，Nginx Service 暴露了这两个端口。 每个容器访问挂载在 /etc/nginx/ssl 卷上的秘钥。这需要在 Nginx server 启动之前安装好。  kubectl delete deployments,svc my-nginx; kubectl create -f ./nginx-secure-app.yaml 这时可以从任何节点访问到 Nginx server。\nkubectl get pods -o yaml | grep -i podip podIP: 10.244.3.5 node $ curl -k https://10.244.3.5 ... \u0026lt;h1\u0026gt;Welcome to nginx!\u0026lt;/h1\u0026gt; 注意最后一步我们是如何提供 -k 参数执行 curl命令的，这是因为在证书生成时，我们不知道任何关于运行 Nginx 的 Pod 的信息，所以不得不在执行 curl 命令时忽略 CName 不匹配的情况。 通过创建 Service，我们连接了在证书中的 CName 与在 Service 查询时被 Pod使用的实际 DNS 名字。 让我们从一个 Pod 来测试（为了简化使用同一个秘钥，Pod 仅需要使用 nginx.crt 去访问 Service）：\ncodenew file=\u0026quot;service/networking/curlpod.yaml\u0026rdquo; \u0026gt;}}\nkubectl apply -f ./curlpod.yaml kubectl get pods -l app=curlpod NAME READY STATUS RESTARTS AGE curl-deployment-1515033274-1410r 1/1 Running 0 1m kubectl exec curl-deployment-1515033274-1410r -- curl https://my-nginx --cacert /etc/nginx/ssl/nginx.crt ... \u0026lt;title\u0026gt;Welcome to nginx!\u0026lt;/title\u0026gt; ... 暴露 Service 对我们应用的某些部分，可能希望将 Service 暴露在一个外部 IP 地址上。 Kubernetes 支持两种实现方式：NodePort 和 LoadBalancer。 在上一段创建的 Service 使用了 NodePort，因此 Nginx https 副本已经就绪，如果使用一个公网 IP，能够处理 Internet 上的流量。\nkubectl get svc my-nginx -o yaml | grep nodePort -C 5 uid: 07191fb3-f61a-11e5-8ae5-42010af00002 spec: clusterIP: 10.0.162.149 ports: - name: http nodePort: 31704 port: 8080 protocol: TCP targetPort: 80 - name: https nodePort: 32453 port: 443 protocol: TCP targetPort: 443 selector: run: my-nginx kubectl get nodes -o yaml | grep ExternalIP -C 1 - address: 104.197.41.11 type: ExternalIP allocatable: -- - address: 23.251.152.56 type: ExternalIP allocatable: ... $ curl https://\u0026lt;EXTERNAL-IP\u0026gt;:\u0026lt;NODE-PORT\u0026gt; -k ... \u0026lt;h1\u0026gt;Welcome to nginx!\u0026lt;/h1\u0026gt; 让我们重新创建一个 Service，使用一个云负载均衡器，只需要将 my-nginx Service 的 Type 由 NodePort 改成 LoadBalancer。\nkubectl edit svc my-nginx kubectl get svc my-nginx NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE my-nginx ClusterIP 10.0.162.149 162.222.184.144 80/TCP,81/TCP,82/TCP 21s curl https://\u0026lt;EXTERNAL-IP\u0026gt; -k ... \u0026lt;title\u0026gt;Welcome to nginx!\u0026lt;/title\u0026gt; 在 EXTERNAL-IP 列指定的 IP 地址是在公网上可用的。CLUSTER-IP 只在集群/私有云网络中可用。\n注意，在 AWS 上类型 LoadBalancer 创建一个 ELB，它使用主机名（比较长），而不是 IP。 它太长以至于不能适配标准 kubectl get svc 的输出，事实上需要通过执行 kubectl describe service my-nginx 命令来查看它。 可以看到类似如下内容：\nkubectl describe service my-nginx ... LoadBalancer Ingress: a320587ffd19711e5a37606cf4a74574-1142138393.us-east-1.elb.amazonaws.com ...  Kubernetes 也支持联合 Service，能够跨多个集群和云提供商，为 Service 提供逐步增强的可用性、更优的容错、更好的可伸缩性。 查看 联合 Service 用户指南 获取更进一步信息。\n"
},
{
	"uri": "https://lijun.in/concepts/architecture/controller/",
	"title": "控制器",
	"tags": [],
	"description": "",
	"content": "在机器人技术和自动化中，控制环是一个控制系统状态的不终止的循环。\n这是一个控制环的例子：房间里的温度自动调节器。\n当你设置了温度，告诉了温度自动调节器你的期望状态。房间的实际温度是当前状态。通过对设备的开关控制，温度自动调节器让其当前状态接近期望状态。\n控制器模式 一个控制器至少追踪一种类型的 Kubernetes 资源。这些对象有一个代表期望状态的指定字段。控制器负责确保其追踪的资源对象的当前状态接近期望状态。\n控制器可能会自行执行操作；在 Kubernetes 中更常见的是一个控制器会发送信息给 API 服务器，这会有副作用。看下面这个例子。\n一些内置的控制器，比如命名空间控制器，针对没有指定命名空间的对象。为了简单起见，这篇文章没有详细介绍这些细节。\n通过 API 服务器来控制 控制器是一个 Kubernetes 内置控制器的例子。内置控制器通过和集群 API 服务器交互来管理状态。\nJob 是一种 Kubernetes 资源，它运行一个 ，或者可能是多个 Pod，来执行一个任务然后停止。\n（一旦被调度了），对 kubelet 来说 Pod 对象就会变成了期望状态的一部分。\n在集群中，当 Job 控制器拿到新任务时，它会保证一组 Node 节点上的 kubelet 可以运行正确数量的 Pod 来完成工作。 Job 控制器不会自己运行任何的 Pod 或者容器。Job 控制器是通知 API 服务器来创建或者移除 Pod。 control-plane中的其它组件根据新的消息而反应（调度新的 Pod 并且运行它）并且最终完成工作。\n创建新 Job 后，所期望的状态就是完成这个 Job。Job 控制器会让 Job 的当前状态不断接近期望状态：创建为 Job 要完成工作所需要的 Pod，使 Job 的状态接近完成。\n控制器也会更新配置对象。例如：一旦 Job 的工作完成了，Job 控制器会更新 Job 对象的状态为 Finished。\n（这有点像温度自动调节器关闭了一个灯，以此来告诉你房间的温度现在到你设定的值了）。\n直接控制 相比 Job 控制器，有些控制器需要对集群外的一些东西进行修改。\n例如，如果你使用一个控制环来保证集群中有足够的节点，那么控制就需要当前集群外的一些服务在需要时创建新节点。\n和外部状态交互的控制器从 API 服务器获取到它想要的状态，然后直接和外部系统进行通信并使当前状态更接近期望状态。\n（实际上有一个控制器可以水平地扩展集群中的节点。请看集群自动扩缩容）。\n期望状态与当前状态 Kubernetes 采用了系统的云原生视图，并且可以处理持续的变化。\n在任务执行时，集群随时都可能被修改，并且控制环会自动的修复故障。这意味着很可能集群永远不会达到稳定状态。\n只要集群中控制器的在运行并且进行有效的修改，整体状态的稳定与否是无关紧要的。\n设计 作为设计的一个原则，Kubernetes 使用了很多控制器，每个控制器管理集群状态的一个特定方面。最常见的一个特定的控制器使用一种类型的资源作为它的期望状态，控制器管理控制另外一种类型的资源向它的期望状态发展。\n使用简单的控制器而不是一组相互连接的单体控制环是很有用的。控制器会失败，所以 Kubernetes 的设计是考虑到了这一点。\n例如：为 Job 追踪 Job 对象（发现新工作）和 Pod 对象（运行 Job，并且等工作完成）的控制器。在本例中，其它东西创建作业，而作业控制器创建 Pod。\n可以有多个控制器来创建或者更新相同类型的对象。在这之后，Kubernetes 控制器确保他们只关心和它们控制资源相关联的资源。\n例如，你可以有 Deployments 和 Jobs；它们都可以创建 Pod。Job 控制器不删除 Deployment 创建的 Pod，因为有信息标签\u0026quot;让控制器可以区分这些 Pod。\n运行控制器的方式 Kubernetes 自带有一组内置的控制器，运行在 kube-controller-manager} 内。这些内置的控制器提供了重要的核心功能。\nDeployment 控制器和 Job 控制器是 Kubernetes 内置控制器的典型例子。Kubernetes 运行一个弹性的控制平面，所以如果任意内置控制器失败了，控制平面的另外一部分会接替它的工作。\n你会发现控制平面外面运行的控制器，扩展了 Kubernetes 的能力。或者，如果你愿意，你也可以写一个新控制器。你可以以一组 Pod 来运行你的控制器，或者运行在 Kubernetes 外面。什么是最合适的控制器，这将取决于特定控制器的功能。\n 请阅读 Kubernetes 控制平面 了解一些基本的 Kubernetes 对象 学习更多的 Kubernetes API 如果你想写自己的控制器，请看 Kubernetes 的扩展模式。  "
},
{
	"uri": "https://lijun.in/setup/best-practices/node-conformance/",
	"title": "校验节点设置",
	"tags": [],
	"description": "",
	"content": ". toc \u0026gt;}}\n节点一致性测试 节点一致性测试 是一个容器化的测试框架，提供了针对节点的系统验证和功能测试。\n该测试主要检测节点是否满足 Kubernetes 的最低要求，通过检测的节点有资格加入 Kubernetes 集群。\n限制 在 Kubernetes 1.5 版中，节点一致性测试具有以下限制：\n 节点一致性测试只支持 Docker 作为容器运行时环境。  节点的前提条件 要运行节点一致性测试，节点必须满足与标准 Kubernetes 节点相同的前提条件。节点至少应安装以下守护程序：\n 容器运行时 (Docker) Kubelet  运行节点一致性测试 要运行节点一致性测试，请执行以下步骤：\n 因为测试框架会启动一个本地的 master 来测试 Kubelet，所以将 Kubelet 指向本机 `\u0026ndash;api-servers=\u0026quot;http://localhost:8080\u0026rdquo;。 还有一些其他 Kubelet 参数可能需要注意：   --pod-cidr： 如果使用 kubenet， 需要为 Kubelet 任意指定一个 CIDR， 例如 --pod-cidr=10.180.0.0/24。 --cloud-provider： 如果使用 --cloud-provider=gce，需要移除这个参数来运行测试。  使用以下命令运行节点一致性测试：  # $CONFIG_DIR is the pod manifest path of your Kubelet. # $LOG_DIR is the test output path. sudo docker run -it --rm --privileged --net=host \\  -v /:/rootfs -v $CONFIG_DIR:$CONFIG_DIR -v $LOG_DIR:/var/result \\  k8s.gcr.io/node-test:0.2 针对其他硬件体系结构运行节点一致性测试 Kubernetes 也为其他硬件体系结构的系统提供了节点一致性测试的 Docker 镜像：\n   Arch Image     amd64 node-test-amd64   arm node-test-arm   arm64 node-test-arm64    运行特定的测试 要运行特定测试，请使用您希望运行的测试的特定表达式覆盖环境变量 FOCUS。\nsudo docker run -it --rm --privileged --net=host \\  -v /:/rootfs:ro -v $CONFIG_DIR:$CONFIG_DIR -v $LOG_DIR:/var/result \\  -e FOCUS=MirrorPod \\ # Only run MirrorPod test k8s.gcr.io/node-test:0.2 要跳过特定的测试，请使用您希望跳过的测试的常规表达式覆盖环境变量 SKIP。\nsudo docker run -it --rm --privileged --net=host \\  -v /:/rootfs:ro -v $CONFIG_DIR:$CONFIG_DIR -v $LOG_DIR:/var/result \\  -e SKIP=MirrorPod \\ # Run all conformance tests but skip MirrorPod test k8s.gcr.io/node-test:0.2 节点一致性测试是[节点端到端测试](https://github.com/kubernetes/community/blob/. param \u0026ldquo;githubbranch\u0026rdquo; \u0026gt;}}/contributors/devel/e2e-node-tests.md)的容器化版本。\n默认情况下，它会运行所有一致性测试。\n理论上，只要合理地配置容器和挂载所需的卷，就可以运行任何的节点端到端测试用例。 但是这里强烈建议只运行一致性测试，因为运行非一致性测试需要很多复杂的配置。\n注意  测试会在节点上遗留一些 Docker 镜像， 包括节点一致性测试本身的镜像和功能测试相关的镜像。 测试会在节点上遗留一些死的容器。这些容器是在功能测试的过程中创建的。  "
},
{
	"uri": "https://lijun.in/tasks/access-kubernetes-api/custom-resources/custom-resource-definition-versioning/",
	"title": "用户自定义资源版本",
	"tags": [],
	"description": "",
	"content": "本页介绍了如何添加版本信息到 [CustomResourceDefinitions](/docs/reference/generated/kubernetes-api/. param \u0026ldquo;version\u0026rdquo; \u0026gt;}}/#customresourcedefinition-v1beta1-apiextensions)，如何表示 CustomResourceDefinitions 的稳定水平或者用 API 之间的表征的转换提高您的 API 到一个新的版本。它还描述了如何将对象从一个版本升级到另一个版本。\n. heading \u0026ldquo;prerequisites\u0026rdquo; %}} . include \u0026ldquo;task-tutorial-prereqs.md\u0026rdquo; \u0026gt;}} . version-check \u0026gt;}}\n 确保您的 Kubernetes 集群的主版本为apiextensions.k8s.io/v1的1.16.0或更高版本，apiextensions.k8s.io/v1beta1的1.11.0或更高版本。   阅读 custom resources。  概览 . feature-state state=\u0026quot;stable\u0026rdquo; for_kubernetes_version=\u0026quot;1.16\u0026rdquo; \u0026gt;}}\nCustomResourceDefinition API 提供了用于引入和升级的工作流程到 CustomResourceDefinition 的新版本。\n创建 CustomResourceDefinition 时，会在 CustomResourceDefinition spec.versions 列表设置适当的稳定级和版本号。例如v1beta1表示第一个版本尚未稳定。所有自定义资源对象将首先存储在这个版本\n创建 CustomResourceDefinition 后，客户端可以开始使用 v1beta1 API。\n稍后可能需要添加新版本，例如 v1。\n增加一个新版本：\n 选择一种转化策略。由于自定义资源对象需要能够两种版本都可用，这意味着它们有时会以与存储版本不同的版本。为了能够做到这一点， 有时必须在它们存储的版本和提供的版本。如果转换涉及结构变更，并且需要自定义逻辑，转换应该使用 webhook。如果没有结构变更， 则使用 None 默认转换策略，不同版本时只有apiVersion字段有变更。 如果使用转换 Webhook，请创建并部署转换 Webhook。希望看到更多详细信息，请参见 Webhook conversion。 更新 CustomResourceDefinition，来将新版本包含在具有served：true的 spec.versions 列表。另外，设置spec.conversion字段 到所选的转换策略。如果使用转换 Webhook，请配置spec.conversion.webhookClientConfig来调用 webhook。  添加新版本后，客户端可以逐步迁移到新版本。对于某些客户而言，在使用旧版本的同时支持其他人使用新版本。\n将存储的对象迁移到新版本：\n 请参阅 将现有对象升级到新的存储版本 章节。  对于客户来说，在将对象升级到新的存储版本之前，期间和之后使用旧版本和新版本都是安全的。\n删除旧版本：\n 确保所有客户端都已完全迁移到新版本。kube-apiserver 可以查看日志以帮助识别仍通过进行访问的所有客户端旧版本。 在spec.versions列表中将旧版本的 served 设置为 false。如果任何客户端仍然意外地使用他们可能开始报告的旧版本尝试访问旧版本的自定义资源对象时出错。 如果发生这种情况，请切换回在旧版本上使用served：true，然后迁移其他客户使用新版本，然后重复此步骤。 确保已完成 将现有对象升级到新存储版本 的步骤。  在 CustomResourceDefinition 的spec.versions列表中，确认新版本的stored已设置为true。 确认旧版本不再列在 CustomResourceDefinition status.storedVersions。   从 CustomResourceDefinitionspec.versions 列表中删除旧版本。 在转换 webhooks 中放弃对旧版本的转换支持。  指定多个版本 CustomResourceDefinition API 的versions字段可用于支持您自定义资源的多个版本已经开发的。版本可以具有不同的架构，并且转换 Webhooks 可以在版本之间转换自定义资源。 在适用的情况下，Webhook 转换应遵循 Kubernetes API。\n. note \u0026gt;}} In apiextensions.k8s.io/v1beta1, there was a version field instead of versions. The version field is deprecated and optional, but if it is not empty, it must match the first item in the versions field.\n. /note \u0026gt;}}\n此示例显示了两个版本的 CustomResourceDefinition。第一个例子，假设所有的版本共享相同的模式而它们之间没有转换。YAML 中的评论提供了更多背景信息。 . tabs name=\u0026quot;CustomResourceDefinition_versioning_example_1\u0026rdquo; \u0026gt;}} . tab name=\u0026quot;apiextensions.k8s.io/v1\u0026rdquo; %}}\napiVersion: apiextensions.k8s.io/v1beta1 kind: CustomResourceDefinition metadata: # name must match the spec fields below, and be in the form: \u0026lt;plural\u0026gt;.\u0026lt;group\u0026gt; name: crontabs.example.com spec: # group name to use for REST API: /apis/\u0026lt;group\u0026gt;/\u0026lt;version\u0026gt; group: example.com # list of versions supported by this CustomResourceDefinition versions: - name: v1beta1 # Each version can be enabled/disabled by Served flag. served: true # One and only one version must be marked as the storage version. storage: true # A schema is required schema: openAPIV3Schema: type: object properties: host: type: string port: type: string - name: v1 served: true storage: false schema: openAPIV3Schema: type: object properties: host: type: string port: type: string # The conversion section is introduced in Kubernetes 1.13+ with a default value of # None conversion (strategy sub-field set to None). conversion: # None conversion assumes the same schema for all versions and only sets the apiVersion # field of custom resources to the proper value strategy: None # either Namespaced or Cluster scope: Namespaced names: # plural name to be used in the URL: /apis/\u0026lt;group\u0026gt;/\u0026lt;version\u0026gt;/\u0026lt;plural\u0026gt; plural: crontabs # singular name to be used as an alias on the CLI and for display singular: crontab # kind is normally the CamelCased singular type. Your resource manifests use this. kind: CronTab # shortNames allow shorter string to match your resource on the CLI shortNames: - ct . /tab %}} . tab name=\u0026quot;apiextensions.k8s.io/v1beta1\u0026rdquo; %}}\n# Deprecated in v1.16 in favor of apiextensions.k8s.io/v1 apiVersion: apiextensions.k8s.io/v1beta1 kind: CustomResourceDefinition metadata: # name must match the spec fields below, and be in the form: \u0026lt;plural\u0026gt;.\u0026lt;group\u0026gt; name: crontabs.example.com spec: # group name to use for REST API: /apis/\u0026lt;group\u0026gt;/\u0026lt;version\u0026gt; group: example.com # list of versions supported by this CustomResourceDefinition versions: - name: v1beta1 # Each version can be enabled/disabled by Served flag. served: true # One and only one version must be marked as the storage version. storage: true - name: v1 served: true storage: false validation: openAPIV3Schema: type: object properties: host: type: string port: type: string # The conversion section is introduced in Kubernetes 1.13+ with a default value of # None conversion (strategy sub-field set to None). conversion: # None conversion assumes the same schema for all versions and only sets the apiVersion # field of custom resources to the proper value strategy: None # either Namespaced or Cluster scope: Namespaced names: # plural name to be used in the URL: /apis/\u0026lt;group\u0026gt;/\u0026lt;version\u0026gt;/\u0026lt;plural\u0026gt; plural: crontabs # singular name to be used as an alias on the CLI and for display singular: crontab # kind is normally the CamelCased singular type. Your resource manifests use this. kind: CronTab # shortNames allow shorter string to match your resource on the CLI shortNames: - ct . /tab %}} . /tabs \u0026gt;}}\n你可以将 CustomResourceDefinition 存储在 YAML 文件中，然后使用kubectl apply来创建它。\nkubectl apply -f my-versioned-crontab.yaml 在创建之后，apiserver 开始在 HTTP REST 端点上为每个启用的版本提供服务。在上面的示例中，API 版本可以在/apis/example.com/v1beta1 和 /apis/example.com/v1中获得。\n版本优先级 不考虑 CustomResourceDefinition 中版本被定义的顺序，kubectl 使用具有最高优先级的版本作为访问对象的默认版本。 通过解析 name 字段确定优先级来决定版本号，稳定性（GA，Beta，或者 Alpha），以及该稳定性水平内的序列。\n用于对版本进行排序的算法被设计成与 Kubernetes 项目对 Kubernetes 版本进行排序的方式相同。 版本以v开头跟一个数字，一个可选的beta 或者 alpha命名，和一个可选的附加的数字型的版本信息。 从广义上讲，版本字符串可能看起来像v2或者v2beta1。 使用以下算法对版本进行排序：\n 遵循 Kubernetes 版本模式的条目在不符合条件的条目之前进行排序。 对于遵循 Kubernetes 版本模式的条目，版本字符串的数字部分从最大到最小排序。 如果字符串beta 或 alpha跟随第一数字部分，它们按顺序排序，在没有beta 或 alpha后缀（假定为 GA 版本）的等效字符串后面。 如果另一个数字跟在beta或alpha之后，那么这些数字也是从最大到最小排序。 不符合上述格式的字符串按字母顺序排序，数字部分不经过特殊处理。请注意，在下面的示例中，foo1在 foo10上方排序。这与遵循 Kubernetes 版本模式的条目的数字部分排序不同。  如果查看以下排序版本列表可以明白：\n- v10 - v2 - v1 - v11beta2 - v10beta3 - v3beta1 - v12alpha1 - v11alpha2 - foo1 - foo10 对于 指定多个版本 中的示例，版本排序顺序为v1，后跟着v1beta1。 这导致了 kubectl 命令使用v1作为默认版本，除非提供对象指定版本。\nWebhook转换 . note \u0026gt;}}\nWebhook 转换在 Kubernetes 1.13 中作为 alpha 功能引入。要使用它，应启用CustomResourceWebhookConversion功能。请参阅 feature gate 文档以获得更多信息。\n. /note \u0026gt;}}\n上面的例子在版本之间有一个 None 转换，它只在转换时设置apiVersion字段而不改变对象的其余部分。apiserver 还支持在需要转换时调用外部服务的 webhook 转换。例如：\n 自定义资源被要求在一个不同的版本里而不是一个存储的版本里。 Watch 在一个版本中创建，但更改对象存储在另一个版本中。 自定义资源 PUT 请求在一个不同的版本里而不是一个存储的版本。  为了涵盖所有这些情况并通过 API 服务优化转换，转换对象可能包含多个对象，以便最大限度地减少外部调用。webhook 应该独立执行这些转换。\n编写一个转换webhook服务 请参考 自定义资源转换 webhook 服务 的实施，这在 Kubernetes e2e 测试中得到验证。webhook 处理由 apiserver 发送的ConversionReview请求，并发送回包含在ConversionResponse中的转换结果。请注意，请求包含需要独立转换不改变对象顺序的自定义资源列表。示例服务器的组织方式使其可以重用于其他转换。大多数常见代码都位于 框架文件 中，只留下 示例 用于实施不同的转换。\n. note \u0026gt;}}\n示例转换 webhook 服务器留下ClientAuth字段 empty，默认为NoClientCert。\n这意味着 webhook 服务器没有验证客户端的身份，据称是 apiserver。\n如果您需要相互 TLS 或者其他方式来验证客户端，请参阅如何 验证 API 服务。\n. /note \u0026gt;}}\n部署转换webhook服务 用于部署转换 webhook 的文档与 准入webhook示例服务。 下一节的假设是转换 webhook 服务器部署到default命名空间中名为example-conversion-webhook-server的服务器上，并在路径/crdconvert上提供流量。\n. note \u0026gt;}} 当 webhook 服务器作为一个部署到 Kubernetes 集群中的服务器时，它必须通过端口443上的服务器公开(服务器本身可以有一个任意端口，但是服务器对象应该将它映射到端口443)。 如果为服务器使用不同的端口，则 apiserver 和 webhook 服务器之间的通信可能会失败。 . /note \u0026gt;}}\n配置CustomResourceDefinition以使用转换webhooks 通过修改spec中的conversion部分，可以扩展None转换示例来使用转换 webhook。\n. tabs name=\u0026quot;CustomResourceDefinition_versioning_example_2\u0026rdquo; \u0026gt;}} . tab name=\u0026quot;apiextensions.k8s.io/v1\u0026rdquo; %}}\napiVersion: apiextensions.k8s.io/v1 kind: CustomResourceDefinition metadata: # name must match the spec fields below, and be in the form: \u0026lt;plural\u0026gt;.\u0026lt;group\u0026gt; name: crontabs.example.com spec: # group name to use for REST API: /apis/\u0026lt;group\u0026gt;/\u0026lt;version\u0026gt; group: example.com # list of versions supported by this CustomResourceDefinition versions: - name: v1beta1 # Each version can be enabled/disabled by Served flag. served: true # One and only one version must be marked as the storage version. storage: true # Each version can define it\u0026#39;s own schema when there is no top-level # schema is defined. schema: openAPIV3Schema: type: object properties: hostPort: type: string - name: v1 served: true storage: false schema: openAPIV3Schema: type: object properties: host: type: string port: type: string conversion: # a Webhook strategy instruct API server to call an external webhook for any conversion between custom resources. strategy: Webhook # webhook is required when strategy is `Webhook` and it configures the webhook endpoint to be called by API server. webhook: # conversionReviewVersions indicates what ConversionReview versions are understood/preferred by the webhook. # The first version in the list understood by the API server is sent to the webhook. # The webhook must respond with a ConversionReview object in the same version it received. conversionReviewVersions: [\u0026#34;v1\u0026#34;,\u0026#34;v1beta1\u0026#34;] clientConfig: service: namespace: default name: example-conversion-webhook-server path: /crdconvert caBundle: \u0026#34;Ci0tLS0tQk...\u0026lt;base64-encoded PEM bundle\u0026gt;...tLS0K\u0026#34; # either Namespaced or Cluster scope: Namespaced names: # plural name to be used in the URL: /apis/\u0026lt;group\u0026gt;/\u0026lt;version\u0026gt;/\u0026lt;plural\u0026gt; plural: crontabs # singular name to be used as an alias on the CLI and for display singular: crontab # kind is normally the CamelCased singular type. Your resource manifests use this. kind: CronTab # shortNames allow shorter string to match your resource on the CLI shortNames: - ct . /tab %}} . tab name=\u0026quot;apiextensions.k8s.io/v1beta1\u0026rdquo; %}}\n# Deprecated in v1.16 in favor of apiextensions.k8s.io/v1 apiVersion: apiextensions.k8s.io/v1beta1 kind: CustomResourceDefinition metadata: # name must match the spec fields below, and be in the form: \u0026lt;plural\u0026gt;.\u0026lt;group\u0026gt; name: crontabs.example.com spec: # group name to use for REST API: /apis/\u0026lt;group\u0026gt;/\u0026lt;version\u0026gt; group: example.com # prunes object fields that are not specified in OpenAPI schemas below. preserveUnknownFields: false # list of versions supported by this CustomResourceDefinition versions: - name: v1beta1 # Each version can be enabled/disabled by Served flag. served: true # One and only one version must be marked as the storage version. storage: true # Each version can define it\u0026#39;s own schema when there is no top-level # schema is defined. schema: openAPIV3Schema: type: object properties: hostPort: type: string - name: v1 served: true storage: false schema: openAPIV3Schema: type: object properties: host: type: string port: type: string conversion: # a Webhook strategy instruct API server to call an external webhook for any conversion between custom resources. strategy: Webhook # webhookClientConfig is required when strategy is `Webhook` and it configures the webhook endpoint to be called by API server. webhookClientConfig: service: namespace: default name: example-conversion-webhook-server path: /crdconvert caBundle: \u0026#34;Ci0tLS0tQk...\u0026lt;base64-encoded PEM bundle\u0026gt;...tLS0K\u0026#34; # either Namespaced or Cluster scope: Namespaced names: # plural name to be used in the URL: /apis/\u0026lt;group\u0026gt;/\u0026lt;version\u0026gt;/\u0026lt;plural\u0026gt; plural: crontabs # singular name to be used as an alias on the CLI and for display singular: crontab # kind is normally the CamelCased singular type. Your resource manifests use this. kind: CronTab # shortNames allow shorter string to match your resource on the CLI shortNames: - ct . /tab %}} . /tabs \u0026gt;}}\n您可以将 CustomResourceDefinition 保存在 YAML 文件中，然后使用kubectl apply来应用它。\nkubectl apply -f my-versioned-crontab-with-conversion.yaml 在应用新更改之前，请确保转换服务器已启动并正在运行。\n调用-webhook apiserver 一旦确定请求应发送到转换 webhook，它需要知道如何调用 webhook。这是在webhookClientConfig中指定的 webhook 配置。\n转换 webhook 可以通过调用 URL 或服务，并且可以选择包含自定义 CA 包，以用于验证 TLS 连接。\nURL url 以标准 URL 形式给出 webhook 的位置（scheme://host:port/path）。 host不应引用集群中运行的服务；采用通过指定service字段来提供服务引用。 host可以通过某些 apiserver 中的外部 DNS 进行解析（即kube-apiserver无法解析集群内 DNS 违反分层规则）。主机也可以是 IP 地址。\n请注意，除非您非常小心在所有主机上运行此 Webhook，否则 localhost 或 127.0.0.1 用作主机是风险很大的，运行一个 apiserver 可能需要对此进行调用 webhook。这样的安装很可能是不可移植的，即不容易出现在新集群中。\n方案必须为https：URL 必须以https://开头。\n尝试使用用户或基本身份验证，例如不允许使用user:password@。片段（#...）和查询参数（?...）也不允许。\n这是配置为调用 URL 的转换 Webhook 的示例（并且期望使用系统信任根来验证 TLS 证书，因此不指定 caBundle）：\n. tabs name=\u0026quot;CustomResourceDefinition_versioning_example_3\u0026rdquo; \u0026gt;}} . tab name=\u0026quot;apiextensions.k8s.io/v1\u0026rdquo; %}}\napiVersion: apiextensions.k8s.io/v1 kind: CustomResourceDefinition ... spec: ... conversion: strategy: Webhook webhook: clientConfig: url: \u0026#34;https://my-webhook.example.com:9443/my-webhook-path\u0026#34; ... . /tab %}} . tab name=\u0026quot;apiextensions.k8s.io/v1beta1\u0026rdquo; %}}\n# Deprecated in v1.16 in favor of apiextensions.k8s.io/v1 apiVersion: apiextensions.k8s.io/v1beta1 kind: CustomResourceDefinition ... spec: ... conversion: strategy: Webhook webhookClientConfig: url: \u0026#34;https://my-webhook.example.com:9443/my-webhook-path\u0026#34; ... . /tab %}} . /tabs \u0026gt;}}\n服务引用 webhookClientConfig内部的service段是对转换 webhook 服务的引用。如果 Webhook 在集群中运行，则应使用service而不是url。 服务名称空间和名称是必需的。端口是可选的，默认为 443。该路径是可选的，默认为/。\n这是一个配置为在端口1234上调用服务的 Webhook 的示例在子路径/my-path下，并针对 ServerName 验证 TLS 连接 使用自定义 CA 捆绑包的my-service-name.my-service-namespace.svc。\n. tabs name=\u0026quot;CustomResourceDefinition_versioning_example_4\u0026rdquo; \u0026gt;}} . tab name=\u0026quot;apiextensions.k8s.io/v1\u0026rdquo; %}}\napiVersion: apiextensions.k8s.io/v1b kind: CustomResourceDefinition ... spec: ... conversion: strategy: Webhook webhook: clientConfig: service: namespace: my-service-namespace name: my-service-name path: /my-path port: 1234 caBundle: \u0026#34;Ci0tLS0tQk...\u0026lt;base64-encoded PEM bundle\u0026gt;...tLS0K\u0026#34; ... . /tab %}} . tab name=\u0026quot;apiextensions.k8s.io/v1beta1\u0026rdquo; %}}\n# Deprecated in v1.16 in favor of apiextensions.k8s.io/v1 apiVersion: apiextensions.k8s.io/v1beta1 kind: CustomResourceDefinition ... spec: ... conversion: strategy: Webhook webhookClientConfig: service: namespace: my-service-namespace name: my-service-name path: /my-path port: 1234 caBundle: \u0026#34;Ci0tLS0tQk...\u0026lt;base64-encoded PEM bundle\u0026gt;...tLS0K\u0026#34; ... . /tab %}} . /tabs \u0026gt;}}\nWebhook-请求和响应 请求 向 Webhooks 发送 POST 请求，请求的内容类型为：application/json，与 APIapitensions.k8s.io API 组中的 ConversionReview API 对象一起使用序列化为 JSON 作为主体。\nWebhooks 可以指定他们接受的ConversionReview对象的版本在其 CustomResourceDefinition 中使用conversionReviewVersions字段：\n. tabs name=\u0026quot;conversionReviewVersions\u0026rdquo; \u0026gt;}} . tab name=\u0026quot;apiextensions.k8s.io/v1\u0026rdquo; %}}\napiVersion: apiextensions.k8s.io/v1 kind: CustomResourceDefinition ... spec: ... conversion: strategy: Webhook webhook: conversionReviewVersions: [\u0026#34;v1\u0026#34;, \u0026#34;v1beta1\u0026#34;] ... 创建时，conversionReviewVersions是必填字段apiextensions.k8s.io/v1自定义资源定义。 需要 Webhooks 支持至少一个ConversionReview当前和以前的 apiserver 可以理解的版本。\n. /tab %}} . tab name=\u0026quot;apiextensions.k8s.io/v1beta1\u0026rdquo; %}}\n# Deprecated in v1.16 in favor of apiextensions.k8s.io/v1 apiVersion: apiextensions.k8s.io/v1beta1 kind: CustomResourceDefinition ... spec: ... conversion: strategy: Webhook conversionReviewVersions: [\u0026#34;v1\u0026#34;, \u0026#34;v1beta1\u0026#34;] ... 如果未指定conversionReviewVersions，则创建时的默认值 apiextensions.k8s.io/v1beta1 自定义资源定义为 v1beta1。 . /tab %}} . /tabs \u0026gt;}}\nAPI servers send the first ConversionReview version in the conversionReviewVersions list they support. If none of the versions in the list are supported by the API server, the custom resource definition will not be allowed to be created. If an API server encounters a conversion webhook configuration that was previously created and does not support any of the ConversionReview versions the API server knows how to send, attempts to call to the webhook will fail.\n此示例显示了包含在ConversionReview对象中的数据请求将CronTab对象转换为example.com/v1：\n. tabs name=\u0026quot;ConversionReview_request\u0026rdquo; \u0026gt;}} . tab name=\u0026quot;apiextensions.k8s.io/v1\u0026rdquo; %}}\n{ \u0026#34;apiVersion\u0026#34;: \u0026#34;apiextensions.k8s.io/v1\u0026#34;, \u0026#34;kind\u0026#34;: \u0026#34;ConversionReview\u0026#34;, \u0026#34;request\u0026#34;: { # Random uid uniquely identifying this conversion call \u0026#34;uid\u0026#34;: \u0026#34;705ab4f5-6393-11e8-b7cc-42010a800002\u0026#34;, # The API group and version the objects should be converted to \u0026#34;desiredAPIVersion\u0026#34;: \u0026#34;example.com/v1\u0026#34;, # The list of objects to convert. # May contain one or more objects, in one or more versions. \u0026#34;objects\u0026#34;: [ { \u0026#34;kind\u0026#34;: \u0026#34;CronTab\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;example.com/v1beta1\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;creationTimestamp\u0026#34;: \u0026#34;2019-09-04T14:03:02Z\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;local-crontab\u0026#34;, \u0026#34;namespace\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;resourceVersion\u0026#34;: \u0026#34;143\u0026#34;, \u0026#34;uid\u0026#34;: \u0026#34;3415a7fc-162b-4300-b5da-fd6083580d66\u0026#34; }, \u0026#34;hostPort\u0026#34;: \u0026#34;localhost:1234\u0026#34; }, { \u0026#34;kind\u0026#34;: \u0026#34;CronTab\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;example.com/v1beta1\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;creationTimestamp\u0026#34;: \u0026#34;2019-09-03T13:02:01Z\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;remote-crontab\u0026#34;, \u0026#34;resourceVersion\u0026#34;: \u0026#34;12893\u0026#34;, \u0026#34;uid\u0026#34;: \u0026#34;359a83ec-b575-460d-b553-d859cedde8a0\u0026#34; }, \u0026#34;hostPort\u0026#34;: \u0026#34;example.com:2345\u0026#34; } ] } } . /tab %}} . tab name=\u0026quot;apiextensions.k8s.io/v1beta1\u0026rdquo; %}}\n{ # Deprecated in v1.16 in favor of apiextensions.k8s.io/v1 \u0026#34;apiVersion\u0026#34;: \u0026#34;apiextensions.k8s.io/v1beta1\u0026#34;, \u0026#34;kind\u0026#34;: \u0026#34;ConversionReview\u0026#34;, \u0026#34;request\u0026#34;: { # Random uid uniquely identifying this conversion call \u0026#34;uid\u0026#34;: \u0026#34;705ab4f5-6393-11e8-b7cc-42010a800002\u0026#34;, # The API group and version the objects should be converted to \u0026#34;desiredAPIVersion\u0026#34;: \u0026#34;example.com/v1\u0026#34;, # The list of objects to convert. # May contain one or more objects, in one or more versions. \u0026#34;objects\u0026#34;: [ { \u0026#34;kind\u0026#34;: \u0026#34;CronTab\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;example.com/v1beta1\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;creationTimestamp\u0026#34;: \u0026#34;2019-09-04T14:03:02Z\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;local-crontab\u0026#34;, \u0026#34;namespace\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;resourceVersion\u0026#34;: \u0026#34;143\u0026#34;, \u0026#34;uid\u0026#34;: \u0026#34;3415a7fc-162b-4300-b5da-fd6083580d66\u0026#34; }, \u0026#34;hostPort\u0026#34;: \u0026#34;localhost:1234\u0026#34; }, { \u0026#34;kind\u0026#34;: \u0026#34;CronTab\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;example.com/v1beta1\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;creationTimestamp\u0026#34;: \u0026#34;2019-09-03T13:02:01Z\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;remote-crontab\u0026#34;, \u0026#34;resourceVersion\u0026#34;: \u0026#34;12893\u0026#34;, \u0026#34;uid\u0026#34;: \u0026#34;359a83ec-b575-460d-b553-d859cedde8a0\u0026#34; }, \u0026#34;hostPort\u0026#34;: \u0026#34;example.com:2345\u0026#34; } ] } } . /tab %}} . /tabs \u0026gt;}}\n响应 Webhooks 响应是以 200 HTTP 状态代码，Content-Type:application/json，和包含 ConversionReview 对象的主体（与发送的版本相同）， 带有response节的序列，并序列化为 JSON。\n如果转换成功，则 Webhook 应该返回包含以下字段的response节：\n uid，从发送到 webhook 的request.uid复制而来 result，设置为{\u0026quot;status\u0026quot;:\u0026quot;Success\u0026quot;}} convertedObjects，包含来自request.objects的所有对象，转换为request.desiredVersion  Webhook 的最简单成功响应示例：\n. tabs name=\u0026quot;ConversionReview_response_success\u0026rdquo; \u0026gt;}} . tab name=\u0026quot;apiextensions.k8s.io/v1\u0026rdquo; %}}\n{ \u0026#34;apiVersion\u0026#34;: \u0026#34;apiextensions.k8s.io/v1\u0026#34;, \u0026#34;kind\u0026#34;: \u0026#34;ConversionReview\u0026#34;, \u0026#34;response\u0026#34;: { # must match \u0026lt;request.uid\u0026gt; \u0026#34;uid\u0026#34;: \u0026#34;705ab4f5-6393-11e8-b7cc-42010a800002\u0026#34;, \u0026#34;result\u0026#34;: { \u0026#34;status\u0026#34;: \u0026#34;Success\u0026#34; }, # Objects must match the order of request.objects, and have apiVersion set to \u0026lt;request.desiredAPIVersion\u0026gt;. # kind, metadata.uid, metadata.name, and metadata.namespace fields must not be changed by the webhook. # metadata.labels and metadata.annotations fields may be changed by the webhook. # All other changes to metadata fields by the webhook are ignored. \u0026#34;convertedObjects\u0026#34;: [ { \u0026#34;kind\u0026#34;: \u0026#34;CronTab\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;example.com/v1\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;creationTimestamp\u0026#34;: \u0026#34;2019-09-04T14:03:02Z\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;local-crontab\u0026#34;, \u0026#34;namespace\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;resourceVersion\u0026#34;: \u0026#34;143\u0026#34;, \u0026#34;uid\u0026#34;: \u0026#34;3415a7fc-162b-4300-b5da-fd6083580d66\u0026#34; }, \u0026#34;host\u0026#34;: \u0026#34;localhost\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;1234\u0026#34; }, { \u0026#34;kind\u0026#34;: \u0026#34;CronTab\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;example.com/v1\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;creationTimestamp\u0026#34;: \u0026#34;2019-09-03T13:02:01Z\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;remote-crontab\u0026#34;, \u0026#34;resourceVersion\u0026#34;: \u0026#34;12893\u0026#34;, \u0026#34;uid\u0026#34;: \u0026#34;359a83ec-b575-460d-b553-d859cedde8a0\u0026#34; }, \u0026#34;host\u0026#34;: \u0026#34;example.com\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;2345\u0026#34; } ] } } . /tab %}} . tab name=\u0026quot;apiextensions.k8s.io/v1beta1\u0026rdquo; %}}\n{ # Deprecated in v1.16 in favor of apiextensions.k8s.io/v1 \u0026#34;apiVersion\u0026#34;: \u0026#34;apiextensions.k8s.io/v1beta1\u0026#34;, \u0026#34;kind\u0026#34;: \u0026#34;ConversionReview\u0026#34;, \u0026#34;response\u0026#34;: { # must match \u0026lt;request.uid\u0026gt; \u0026#34;uid\u0026#34;: \u0026#34;705ab4f5-6393-11e8-b7cc-42010a800002\u0026#34;, \u0026#34;result\u0026#34;: { \u0026#34;status\u0026#34;: \u0026#34;Failed\u0026#34; }, # Objects must match the order of request.objects, and have apiVersion set to \u0026lt;request.desiredAPIVersion\u0026gt;. # kind, metadata.uid, metadata.name, and metadata.namespace fields must not be changed by the webhook. # metadata.labels and metadata.annotations fields may be changed by the webhook. # All other changes to metadata fields by the webhook are ignored. \u0026#34;convertedObjects\u0026#34;: [ { \u0026#34;kind\u0026#34;: \u0026#34;CronTab\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;example.com/v1\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;creationTimestamp\u0026#34;: \u0026#34;2019-09-04T14:03:02Z\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;local-crontab\u0026#34;, \u0026#34;namespace\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;resourceVersion\u0026#34;: \u0026#34;143\u0026#34;, \u0026#34;uid\u0026#34;: \u0026#34;3415a7fc-162b-4300-b5da-fd6083580d66\u0026#34; }, \u0026#34;host\u0026#34;: \u0026#34;localhost\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;1234\u0026#34; }, { \u0026#34;kind\u0026#34;: \u0026#34;CronTab\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;example.com/v1\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;creationTimestamp\u0026#34;: \u0026#34;2019-09-03T13:02:01Z\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;remote-crontab\u0026#34;, \u0026#34;resourceVersion\u0026#34;: \u0026#34;12893\u0026#34;, \u0026#34;uid\u0026#34;: \u0026#34;359a83ec-b575-460d-b553-d859cedde8a0\u0026#34; }, \u0026#34;host\u0026#34;: \u0026#34;example.com\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;2345\u0026#34; } ] } } . /tab %}} . /tabs \u0026gt;}}\n如果转换失败，则 Webhook 应该返回包含以下字段的response节： *uid，从发送到 webhook 的request.uid复制而来 *result，设置为{\u0026quot;status\u0026quot;:\u0026quot;Failed\u0026quot;}\n. warning \u0026gt;}}\n转换失败会破坏对自定义资源的读写访问，包括更新或删除资源的能力。转换失败应尽可能避免使用，并且不应用于强制验证 约束（改用验证模式 或 Webhook admission）。 . /warning \u0026gt;}}\n来自 Webhook 的响应示例，指示转换请求失败，并带有可选消息：\n. tabs name=\u0026quot;ConversionReview_response_failure\u0026rdquo; \u0026gt;}} . tab name=\u0026quot;apiextensions.k8s.io/v1\u0026rdquo; %}}\n{ \u0026#34;apiVersion\u0026#34;: \u0026#34;apiextensions.k8s.io/v1\u0026#34;, \u0026#34;kind\u0026#34;: \u0026#34;ConversionReview\u0026#34;, \u0026#34;response\u0026#34;: { \u0026#34;uid\u0026#34;: \u0026#34;\u0026lt;value from request.uid\u0026gt;\u0026#34;, \u0026#34;result\u0026#34;: { \u0026#34;status\u0026#34;: \u0026#34;Failed\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;hostPort could not be parsed into a separate host and port\u0026#34; } } } . /tab %}} . tab name=\u0026quot;apiextensions.k8s.io/v1beta1\u0026rdquo; %}}\n{ # Deprecated in v1.16 in favor of apiextensions.k8s.io/v1 \u0026#34;apiVersion\u0026#34;: \u0026#34;apiextensions.k8s.io/v1beta1\u0026#34;, \u0026#34;kind\u0026#34;: \u0026#34;ConversionReview\u0026#34;, \u0026#34;response\u0026#34;: { \u0026#34;uid\u0026#34;: \u0026#34;\u0026lt;value from request.uid\u0026gt;\u0026#34;, \u0026#34;result\u0026#34;: { \u0026#34;status\u0026#34;: \u0026#34;Failed\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;hostPort could not be parsed into a separate host and port\u0026#34; } } } . /tab %}} . /tabs \u0026gt;}}\n编写、读取和更新版本化的 CustomResourceDefinition 对象 写入对象时，它将保留在写入时指定为存储版本的版本中。如果存储版本发生变化，现有对象永远不会自动转换。然而，新创建或更新的对象将在新的存储版本中编写。对象可能已在不再被服务的版本中编写。\n当读取对象时，将版本指定为路径的一部分。 如果指定的版本与对象的持久版本不同，Kubernetes 会在您请求的版本里将对象返还给您，但是在提供请求时，持久化对象既不会在磁盘上更改，也不会以任何方式进行转换（除了更改apiVersion字符串）。您可以在当前提供的任何版本中请求对象。\n如果您更新了一个现有对象，它将在现在的存储版本中被重写。 这是对象可以从一个版本改到另一个版本的唯一办法。\n为了说明这一点，请考虑以下假设的一系列事件：\n 存储版本是v1beta1。 它保存在版本v1beta1的存储中。 您将版本v1添加到 CustomResourceDefinition 中，并将其指定为存储版本。 您在版本v1beta1中读取您的对象，然后您再次在版本v1中读取对象。 除了 apiVersion 字段之外，两个返回的对象都是相同的。 您创建一个新对象。 它存储在版本v1的存储中。 您现在有两个对象，其中一个位于v1beta1，另一个位于v1。 您更新第一个对象。 它现在保存在版本v1中，因为那是当前的存储版本。  以前的存储版本 API 服务在状态字段storedVersions中记录曾被标记为存储版本的每个版本。 对象可能已被保留在任何曾被指定为存储版本的版本中。 从未成为存储版本的版本的存储中不能存在任何对象。\n将现有对象升级到新的存储版本 弃用版本并删除支持时，请设计存储升级过程。 以下是从v1beta1升级到v1的示例过程。\n 将v1设置为 CustomResourceDefinition 文件中的存储，并使用 kubectl 应用它。 storedVersions现在是v1beta1、 v1。 编写升级过程以列出所有现有对象并使用相同内容编写它们。 这会强制后端在当前存储版本中写入对象，即v1。 通过从storedVersions字段中删除v1beta1来更新 CustomResourceDefinitionStatus。  "
},
{
	"uri": "https://lijun.in/concepts/extend-kubernetes/compute-storage-net/",
	"title": "计算、存储和网络扩展",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lijun.in/tasks/run-application/run-replicated-stateful-application/",
	"title": "运行一个有状态的应用程序",
	"tags": [],
	"description": "",
	"content": "该页面显示如何使用StatefulSet 控制器去运行一个有状态的应用程序。此例是一主多从的 MySQL 集群。\n请注意 这不是生产配置。 重点是， MySQL 设置保留在不安全的默认值上，使重点放在 Kubernetes 中运行有状态应用程序的常规模式。\n. heading \u0026ldquo;prerequisites\u0026rdquo; %}}  . include \u0026ldquo;task-tutorial-prereqs.md\u0026rdquo; \u0026gt;}} . version-check \u0026gt;}} . include \u0026ldquo;default-storage-class-prereqs.md\u0026rdquo; \u0026gt;}} 本教程假定您熟悉 PersistentVolumes 与 StatefulSets, 以及其他核心概念，例如Pods, Services, 与 ConfigMaps. 熟悉 MySQL 会有所帮助，但是本教程旨在介绍对其他系统应该有用的常规模式。  . heading \u0026ldquo;objectives\u0026rdquo; %}}  使用 StatefulSet 控制器部署复制的 MySQL 拓扑。 发送 MySQL 客户端流量。 观察对宕机的抵抗力。 缩放 StatefulSet 的大小。  部署 MySQL 部署 MySQL 示例，包含一个 ConfigMap，两个 Services，与一个 StatefulSet。\nConfigMap 从以下的 YAML 配置文件创建 ConfigMap ：\n. codenew file=\u0026quot;application/mysql/mysql-configmap.yaml\u0026rdquo; \u0026gt;}}\nkubectl apply -f https://k8s.io/examples/application/mysql/mysql-configmap.yaml 这个 ConfigMap 提供 my.cnf 覆盖，使您可以独立控制 MySQL 主服务器和从服务器的配置。 在这种情况下，您希望主服务器能够将复制日志提供给从服务器，并且希望从服务器拒绝任何不是通过复制进行的写操作。\nConfigMap 本身没有什么特别之处，它可以使不同部分应用于不同的 Pod。 每个 Pod 都会决定在初始化时要看基于 StatefulSet 控制器提供的信息。\nServices 从以下 YAML 配置文件创建服务：\n. codenew file=\u0026quot;application/mysql/mysql-services.yaml\u0026rdquo; \u0026gt;}}\nkubectl apply -f https://k8s.io/examples/application/mysql/mysql-services.yaml Headless Service 给 StatefulSet 控制器为集合中每个 Pod 创建的 DNS 条目提供了一个宿主。因为 Headless Service 名为 mysql，所以可以通过在同一 Kubernetes 集群和 namespace 中的任何其他 Pod 内解析 \u0026lt;pod-name\u0026gt;.mysql 来访问 Pod。\n客户端 Service 称为 mysql-read，是一种常规 Service，具有其自己的群集 IP，该群集 IP 在报告为就绪的所有MySQL Pod 中分配连接。可能端点的集合包括 MySQL 主节点和所有从节点。\n请注意，只有读取查询才能使用负载平衡的客户端 Service。因为只有一个 MySQL 主服务器，所以客户端应直接连接到 MySQL 主服务器 Pod (通过其在 Headless Service 中的 DNS 条目)以执行写入操作。\nStatefulSet 最后，从以下 YAML 配置文件创建 StatefulSet：\n. codenew file=\u0026quot;application/mysql/mysql-statefulset.yaml\u0026rdquo; \u0026gt;}}\nkubectl apply -f https://k8s.io/examples/application/mysql/mysql-statefulset.yaml 您可以通过运行以下命令查看启动进度：\nkubectl get pods -l app=mysql --watch 一段时间后，您应该看到所有3个 Pod 都开始运行：\nNAME READY STATUS RESTARTS AGE mysql-0 2/2 Running 0 2m mysql-1 2/2 Running 0 1m mysql-2 2/2 Running 0 1m 输入Ctrl+C取消观察。 如果您看不到任何进度，确保已启用前提条件中提到的动态 PersistentVolume 预配器。\n该清单使用多种技术来管理作为 StatefulSet 一部分的有状态 Pod。下一节重点介绍其中一些技巧，以解释 StatefulSet 创建 Pod 时发生的状况。\n了解有状态的 Pod 初始化 StatefulSet 控制器一次按顺序启动 Pod 序数索引。它一直等到每个 Pod 报告就绪为止，然后再开始下一个 Pod。\n此外，控制器为每个 Pod 分配一个唯一，稳定的表单名称 \u0026lt;statefulset-name\u0026gt;-\u0026lt;ordinal-index\u0026gt; 其结果是 Pods 名为 mysql-0，mysql-1 和 mysql-2。\n上述 StatefulSet 清单中的 Pod 模板利用这些属性来执行 MySQL 复制的有序启动。\n生成配置 在启动 Pod 规范中的任何容器之前， Pod 首先运行任何初始容器按照定义的顺序。\n第一个名为 init-mysql 的初始化容器，根据序号索引生成特殊的 MySQL 配置文件。\n该脚本通过从 Pod 名称的末尾提取索引来确定自己的序号索引，该名称由 hostname 命令返回。 然后将序数(带有数字偏移量以避免保留值)保存到 MySQL conf.d 目录中的文件 server-id.cnf 中。 这将转换 StatefulSet 提供的唯一，稳定的身份控制器进入需要相同属性的 MySQL 服务器 ID 的范围。\n通过将内容复制到 conf.d 中，init-mysql 容器中的脚本也可以应用 ConfigMap 中的 master.cnf 或 slave.cnf。由于示例拓扑由单个 MySQL 主节点和任意数量的从节点组成，因此脚本仅将序数 0 指定为主节点，而将其他所有人指定为从节点。与 StatefulSet 控制器的部署顺序保证,这样可以确保 MySQL 主服务器在创建从服务器之前已准备就绪，以便它们可以开始复制。\n克隆现有数据 通常，当新的 Pod 作为从节点加入集合时，必须假定 MySQL 主节点可能已经有数据。还必须假设复制日志可能不会一直追溯到时间的开始。\n这些保守假设的关键是允许正在运行的 StatefulSet 随时间扩大和缩小而不是固定在其初始大小。\n第二个名为 clone-mysql 的初始化容器，第一次在从属 Pod 上以空 PersistentVolume 启动时，会对从属 Pod 执行克隆操作。这意味着它将从另一个运行的 Pod 复制所有现有数据，因此其本地状态足够一致，可以开始主从服务器复制。\nMySQL 本身不提供执行此操作的机制，因此该示例使用了一种流行的开源工具 Percona XtraBackup。 在克隆期间，源 MySQL 服务器可能会降低性能。 为了最大程度地减少对 MySQL 主机的影响，该脚本指示每个 Pod 从序号较低的 Pod 中克隆。 可以这样做的原因是 StatefulSet 控制器始终确保在启动 Pod N + 1 之前 Pod N 已准备就绪。\n开始复制 初始化容器成功完成后，常规容器将运行。 MySQL Pods 由运行实际 mysqld 服务器的 mysql 容器和充当辅助工具的 xtrabackup 容器组成。\nxtrabackup 辅助工具查看克隆的数据文件，并确定是否有必要在从属服务器上初始化 MySQL 复制。 如果是这样，它将等待 mysqld 准备就绪，然后执行带有从 XtraBackup 克隆文件中提取的复制参数 CHANGE MASTER TO 和 START SLAVE 命令。\n一旦从服务器开始复制后，它会记住其 MySQL 主服务器。并且如果服务器重新启动或连接中断，则会自动重新连接。 另外，因为从服务器会以其稳定的 DNS 名称查找主服务器(mysql-0.mysql)，即使由于重新安排而获得新的 Pod IP，他们也会自动找到主服务器。\n最后，开始复制后，xtrabackup 容器监听来自其他 Pod 的连接数据克隆请求。 如果 StatefulSet 扩大规模，或者下一个 Pod 失去其 PersistentVolumeClaim 并需要重新克隆，则此服务器将无限期保持运行。\n发送客户端流量 您可以通过运行带有 mysql:5.7 镜像的临时容器并运行 mysql 客户端二进制文件，将测试查询发送到 MySQL 主服务器(主机名 mysql-0.mysql )。\nkubectl run mysql-client --image=mysql:5.7 -i --rm --restart=Never --\\  mysql -h mysql-0.mysql \u0026lt;\u0026lt;EOF CREATE DATABASE test; CREATE TABLE test.messages (message VARCHAR(250)); INSERT INTO test.messages VALUES (\u0026#39;hello\u0026#39;); EOF 使用主机名 mysql-read 将测试查询发送到任何报告为就绪的服务器：\nkubectl run mysql-client --image=mysql:5.7 -i -t --rm --restart=Never --\\  mysql -h mysql-read -e \u0026#34;SELECT * FROM test.messages\u0026#34; 您应该获得如下输出：\nWaiting for pod default/mysql-client to be running, status is Pending, pod ready: false +---------+ | message | +---------+ | hello | +---------+ pod \u0026quot;mysql-client\u0026quot; deleted 为了演示 mysql-read 服务在服务器之间分配连接，您可以在循环中运行 SELECT @@server_id：\nkubectl run mysql-client-loop --image=mysql:5.7 -i -t --rm --restart=Never --\\  bash -ic \u0026#34;while sleep 1; do mysql -h mysql-read -e \u0026#39;SELECT @@server_id,NOW()\u0026#39;; done\u0026#34; 您应该看到报告的 @@server_id 发生随机变化，因为每次尝试连接时都可能选择了不同的端点：\n+-------------+---------------------+ | @@server_id | NOW() | +-------------+---------------------+ | 100 | 2006-01-02 15:04:05 | +-------------+---------------------+ +-------------+---------------------+ | @@server_id | NOW() | +-------------+---------------------+ | 102 | 2006-01-02 15:04:06 | +-------------+---------------------+ +-------------+---------------------+ | @@server_id | NOW() | +-------------+---------------------+ | 101 | 2006-01-02 15:04:07 | +-------------+---------------------+ 要停止循环时可以按 Ctrl+C ，但是让它在另一个窗口中运行非常有用，这样您就可以看到以下步骤的效果。\n模拟 Pod 和 Node 的宕机时间 为了证明从从节点缓存而不是单个服务器读取数据的可用性提高，请在使 Pod 退出 Ready 状态时，保持 SELECT @@server_id 循环从上面运行。\n退出 Readiness Probe mysql 容器的readiness probe运行命令 mysql -h 127.0.0.1 -e 'SELECT 1'，以确保服务器已启动并能够执行查询。\n迫使 readiness probe 失败的一种方法就是执行该命令：\nkubectl exec mysql-2 -c mysql -- mv /usr/bin/mysql /usr/bin/mysql.off 这进入 Pod mysql-2 的实际容器文件系统，并重命名 mysql 命令，以便 readiness probe 无法找到它。 几秒钟后， Pod 会将其中一个容器报告为未就绪，您可以通过运行以下命令进行检查：\nkubectl get pod mysql-2 在 就绪 列中查找  1/2 ：\nNAME READY STATUS RESTARTS AGE mysql-2 1/2 Running 0 3m 此时，您应该会看到 SELECT @@server_id 循环继续运行，尽管它不再报告 102 。 回想一下，init-mysql 脚本将 server-id 定义为 100 + $ordinal ，因此服务器 ID 102 对应于 Pod mysql-2。\n现在修复 Pod，几秒钟后它应该重新出现在循环输出中：\nkubectl exec mysql-2 -c mysql -- mv /usr/bin/mysql.off /usr/bin/mysql 删除 Pods 如果删除了 Pod，则 StatefulSet 还会重新创建 Pod，类似于 ReplicaSet 对无状态 Pod 所做的操作。\nkubectl delete pod mysql-2 StatefulSet 控制器注意到不再存在 mysql-2 Pod，并创建了一个具有相同名称并链接到相同 PersistentVolumeClaim 的新 Pod。 您应该看到服务器 ID 102 从循环输出中消失了一段时间，然后自行返回。\n排除 Node 如果您的 Kubernetes 集群具有多个节点，则可以通过发出以下命令drain来模拟节点停机时间(例如升级节点时)。\n首先确定 MySQL Pods 之一在哪个节点上：\nkubectl get pod mysql-2 -o wide 节点名称应显示在最后一列中：\nNAME READY STATUS RESTARTS AGE IP NODE mysql-2 2/2 Running 0 15m 10.244.5.27 kubernetes-node-9l2t 然后通过运行以下命令耗尽节点，该命令将其封锁，以使新的 Pod 不能在那里调度，然后驱逐任何现有的 Pod。 将 \u0026lt;node-name\u0026gt; 替换为在上一步中找到的 Node 的名称。\n这可能会影响节点上的其他应用程序，因此最好 仅在测试集群中执行此操作。\nkubectl drain \u0026lt;node-name\u0026gt; --force --delete-local-data --ignore-daemonsets 现在，您可以观察 Pod 在其他节点上的重新安排：\nkubectl get pod mysql-2 -o wide --watch 它看起来应该像这样：\nNAME READY STATUS RESTARTS AGE IP NODE mysql-2 2/2 Terminating 0 15m 10.244.1.56 kubernetes-node-9l2t [...] mysql-2 0/2 Pending 0 0s \u0026lt;none\u0026gt; kubernetes-node-fjlm mysql-2 0/2 Init:0/2 0 0s \u0026lt;none\u0026gt; kubernetes-node-fjlm mysql-2 0/2 Init:1/2 0 20s 10.244.5.32 kubernetes-node-fjlm mysql-2 0/2 PodInitializing 0 21s 10.244.5.32 kubernetes-node-fjlm mysql-2 1/2 Running 0 22s 10.244.5.32 kubernetes-node-fjlm mysql-2 2/2 Running 0 30s 10.244.5.32 kubernetes-node-fjlm 再次，您应该看到服务器 ID 102 从 SELECT @@server_id 循环输出一段时间，然后返回。\n现在 uncordon 节点,使其恢复为正常模式:\nkubectl uncordon \u0026lt;node-name\u0026gt; 扩展从节点数量 使用 MySQL 复制，您可以通过添加从节点来扩展读取查询的能力。 使用 StatefulSet，您可以使用单个命令执行此操作：\nkubectl scale statefulset mysql --replicas=5 查看新的 Pod 的运行情况：\nkubectl get pods -l app=mysql --watch 一旦 Pod 启动，您应该看到服务器 IDs 103 和 104 开始出现在 SELECT @@server_id 循环输出中。\n您还可以验证这些新服务器在存在之前已添加了数据：\nkubectl run mysql-client --image=mysql:5.7 -i -t --rm --restart=Never --\\  mysql -h mysql-3.mysql -e \u0026#34;SELECT * FROM test.messages\u0026#34; Waiting for pod default/mysql-client to be running, status is Pending, pod ready: false +---------+ | message | +---------+ | hello | +---------+ pod \u0026quot;mysql-client\u0026quot; deleted 向下缩放也是不停顿的：\nkubectl scale statefulset mysql --replicas=3 但是请注意，按比例放大会自动创建新的 PersistentVolumeClaims，而按比例缩小不会自动删除这些 PVC。 这使您可以选择保留那些初始化的 PVC，以更快地进行缩放，或者在删除它们之前提取数据。\n您可以通过运行以下命令查看此信息：\nkubectl get pvc -l app=mysql 这表明，尽管将 StatefulSet 缩小为3，所有5个 PVC 仍然存在：\nNAME STATUS VOLUME CAPACITY ACCESSMODES AGE data-mysql-0 Bound pvc-8acbf5dc-b103-11e6-93fa-42010a800002 10Gi RWO 20m data-mysql-1 Bound pvc-8ad39820-b103-11e6-93fa-42010a800002 10Gi RWO 20m data-mysql-2 Bound pvc-8ad69a6d-b103-11e6-93fa-42010a800002 10Gi RWO 20m data-mysql-3 Bound pvc-50043c45-b1c5-11e6-93fa-42010a800002 10Gi RWO 2m data-mysql-4 Bound pvc-500a9957-b1c5-11e6-93fa-42010a800002 10Gi RWO 2m 如果您不打算重复使用多余的 PVC，则可以删除它们：\nkubectl delete pvc data-mysql-3 kubectl delete pvc data-mysql-4 . heading \u0026ldquo;cleanup\u0026rdquo; %}}   通过在终端上按 Ctrl+C 取消 SELECT @@server_id 循环，或从另一个终端运行以下命令：\nkubectl delete pod mysql-client-loop --now     删除 StatefulSet。这也开始终止 Pod。\nkubectl delete statefulset mysql     验证 Pod 消失。 他们可能需要一些时间才能完成终止。\nkubectl get pods -l app=mysql 当以上内容返回时，您将知道 Pod 已终止：\nNo resources found.     删除 ConfigMap，Services 和 PersistentVolumeClaims。\nkubectl delete configmap,service,pvc -l app=mysql    如果您手动设置 PersistentVolume，则还需要手动删除它们，并释放基础资源。 如果您使用了动态预配器，当得知您删除 PersistentVolumeClaims 时，它将自动删除 PersistentVolumes。 一些动态预配器(例如用于 EBS 和 PD 的预配器)也会在删除 PersistentVolumes 时释放基础资源。  . heading \u0026ldquo;whatsnext\u0026rdquo; %}}  在Helm Charts 存储库中查找其他有状态的应用程序示例。  "
},
{
	"uri": "https://lijun.in/tasks/configure-pod-container/quality-service-pod/",
	"title": "配置 Pod 的服务质量",
	"tags": [],
	"description": "",
	"content": "本文介绍怎样配置 Pod 让其获得特定的服务质量（QoS）类。Kubernetes 使用 QoS 类来决定 Pod 的调度和驱逐策略。\n. heading \u0026ldquo;prerequisites\u0026rdquo; %}} . include \u0026ldquo;task-tutorial-prereqs.md\u0026rdquo; \u0026gt;}} . version-check \u0026gt;}}\nQoS 类 Kubernetes 创建 Pod 时就给它指定了下列一种 QoS 类：\n Guaranteed Burstable BestEffort  创建命名空间 创建一个命名空间，以便将本练习所创建的资源与集群的其余资源相隔离。\nkubectl create namespace qos-example 创建一个 QoS 类为 Guaranteed 的 Pod 对于 QoS 类为 Guaranteed 的 Pod：\n Pod 中的每个容器必须指定内存请求和内存限制，并且两者要相等。 Pod 中的每个容器必须指定 CPU 请求和 CPU 限制，并且两者要相等。  下面是包含一个容器的 Pod 配置文件。 容器设置了内存请求和内存限制，值都是 200 MiB。 容器设置了 CPU 请求和 CPU 限制，值都是 700 milliCPU：\n. codenew file=\u0026quot;pods/qos/qos-pod.yaml\u0026rdquo; \u0026gt;}}\n创建 Pod：\nkubectl create -f https://k8s.io/examples/pods/qos/qos-pod.yaml --namespace=qos-example 查看 Pod 详情：\nkubectl get pod qos-demo --namespace=qos-example --output=yaml 结果表明 Kubernetes 为 Pod 配置的 QoS 类为 Guaranteed。 结果也确认了 Pod 容器设置了与内存限制匹配的内存请求，设置了与 CPU 限制匹配的 CPU 请求。\nspec: containers: ... resources: limits: cpu: 700m memory: 200Mi requests: cpu: 700m memory: 200Mi ... qosClass: Guaranteed . note \u0026gt;}}\n如果容器指定了自己的内存限制，但没有指定内存请求，Kubernetes 会自动为它指定与内存限制匹配的内存请求。 同样，如果容器指定了自己的 CPU 限制，但没有指定 CPU 请求，Kubernetes 会自动为它指定与 CPU 限制匹配的 CPU 请求。 . /note \u0026gt;}}\n删除 Pod：\nkubectl delete pod qos-demo --namespace=qos-example 创建一个 QoS 类为 Burstable 的 Pod 如果满足下面条件，将会指定 Pod 的 QoS 类为 Burstable：\n Pod 不符合 Guaranteed QoS 类的标准。 Pod 中至少一个容器具有内存或 CPU 请求。  下面是包含一个容器的 Pod 配置文件。 容器设置了内存限制 200 MiB 和内存请求 100 MiB。\n. codenew file=\u0026quot;pods/qos/qos-pod-2.yaml\u0026rdquo; \u0026gt;}}\n创建 Pod：\nkubectl create -f https://k8s.io/examples/pods/qos/qos-pod-2.yaml --namespace=qos-example 查看 Pod 详情：\nkubectl get pod qos-demo-2 --namespace=qos-example --output=yaml 结果表明 Kubernetes 为 Pod 配置的 QoS 类为 Burstable。\nspec: containers: - image: nginx imagePullPolicy: Always name: qos-demo-2-ctr resources: limits: memory: 200Mi requests: memory: 100Mi ... qosClass: Burstable 删除 Pod：\nkubectl delete pod qos-demo-2 --namespace=qos-example 创建一个 QoS 类为 BestEffort 的 Pod 对于 QoS 类为 BestEffort 的 Pod，Pod 中的容器必须没有设置内存和 CPU 限制或请求。\n下面是包含一个容器的 Pod 配置文件。 容器没有设置内存和 CPU 限制或请求。\n. codenew file=\u0026quot;pods/qos/qos-pod-3.yaml\u0026rdquo; \u0026gt;}}\n创建 Pod：\nkubectl create -f https://k8s.io/examples/pods/qos/qos-pod-3.yaml --namespace=qos-example 查看 Pod 详情：\nkubectl get pod qos-demo-3 --namespace=qos-example --output=yaml 结果表明 Kubernetes 为 Pod 配置的 QoS 类为 BestEffort。\nspec: containers: ... resources: {} ... qosClass: BestEffort 删除 Pod：\nkubectl delete pod qos-demo-3 --namespace=qos-example 创建包含两个容器的 Pod 下面是包含两个容器的 Pod 配置文件。 一个容器指定了内存请求 200 MiB。 另外一个容器没有指定任何请求和限制。\n. codenew file=\u0026quot;pods/qos/qos-pod-4.yaml\u0026rdquo; \u0026gt;}}\n注意此 Pod 满足 Burstable QoS 类的标准。 也就是说它不满足 Guaranteed QoS 类标准，因为它的一个容器设有内存请求。\n创建 Pod：\nkubectl create -f https://k8s.io/examples/pods/qos/qos-pod-4.yaml --namespace=qos-example 查看 Pod 详情：\nkubectl get pod qos-demo-4 --namespace=qos-example --output=yaml 结果表明 Kubernetes 为 Pod 配置的 QoS 类为 Burstable：\nspec: containers: ... name: qos-demo-4-ctr-1 resources: requests: memory: 200Mi ... name: qos-demo-4-ctr-2 resources: {} ... qosClass: Burstable 删除 Pod：\nkubectl delete pod qos-demo-4 --namespace=qos-example 环境清理 删除命名空间：\nkubectl delete namespace qos-example . heading \u0026ldquo;whatsnext\u0026rdquo; %}} 应用开发者参考   为 Pod 和容器分配内存资源\n  为 Pod 和容器分配 CPU 资源\n  集群管理员参考   为命名空间配置默认的内存请求和限制\n  为命名空间配置默认的 CPU 请求和限制\n  为命名空间配置最小和最大内存限制\n  为命名空间配置最小和最大 CPU 限制\n  为命名空间配置内存和 CPU 配额\n  为命名空间配置 Pod 配额\n  为 API 对象配置配额\n  控制节点上的拓扑管理策略\n  "
},
{
	"uri": "https://lijun.in/tasks/administer-cluster/manage-resources/memory-constraint-namespace/",
	"title": "配置命名空间的最小和最大内存约束",
	"tags": [],
	"description": "",
	"content": "此页面介绍如何设置在命名空间中运行的容器使用的内存的最小值和最大值。 您可以在 [LimitRange](/docs/reference/generated/kubernetes-api/. param \u0026ldquo;version\u0026rdquo; \u0026gt;}}/#limitrange-v1-core)对象中指定最小和最大内存值。 如果 Pod 不满足 LimitRange 施加的约束，则无法在命名空间中创建它。\n. heading \u0026ldquo;prerequisites\u0026rdquo; %}} . include \u0026ldquo;task-tutorial-prereqs.md\u0026rdquo; \u0026gt;}} . version-check \u0026gt;}}\n集群中每个节点必须至少要有1 GiB 的内存。\n创建命名空间 创建一个命名空间，以便在此练习中创建的资源与群集的其余资源隔离。\nkubectl create namespace constraints-mem-example 创建 LimitRange 和 Pod 下面是 LimitRange 的配置文件：\n. codenew file=\u0026quot;admin/resource/memory-constraints.yaml\u0026rdquo; \u0026gt;}}\n创建 LimitRange:\nkubectl create -f https://k8s.io/examples/admin/resource/memory-constraints.yaml --namespace=constraints-mem-example 查看 LimitRange 的详情：\nkubectl get limitrange mem-min-max-demo-lr --namespace=constraints-mem-example --output=yaml 输出显示预期的最小和最大内存约束。 但请注意，即使您没有在 LimitRange 的配置文件中指定默认值，也会自动创建它们。\n limits: - default: memory: 1Gi defaultRequest: memory: 1Gi max: memory: 1Gi min: memory: 500Mi type: Container 现在，只要在 constraints-mem-example 命名空间中创建容器，Kubernetes 就会执行下面的步骤：\n  如果 Container 未指定自己的内存请求和限制，将为它指定默认的内存请求和限制。\n  验证 Container 的内存请求是否大于或等于500 MiB。\n  验证 Container 的内存限制是否小于或等于1 GiB。\n  这里给出了包含一个 Container 的 Pod 配置文件。Container 声明了600 MiB 的内存请求和800 MiB 的内存限制， 这些满足了 LimitRange 施加的最小和最大内存约束。\n. codenew file=\u0026quot;admin/resource/memory-constraints-pod.yaml\u0026rdquo; \u0026gt;}}\n创建 Pod：\nkubectl create -f https://k8s.io/examples/admin/resource/memory-constraints-pod.yaml --namespace=constraints-mem-example 确认下 Pod 中的容器在运行：\nkubectl get pod constraints-mem-demo --namespace=constraints-mem-example 查看 Pod 详情：\nkubectl get pod constraints-mem-demo --output=yaml --namespace=constraints-mem-example 输出结果显示容器的内存请求为600 MiB，内存限制为800 MiB。这些满足了 LimitRange 设定的限制范围。\nresources: limits: memory: 800Mi requests: memory: 600Mi 删除你创建的 Pod：\nkubectl delete pod constraints-mem-demo --namespace=constraints-mem-example 尝试创建一个超过最大内存限制的 Pod 这里给出了包含一个容器的 Pod 的配置文件。容器声明了800 MiB 的内存请求和1.5 GiB 的内存限制。\n. codenew file=\u0026quot;admin/resource/memory-constraints-pod-2.yaml\u0026rdquo; \u0026gt;}}\n尝试创建 Pod:\nkubectl create -f https://k8s.io/examples/admin/resource/memory-constraints-pod-2.yaml --namespace=constraints-mem-example 输出结果显示 Pod 没有创建成功，因为容器声明的内存限制太大了：\nError from server (Forbidden): error when creating \u0026quot;examples/admin/resource/memory-constraints-pod-2.yaml\u0026quot;: pods \u0026quot;constraints-mem-demo-2\u0026quot; is forbidden: maximum memory usage per Container is 1Gi, but limit is 1536Mi. 尝试创建一个不满足最小内存请求的 Pod 这里给出了包含一个容器的 Pod 的配置文件。容器声明了100 MiB 的内存请求和800 MiB 的内存限制。\n. codenew file=\u0026quot;admin/resource/memory-constraints-pod-3.yaml\u0026rdquo; \u0026gt;}}\n尝试创建 Pod：\nkubectl create -f https://k8s.io/examples/admin/resource/memory-constraints-pod-3.yaml --namespace=constraints-mem-example 输出结果显示 Pod 没有创建成功，因为容器声明的内存请求太小了：\nError from server (Forbidden): error when creating \u0026quot;examples/admin/resource/memory-constraints-pod-3.yaml\u0026quot;: pods \u0026quot;constraints-mem-demo-3\u0026quot; is forbidden: minimum memory usage per Container is 500Mi, but request is 100Mi. 创建一个没有声明内存请求和限制的 Pod 这里给出了包含一个容器的 Pod 的配置文件。容器没有声明内存请求，也没有声明内存限制。\n. codenew file=\u0026quot;admin/resource/memory-constraints-pod-4.yaml\u0026rdquo; \u0026gt;}}\n创建 Pod：\nkubectl create -f https://k8s.io/examples/admin/resource/memory-constraints-pod-4.yaml --namespace=constraints-mem-example 查看 Pod 详情：\nkubectl get pod constraints-mem-demo-4 --namespace=constraints-mem-example --output=yaml 输出结果显示 Pod 的内存请求为1 GiB，内存限制为1 GiB。容器怎样获得哪些数值呢？\nresources: limits: memory: 1Gi requests: memory: 1Gi 因为你的容器没有声明自己的内存请求和限制，它从 LimitRange 那里获得了默认的内存请求和限制。\n此时，你的容器可能运行起来也可能没有运行起来。回想一下我们本次任务的先决条件是你的每个节点都至少有1 GiB 的内存。如果你的每个节点都只有1 GiB 的内存，那将没有一个节点拥有足够的可分配内存来满足1 GiB 的内存请求。\n删除你的 Pod：\nkubectl delete pod constraints-mem-demo-4 --namespace=constraints-mem-example 强制执行内存最小和最大限制 LimitRange 为命名空间设定的最小和最大内存限制只有在 Pod 创建和更新时才会强制执行。如果你更新 LimitRange，它不会影响此前创建的 Pod。\n设置内存最小和最大限制的动因 做为集群管理员，你可能想规定 Pod 可以使用的内存总量限制。例如：\n 集群的每个节点有2 GB 内存。你不想接受任何请求超过2 GB 的 Pod，因为集群中没有节点可以满足。  数据清理 删除你的命名空间：\nkubectl delete namespace constraints-mem-example . heading \u0026ldquo;whatsnext\u0026rdquo; %}} 集群管理员参考   为命名空间配置默认内存请求和限制\n  为命名空间配置内存限制的最小值和最大值\n  为命名空间配置 CPU 限制的最小值和最大值\n  为命名空间配置内存和 CPU 配额\n  为命名空间配置 Pod 配额\n  为 API 对象配置配额\n  应用开发者参考   为容器和 Pod 分配内存资源\n  为容器和 Pod 分配 CPU 资源\n  为 Pod 配置 Service 数量\n  "
},
{
	"uri": "https://lijun.in/setup/independent/create-cluster-kubeadm/",
	"title": "💖 - 使用 kubeadm 创建一个单主集群",
	"tags": [],
	"description": "",
	"content": "kubeadm 能帮助您建立一个小型的符合最佳实践的 Kubernetes 集群。通过使用 kubeadm, 您的集群会符合 Kubernetes 合规性测试的要求. Kubeadm 也支持其他的集群生命周期操作，比如升级、降级和管理启动引导令牌。\n因为您可以在不同类型的机器（比如笔记本、服务器和树莓派等）上安装 kubeadm，因此它非常适合与 Terraform 或 Ansible 这类自动化管理系统集成。\nkubeadm 的简单便捷为大家带来了广泛的用户案例：\n 新用户可以从 kubeadm 开始来试用 Kubernetes。 熟悉 Kubernetes 的用户可以使用 kubeadm 快速搭建集群并测试他们的应用。 大型的项目可以将 kubeadm 和其他的安装工具一起形成一个比较复杂的系统。  kubeadm 的设计初衷是为新用户提供一种便捷的方式来首次试用 Kubernetes， 同时也方便老用户搭建集群测试他们的应用。 此外 kubeadm 也可以跟其它生态系统与/或安装工具集成到一起，提供更强大的功能。\n您可以很方便地在支持 rpm 或 deb 软件包的操作系统上安装 kubeadm。对应 kubeadm 的 SIG， SIG Cluster Lifecycle， 提供了预编译的这类安装包，当然您也可以自己基于源码为其它操作系统来构造安装包。\nkubeadm 成熟程度    功能 成熟程度     命令行用户体验 beta   功能实现 beta   配置文件 API alpha   自托管 alpha   kubeadm alpha 子命令 alpha   CoreDNS GA   动态 Kubelet 配置 alpha    kubeadm 的整体功能目前还是 Beta 状态，然而很快在 2018 年就会转换成正式发布 (GA) 状态。 一些子功能，比如自托管或者配置文件 API 还在开发过程当中。 随着工具的发展，创建集群的方法可能会有所变化，但是整体部署方案还是比较稳定的。 在 kubeadm alpha 下面的任何命令都只是 alpha 状态，目前只提供初期阶段的服务。\n维护周期 Kubernetes 发布的版本通常只维护支持九个月，在维护周期内，如果发现有比较重大的 bug 或者安全问题的话， 可能会发布一个补丁版本。下面是 Kubernetes 的发布和维护周期，同时也适用于 kubeadm。\n   Kubernetes 版本 发行月份 终止维护月份     v1.6.x 2017 年 3 月 2017 年 12 月   v1.7.x 2017 年 6 月 2018 年 3 月   v1.8.x 2017 年 9 月 2018 年 6 月   v1.9.x 2017 年 12 月 2018 年 9 月   v1.10.x 2018 年 3 月 2018 年 12 月   v1.11.x 2018 年 6 月 2019 年 3 月   v1.12.x 2018 年 9 月 2019 年 6 月    . heading \u0026ldquo;prerequisites\u0026rdquo; %}}  一个或者多个兼容 deb 或者 rpm 软件包的操作系统，比如 Ubuntu 或者 CentOS 每台机器 2 GB 以上的内存，内存不足时应用会受限制 主节点上 2 CPU 以上 集群里所有的机器有完全的网络连接，公有网络或者私有网络都可以  目标  搭建一个单主 Kubernetes 集群或者高可用集群 在集群上安装 Pod 网络组件以便 Pod 之间可以互相通信  步骤 在您的机器上安装 kubeadm 请查阅安装 kubeadm。\n. note \u0026gt;}} 如果您的机器已经安装了 kubeadm, 请运行 apt-get update \u0026amp;\u0026amp; apt-get upgrade 或者 yum update 来升级至最新版本的 kubeadm.\n升级过程中，kubelet 会每隔几秒钟重启并陷入了不断循环等待 kubeadm 发布指令的状态。 这个死循环的过程是正常的，当升级并初始化完成您的主节点之后，kubelet 才会正常运行。 . /note \u0026gt;}}\n初始化您的主节点 主节点是集群里运行控制面的机器，包括 etcd (集群的数据库)和 API 服务（kubectl CLI 与之交互）。\n 选择一个 Pod 网络插件，并检查是否在 kubeadm 初始化过程中需要传入什么参数。这个取决于 您选择的网络插件，您可能需要设置 --Pod-network-cidr 来指定网络驱动的 CIDR。请参阅安装网络插件。 (可选) 除非特别指定，kubeadm 会使用默认网关所在的网络接口广播其主节点的 IP 地址。若需使用其他网络接口，请 给 kubeadm init 设置 --apiserver-advertise-address=\u0026lt;ip-address\u0026gt; 参数。如果需要部署 IPv6 的集群，则需要指定一个 IPv6 地址，比如 --apiserver-advertise-address=fd00::101。 (可选) 在运行 kubeadm init 之前请先执行 kubeadm config images pull 来测试与 gcr.io 的连接。  现在运行:\n​```bash kubeadm init  \u0026lt;!-- ### More information For more information about `kubeadm init` arguments, see the [kubeadm reference guide](/docs/reference/setup-tools/kubeadm/kubeadm/). For a complete list of configuration options, see the [configuration file documentation](/docs/reference/setup-tools/kubeadm/kubeadm-init/#config-file). To customize control plane components, including optional IPv6 assignment to liveness probe for control plane components and etcd server, provide extra arguments to each component as documented in [custom arguments](/docs/admin/kubeadm#custom-args). --\u0026gt; ### 补充信息 想了解更多关于 `kubeadm init` 的参数, 请参阅[kubeadm 参考指南](/docs/reference/setup-tools/kubeadm/kubeadm/)。 想了解完整的配置选项，请参阅[配置文件](/docs/reference/setup-tools/kubeadm/kubeadm-init/#config-file)。 如果想定制控制面组件，包括为活跃性探测和 etcd 服务提供 IPv6 支持以及为各组件提供额外参数，请参阅[定制参数](/docs/admin/kubeadm#custom-args)。 \u0026lt;!-- To run `kubeadm init` again, you must first [tear down the cluster](#tear-down). If you join a node with a different architecture to your cluster, create a separate Deployment or DaemonSet for `kube-proxy` and `kube-dns` on the node. This is because the Docker images for these components do not currently support multi-architecture. `kubeadm init` first runs a series of prechecks to ensure that the machine is ready to run Kubernetes. These prechecks expose warnings and exit on errors. `kubeadm init` then downloads and installs the cluster control plane components. This may take several minutes. The output should look like: --\u0026gt; 如果需要再次运行 `kubeadm init`，您必须先[卸载集群](#tear-down)。 如果您需要将不同架构的节点加入您的集群，请单独在这类节点上为 `kube-proxy` 和 `kube-dns` 创建 Deployment 或 DaemonSet。 这是因为这些组件的 Docker 镜像并不支持多架构。 `kubeadm init` 首先会执行一系列的运行前检查来确保机器满足运行 Kubernetes 的条件。 这些检查会抛出警告并在发现错误的时候终止整个初始化进程。 然后 `kubeadm init` 会下载并安装集群的控制面组件，这可能会花费几分钟时间，其输出如下所示： ```none [init] Using Kubernetes version: vX.Y.Z [preflight] Running pre-flight checks [kubeadm] WARNING: starting in 1.8, tokens expire after 24 hours by default (if you require a non-expiring token use --token-ttl 0) [certificates] Generated ca certificate and key. [certificates] Generated apiserver certificate and key. [certificates] apiserver serving cert is signed for DNS names [kubeadm-master kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 10.138.0.4] [certificates] Generated apiserver-kubelet-client certificate and key. [certificates] Generated sa key and public key. [certificates] Generated front-proxy-ca certificate and key. [certificates] Generated front-proxy-client certificate and key. [certificates] Valid certificates and keys now exist in \u0026quot;/etc/kubernetes/pki\u0026quot; [kubeconfig] Wrote KubeConfig file to disk: \u0026quot;admin.conf\u0026quot; [kubeconfig] Wrote KubeConfig file to disk: \u0026quot;kubelet.conf\u0026quot; [kubeconfig] Wrote KubeConfig file to disk: \u0026quot;controller-manager.conf\u0026quot; [kubeconfig] Wrote KubeConfig file to disk: \u0026quot;scheduler.conf\u0026quot; [controlplane] Wrote Static Pod manifest for component kube-apiserver to \u0026quot;/etc/kubernetes/manifests/kube-apiserver.yaml\u0026quot; [controlplane] Wrote Static Pod manifest for component kube-controller-manager to \u0026quot;/etc/kubernetes/manifests/kube-controller-manager.yaml\u0026quot; [controlplane] Wrote Static Pod manifest for component kube-scheduler to \u0026quot;/etc/kubernetes/manifests/kube-scheduler.yaml\u0026quot; [etcd] Wrote Static Pod manifest for a local etcd instance to \u0026quot;/etc/kubernetes/manifests/etcd.yaml\u0026quot; [init] Waiting for the kubelet to boot up the control plane as Static Pods from directory \u0026quot;/etc/kubernetes/manifests\u0026quot; [init] This often takes around a minute; or longer if the control plane images have to be pulled. [apiclient] All control plane components are healthy after 39.511972 seconds [uploadconfig] Storing the configuration used in ConfigMap \u0026quot;kubeadm-config\u0026quot; in the \u0026quot;kube-system\u0026quot; Namespace [markmaster] Will mark node master as master by adding a label and a taint [markmaster] Master master tainted and labelled with key/value: node-role.kubernetes.io/master=\u0026quot;\u0026quot; [bootstraptoken] Using token: \u0026lt;token\u0026gt; [bootstraptoken] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials [bootstraptoken] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token [bootstraptoken] Creating the \u0026quot;cluster-info\u0026quot; ConfigMap in the \u0026quot;kube-public\u0026quot; namespace [addons] Applied essential addon: CoreDNS [addons] Applied essential addon: kube-proxy Your Kubernetes master has initialized successfully! To start using your cluster, you need to run (as a regular user): mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config You should now deploy a Pod network to the cluster. Run \u0026quot;kubectl apply -f [Podnetwork].yaml\u0026quot; with one of the addon options listed at: http://kubernetes.io/docs/admin/addons/ You can now join any number of machines by running the following on each node as root: kubeadm join --token \u0026lt;token\u0026gt; \u0026lt;master-ip\u0026gt;:\u0026lt;master-port\u0026gt; --discovery-token-ca-cert-hash sha256:\u0026lt;hash\u0026gt; 如果需要让普通用户可以运行 kubectl，请运行如下命令，其实这也是 kubeadm init 输出的一部分：\n​```bash mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config\n \u0026lt;!-- Alternatively, if you are the `root` user, you can run: ```bash export KUBECONFIG=/etc/kubernetes/admin.conf ​``` --\u0026gt; 或者，如果您是 `root` 用户，则可以运行： ​```bash export KUBECONFIG=/etc/kubernetes/admin.conf 请备份好 kubeadm init 输出中的 kubeadm join 命令，因为您会需要这个命令来给集群添加节点。\n令牌是主节点和新添加的节点之间进行相互身份验证的，因此请确保其安全。任何人只要知道了这些令牌，就可以随便给您的集群添加节点。 可以使用 kubeadm token 命令来列出、创建和删除这类令牌。 请参阅kubeadm 参考指南。\n安装 Pod 网络插件 . caution \u0026gt;}} 注意: 这一节包含了安装和部署顺序的重要信息，执行之前请仔细阅读。 . /caution \u0026gt;}}\n您必须先安装 Pod 网络插件，以便您的 Pod 可以互相通信。\n网络必须在部署任何应用之前部署好。此外，在网络安装之前是 CoreDNS 不会启用的。 kubeadm 只支持基于容器网络接口（CNI）的网络而且不支持 kubenet 。\n有一些项目为 Kubernetes 提供使用 CNI 的 Pod 网络，其中一些也支持网络策略. 请参阅插件页面了解可用网络插件的完整列表。\n CNI v0.6.0 也提供了 IPv6 的支持。 CNI 网桥 和 local-ipam 是 Kubernetes 1.9 版本里提供的唯一支持 IPv6 的网络插件。  注意 kubeadm 默认会创建一个比较安全的集群并强制启用RBAC。 请确保您的网络方案支持 RBAC。\n您可以使用下列命令安装网络插件：\n​```bash kubectl apply -f \u0026lt;add-on.yaml\u0026gt;\n \u0026lt;!-- You can install only one Pod network per cluster. . tabs name=\u0026quot;tabs-Pod-install\u0026quot; \u0026gt;}} . tab name=\u0026quot;Choose one...\u0026quot; %}} Please select one of the tabs to see installation instructions for the respective third-party Pod Network Provider. . /tab %}} --\u0026gt; 您仅可以给任何一个集群安装一个网络插件。 . tabs name=\u0026quot;tabs-Pod-install\u0026quot; \u0026gt;}} . tab name=\u0026quot;Choose one...\u0026quot; %}} 请选择一个选项来查看对应的第三方网络插件驱动的安装向导。 . /tab %}} \u0026lt;!-- For more information about using Calico, see [Quickstart for Calico on Kubernetes](https://docs.projectcalico.org/latest/getting-started/kubernetes/), [Installing Calico for policy and networking](https://docs.projectcalico.org/latest/getting-started/kubernetes/installation/calico), and other related resources. For Calico to work correctly, you need to pass `--Pod-network-cidr=192.168.0.0/16` to `kubeadm init` or update the `calico.yml` file to match your Pod network. Note that Calico works on `amd64` only. ```shell kubectl apply -f https://docs.projectcalico.org/v3.8/manifests/calico.yaml \u0026ndash;\u0026gt;\n. tab name=\u0026quot;Calico\u0026rdquo; %}} 想了解关于 Calico 的使用的更多信息, 请参阅Kubernetes上的Calico快速实践、安装 Calico 实现网络策略和其他相关资源。\n为了 Calico 可以正确工作，您需要给 kubeadm init 传递 --Pod-network-cidr=192.168.0.0/16 这样的选项， 或者根据您的网络方案更新 calico.yml 。注意 Calico 只适用于 amd64 架构。\nkubectl apply -f https://docs.projectcalico.org/v3.8/manifests/calico.yaml . /tab %}}\n. tab name=\u0026quot;Canal\u0026rdquo; %}} Canal 使用 Calico 提供的网络策略和 Flannel 提供的网络功能。请查阅 Calico 的官方文档 入门指引。\n为了 Canal 可以正确运行，kubeadm init 运行时需要设置--Pod-network-cidr=10.244.0.0/16，同时注意它只适用于 amd64 架构。\nkubectl apply -f https://docs.projectcalico.org/v3.8/manifests/canal.yaml . /tab %}}\n. tab name=\u0026quot;Cilium\u0026rdquo; %}} 想了解 Kubernetes 上使用 Cilium 的更多相关信息，请查参阅Kubernetes 上 Cilium 的快速指南 和 Kubernetes 上 Cilium 的安装向导。\n尽管这里并不要求给 kubeadm init 设置 --Pod-network-cidr 参数，但是这是一个高度推荐操作的步骤。\n这些命令会部署 Cilium 和它自己受 etcd 操作者管理的 etcd。\n# 从 Cilium 库下载所需清单文件 wget https://github.com/cilium/cilium/archive/v1.2.0.zip unzip v1.2.0.zip cd cilium-1.2.0/examples/kubernetes/addons/etcd-operator # 生成并部署 etcd 证书 export CLUSTER_DOMAIN=$(kubectl get ConfigMap --namespace kube-system coredns -o yaml | awk \u0026#39;/kubernetes/ {print $2}\u0026#39;) tls/certs/gen-cert.sh $CLUSTER_DOMAIN tls/deploy-certs.sh # 为 kube-dns 设置固定的标识标签 kubectl label -n kube-system Pod $(kubectl -n kube-system get Pods -l k8s-app=kube-dns -o jsonpath=\u0026#39;{range .items[]}{.metadata.name}{\u0026#34; \u0026#34;}{end}\u0026#39;) io.cilium.fixed-identity=kube-dns kubectl create -f ./ # 等待几分钟，Cilium、coredns 和 etcd 的 Pods 会收敛到工作状态 . /tab %}}\n. tab name=\u0026quot;Flannel\u0026rdquo; %}}\n为了让 flannel 能正确工作，您必须在运行 kubeadm init 时设置 --Pod-network-cidr=10.244.0.0/16。\n通过运行 sysctl net.bridge.bridge-nf-call-iptables=1 将 /proc/sys/net/bridge/bridge-nf-call-iptables 设置成 1， 进而确保桥接的 IPv4 流量会传递给 iptables。 这是一部分 CNI 插件运行的要求条件，请查看这篇文档获取更详细信息。\n注意 flannel 适用于 amd64、arm、arm64 和 ppc64le 架构平台。\nkubectl apply -f https://raw.githubusercontent.com/coreos/flannel/bc79dd1505b0c8681ece4de4c0d86c5cd2643275/Documentation/kube-flannel.yml 想了解更多关于 flannel 的信息,请查阅GitHub 上的 CoreOS flannel 仓库。 . /tab %}}\n. tab name=\u0026quot;Kube-router\u0026rdquo; %}} 通过运行 sysctl net.bridge.bridge-nf-call-iptables=1 将 /proc/sys/net/bridge/bridge-nf-call-iptables 设置成 1， 确保桥接的 IPv4 流量会传递给 iptables。 这是一部分 CNI 插件的运行条件。请查看这篇文档了解更详细的信息。\nKube-router 依赖于 kube-controller-manager 来给节点分配 CIDR， 因此需要设置 kubeadm init 的 --Pod-network-cidr 参数。\nKube-router 提供 Pod 间联网、网络策略和和高效的基于 IPVS/LVS 的服务代理功能。\n想了解关于使用 kubeadm 搭建 Kubernetes 和 Kube-router 的更多信息。请查看官方的安装指引。 . /tab %}}\n. tab name=\u0026quot;Romana\u0026rdquo; %}} 通过运行 sysctl net.bridge.bridge-nf-call-iptables=1 将 /proc/sys/net/bridge/bridge-nf-call-iptables 设置成 1， 确保桥接的 IPv4 流量会传递给 iptables。这是一部分 CNI 插件的运行条件。 请查看这篇文档 获取更详细的信息。\n官方的 Romana 安装指引在这里。\n注意，Romana 只适用于 amd64 架构。\nkubectl apply -f https://raw.githubusercontent.com/romana/romana/master/containerize/specs/romana-kubeadm.yml . /tab %}}\n. tab name=\u0026quot;Weave Net\u0026rdquo; %}}\n通过运行 sysctl net.bridge.bridge-nf-call-iptables=1 将 /proc/sys/net/bridge/bridge-nf-call-iptables 设置成 1， 将桥接的 IPv4 流量传递给 iptables。这是一部分 CNI 插件的运行条件。 请查看这篇文档 获取更详细的信息。\n官方的 Weave Net 配置向导在这里。\nWeave Net 适用于amd64、arm、arm64 和 ppc64le 而不需要其它额外的配置。 Weave Net 默认启用 hairpin 模式，可以让 Pod 在不知道他们自己的 PodIP 的时候仍可以使用服务的 IP 地址来访问他们自己。\nkubectl apply -f \u0026#34;https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d \u0026#39;\\n\u0026#39;)\u0026#34; . /tab %}}\n. tab name=\u0026quot;JuniperContrail/TungstenFabric\u0026rdquo; %}} 提供了支持 overlay 的 SDN 解决方案，支持多云环境和混合云环境的网络方案，同时支持 overlay 和 underlay、网络策略、 网络隔离、服务链和灵活的负载均衡。\n安装 JuniperContrail/TungstenFabric CNI 有很多灵活的方式。\n请查阅这个安装指引。 . /tab %}} . /tabs \u0026gt;}}\n一旦 Pod 网络安装完成，您就可以通过 kubectl get Pods --all-namespaces 的输出来验证 CoreDNS Pod 是否正常运行。 只要确认了 CoreDNS 正常运行，您就可以向集群中添加节点了。\n如果您的网络不能工作或者 CoreDNS 不在运行状态，请查阅查错方案。\n主节点隔离 出于安全原因，默认您的主节点不会被调度运行任何 Pod。 如果您需要在主节点上运行 Pod，比如说部署环境是一个单机器集群，运行：\nkubectl taint nodes --all node-role.kubernetes.io/master- 输出类似这样：\nnode \u0026quot;test-01\u0026quot; untainted taint \u0026quot;node-role.kubernetes.io/master:\u0026quot; not found taint \u0026quot;node-role.kubernetes.io/master:\u0026quot; not found 这个操作会从任何有 node-role.kubernetes.io/master 这种标签的节点移除该标签，包括主节点， 标签的移除意味着集群调度器可以将 Pod 调度到任何节点。\n添加节点 节点就是工作负载（容器和 Pod 等）运行的地方。如需向集群添加新节点，可以在每台机器上面执行如下操作：\n SSH 连接到机器上 成为 root 用户（比如 sudo su -） 运行 kubeadm init 输出里的命令，即：  ​``` bash kubeadm join \u0026ndash;token :\u0026ndash;discovery-token-ca-cert-hash sha256:\u0026lt;!-- If you do not have the token, you can get it by running the following command on the master node: ``` bash kubeadm token list The output is similar to this:\nTOKEN TTL EXPIRES USAGES DESCRIPTION EXTRA GROUPS 8ewj1p.9r9hcjoqgajrj4gi 23h 2018-06-12T02:51:28Z authentication, The default bootstrap system: signing token generated by bootstrappers: 'kubeadm init'. kubeadm: default-node-token ​``` --\u0026gt; 如果您没有保存令牌的话，可以通过在主节点上执行下面的命令来获取： ​``` bash kubeadm token list 输出类似这样:\nTOKEN TTL EXPIRES USAGES DESCRIPTION EXTRA GROUPS 8ewj1p.9r9hcjoqgajrj4gi 23h 2018-06-12T02:51:28Z authentication, The default bootstrap system: signing token generated by bootstrappers: 'kubeadm init'. kubeadm: default-node-token 默认情况下，令牌会在 24 小时内过期。如果在令牌过期之后添加节点，您可以在主节点上执行下面的命令创建一个新令牌：\n​``` bash kubeadm token create\n 输出类似这样： ``` console 5didvk.d09sbcov8ph2amjw 如果您也不知道这个 --discovery-token-ca-cert-hash 的值，您也可以在主节点上运行下面的命令来获取：\nopenssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2\u0026gt;/dev/null | \\  openssl dgst -sha256 -hex | sed \u0026#39;s/^.* //\u0026#39; 输出类似这样：\n8cb2de97839780a412b93877f8507ad6c94f73add17d5d7058e91741c9d5ec78 . note \u0026gt;}} 若需为 \u0026lt;master-ip\u0026gt;:\u0026lt;master-port\u0026gt; 参数设定一个 IPv6 的元组，地址必须写在一对方括号里面，比如: [fd00::101]:2073。 . /note \u0026gt;}}\n输出类似这样:\n[preflight] Running pre-flight checks ... (log output of join workflow) ... Node join complete: * Certificate signing request sent to master and response received. * Kubelet informed of new secure connection details. Run 'kubectl get nodes' on the master to see this machine join. 几秒钟之后，您将能在主节点上的 kubectl get nodes 的输出里发现新添加的节点。\n(可选) 在非主节点上控制集群 为了能在其他机器（比如，笔记本）上使用 kubectl 来控制您的集群，您可以从主节点上复制管理员的 kubeconfig 到您的机器上，像下面这样操作：\nscp root@\u0026lt;master ip\u0026gt;:/etc/kubernetes/admin.conf . kubectl --kubeconfig ./admin.conf get nodes . note \u0026gt;}} 上面的例子生效的前提是 SSH 允许 root 用户连接登录。 如果root 用户不能连接的话，您可以将 admin.conf 复制到允许其他用户访问的其他地方并将 scp 命令里的用户改成相对应的用户再复制。\n这个 admin.conf 文件给予了用户整个集群的超级用户权限，因此这个操作必须小心谨慎。对于普通用户来说， 更建议创建一个适用于白名单某些权限的验证文件。您可以通过这个命令来生成 kubeadm alpha phase kubeconfig user --client-name \u0026lt;CN\u0026gt;。 这个命令会打印 KubeConfig 的内容到标准输出，然后您需要将它保存到一个文件里并分发给您的用户。然后再创建权限的白名单列表， 命令如下： kubectl create (cluster)rolebinding 。 . /note \u0026gt;}}\n(可选) 将 API 服务代理到本地 如果您需要从集群外部连接到您的 API 服务器，请运行kubectl proxy:\nscp root@\u0026lt;master ip\u0026gt;:/etc/kubernetes/admin.conf . kubectl --kubeconfig ./admin.conf proxy 现在您就可以在本地访问 http://localhost:8001/api/v1 来连接 API 服务器了。\n卸载集群 想要回退 kubeadm 做出的修改，您需要首先腾空节点 而且必须确保在关闭节点之前没有任何工作负载在运行。\n使用正确的登录凭据来连接到主节点：\nkubectl drain \u0026lt;node name\u0026gt; --delete-local-data --force --ignore-daemonsets kubectl delete node \u0026lt;node name\u0026gt; 然后在待移除的节点上，重置所有 kubeadm 的安装状态：\nkubeadm reset 如果您只是想重新运行 kubeadm init 或者 kubeadm join，kubeadm reset页面有更多的信息可供参考.\n集群维护 维护集群（比如升级，降级）的详细指令，可参考这里。\n探索其他插件 查看插件列表来发现其他插件，包括日志、监控、网络策略、 可视化和集群管理工具等等。\n后续  使用 Sonobuoy 验证集群是否正确运行 阅读kubeadm 参考文档，学习 kubeadm 的高级应用 进一步了解 Kubernetes 的概念和 kubectl 您可以使用 logrotate 配置日志轮转。使用 Docker 的时候，您可以给 Docker 守护进程设置日志轮转的选项， 比如 --log-driver=json-file --log-opt=max-size=10m --log-opt=max-file=5。请查阅 Docker 守护进程的配置和纠错。  反馈  如果发现故障，请访问 kubeadm GitHub issue tracker 如果需要支持，请访问 kubeadm 的 Slack 频道：#kubeadm 访问 SIG cluster-lifecycle 开发者所使用的 Slack 频道： #sig-cluster-lifecycle SIG cluster-lifecycle 的 SIG 信息 SIG cluster-lifecycle 的邮件列表: kubernetes-sig-cluster-lifecycle  版本偏差策略 vX.Y 版本的 kubeadm 命令行工具可能会部署一个控制面版本为 vX.Y 或者 vX.(Y-1) 的集群，也可以用于升级一个 vX.(Y-1) 的由 kubeadm 创建的集群。 因为我们无法预见未来，版本为 vX.Y 的 kubeadm 可能可以也可能无法用于部署 vX.(Y+1) 版本的集群。 例子: kubeadm v1.8 可以用于部署 v1.7 和 v1.8 的集群，也可以升级 v1.7 的由 kubeadm 创建的集群到 1.8 版本。 请查看我们的安装向导，其中提供了关于 kubelet 和控制面版本偏差的更多信息。\n跨多平台上使用 kubeadm kubeadm 的 deb/rpm 包和可执行文件都是适用于 amd64、arm (32位)、arm64、ppc64le 和 s390x等架构平台的， 请查阅多平台方案。\n只有一部分的网络驱动提供了所有平台的网络解决方案，请查询上面的网络驱动列表或者对应的官方文档来确定是否支持您的平台。\n局限 请注意，kubeadm 还处于正在开发的状态中，这些局限将会在适当的时间修正。\n 这篇文档介绍的创建的集群只能有单一的主节点和单一的 etcd 数据库，这表示如果主节点宕机，您的集群将会丢失数据 而且可能无法重新创建。目前给 kubeadm 添加高可用支持（比如多 etcd 多 API服务等等）的功能还在开发当中，因此可先参照下面的 临时解决方案: 经常性地备份 etcd。 由 kubeadm 配置的 etcd 数据位于主节点上的 /var/lib/etcd 目录。  查错 如果您在使用 kubeadm 发现任何问题，请查阅我们的纠错文档。\n"
},
{
	"uri": "https://lijun.in/setup/production-environment/",
	"title": "💖 - 生产环境",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lijun.in/concepts/architecture/",
	"title": "😊 - Kubernetes 架构",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lijun.in/concepts/cluster-administration/",
	"title": "😊 - 计算、存储和网络扩展",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lijun.in/reference/kubernetes-api/",
	"title": "😍 - API 参考",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lijun.in/tutorials/configuration/",
	"title": "😎 - 配置",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lijun.in/tasks/inject-data-application/",
	"title": "😝 - 给应用注入数据",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lijun.in/concepts/services-networking/ingress/",
	"title": "Ingress",
	"tags": [],
	"description": "",
	"content": "feature-state for_k8s_version=\u0026quot;v1.1\u0026rdquo; state=\u0026quot;beta\u0026rdquo; \u0026gt;}} glossary_definition term_id=\u0026quot;ingress\u0026rdquo; length=\u0026quot;all\u0026rdquo; \u0026gt;}}\n 节点（Node）: Kubernetes 集群中其中一台工作机器，是集群的一部分。 集群（Cluster）: 一组运行程序（这些程序是容器化的，被 Kubernetes 管理的）的节点。 在此示例中，和在大多数常见的Kubernetes部署方案，集群中的节点都不会是公共网络。 边缘路由器（Edge router）: 在集群中强制性执行防火墙策略的路由器（router）。可以是由云提供商管理的网关，也可以是物理硬件。 集群网络（Cluster network）: 一组逻辑或物理的链接，根据 Kubernetes 网络模型 在集群内实现通信。 服务（Service）：Kubernetes Service标签选择器（selectors）标识的一组 Pod。除非另有说明，否则假定服务只具有在集群网络中可路由的虚拟 IP。  可以将 Ingress 配置为提供服务外部可访问的 URL、负载均衡流量、终止 SSL / TLS，以及提供基于名称的虚拟主机。Ingress 控制器 通常负责通过负载均衡器来实现 Ingress，尽管它也可以配置边缘路由器或其他前端来帮助处理流量。\nIngress 不会公开任意端口或协议。 将 HTTP 和 HTTPS 以外的服务公开到 Internet 时，通常使用 Service.Type=NodePort 或者 Service.Type=LoadBalancer 类型的服务。\n环境准备 您必须具有 ingress 控制器 才能满足 Ingress 的要求。仅创建 Ingress 资源无效。\n您可能需要部署 Ingress 控制器，例如 ingress-nginx。您可以从许多Ingress 控制器 中进行选择。\n理想情况下，所有 Ingress 控制器都应符合参考规范。但实际上，不同的 Ingress 控制器操作略有不同。\n确保您查看了 Ingress 控制器的文档，以了解选择它的注意事项。\nIngress 资源 一个最小的 Ingress 资源示例：\napiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: test-ingress annotations: nginx.ingress.kubernetes.io/rewrite-target: / spec: rules: - http: paths: - path: /testpath pathType: Prefix backend: serviceName: test servicePort: 80 与所有其他 Kubernetes 资源一样，Ingress 需要使用 apiVersion、kind 和 metadata 字段。 Ingress 对象的命名必须是合法的 DNS 子域名名称。 有关使用配置文件的一般信息，请参见部署应用、 配置容器、管理资源。 Ingress 经常使用注解（annotations）来配置一些选项，具体取决于 Ingress 控制器，例如 rewrite-target annotation。 不同的 Ingress 控制器 支持不同的注解（annotations）。查看文档以供您选择 Ingress 控制器，以了解支持哪些注解（annotations）。\nIngress 规范 具有配置负载均衡器或者代理服务器所需的所有信息。最重要的是，它包含与所有传入请求匹配的规则列表。Ingress 资源仅支持用于定向 HTTP 流量的规则。\nIngress 规则 每个 HTTP 规则都包含以下信息：\n 可选主机。在此示例中，未指定主机，因此该规则适用于通过指定 IP 地址的所有入站 HTTP 通信。如果提供了主机（例如 foo.bar.com），则规则适用于该主机。 路径列表（例如，/testpath）,每个路径都有一个由 serviceName 和 servicePort 定义的关联后端。在负载均衡器将流量定向到引用的服务之前，主机和路径都必须匹配传入请求的内容。 后端是 Service 文档中所述的服务和端口名称的组合。与规则的主机和路径匹配的对 Ingress 的 HTTP（和 HTTPS ）请求将发送到列出的后端。  通常在 Ingress 控制器中配置默认后端，以服务任何不符合规范中路径的请求。\n默认后端 没有规则的 Ingress 将所有流量发送到单个默认后端。默认后端通常是 Ingress 控制器的配置选项，并且未在 Ingress 资源中指定。\n如果没有主机或路径与 Ingress 对象中的 HTTP 请求匹配，则流量将路由到您的默认后端。\n路径类型 Ingress 中的每个路径都有对应的路径类型。支持三种类型：\n  ImplementationSpecific （默认）：对于这种类型，匹配取决于 IngressClass. 具体实现可以将其作为单独的 pathType 处理或者与 Prefix 或 Exact 类型作相同处理。\n  Exact：精确匹配 URL 路径且对大小写敏感。\n  Prefix：基于以 / 分割的 URL 路径前缀匹配。匹配对大小写敏感，并且对路径中的元素逐个完成。路径元素指的是由 / 分隔符分割的路径中的标签列表。如果每个 p 都是请求路径 p 的元素前缀，则请求与路径 p 匹配。\n如果路径的最后一个元素是请求路径中最后一个元素的子字符串，则不会匹配（例如：/foo/bar 匹配 /foo/bar/baz, 但不匹配 /foo/barbaz）。\n  多重匹配 在某些情况下，Ingress 中的多条路径会匹配同一个请求。这种情况下最长的匹配路径优先。如果仍然有两条同等的匹配路径，则精确路径类型优先于前缀路径类型。\nIngress 类 Ingress 可以由不同的控制器实现，通常使用不同的配置。每个 Ingress 应当指定一个类，一个对 IngressClass 资源的引用，该资源包含额外的配置，其中包括应当实现该类的控制器名称。\napiVersion: networking.k8s.io/v1beta1 kind: IngressClass metadata: name: external-lb spec: controller: example.com/ingress-controller parameters: apiGroup: k8s.example.com/v1alpha kind: IngressParameters name: external-lb IngressClass 资源包含一个可选的参数字段。可用于引用该类的额外配置。\n废弃的注解 在 IngressClass 资源和 ingressClassName 字段被引入 Kubernetes 1.18 之前，Ingress 类是通过 Ingress 中的一个 kubernetes.io/ingress.class 注解来指定的。这个注解从未被正式定义过，但是得到了 Ingress 控制器的广泛支持。\nIngress 中新的 ingressClassName 字段是该注解的替代品，但并非完全等价。该注解通常用于引用实现该 Ingress 的控制器的名称， 而这个新的字段则是对一个包含额外 Ingress 配置的 IngressClass 资源的引用，包括 Ingress 控制器的名称。\n默认 Ingress 类 您可以将一个特定的 IngressClass 标记为集群默认项。将一个 IngressClass 资源的 ingressclass.kubernetes.io/is-default-class 注解设置为 true 将确保新的未指定 ingressClassName 字段的 Ingress 能够分配为这个默认的 IngressClass.\n如果集群中有多个 IngressClass 被标记为默认，准入控制器将阻止创建新的未指定 ingressClassName 字段的 Ingress 对象。 解决这个问题只需确保集群中最多只能有一个 IngressClass 被标记为默认。\nIngress 类型 单服务 Ingress 现有的 Kubernetes 概念允许您暴露单个 Service (查看替代方案)，您也可以通过指定无规则的 默认后端 来对 Ingress 进行此操作。\ncodenew file=\u0026quot;service/networking/ingress.yaml\u0026rdquo; \u0026gt;}}\n如果使用 kubectl apply -f 创建它，则应该能够查看刚刚添加的 Ingress 的状态：\nkubectl get ingress test-ingress NAME HOSTS ADDRESS PORTS AGE test-ingress * 203.0.113.123 80 59s 其中 203.0.113.123 是由 Ingress 控制器分配以满足该 Ingress 的 IP。\n入口控制器和负载平衡器可能需要一两分钟才能分配IP地址。 在此之前，您通常会看到地址字段的值被设定为 \u0026lt;pending\u0026gt;。\n简单分列 一个分列配置根据请求的 HTTP URI 将流量从单个 IP 地址路由到多个服务。 Ingress 允许您将负载均衡器的数量降至最低。例如，这样的设置：\nfoo.bar.com -\u0026gt; 178.91.123.132 -\u0026gt; / foo service1:4200 / bar service2:8080 将需要一个 Ingress，例如：\napiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: simple-fanout-example annotations: nginx.ingress.kubernetes.io/rewrite-target: / spec: rules: - host: foo.bar.com http: paths: - path: /foo backend: serviceName: service1 servicePort: 4200 - path: /bar backend: serviceName: service2 servicePort: 8080 当您使用 kubectl apply -f 创建 Ingress 时：\nkubectl describe ingress simple-fanout-example Name: simple-fanout-example Namespace: default Address: 178.91.123.132 Default backend: default-http-backend:80 (10.8.2.3:8080) Rules: Host Path Backends ---- ---- -------- foo.bar.com /foo service1:4200 (10.8.0.90:4200) /bar service2:8080 (10.8.0.91:8080) Annotations: nginx.ingress.kubernetes.io/rewrite-target: / Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal ADD 22s loadbalancer-controller default/test Ingress 控制器将提供实现特定的负载均衡器来满足 Ingress，只要 Service (service1，service2) 存在。 当它这样做了，您会在地址栏看到负载均衡器的地址。\n根据您使用的 Ingress 控制器，您可能需要创建默认 HTTP 后端 Service。\n基于名称的虚拟托管 基于名称的虚拟主机支持将 HTTP 流量路由到同一 IP 地址上的多个主机名。\nfoo.bar.com --| |-\u0026gt; foo.bar.com service1:80 | 178.91.123.132 | bar.foo.com --| |-\u0026gt; bar.foo.com service2:80 以下 Ingress 让后台负载均衡器基于主机 header 路由请求。\napiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: name-virtual-host-ingress spec: rules: - host: foo.bar.com http: paths: - backend: serviceName: service1 servicePort: 80 - host: bar.foo.com http: paths: - backend: serviceName: service2 servicePort: 80 如果您创建的 Ingress 资源没有规则中定义的任何主机，则可以匹配到您 Ingress 控制器 IP 地址的任何网络流量，而无需基于名称的虚拟主机。\n例如，以下 Ingress 资源会将 first.bar.com 请求的流量路由到 service1，将 second.foo.com 请求的流量路由到 service2，而没有在请求中定义主机名的 IP 地址的流量路由（即，不提供请求标头）到 service3。\napiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: name-virtual-host-ingress spec: rules: - host: first.bar.com http: paths: - backend: serviceName: service1 servicePort: 80 - host: second.foo.com http: paths: - backend: serviceName: service2 servicePort: 80 - http: paths: - backend: serviceName: service3 servicePort: 80 TLS 您可以通过指定包含 TLS 私钥和证书的 secret _tooltip term_id=\u0026quot;secret\u0026rdquo; \u0026gt;}} 来加密 Ingress。 目前，Ingress 只支持单个 TLS 端口 443，并假定 TLS 终止。\n如果 Ingress 中的 TLS 配置部分指定了不同的主机，那么它们将根据通过 SNI TLS 扩展指定的主机名（如果 Ingress 控制器支持 SNI）在同一端口上进行复用。 TLS Secret 必须包含名为 tls.crt 和 tls.key 的密钥，这些密钥包含用于 TLS 的证书和私钥，例如：\napiVersion: v1 kind: Secret metadata: name: testsecret-tls namespace: default data: tls.crt: base64 encoded cert tls.key: base64 encoded key type: kubernetes.io/tls 在 Ingress 中引用此 Secret 将会告诉 Ingress 控制器使用 TLS 加密从客户端到负载均衡器的通道。您需要确保创建的 TLS secret 来自包含 sslexample.foo.com 的公用名称（CN）的证书，也被称为全限定域名（FQDN）。\napiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: tls-example-ingress spec: tls: - hosts: - sslexample.foo.com secretName: testsecret-tls rules: - host: sslexample.foo.com http: paths: - path: / backend: serviceName: service1 servicePort: 80 各种 Ingress 控制器所支持的 TLS 功能之间存在差异。请参阅有关文件 nginx、 GCE 或者任何其他平台特定的 Ingress 控制器，以了解 TLS 如何在您的环境中工作。\n负载均衡 Ingress 控制器使用一些适用于所有 Ingress 的负载均衡策略设置进行自举，例如负载均衡算法、后端权重方案和其他等。更高级的负载均衡概念（例如，持久会话、动态权重）尚未通过 Ingress 公开。您可以通过用于服务的负载均衡器来获取这些功能。\n值得注意的是，即使健康检查不是通过 Ingress 直接暴露的，但是在 Kubernetes 中存在并行概念，比如 就绪检查，它允许您实现相同的最终结果。 请检查控制器特殊说明文档，以了解他们是怎样处理健康检查的 ( nginx， GCE)。\n更新 Ingress 要更新现有的 Ingress 以添加新的 Host，可以通过编辑资源来对其进行更新：\nkubectl describe ingress test Name: test Namespace: default Address: 178.91.123.132 Default backend: default-http-backend:80 (10.8.2.3:8080) Rules: Host Path Backends ---- ---- -------- foo.bar.com /foo service1:80 (10.8.0.90:80) Annotations: nginx.ingress.kubernetes.io/rewrite-target: / Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal ADD 35s loadbalancer-controller default/test kubectl edit ingress test 这将弹出具有 YAML 格式的现有配置的编辑器。 修改它来增加新的主机：\nspec: rules: - host: foo.bar.com http: paths: - backend: serviceName: service1 servicePort: 80 path: /foo - host: bar.baz.com http: paths: - backend: serviceName: service2 servicePort: 80 path: /foo .. 保存更改后，kubectl 将更新 API 服务器中的资源，该资源将告诉 Ingress 控制器重新配置负载均衡器。\n验证：\nkubectl describe ingress test Name: test Namespace: default Address: 178.91.123.132 Default backend: default-http-backend:80 (10.8.2.3:8080) Rules: Host Path Backends ---- ---- -------- foo.bar.com /foo service1:80 (10.8.0.90:80) bar.baz.com /foo service2:80 (10.8.0.91:80) Annotations: nginx.ingress.kubernetes.io/rewrite-target: / Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal ADD 45s loadbalancer-controller default/test 您可以通过 kubectl replace -f 命令调用修改后的 Ingress yaml 文件来获得同样的结果。\n跨可用区失败 用于跨故障域传播流量的技术在云提供商之间是不同的。详情请查阅相关 Ingress 控制器的文档。 请查看相关Ingress 控制器 的文档以了解详细信息。 您还可以参考联邦文档，以获取有关在联合集群中部署 Ingress 的详细信息。\n未来工作 跟踪 SIG 网络以获得有关 Ingress 和相关资源演变的更多细节。您还可以跟踪 Ingress 仓库以获取有关各种 Ingress 控制器的更多细节。\n替代方案 不直接使用 Ingress 资源，也有多种方法暴露 Service：\n 使用 Service.Type=LoadBalancer 使用 Service.Type=NodePort  whatsnext  了解更多 [Ingress API](/docs/reference/generated/kubernetes-api/\u0026lt; param \u0026ldquo;version\u0026rdquo; \u0026gt;}}/#ingress-v1beta1-networking-k8s-io) 了解更多 Ingress 控制器 使用 NGINX 控制器在 Minikube 上安装 Ingress  "
},
{
	"uri": "https://lijun.in/concepts/services-networking/ingress-controllers/",
	"title": "Ingress 控制器",
	"tags": [],
	"description": "",
	"content": "为了让 Ingress 资源工作，集群必须有一个正在运行的 Ingress 控制器。\n与作为 kube-controller-manager 可执行文件的一部分运行的其他类型的控制器不同，Ingress 控制器不是随集群自动启动的。 基于此页面，您可选择最适合您的集群的 ingress 控制器实现。\nKubernetes 作为一个项目，目前支持和维护 GCE 和 nginx 控制器。\n其他控制器  [AKS 应用程序网关 Ingress 控制器]使用 Azure 应用程序网关启用AKS 集群 ingress。 Ambassador API 网关， 一个基于 Envoy 的 ingress 控制器，有着来自社区 的支持和来自 Datawire 的商业 支持。 AppsCode Inc. 为最广泛使用的基于 HAProxy 的 ingress 控制器 Voyager 提供支持和维护。 AWS ALB Ingress 控制器通过 AWS 应用 Load Balancer 启用 ingress。 Contour 是一个基于 Envoy 的 ingress 控制器，它由 VMware 提供和支持。 Citrix 为其硬件（MPX），虚拟化（VPX）和 免费容器化 (CPX) ADC 提供了一个 Ingress 控制器，用于裸金属和云部署。 F5 Networks 为 用于 Kubernetes 的 F5 BIG-IP 控制器提供支持和维护。 Gloo 是一个开源的基于 Envoy 的 ingress 控制器，它提供了 API 网关功能，有着来自 solo.io 的企业级支持。 HAProxy Ingress 是 HAProxy 高度可定制的、由社区驱动的 Ingress 控制器。 HAProxy Technologies 为用于 Kubernetes 的 HAProxy Ingress 控制器 提供支持和维护。具体信息请参考官方文档。 基于 Istio 的 ingress 控制器控制 Ingress 流量。 Kong 为用于 Kubernetes 的 Kong Ingress 控制器 提供社区或商业支持和维护。 NGINX, Inc. 为用于 Kubernetes 的 NGINX Ingress 控制器提供支持和维护。 Skipper HTTP 路由器和反向代理，用于服务组合，包括诸如 Kubernetes Ingress 之类的用例，被设计为用于构建自定义代理的库。 Traefik 是一个全功能的 ingress 控制器 （Let\u0026rsquo;s Encrypt，secrets，http2，websocket），并且它也有来自 Containous 的商业支持。  使用多个 Ingress 控制器 你可以在集群中部署任意数量的 ingress 控制器。 创建 ingress 时，应该使用适当的 ingress.class 注解每个 ingress 以表明在集群中如果有多个 ingress 控制器时，应该使用哪个 ingress 控制器。\n如果不定义 ingress.class，云提供商可能使用默认的 ingress 控制器。\n理想情况下，所有 ingress 控制器都应满足此规范，但各种 ingress 控制器的操作略有不同。\n确保您查看了 ingress 控制器的文档，以了解选择它的注意事项。\nwhatsnext  进一步了解 Ingress。 在 Minikube 上使用 NGINX 控制器安装 Ingress。  "
},
{
	"uri": "https://lijun.in/reference/setup-tools/kubeadm/kubeadm-upgrade/",
	"title": "kubeadm upgrade",
	"tags": [],
	"description": "",
	"content": "kubeadm upgrade 是一个对用户友好的命令，它将复杂的升级逻辑包装在一个命令后面，支持升级的规划和实际执行。\nkubeadm 升级指南 本文档概述了使用 kubeadm 执行升级的步骤。 有关 kubeadm 旧版本，请参阅 Kubernetes 网站的旧版文档。\n您可以使用 kubeadm upgrade diff 来查看将应用于静态 pod 清单的更改。\n要在 Kubernetes v1.13.0 及更高版本中使用 kube-dns 进行升级，请遵循本指南。\n在 Kubernetes v1.15.0 和更高版本中，kubeadm upgrade apply 和 kubeadm upgrade node 也将自动续订该节点上的 kubeadm 托管证书，包括存储在 kubeconfig 文件中的证书。 要选择退出，可以传递参数 --certificate-renewal=false。有关证书续订的更多详细信息请参见证书管理文档。\nkubeadm upgrade plan . include \u0026ldquo;generated/kubeadm_upgrade_plan.md\u0026rdquo; \u0026gt;}}\nkubeadm upgrade apply  . include \u0026ldquo;generated/kubeadm_upgrade_apply.md\u0026rdquo; \u0026gt;}}\nkubeadm upgrade diff . include \u0026ldquo;generated/kubeadm_upgrade_diff.md\u0026rdquo; \u0026gt;}}\nkubeadm upgrade node . include \u0026ldquo;generated/kubeadm_upgrade_node.md\u0026rdquo; \u0026gt;}}\n. heading \u0026ldquo;whatsnext\u0026rdquo; %}}  如果您使用 kubeadm v1.7.x 或更低版本初始化集群，则可以参考kubeadm 配置配置集群用于 kubeadm upgrade。  "
},
{
	"uri": "https://lijun.in/setup/best-practices/certificates/",
	"title": "PKI 证书和要求",
	"tags": [],
	"description": "",
	"content": "Kubernetes 需要 PKI 证书才能进行基于 TLS 的身份验证。如果您是使用 kubeadm 安装的 Kubernetes，则会自动生成集群所需的证书。您还可以生成自己的证书。例如，不将私钥存储在 API 服务器上，可以让私钥更加安全。此页面说明了集群必需的证书。\n集群是如何使用证书的 Kubernetes 需要 PKI 才能执行以下操作：\n Kubelet 的客户端证书，用于 API 服务器身份验证 API 服务器端点的证书 集群管理员的客户端证书，用于 API 服务器身份认证 API 服务器的客户端证书，用于和 Kubelet 的会话 API 服务器的客户端证书，用于和 etcd 的会话 控制器管理器的客户端证书/kubeconfig，用于和 API server 的会话 调度器的客户端证书/kubeconfig，用于和 API server 的会话 前端代理 的客户端及服务端证书  . note \u0026gt;}}\n只有当您运行 kube-proxy 并要支持扩展 API 服务器时，才需要 front-proxy 证书 . /note \u0026gt;}}\netcd 还实现了双向 TLS 来对客户端和对其他对等节点进行身份验证。\n证书存放的位置 如果你是通过 kubeadm 安装的 Kubernetes，所有证书都存放在 /etc/kubernetes/pki 目录下。本文所有相关的路径都是基于该路径的相对路径。\n手动配置证书 如果你不想通过 kubeadm 生成这些必需的证书，你可以通过下面两种方式之一来手动创建他们。\n单根 CA 你可以创建一个单根 CA，由管理员控制器它。该根 CA 可以创建多个中间 CA，并将所有进一步的创建委托给 Kubernetes。\n需要这些 CA：\n   路径 默认 CN 描述     ca.crt,key kubernetes-ca Kubernetes 通用 CA   etcd/ca.crt,key etcd-ca 与 etcd 相关的所有功能   front-proxy-ca.crt,key kubernetes-front-proxy-ca 用于 前端代理    上面的 CA 之外，还需要获取用于服务账户管理的密钥对，也就是 sa.key 和 sa.pub。\n所有的证书 如果你不想将 CA 的私钥拷贝至你的集群中，你也可以自己生成全部的证书。\n需要这些证书：\n   默认 CN 父级 CA O (位于 Subject 中) 类型 主机 (SAN)     kube-etcd etcd-ca  server, client localhost, 127.0.0.1   kube-etcd-peer etcd-ca  server, client \u0026lt;hostname\u0026gt;, \u0026lt;Host_IP\u0026gt;, localhost, 127.0.0.1   kube-etcd-healthcheck-client etcd-ca  client    kube-apiserver-etcd-client etcd-ca system:masters client    kube-apiserver kubernetes-ca  server \u0026lt;hostname\u0026gt;, \u0026lt;Host_IP\u0026gt;, \u0026lt;advertise_IP\u0026gt;, [1]   kube-apiserver-kubelet-client kubernetes-ca system:masters client    front-proxy-client kubernetes-front-proxy-ca  client     [1]: 用来连接到集群的不同 IP 或 DNS 名（就像 kubeadm 为负载均衡所使用的固定 IP 或 DNS 名，kubernetes、kubernetes.default、kubernetes.default.svc、kubernetes.default.svc.cluster、kubernetes.default.svc.cluster.local）\n其中，kind 对应一种或多种类型的 x509 密钥用途：\n   kind 密钥用途     server 数字签名、密钥加密、服务端认证   client 数字签名、密钥加密、客户端认证    . note \u0026gt;}}\n上面列出的 Hosts/SAN 是推荐的配置方式；如果需要特殊安装，则可以在所有服务器证书上添加其他 SAN。 . /note \u0026gt;}}\n. note \u0026gt;}}\n对于 kubeadm 用户：\n 不使用私钥，将证书复制到集群 CA 的方案，在 kubeadm 文档中将这种方案称为外部 CA。 如果将以上列表与 kubeadm 生成的 PKI 进行比较，你会注意到，如果使用外部 etcd，则不会生成 kube-etcd、kube-etcd-peer 和 kube-etcd-healthcheck-client 证书。  . /note \u0026gt;}}\n证书路径 证书应放置在建议的路径中（以便 kubeadm使用）。无论使用什么位置，都应使用给定的参数指定路径。\n   默认 CN 建议的密钥路径 建议的证书路径 命令 密钥参数 证书参数     etcd-ca etcd/ca.key etcd/ca.crt kube-apiserver  \u0026ndash;etcd-cafile   kube-apiserver-etcd-client apiserver-etcd-client.key apiserver-etcd-client.crt kube-apiserver \u0026ndash;etcd-keyfile \u0026ndash;etcd-certfile   kubernetes-ca ca.key ca.crt kube-apiserver  \u0026ndash;client-ca-file   kubernetes-ca ca.key ca.crt kube-controller-manager \u0026ndash;cluster-signing-key-file \u0026ndash;client-ca-file, \u0026ndash;root-ca-file, \u0026ndash;cluster-signing-cert-file   kube-apiserver apiserver.key apiserver.crt kube-apiserver \u0026ndash;tls-private-key-file \u0026ndash;tls-cert-file   kube-apiserver-kubelet-client apiserver-kubelet-client.key apiserver-kubelet-client.crt kube-apiserver \u0026ndash;kubelet-client-key \u0026ndash;kubelet-client-certificate   front-proxy-ca front-proxy-ca.key front-proxy-ca.crt kube-apiserver  \u0026ndash;requestheader-client-ca-file   front-proxy-ca front-proxy-ca.key front-proxy-ca.crt kube-controller-manager  \u0026ndash;requestheader-client-ca-file   front-proxy-client front-proxy-client.key front-proxy-client.crt kube-apiserver \u0026ndash;proxy-client-key-file \u0026ndash;proxy-client-cert-file   etcd-ca etcd/ca.key etcd/ca.crt etcd  \u0026ndash;trusted-ca-file, \u0026ndash;peer-trusted-ca-file   kube-etcd etcd/server.key etcd/server.crt etcd \u0026ndash;key-file \u0026ndash;cert-file   kube-etcd-peer etcd/peer.key etcd/peer.crt etcd \u0026ndash;peer-key-file \u0026ndash;peer-cert-file   etcd-ca  etcd/ca.crt etcdctl  \u0026ndash;cacert   kube-etcd-healthcheck-client etcd/healthcheck-client.key etcd/healthcheck-client.crt etcdctl \u0026ndash;key \u0026ndash;cert    注意事项同样适用于服务帐户密钥对：\n   私钥路径 公钥路径 命令 参数     sa.key  kube-controller-manager service-account-private    sa.pub kube-apiserver service-account-key    为用户帐户配置证书 您必须手动配置以下管理员帐户和服务帐户：\n   文件名 凭据名称 默认 CN O (位于 Subject 中)     admin.conf default-admin kubernetes-admin system:masters   kubelet.conf default-auth system:node:\u0026lt;nodeName\u0026gt; (see note) system:nodes   controller-manager.conf default-controller-manager system:kube-controller-manager    scheduler.conf default-scheduler system:kube-scheduler     . note \u0026gt;}}\nkubelet.conf 中 \u0026lt;nodeName\u0026gt; 的值 必须 与 kubelet 向 apiserver 注册时提供的节点名称的值完全匹配。有关更多详细信息，请阅读节点授权。 . /note \u0026gt;}}\n  对于每个配置，请都使用给定的 CN 和 O 生成 x509 证书/密钥偶对。\n  为每个配置运行下面的 kubectl 命令：\n  KUBECONFIG=\u0026lt;filename\u0026gt; kubectl config set-cluster default-cluster --server=https://\u0026lt;host ip\u0026gt;:6443 --certificate-authority \u0026lt;path-to-kubernetes-ca\u0026gt; --embed-certs KUBECONFIG=\u0026lt;filename\u0026gt; kubectl config set-credentials \u0026lt;credential-name\u0026gt; --client-key \u0026lt;path-to-key\u0026gt;.pem --client-certificate \u0026lt;path-to-cert\u0026gt;.pem --embed-certs KUBECONFIG=\u0026lt;filename\u0026gt; kubectl config set-context default-system --cluster default-cluster --user \u0026lt;credential-name\u0026gt; KUBECONFIG=\u0026lt;filename\u0026gt; kubectl config use-context default-system 这些文件用途如下：\n   文件名 命令 说明     admin.conf kubectl 配置集群的管理员   kubelet.conf kubelet 集群中的每个节点都需要一份   controller-manager.conf kube-controller-manager 必需添加到 manifests/kube-controller-manager.yaml 清单中   scheduler.conf kube-scheduler 必需添加到 manifests/kube-scheduler.yaml 清单中    "
},
{
	"uri": "https://lijun.in/concepts/workloads/controllers/statefulset/",
	"title": "StatefulSets",
	"tags": [],
	"description": "",
	"content": "StatefulSet 是用来管理有状态应用的工作负载 API 对象。\nglossary_definition term_id=\u0026quot;statefulset\u0026rdquo; length=\u0026quot;all\u0026rdquo; \u0026gt;}}\n使用 StatefulSets StatefulSets 对于需要满足以下一个或多个需求的应用程序很有价值：\n 稳定的、唯一的网络标识符。 稳定的、持久的存储。 有序的、优雅的部署和缩放。 有序的、自动的滚动更新。  在上面，稳定意味着 Pod 调度或重调度的整个过程是有持久性的。如果应用程序不需要任何稳定的标识符或有序的部署、删除或伸缩，则应该使用由一组无状态的副本控制器提供的工作负载来部署应用程序，比如 Deployment 或者 ReplicaSet 可能更适用于您的无状态应用部署需要。\n限制  给定 Pod 的存储必须由 [PersistentVolume 驱动](https://github.com/kubernetes/examples/tree/\u0026lt; param \u0026ldquo;githubbranch\u0026rdquo; \u0026gt;}}/staging/persistent-volume-provisioning/README.md) 基于所请求的 storage class 来提供，或者由管理员预先提供。 删除或者收缩 StatefulSet 并不会删除它关联的存储卷。这样做是为了保证数据安全，它通常比自动清除 StatefulSet 所有相关的资源更有价值。 StatefulSet 当前需要 headless 服务 来负责 Pod 的网络标识。您需要负责创建此服务。 当删除 StatefulSets 时，StatefulSet 不提供任何终止 Pod 的保证。为了实现 StatefulSet 中的 Pod 可以有序和优雅的终止，可以在删除之前将 StatefulSet 缩放为 0。 在默认 Pod 管理策略(OrderedReady) 时使用 滚动更新，可能进入需要 人工干预 才能修复的损坏状态。  组件 下面的示例演示了 StatefulSet 的组件。\napiVersion: v1 kind: Service metadata: name: nginx labels: app: nginx spec: ports: - port: 80 name: web clusterIP: None selector: app: nginx --- apiVersion: apps/v1 kind: StatefulSet metadata: name: web spec: selector: matchLabels: app: nginx # has to match .spec.template.metadata.labels serviceName: \u0026#34;nginx\u0026#34; replicas: 3 # by default is 1 template: metadata: labels: app: nginx # has to match .spec.selector.matchLabels spec: terminationGracePeriodSeconds: 10 containers: - name: nginx image: k8s.gcr.io/nginx-slim:0.8 ports: - containerPort: 80 name: web volumeMounts: - name: www mountPath: /usr/share/nginx/html volumeClaimTemplates: - metadata: name: www spec: accessModes: [ \u0026#34;ReadWriteOnce\u0026#34; ] storageClassName: \u0026#34;my-storage-class\u0026#34; resources: requests: storage: 1Gi  名为 nginx 的 Headless Service 用来控制网络域名。 名为 web 的 StatefulSet 有一个 Spec，它表明将在独立的 3 个 Pod 副本中启动 nginx 容器。 volumeClaimTemplates 将通过 PersistentVolumes 驱动提供的 PersistentVolumes 来提供稳定的存储。  Pod 选择器 您必须设置 StatefulSet 的 .spec.selector 字段，使之匹配其在 .spec.template.metadata.labels 中设置的标签。在 Kubernetes 1.8 版本之前，被忽略 .spec.selector 字段会获得默认设置值。在 1.8 和以后的版本中，未指定匹配的 Pod 选择器将在创建 StatefulSet 期间导致验证错误。\nPod 标识 StatefulSet Pod 具有唯一的标识，该标识包括顺序标识、稳定的网络标识和稳定的存储。该标识和 Pod 是绑定的，不管它被调度在哪个节点上。\n有序索引 对于具有 N 个副本的 StatefulSet，StatefulSet 中的每个 Pod 将被分配一个整数序号，从 0 到 N-1，该序号在 StatefulSet 上是唯一的。\n稳定的网络 ID StatefulSet 中的每个 Pod 根据 StatefulSet 的名称和 Pod 的序号派生出它的主机名。组合主机名的格式为$(StatefulSet 名称)-$(序号)。上例将会创建三个名称分别为 web-0、web-1、web-2 的 Pod。 StatefulSet 可以使用 headless 服务 控制它的 Pod 的网络域。管理域的这个服务的格式为： $(服务名称).$(命名空间).svc.cluster.local，其中 cluster.local 是集群域。 一旦每个 Pod 创建成功，就会得到一个匹配的 DNS 子域，格式为：$(pod 名称).$(所属服务的 DNS 域名)，其中所属服务由 StatefulSet 的 serviceName 域来设定。\n下面给出一些选择集群域、服务名、StatefulSet 名、及其怎样影响 StatefulSet 的 Pod 上的 DNS 名称的示例：\n   Cluster Domain Service (ns/name) StatefulSet (ns/name) StatefulSet Domain Pod DNS Pod Hostname     cluster.local default/nginx default/web nginx.default.svc.cluster.local web-{0..N-1}.nginx.default.svc.cluster.local web-{0..N-1}   cluster.local foo/nginx foo/web nginx.foo.svc.cluster.local web-{0..N-1}.nginx.foo.svc.cluster.local web-{0..N-1}   kube.local foo/nginx foo/web nginx.foo.svc.kube.local web-{0..N-1}.nginx.foo.svc.kube.local web-{0..N-1}    集群域会被设置为 cluster.local，除非有其他配置。\n稳定的存储 Kubernetes 为每个 VolumeClaimTemplate 创建一个 PersistentVolume。在上面的 nginx 示例中，每个 Pod 将会得到基于 StorageClass my-storage-class 提供的 1 Gib 的 PersistentVolume。如果没有声明 StorageClass，就会使用默认的 StorageClass。当一个 Pod 被调度（重新调度）到节点上时，它的 volumeMounts 会挂载与其 PersistentVolumeClaims 相关联的 PersistentVolume。请注意，当 Pod 或者 StatefulSet 被删除时，与 PersistentVolumeClaims 相关联的 PersistentVolume 并不会被删除。要删除它必须通过手动方式来完成。\nPod 名称标签 当 StatefulSet glossary_tooltip term_id=\u0026quot;controller\u0026rdquo; \u0026gt;}} 创建 Pod 时，它会添加一个标签 statefulset.kubernetes.io/pod-name，该标签设置为 Pod 名称。这个标签允许您给 StatefulSet 中的特定 Pod 绑定一个 Service。\n部署和扩缩保证  对于包含 N 个 副本的 StatefulSet，当部署 Pod 时，它们是依次创建的，顺序为 0..N-1。 当删除 Pod 时，它们是逆序终止的，顺序为 N-1..0。 在将缩放操作应用到 Pod 之前，它前面的所有 Pod 必须是 Running 和 Ready 状态。 在 Pod 终止之前，所有的继任者必须完全关闭。  StatefulSet 不应将 pod.Spec.TerminationGracePeriodSeconds 设置为 0。这种做法是不安全的，要强烈阻止。更多的解释请参考 强制删除 StatefulSet Pod。\n在上面的 nginx 示例被创建后，会按照 web-0、web-1、web-2 的顺序部署三个 Pod。在 web-0 进入 Running 和 Ready 状态前不会部署 web-1。在 web-1 进入 Running 和 Ready 状态前不会部署 web-2。如果 web-1 已经处于 Running 和 Ready 状态，而 web-2 尚未部署，在此期间发生了 web-0 运行失败，那么 web-2 将不会被部署，要等到 web-0 部署完成并进入 Running 和 Ready 状态后，才会部署 web-2。\n如果用户想将示例中的 StatefulSet 收缩为 replicas=1，首先被终止的是 web-2。在 web-2 没有被完全停止和删除前，web-1 不会被终止。当 web-2 已被终止和删除、web-1 尚未被终止，如果在此期间发生 web-0 运行失败，那么就不会终止 web-1，必须等到 web-0 进入 Running 和 Ready 状态后才会终止 web-1。\nPod 管理策略 在 Kubernetes 1.7 及以后的版本中，StatefulSet 允许您不要求其排序保证，同时通过它的 .spec.podManagementPolicy 域保持其唯一性和身份保证。 在 Kubernetes 1.7 及以后的版本中，StatefulSet 允许您放宽其排序保证，同时通过它的 .spec.podManagementPolicy 域保持其唯一性和身份保证。\nOrderedReady Pod 管理 OrderedReady Pod 管理是 StatefulSet 的默认设置。它实现了上面描述的功能。\nParallel Pod 管理 Parallel Pod 管理让 StatefulSet 控制器并行的启动或终止所有的 Pod，启动或者终止其他 Pod 前，无需等待 Pod 进入 Running 和 ready 或者完全停止状态。\n更新策略 在 Kubernetes 1.7 及以后的版本中，StatefulSet 的 .spec.updateStrategy 字段让您可以配置和禁用掉自动滚动更新 Pod 的容器、标签、资源请求或限制、以及注解。\n关于删除策略 OnDelete 更新策略实现了 1.6 及以前版本的历史遗留行为。当 StatefulSet 的 .spec.updateStrategy.type 设置为 OnDelete 时，它的控制器将不会自动更新 StatefulSet 中的 Pod。用户必须手动删除 Pod 以便让控制器创建新的 Pod，以此来对 StatefulSet 的 .spec.template 的变动作出反应。\n滚动更新 RollingUpdate 更新策略对 StatefulSet 中的 Pod 执行自动的滚动更新。在没有声明 .spec.updateStrategy 时，RollingUpdate 是默认配置。 当 StatefulSet 的 .spec.updateStrategy.type 被设置为 RollingUpdate 时，StatefulSet 控制器会删除和重建 StatefulSet 中的每个 Pod。 它将按照与 Pod 终止相同的顺序（从最大序号到最小序号）进行，每次更新一个 Pod。它会等到被更新的 Pod 进入 Running 和 Ready 状态，然后再更新其前身。\n分区 通过声明 .spec.updateStrategy.rollingUpdate.partition 的方式，RollingUpdate 更新策略可以实现分区。如果声明了一个分区，当 StatefulSet 的 .spec.template 被更新时，所有序号大于等于该分区序号的 Pod 都会被更新。所有序号小于该分区序号的 Pod 都不会被更新，并且，即使他们被删除也会依据之前的版本进行重建。如果 StatefulSet 的 .spec.updateStrategy.rollingUpdate.partition 大于它的 .spec.replicas，对它的 .spec.template 的更新将不会传递到它的 Pod。 在大多数情况下，您不需要使用分区，但如果您希望进行阶段更新、执行金丝雀或执行分阶段展开，则这些分区会非常有用。\n强制回滚 在默认 Pod 管理策略(OrderedReady) 时使用 滚动更新 ，可能进入需要人工干预才能修复的损坏状态。\n如果更新后 Pod 模板配置进入无法运行或就绪的状态（例如，由于错误的二进制文件或应用程序级配置错误），StatefulSet 将停止回滚并等待。\n在这种状态下，仅将 Pod 模板还原为正确的配置是不够的。由于已知问题，StatefulSet 将继续等待损坏状态的 Pod 准备就绪（永远不会发生），然后再尝试将其恢复为正常工作配置。\n恢复模板后，还必须删除 StatefulSet 尝试使用错误的配置来运行的 Pod。这样，StatefulSet 才会开始使用被还原的模板来重新创建 Pod。\nheading \u0026ldquo;whatsnext  示例一：部署有状态应用。 示例二：使用 StatefulSet 部署 Cassandra。 示例三：运行多副本的有状态应用程序。  "
},
{
	"uri": "https://lijun.in/concepts/configuration/taint-and-toleration/",
	"title": "Taint 和 Toleration",
	"tags": [],
	"description": "",
	"content": "节点亲和性（详见这里），是 pod 的一种属性（偏好或硬性要求），它使 pod 被吸引到一类特定的节点。Taint 则相反，它使 节点 能够 排斥 一类特定的 pod。\nTaint 和 toleration 相互配合，可以用来避免 pod 被分配到不合适的节点上。每个节点上都可以应用一个或多个 taint ，这表示对于那些不能容忍这些 taint 的 pod，是不会被该节点接受的。如果将 toleration 应用于 pod 上，则表示这些 pod 可以（但不要求）被调度到具有匹配 taint 的节点上。\n概念 您可以使用命令 kubectl taint 给节点增加一个 taint。比如，\nkubectl taint nodes node1 key=value:NoSchedule 给节点 node1 增加一个 taint，它的 key 是 key，value 是 value，effect 是 NoSchedule。这表示只有拥有和这个 taint 相匹配的 toleration 的 pod 才能够被分配到 node1 这个节点。您可以在 PodSpec 中定义 pod 的 toleration。下面两个 toleration 均与上面例子中使用 kubectl taint 命令创建的 taint 相匹配，因此如果一个 pod 拥有其中的任何一个 toleration 都能够被分配到 node1 ：\n想删除上述命令添加的 taint ，您可以运行：\nkubectl taint nodes node1 key:NoSchedule- 您可以在 PodSpec 中为容器设定容忍标签。以下两个容忍标签都与上面的 kubectl taint 创建的污点“匹配”， 因此具有任一容忍标签的Pod都可以将其调度到 node1 上：\ntolerations: - key: \u0026#34;key\u0026#34; operator: \u0026#34;Equal\u0026#34; value: \u0026#34;value\u0026#34; effect: \u0026#34;NoSchedule\u0026#34; tolerations: - key: \u0026#34;key\u0026#34; operator: \u0026#34;Exists\u0026#34; effect: \u0026#34;NoSchedule\u0026#34; 一个 toleration 和一个 taint 相“匹配”是指它们有一样的 key 和 effect ，并且：\n 如果 operator 是 Exists （此时 toleration 不能指定 value），或者 如果 operator 是 Equal ，则它们的 value 应该相等  存在两种特殊情况：\n 如果一个 toleration 的 key 为空且 operator 为 Exists，表示这个 toleration 与任意的 key 、value 和 effect 都匹配，即这个 toleration 能容忍任意 taint。  tolerations: - operator: \u0026#34;Exists\u0026#34;  如果一个 toleration 的 effect 为空，则 key 值与之相同的相匹配 taint 的 effect 可以是任意值。  tolerations: - key: \u0026#34;key\u0026#34; operator: \u0026#34;Exists\u0026#34; 上述例子使用到的 effect 的一个值 NoSchedule，您也可以使用另外一个值 PreferNoSchedule。这是“优化”或“软”版本的 NoSchedule ——系统会 尽量 避免将 pod 调度到存在其不能容忍 taint 的节点上，但这不是强制的。effect 的值还可以设置为 NoExecute，下文会详细描述这个值。\n您可以给一个节点添加多个 taint ，也可以给一个 pod 添加多个 toleration。Kubernetes 处理多个 taint 和 toleration 的过程就像一个过滤器：从一个节点的所有 taint 开始遍历，过滤掉那些 pod 中存在与之相匹配的 toleration 的 taint。余下未被过滤的 taint 的 effect 值决定了 pod 是否会被分配到该节点，特别是以下情况：\n 如果未被过滤的 taint 中存在一个以上 effect 值为 NoSchedule 的 taint，则 Kubernetes 不会将 pod 分配到该节点。 如果未被过滤的 taint 中不存在 effect 值为 NoSchedule 的 taint，但是存在 effect 值为 PreferNoSchedule 的 taint，则 Kubernetes 会 尝试 将 pod 分配到该节点。 如果未被过滤的 taint 中存在一个以上 effect 值为 NoExecute 的 taint，则 Kubernetes 不会将 pod 分配到该节点（如果 pod 还未在节点上运行），或者将 pod 从该节点驱逐（如果 pod 已经在节点上运行）。  例如，假设您给一个节点添加了如下的 taint\nkubectl taint nodes node1 key1=value1:NoSchedule kubectl taint nodes node1 key1=value1:NoExecute kubectl taint nodes node1 key2=value2:NoSchedule 然后存在一个 pod，它有两个 toleration：\ntolerations: - key: \u0026#34;key1\u0026#34; operator: \u0026#34;Equal\u0026#34; value: \u0026#34;value1\u0026#34; effect: \u0026#34;NoSchedule\u0026#34; - key: \u0026#34;key1\u0026#34; operator: \u0026#34;Equal\u0026#34; value: \u0026#34;value1\u0026#34; effect: \u0026#34;NoExecute\u0026#34; 在这个例子中，上述 pod 不会被分配到上述节点，因为其没有 toleration 和第三个 taint 相匹配。但是如果在给节点添加上述 taint 之前，该 pod 已经在上述节点运行，那么它还可以继续运行在该节点上，因为第三个 taint 是三个 taint 中唯一不能被这个 pod 容忍的。\n通常情况下，如果给一个节点添加了一个 effect 值为 NoExecute 的 taint，则任何不能忍受这个 taint 的 pod 都会马上被驱逐，任何可以忍受这个 taint 的 pod 都不会被驱逐。但是，如果 pod 存在一个 effect 值为 NoExecute 的 toleration 指定了可选属性 tolerationSeconds 的值，则表示在给节点添加了上述 taint 之后，pod 还能继续在节点上运行的时间。例如，\ntolerations: - key: \u0026#34;key1\u0026#34; operator: \u0026#34;Equal\u0026#34; value: \u0026#34;value1\u0026#34; effect: \u0026#34;NoExecute\u0026#34; tolerationSeconds: 3600 这表示如果这个 pod 正在运行，然后一个匹配的 taint 被添加到其所在的节点，那么 pod 还将继续在节点上运行 3600 秒，然后被驱逐。如果在此之前上述 taint 被删除了，则 pod 不会被驱逐。\n使用例子 通过 taint 和 toleration，可以灵活地让 pod 避开 某些节点或者将 pod 从某些节点驱逐。下面是几个使用例子：\n 专用节点：如果您想将某些节点专门分配给特定的一组用户使用，您可以给这些节点添加一个 taint（即， kubectl taint nodes nodename dedicated=groupName:NoSchedule），然后给这组用户的 pod 添加一个相对应的 toleration（通过编写一个自定义的 admission controller，很容易就能做到）。拥有上述 toleration 的 pod 就能够被分配到上述专用节点，同时也能够被分配到集群中的其它节点。如果您希望这些 pod 只能被分配到上述专用节点，那么您还需要给这些专用节点另外添加一个和上述 taint 类似的 label （例如：dedicated=groupName），同时 还要在上述 admission controller 中给 pod 增加节点亲和性要求上述 pod 只能被分配到添加了 dedicated=groupName 标签的节点上。   配备了特殊硬件的节点：在部分节点配备了特殊硬件（比如 GPU）的集群中，我们希望不需要这类硬件的 pod 不要被分配到这些特殊节点，以便为后继需要这类硬件的 pod 保留资源。要达到这个目的，可以先给配备了特殊硬件的节点添加 taint（例如 kubectl taint nodes nodename special=true:NoSchedule or kubectl taint nodes nodename special=true:PreferNoSchedule)，然后给使用了这类特殊硬件的 pod 添加一个相匹配的 toleration。和专用节点的例子类似，添加这个 toleration 的最简单的方法是使用自定义 admission controller。比如，我们推荐使用 Extended Resources 来表示特殊硬件，给配置了特殊硬件的节点添加 taint 时包含 extended resource 名称，然后运行一个 ExtendedResourceToleration admission controller。此时，因为节点已经被 taint 了，没有对应 toleration 的 Pod 会被调度到这些节点。但当你创建一个使用了 extended resource 的 Pod 时，ExtendedResourceToleration admission controller 会自动给 Pod 加上正确的 toleration ，这样 Pod 就会被自动调度到这些配置了特殊硬件件的节点上。这样就能够确保这些配置了特殊硬件的节点专门用于运行 需要使用这些硬件的 Pod，并且您无需手动给这些 Pod 添加 toleration。   基于 taint 的驱逐: 这是在每个 pod 中配置的在节点出现问题时的驱逐行为，接下来的章节会描述这个特性  基于 taint 的驱逐 for_k8s_version=\u0026quot;v1.18\u0026rdquo; state=\u0026quot;stable\u0026rdquo; \u0026gt;}}\n前文我们提到过 taint 的 effect 值 `NoExecute` ，它会影响已经在节点上运行的 pod   如果 pod 不能忍受 effect 值为 NoExecute 的 taint，那么 pod 将马上被驱逐 如果 pod 能够忍受 effect 值为 NoExecute 的 taint，但是在 toleration 定义中没有指定 tolerationSeconds，则 pod 还会一直在这个节点上运行。 如果 pod 能够忍受 effect 值为 NoExecute 的 taint，而且指定了 tolerationSeconds，则 pod 还能在这个节点上继续运行这个指定的时间长度。  此外，Kubernetes 1.6 已经支持（alpha阶段）节点问题的表示。换句话说，当某种条件为真时，node controller会自动给节点添加一个 taint。当前内置的 taint 包括：   node.kubernetes.io/not-ready：节点未准备好。这相当于节点状态 Ready 的值为 \u0026ldquo;False\u0026rdquo;。 node.kubernetes.io/unreachable：node controller 访问不到节点. 这相当于节点状态 Ready 的值为 \u0026ldquo;Unknown\u0026rdquo;。 node.kubernetes.io/out-of-disk：节点磁盘耗尽。 node.kubernetes.io/memory-pressure：节点存在内存压力。 node.kubernetes.io/disk-pressure：节点存在磁盘压力。 node.kubernetes.io/network-unavailable：节点网络不可用。 node.kubernetes.io/unschedulable: 节点不可调度。 node.cloudprovider.kubernetes.io/uninitialized：如果 kubelet 启动时指定了一个 \u0026ldquo;外部\u0026rdquo; cloud provider，它将给当前节点添加一个 taint 将其标志为不可用。在 cloud-controller-manager 的一个 controller 初始化这个节点后，kubelet 将删除这个 taint。  在节点被驱逐时，节点控制器或者 kubelet 会添加带有 NoExecute 效应的相关污点。如果异常状态恢复正常，kubelet 或节点控制器能够移除相关的污点。\n为了保证由于节点问题引起的 pod 驱逐rate limiting行为正常，系统实际上会以 rate-limited 的方式添加 taint。在像 master 和 node 通讯中断等场景下，这避免了 pod 被大量驱逐。\n使用这个功能特性，结合 tolerationSeconds，pod 就可以指定当节点出现一个或全部上述问题时还将在这个节点上运行多长的时间。\n比如，一个使用了很多本地状态的应用程序在网络断开时，仍然希望停留在当前节点上运行一段较长的时间，愿意等待网络恢复以避免被驱逐。在这种情况下，pod 的 toleration 可能是下面这样的：\ntolerations: - key: \u0026#34;node.kubernetes.io/unreachable\u0026#34; operator: \u0026#34;Exists\u0026#34; effect: \u0026#34;NoExecute\u0026#34; tolerationSeconds: 6000 注意，Kubernetes 会自动给 pod 添加一个 key 为 node.kubernetes.io/not-ready 的 toleration 并配置 tolerationSeconds=300，除非用户提供的 pod 配置中已经已存在了 key 为 node.kubernetes.io/not-ready 的 toleration。同样，Kubernetes 会给 pod 添加一个 key 为 node.kubernetes.io/unreachable 的 toleration 并配置 tolerationSeconds=300，除非用户提供的 pod 配置中已经已存在了 key 为 node.kubernetes.io/unreachable 的 toleration。\n这种自动添加 toleration 机制保证了在其中一种问题被检测到时 pod 默认能够继续停留在当前节点运行 5 分钟。这两个默认 toleration 是由 DefaultTolerationSeconds admission controller添加的。\nDaemonSet 中的 pod 被创建时，针对以下 taint 自动添加的 NoExecute 的 toleration 将不会指定 tolerationSeconds：\n node.kubernetes.io/unreachable node.kubernetes.io/not-ready  这保证了出现上述问题时 DaemonSet 中的 pod 永远不会被驱逐。\n基于节点状态添加 taint Node 生命周期控制器会自动创建与 Node 条件相对应的带有 NoSchedule 效应的污点。 同样，调度器不检查节点条件，而是检查节点污点。这确保了节点条件不会影响调度到节点上的内容。用户可以通过添加适当的 Pod 容忍度来选择忽略某些 Node 的问题(表示为 Node 的调度条件)。\n自 Kubernetes 1.8 起， DaemonSet 控制器自动为所有守护进程添加如下 NoSchedule toleration 以防 DaemonSet 崩溃：\n node.kubernetes.io/memory-pressure node.kubernetes.io/disk-pressure node.kubernetes.io/out-of-disk (只适合 critical pod) node.kubernetes.io/unschedulable (1.10 或更高版本) node.kubernetes.io/network-unavailable (只适合 host network)  添加上述 toleration 确保了向后兼容，您也可以选择自由的向 DaemonSet 添加 toleration。\n"
},
{
	"uri": "https://lijun.in/tasks/administer-cluster/manage-resources/cpu-constraint-namespace/",
	"title": "为命名空间配置CPU最小和最大限制",
	"tags": [],
	"description": "",
	"content": "本章介绍命名空间中可以被容器和Pod使用的CPU资源的最小和最大值。你可以通过 [LimitRange](/docs/reference/generated/kubernetes-api/. param \u0026ldquo;version\u0026rdquo; \u0026gt;}}/#limitrange-v1-core) 对象声明 CPU 的最小和最大值. 如果 Pod 不能满足 LimitRange 的限制，它就不能在命名空间中创建。\n. heading \u0026ldquo;prerequisites\u0026rdquo; %}} . include \u0026ldquo;task-tutorial-prereqs.md\u0026rdquo; \u0026gt;}} . version-check \u0026gt;}}\n你的集群中每个节点至少要有1个CPU。\n创建命名空间 创建一个命名空间，以便本练习中创建的资源和集群的其余资源相隔离。\nkubectl create namespace constraints-cpu-example 创建 LimitRange 和 Pod 这里给出了 LimitRange 的配置文件：\n. codenew file=\u0026quot;admin/resource/cpu-constraints.yaml\u0026rdquo; \u0026gt;}}\n创建 LimitRange:\nkubectl create -f https://k8s.io/examples/admin/resource/cpu-constraints.yaml --namespace=constraints-cpu-example 查看 LimitRange 详情：\nkubectl get limitrange cpu-min-max-demo-lr --output=yaml --namespace=constraints-cpu-example 输出结果显示 CPU 的最小和最大限制符合预期。但需要注意的是，尽管你在 LimitRange 的配置文件中你没有声明默认值，默认值也会被自动创建。\nlimits: - default: cpu: 800m defaultRequest: cpu: 800m max: cpu: 800m min: cpu: 200m type: Container 现在不管什么时候在 constraints-cpu-example 命名空间中创建容器，Kubernetes 都会执行下面这些步骤：\n  如果容器没有声明自己的 CPU 请求和限制，将为容器指定默认 CPU 请求和限制。\n  核查容器声明的 CPU 请求确保其大于或者等于200 millicpu。\n  核查容器声明的 CPU 限制确保其小于或者等于800 millicpu。\n  . note \u0026gt;}} 当创建 LimitRange 对象时，你也可以声明 huge-page 和 GPU 的限制。当这些资源同时声明了 \u0026lsquo;default\u0026rsquo; 和 ‘defaultRequest’ 参数时，两个参数值必须相同。 . /note \u0026gt;}}\n这里给出了包含一个容器的 Pod 的配置文件。该容器声明了500 millicpu的 CPU 请求和800 millicpu的 CPU 限制。这些参数满足了 LimitRange 对象规定的 CPU 最小和最大限制。\n. codenew file=\u0026quot;admin/resource/cpu-constraints-pod.yaml\u0026rdquo; \u0026gt;}}\n创建Pod：\nkubectl create -f https://k8s.io/examples/admin/resource/cpu-constraints-pod.yaml --namespace=constraints-cpu-example 确认一下 Pod 中的容器在运行：\nkubectl get pod constraints-cpu-demo --namespace=constraints-cpu-example 查看 Pod 的详情：\nkubectl get pod constraints-cpu-demo --output=yaml --namespace=constraints-cpu-example 输出结果表明容器的 CPU 请求为500 millicpu，CPU限制为800 millicpu。这些参数满足 LimitRange 规定的限制范围。\nresources: limits: cpu: 800m requests: cpu: 500m 删除 Pod kubectl delete pod constraints-cpu-demo --namespace=constraints-cpu-example 尝试创建一个超过最大 CPU 限制的 Pod 这里给出了包含一个容器的 Pod 的配置文件。容器声明了500 millicpu的CPU请求和1.5 cpu的 CPU 限制。\n. codenew file=\u0026quot;admin/resource/cpu-constraints-pod-2.yaml\u0026rdquo; \u0026gt;}}\n尝试创建 Pod：\nkubectl create -f https://k8s.io/examples/admin/resource/cpu-constraints-pod-2.yaml --namespace=constraints-cpu-example 输出结果表明 Pod 没有创建成功，因为容器声明的 CPU 限制太大了：\nError from server (Forbidden): error when creating \u0026quot;examples/admin/resource/cpu-constraints-pod-2.yaml\u0026quot;: pods \u0026quot;constraints-cpu-demo-2\u0026quot; is forbidden: maximum cpu usage per Container is 800m, but limit is 1500m. 尝试创建一个不满足最小 CPU 请求的 Pod 这里给出了包含一个容器的 Pod 的配置文件。该容器声明了100 millicpu的 CPU 请求和800 millicpu的 CPU 限制。\n. codenew file=\u0026quot;admin/resource/cpu-constraints-pod-3.yaml\u0026rdquo; \u0026gt;}}\n尝试创建 Pod：\nkubectl create -f https://k8s.io/examples/admin/resource/cpu-constraints-pod-3.yaml --namespace=constraints-cpu-example 输出结果显示 Pod 没有创建成功，因为容器声明的 CPU 请求太小了：\nError from server (Forbidden): error when creating \u0026quot;examples/admin/resource/cpu-constraints-pod-3.yaml\u0026quot;: pods \u0026quot;constraints-cpu-demo-4\u0026quot; is forbidden: minimum cpu usage per Container is 200m, but request is 100m. 创建一个没有声明CPU请求和CPU限制的Pod 这里给出了包含一个容器的Pod的配置文件。该容器没有声明CPU请求和CPU限制。\n. codenew file=\u0026quot;admin/resource/cpu-constraints-pod-4.yaml\u0026rdquo; \u0026gt;}}\n创建 Pod：\nkubectl create -f https://k8s.io/examples/admin/resource/cpu-constraints-pod-4.yaml --namespace=constraints-cpu-example 查看 Pod 的详情：\nkubectl get pod constraints-cpu-demo-4 --namespace=constraints-cpu-example --output=yaml 输出结果显示 Pod 的容器有个800 millicpu的 CPU 请求和800 millicpu的 CPU 限制。容器时怎样得到那些值的呢？\nresources: limits: cpu: 800m requests: cpu: 800m 因为你的容器没有声明自己的 CPU 请求和限制，LimitRange 给它指定了默认的CPU请求和限制\n此时，你的容器可能运行也可能没有运行。回想一下，本任务的先决条件是你的节点要有1 个 CPU。如果你的每个节点仅有1个 CPU，那么可能没有任何一个节点可以满足800 millicpu的 CPU 请求。如果你在用的节点恰好有两个 CPU，那么你才可能有足够的 CPU来满足800 millicpu的请求。\nkubectl delete pod constraints-cpu-demo-4 --namespace=constraints-cpu-example CPU 最小和最大限制的强制执行 只有当Pod创建或者更新时，LimitRange为命名空间规定的CPU最小和最大限制才会被强制执行。如果你对LimitRange进行修改，那不会影响此前创建的Pod。\n最小和最大 CPU 限制范围的动机 作为集群管理员，你可能想设定 Pod 可以使用的 CPU 资源限制。例如：\n  集群中的每个节点有两个 CPU。你不想接受任何请求超过2个 CPU 的 Pod，因为集群中没有节点可以支持这种请求。\n  你的生产和开发部门共享一个集群。你想允许生产工作负载消耗3个 CPU，而开发工作负载的消耗限制为1个 CPU。你为生产和开发创建不同的命名空间，并且你为每个命名空间都应用了 CPU 限制。\n  环境清理 删除你的命名空间：\nkubectl delete namespace constraints-cpu-example . heading \u0026ldquo;whatsnext\u0026rdquo; %}} 集群管理员参考：   为命名空间配置默认内存请求和限制\n  为命名空间配置内存限制的最小值和最大值\n  为命名空间配置 CPU 限制的最小值和最大值\n  为命名空间配置内存和 CPU 配额\n  为命名空间配置 Pod 配额\n  为 API 对象配置配额\n  应用开发者参考：   为容器和 Pod 分配内存资源\n  为容器和 Pod 分配 CPU 资源\n  为 Pod 配置 Service 数量\n  "
},
{
	"uri": "https://lijun.in/tasks/configure-pod-container/extended-resource/",
	"title": "为容器分派扩展资源",
	"tags": [],
	"description": "",
	"content": ". feature-state state=\u0026quot;stable\u0026rdquo; \u0026gt;}}\n本文介绍如何为容器指定扩展资源。\n. feature-state state=\u0026quot;stable\u0026rdquo; \u0026gt;}}\n. heading \u0026ldquo;prerequisites\u0026rdquo; %}} . include \u0026ldquo;task-tutorial-prereqs.md\u0026rdquo; \u0026gt;}} . version-check \u0026gt;}}\n在您开始此练习前，请先练习为节点广播扩展资源。 在那个练习中将配置您的一个节点来广播 dongle 资源。\n给 Pod 分派扩展资源 要请求扩展资源，需要在您的容器清单中包括 resources:requests 字段。 扩展资源可以使用任何完全限定名称，只是不能使用 *.kubernetes.io/。 有效的扩展资源名的格式为 example.com/foo，其中 example.com 应被替换为您的组织的域名，而 foo 则是描述性的资源名称。\n下面是包含一个容器的 Pod 配置文件：\n. codenew file=\u0026quot;pods/resource/extended-resource-pod.yaml\u0026rdquo; \u0026gt;}}\n在配置文件中，您可以看到容器请求了 3 个 dongles。\n创建 Pod：\nkubectl apply -f https://k8s.io/examples/pods/resource/extended-resource-pod.yaml 检查 Pod 是否运行正常：\nkubectl get pod extended-resource-demo 描述 Pod:\nkubectl describe pod extended-resource-demo 输出结果显示 dongle 请求如下：\nLimits: example.com/dongle: 3 Requests: example.com/dongle: 3 尝试创建第二个 Pod 下面是包含一个容器的 Pod 配置文件，容器请求了 2 个 dongles。\n. codenew file=\u0026quot;pods/resource/extended-resource-pod-2.yaml\u0026rdquo; \u0026gt;}}\nKubernetes 将不能满足 2 个 dongles 的请求，因为第一个 Pod 已经使用了 4 个可用 dongles 中的 3 个。\n尝试创建 Pod：\nkubectl apply -f https://k8s.io/examples/pods/resource/extended-resource-pod-2.yaml 描述 Pod\nkubectl describe pod extended-resource-demo-2 输出结果表明 Pod 不能被调度，因为没有一个节点上存在两个可用的 dongles。\nConditions: Type Status PodScheduled False ... Events: ... ... Warning FailedScheduling pod (extended-resource-demo-2) failed to fit in any node fit failure summary on nodes : Insufficient example.com/dongle (1) 查看 Pod 的状态：\nkubectl get pod extended-resource-demo-2 输出结果表明 Pod 虽然被创建了，但没有被调度到节点上正常运行。Pod 的状态为 Pending：\nNAME READY STATUS RESTARTS AGE extended-resource-demo-2 0/1 Pending 0 6m 环境清理 删除本练习中创建的 Pod：\nkubectl delete pod extended-resource-demo kubectl delete pod extended-resource-demo-2 . heading \u0026ldquo;whatsnext\u0026rdquo; %}} 应用开发者参考  为容器和 Pod 分配内存资源 为容器和 Pod 分配 CPU 资源  集群管理员参考  为节点广播扩展资源  "
},
{
	"uri": "https://lijun.in/tasks/access-kubernetes-api/http-proxy-access-api/",
	"title": "使用 HTTP 代理访问 Kubernetes API",
	"tags": [],
	"description": "",
	"content": "本文说明如何使用 HTTP 代理访问 Kubernetes API。\n. heading \u0026ldquo;prerequisites\u0026rdquo; %}}  . include \u0026ldquo;task-tutorial-prereqs.md\u0026rdquo; \u0026gt;}} . version-check \u0026gt;}}   如果您的集群中还没有任何应用，使用如下命令启动一个 Hello World 应用：  kubectl run node-hello --image=gcr.io/google-samples/node-hello:1.0 --port=8080 使用 kubectl 启动代理服务器 使用如下命令启动 Kubernetes API 服务器的代理：\nkubectl proxy --port=8080  探究 Kubernetes API 当代理服务器在运行时，你可以通过 curl、wget 或者浏览器访问 API。\n获取 API 版本：\ncurl http://localhost:8080/api/ { \u0026quot;kind\u0026quot;: \u0026quot;APIVersions\u0026quot;, \u0026quot;versions\u0026quot;: [ \u0026quot;v1\u0026quot; ], \u0026quot;serverAddressByClientCIDRs\u0026quot;: [ { \u0026quot;clientCIDR\u0026quot;: \u0026quot;0.0.0.0/0\u0026quot;, \u0026quot;serverAddress\u0026quot;: \u0026quot;10.0.2.15:8443\u0026quot; } ] }  获取 Pod 列表：\ncurl http://localhost:8080/api/v1/namespaces/default/pods { \u0026quot;kind\u0026quot;: \u0026quot;PodList\u0026quot;, \u0026quot;apiVersion\u0026quot;: \u0026quot;v1\u0026quot;, \u0026quot;metadata\u0026quot;: { \u0026quot;resourceVersion\u0026quot;: \u0026quot;33074\u0026quot; }, \u0026quot;items\u0026quot;: [ { \u0026quot;metadata\u0026quot;: { \u0026quot;name\u0026quot;: \u0026quot;kubernetes-bootcamp-2321272333-ix8pt\u0026quot;, \u0026quot;generateName\u0026quot;: \u0026quot;kubernetes-bootcamp-2321272333-\u0026quot;, \u0026quot;namespace\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;uid\u0026quot;: \u0026quot;ba21457c-6b1d-11e6-85f7-1ef9f1dab92b\u0026quot;, \u0026quot;resourceVersion\u0026quot;: \u0026quot;33003\u0026quot;, \u0026quot;creationTimestamp\u0026quot;: \u0026quot;2016-08-25T23:43:30Z\u0026quot;, \u0026quot;labels\u0026quot;: { \u0026quot;pod-template-hash\u0026quot;: \u0026quot;2321272333\u0026quot;, \u0026quot;run\u0026quot;: \u0026quot;kubernetes-bootcamp\u0026quot; }, ... }  . heading \u0026ldquo;whatsnext\u0026rdquo; %}} 想了解更多信息，请参阅 kubectl 代理。\n"
},
{
	"uri": "https://lijun.in/setup/production-environment/tools/kubeadm/control-plane-flags/",
	"title": "使用 kubeadm 定制控制平面配置",
	"tags": [],
	"description": "",
	"content": ". feature-state for_k8s_version=\u0026quot;1.12\u0026rdquo; state=\u0026quot;stable\u0026rdquo; \u0026gt;}}\nkubeadm ClusterConfiguration 对象公开了 extraArgs 字段，它可以覆盖传递给控制平面组件（如 APIServer、ControllerManager 和 Scheduler）的默认参数。各组件配置使用如下字段定义：\n apiServer controllerManager scheduler  extraArgs 字段由 key: value 对组成。 要覆盖控制平面组件的参数:\n 将适当的字段添加到配置中。 向字段添加要覆盖的参数值。 用 --config \u0026lt;YOUR CONFIG YAML\u0026gt; 运行 kubeadm init。  有关配置中的每个字段的详细信息，您可以导航到我们的 API 参考页面。\n. note \u0026gt;}}\n您可以通过运行 kubeadm config print init-defaults 并将输出保存到您选择的文件中，以默认值形式生成 ClusterConfiguration 对象。 . /note \u0026gt;}}\nAPIServer 参数 有关详细信息，请参阅 kube-apiserver 参考文档。\n使用示例：\napiVersion: kubeadm.k8s.io/v1beta2 kind: ClusterConfiguration kubernetesVersion: v1.16.0 apiServer: extraArgs: advertise-address: 192.168.0.103 anonymous-auth: \u0026#34;false\u0026#34; enable-admission-plugins: AlwaysPullImages,DefaultStorageClass audit-log-path: /home/johndoe/audit.log ControllerManager 参数 有关详细信息，请参阅 kube-controller-manager 参考文档。\n使用示例：\napiVersion: kubeadm.k8s.io/v1beta2 kind: ClusterConfiguration kubernetesVersion: v1.16.0 controllerManager: extraArgs: cluster-signing-key-file: /home/johndoe/keys/ca.key bind-address: 0.0.0.0 deployment-controller-sync-period: \u0026#34;50\u0026#34; Scheduler 参数 有关详细信息，请参阅 kube-scheduler 参考文档。\n使用示例：\napiVersion: kubeadm.k8s.io/v1beta2 kind: ClusterConfiguration kubernetesVersion: v1.16.0 scheduler: extraArgs: address: 0.0.0.0 config: /home/johndoe/schedconfig.yaml kubeconfig: /home/johndoe/kubeconfig.yaml "
},
{
	"uri": "https://lijun.in/tasks/run-application/update-api-object-kubectl-patch/",
	"title": "使用 kubectl patch 更新 API 对象",
	"tags": [],
	"description": "使用 kubectl patch 更新 Kubernetes API 对象。做一个策略性的合并 patch 或 JSON 合并 patch。",
	"content": "这个任务展示了如何使用 kubectl patch 就地更新 API 对象。这个任务中的练习演示了一个策略性合并 patch 和一个 JSON 合并 patch。\n. heading \u0026ldquo;prerequisites\u0026rdquo; %}} . include \u0026ldquo;task-tutorial-prereqs.md\u0026rdquo; \u0026gt;}} . version-check \u0026gt;}}\n使用策略合并 patch 更新 Deployment 下面是具有两个副本的 Deployment 的配置文件。每个副本是一个 Pod，有一个容器：\n. codenew file=\u0026quot;application/deployment-patch.yaml\u0026rdquo; \u0026gt;}}\n创建 Deployment：\nkubectl create -f https://k8s.io/examples/application/deployment-patch.yaml 查看与 Deployment 相关的 Pod：\nkubectl get pods 输出显示 Deployment 有两个 Pod。1/1 表示每个 Pod 有一个容器:\nNAME READY STATUS RESTARTS AGE patch-demo-28633765-670qr 1/1 Running 0 23s patch-demo-28633765-j5qs3 1/1 Running 0 23s 把运行的 Pod 的名字记下来。稍后，您将看到这些 Pod 被终止并被新的 Pod 替换。\n此时，每个 Pod 都有一个运行 nginx 镜像的容器。现在假设您希望每个 Pod 有两个容器：一个运行 nginx，另一个运行 redis。\n创建一个名为 patch-file-containers.yaml 的文件。内容如下:\nspec: template: spec: containers: - name: patch-demo-ctr-2 image: redis 修补您的 Deployment：\nkubectl patch deployment patch-demo --patch \u0026#34;$(cat patch-file-containers.yaml)\u0026#34; 查看修补后的 Deployment：\nkubectl get deployment patch-demo --output yaml 输出显示 Deployment 中的 PodSpec 有两个容器:\ncontainers: - image: redis imagePullPolicy: Always name: patch-demo-ctr-2 ... - image: nginx imagePullPolicy: Always name: patch-demo-ctr ... 查看与 patch Deployment 相关的 Pod:\nkubectl get pods 输出显示正在运行的 Pod 与以前运行的 Pod 有不同的名称。Deployment 终止了旧的 Pod，并创建了两个 符合更新的部署规范的新 Pod。2/2 表示每个 Pod 有两个容器:\nNAME READY STATUS RESTARTS AGE patch-demo-1081991389-2wrn5 2/2 Running 0 1m patch-demo-1081991389-jmg7b 2/2 Running 0 1m 仔细查看其中一个 patch-demo Pod:\nkubectl get pod \u0026lt;your-pod-name\u0026gt; --output yaml 输出显示 Pod 有两个容器:一个运行 nginx，一个运行 redis:\ncontainers: - image: redis ... - image: nginx ... 策略性合并类的 patch 您在前面的练习中所做的 patch 称为策略性合并 patch。 请注意，patch 没有替换容器列表。相反，它向列表中添加了一个新容器。换句话说， patch 中的列表与现有列表合并。当您在列表中使用策略性合并 patch 时，并不总是这样。 在某些情况下，列表是替换的，而不是合并的。\n对于策略性合并 patch，列表可以根据其 patch 策略进行替换或合并。patch 策略由 Kubernetes 源代码中字段标记中的 patchStrategy 键的值指定。 例如，PodSpec 结构体的 Containers 字段有 merge 的 patchStrategy：\ntype PodSpec struct { ... Containers []Container `json:\u0026#34;containers\u0026#34; patchStrategy:\u0026#34;merge\u0026#34; patchMergeKey:\u0026#34;name\u0026#34; ...` 您还可以在 OpenApi spec 规范中看到 patch 策略：\n\u0026#34;io.k8s.api.core.v1.PodSpec\u0026#34;: { ... \u0026#34;containers\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;List of containers belonging to the pod. ... }, \u0026#34;x-kubernetes-patch-merge-key\u0026#34;: \u0026#34;name\u0026#34;, \u0026#34;x-kubernetes-patch-strategy\u0026#34;: \u0026#34;merge\u0026#34; }, 您可以在 [Kubernetes API 文档](/docs/reference/generated/kubernetes-api/. param \u0026ldquo;version\u0026rdquo; \u0026gt;}}/#podspec-v1-core) 中看到 patch 策略\n创建一个名为 patch-file-tolerations.yaml 的文件。内容如下:\nspec: template: spec: tolerations: - effect: NoSchedule key: disktype value: ssd patch Deployment：\n. tabs name=\u0026quot;kubectl_patch_example\u0026rdquo; \u0026gt;}} {. tab name=\u0026quot;Bash\u0026rdquo; codelang=\u0026quot;bash\u0026rdquo; \u0026gt;}} kubectl patch deployment patch-demo \u0026ndash;patch \u0026ldquo;$(cat patch-file-containers.yaml)\u0026rdquo; . /tab \u0026gt;}} . tab name=\u0026quot;PowerShell\u0026rdquo; codelang=\u0026quot;posh\u0026rdquo; \u0026gt;}} kubectl patch deployment patch-demo \u0026ndash;patch $(cat patch-file-containers.yaml) . /tab \u0026gt;}}} . /tabs \u0026gt;}}\n查看 patch Deployment：\nkubectl get deployment patch-demo --output yaml 输出结果显示部署中的 PodSpec 只有一个默认：\ncontainers: - image: redis imagePullPolicy: Always name: patch-demo-ctr-2 ... - image: nginx imagePullPolicy: Always name: patch-demo-ctr ... tolerations: - effect: NoSchedule key: disktype value: ssd 请注意，PodSpec 中的 tolerations 列表被替换，而不是合并。这是因为 PodSpec 的 tolerance 字段的字段标签中没有 patchStrategy 键。所以策略合并 patch 使用默认的 patch 策略，也就是 replace。\ntype PodSpec struct { ... Tolerations []Toleration `json:\u0026#34;tolerations,omitempty\u0026#34; protobuf:\u0026#34;bytes,22,opt,name=tolerations\u0026#34;` 使用 JSON 合并 patch 更新部署 策略性合并 patch 不同于 JSON 合并 patch。 使用 JSON 合并 patch，如果您想更新列表，您必须指定整个新列表。新的列表完全取代了现有的列表。\nkubectl patch 命令有一个 type 参数，您可以将其设置为以下值之一:\n有关 JSON patch 和 JSON 合并 patch 的比较，查看JSON patch 和 JSON 合并 patch。\ntype 参数的默认值是 strategic。在前面的练习中，我们做了一个策略性的合并 patch。\n下一步，在相同的部署上执行 JSON 合并 patch。创建一个名为 patch-file-2 的文件。内容如下:\nspec: template: spec: containers: - name: patch-demo-ctr-3 image: gcr.io/google-samples/node-hello:1.0 在 patch 命令中，将 type 设置为 merge：\nkubectl patch deployment patch-demo --type merge --patch \u0026#34;$(cat patch-file-2.yaml)\u0026#34; 查看 patch 部署：\nkubectl get deployment patch-demo --output yaml patch 中指定的容器列表只有一个容器。 输出显示您的一个容器列表替换了现有的容器列表。\nspec: containers: - image: gcr.io/google-samples/node-hello:1.0 ... name: patch-demo-ctr-3 列表中运行的 Pod：\nkubectl get pods 在输出中，您可以看到已经终止了现有的 Pod，并创建了新的 Pod。1/1 表示每个新 Pod只运行一个容器。\nNAME READY STATUS RESTARTS AGE patch-demo-1307768864-69308 1/1 Running 0 1m patch-demo-1307768864-c86dc 1/1 Running 0 1m kubectl patch 命令的其他形式 kubectl patch 命令使用 YAML 或 JSON。它可以将 patch 作为文件，也可以直接在命令行中使用。\n创建一个文件名称是 patch-file.json 内容如下：\n{ \u0026#34;spec\u0026#34;: { \u0026#34;template\u0026#34;: { \u0026#34;spec\u0026#34;: { \u0026#34;containers\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;patch-demo-ctr-2\u0026#34;, \u0026#34;image\u0026#34;: \u0026#34;redis\u0026#34; } ] } } } } 以下命令是相同的：\nkubectl patch deployment patch-demo --patch \u0026#34;$(cat patch-file.yaml)\u0026#34; kubectl patch deployment patch-demo --patch \u0026#39;spec:\\n template:\\n spec:\\n containers:\\n - name: patch-demo-ctr-2\\n image: redis\u0026#39; kubectl patch deployment patch-demo --patch \u0026#34;$(cat patch-file.json)\u0026#34; kubectl patch deployment patch-demo --patch \u0026#39;{\u0026#34;spec\u0026#34;: {\u0026#34;template\u0026#34;: {\u0026#34;spec\u0026#34;: {\u0026#34;containers\u0026#34;: [{\u0026#34;name\u0026#34;: \u0026#34;patch-demo-ctr-2\u0026#34;,\u0026#34;image\u0026#34;: \u0026#34;redis\u0026#34;}]}}}}\u0026#39; 总结 在本练习中，您使用 kubectl patch 更改部署对象的实时配置。您没有更改最初用于创建部署对象的配置文件。 用于更新 API 对象的其他命令包括 kubectl annotate， kubectl edit， kubectl replace， kubectl scale， 和 kubectl apply。\n. heading \u0026ldquo;whatsnext\u0026rdquo; %}}  Kubernetes 对象管理器 使用命令管理 Kubernetes 对象 使用配置文件强制管理 Kubernetes 对象 使用配置文件对 Kubernetes 对象进行声明式管理  "
},
{
	"uri": "https://lijun.in/concepts/overview/working-with-objects/",
	"title": "使用 Kubernetes 对象",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lijun.in/tasks/administer-cluster/network-policy-provider/romana-network-policy/",
	"title": "使用 Romana 作为 NetworkPolicy",
	"tags": [],
	"description": "",
	"content": "本页展示如何使用 Romana 作为 NetworkPolicy。\n. heading \u0026ldquo;prerequisites\u0026rdquo; %}} 完成kubeadm 入门指南中的1、2、3步。\n使用 kubeadm 安装 Romana 按照容器化安装指南获取 kubeadm。\n运用网络策略 使用以下的一种方式去运用网络策略：\n Romana 网络策略  Romana 网络策略例子   NetworkPolicy API  . heading \u0026ldquo;whatsnext\u0026rdquo; %}} Romana 安装完成后，您可以按照声明 Network Policy去尝试使用 Kubernetes NetworkPolicy。\n"
},
{
	"uri": "https://lijun.in/tasks/job/fine-parallel-processing-work-queue/",
	"title": "使用工作队列进行精细的并行处理",
	"tags": [],
	"description": "",
	"content": "在这个例子中，我们会运行一个Kubernetes Job，其中的 Pod 会运行多个并行工作进程。\n在这个例子中，当每个pod被创建时，它会从一个任务队列中获取一个工作单元，处理它，然后重复，直到到达队列的尾部。\n下面是这个示例的步骤概述\n 启动存储服务用于保存工作队列。 在这个例子中，我们使用 Redis 来存储工作项。在上一个例子中，我们使用了 RabbitMQ。在这个例子中，由于 AMQP 不能为客户端提供一个良好的方法来检测一个有限长度的工作队列是否为空，我们使用了 Redis 和一个自定义的工作队列客户端库。在实践中，您可能会设置一个类似于 Redis 的存储库，并将其同时用于多项任务或其他事务的工作队列。  创建一个队列，然后向其中填充消息。 每个消息表示一个将要被处理的工作任务。在这个例子中，消息只是一个我们将用于进行长度计算的整数。  启动一个 Job 对队列中的任务进行处理。这个 Job 启动了若干个 Pod 。每个 Pod 从消息队列中取出一个工作任务，处理它，然后重复，直到到达队列的尾部。  . toc \u0026gt;}}\n. heading \u0026ldquo;prerequisites\u0026rdquo; %}} . include \u0026ldquo;task-tutorial-prereqs.md\u0026rdquo; \u0026gt;}} . version-check \u0026gt;}}\n熟秋基础知识，非并行方式运行 Job。\n启动 Redis 对于这个例子，为了简单起见，我们将启动一个单实例的 Redis。 了解如何部署一个可伸缩、高可用的 Redis 例子，请查看 Redis 样例\n如果您在使用本文档库的源代码目录，您可以进入如下目录，然后启动一个临时的 Pod 用于运行 Redis 和 一个临时的 service 以便我们能够找到这个 Pod\n$ cd content/en/examples/application/job/redis $ kubectl create -f ./redis-pod.yaml pod/redis-master created $ kubectl create -f ./redis-service.yaml service/redis created 如果您没有使用本文档库的源代码目录，您可以直接下载如下文件：\n redis-pod.yaml redis-service.yaml Dockerfile job.yaml rediswq.py worker.py  使用任务填充队列 现在，让我们往队列里添加一些“任务”。在这个例子中，我们的任务只是一些将被打印出来的字符串。\n启动一个临时的可交互的 pod 用于运行 Redis 命令行界面。\n$ kubectl run -i --tty temp --image redis --command \u0026#34;/bin/sh\u0026#34; Waiting for pod default/redis2-c7h78 to be running, status is Pending, pod ready: false Hit enter for command prompt 现在按回车键，启动 redis 命令行界面，然后创建一个存在若干个工作项的列表。\n# redis-cli -h redis redis:6379\u0026gt; rpush job2 \u0026quot;apple\u0026quot; (integer) 1 redis:6379\u0026gt; rpush job2 \u0026quot;banana\u0026quot; (integer) 2 redis:6379\u0026gt; rpush job2 \u0026quot;cherry\u0026quot; (integer) 3 redis:6379\u0026gt; rpush job2 \u0026quot;date\u0026quot; (integer) 4 redis:6379\u0026gt; rpush job2 \u0026quot;fig\u0026quot; (integer) 5 redis:6379\u0026gt; rpush job2 \u0026quot;grape\u0026quot; (integer) 6 redis:6379\u0026gt; rpush job2 \u0026quot;lemon\u0026quot; (integer) 7 redis:6379\u0026gt; rpush job2 \u0026quot;melon\u0026quot; (integer) 8 redis:6379\u0026gt; rpush job2 \u0026quot;orange\u0026quot; (integer) 9 redis:6379\u0026gt; lrange job2 0 -1 1) \u0026quot;apple\u0026quot; 2) \u0026quot;banana\u0026quot; 3) \u0026quot;cherry\u0026quot; 4) \u0026quot;date\u0026quot; 5) \u0026quot;fig\u0026quot; 6) \u0026quot;grape\u0026quot; 7) \u0026quot;lemon\u0026quot; 8) \u0026quot;melon\u0026quot; 9) \u0026quot;orange\u0026quot; 因此，这个键为 job2 的列表就是我们的工作队列。\n注意：如果您还没有正确地配置 Kube DNS，您可能需要将上面的第一步改为 redis-cli -h $REDIS_SERVICE_HOST。\n创建镜像\n现在我们已经准备好创建一个我们要运行的镜像\n我们会使用一个带有 redis 客户端的 python 工作程序从消息队列中读出消息。\n这里提供了一个简单的 Redis 工作队列客户端库，叫 rediswq.py (下载)。\nJob 中每个 Pod 内的 “工作程序” 使用工作队列客户端库获取工作。如下：\n. codenew language=\u0026quot;python\u0026rdquo; file=\u0026quot;application/job/redis/worker.py\u0026rdquo; \u0026gt;}}\n如果您在使用本文档库的源代码目录，请将当前目录切换到 content/en/examples/application/job/redis/。否则，请点击链接下载 worker.py、 rediswq.py 和 Dockerfile。然后构建镜像：\ndocker build -t job-wq-2 . Push 镜像 对于 Docker Hub，请先用您的用户名给镜像打上标签，然后使用下面的命令 push 您的镜像到仓库。请将 \u0026lt;username\u0026gt; 替换为您自己的用户名。\ndocker tag job-wq-2 \u0026lt;username\u0026gt;/job-wq-2 docker push \u0026lt;username\u0026gt;/job-wq-2 您需要将镜像 push 到一个公共仓库或者 配置集群访问您的私有仓库。\n如果您使用的是 Google Container Registry，请先用您的 project ID 给您的镜像打上标签，然后 push 到 GCR 。请将 \u0026lt;project\u0026gt; 替换为您自己的 project ID\ndocker tag job-wq-2 gcr.io/\u0026lt;project\u0026gt;/job-wq-2 gcloud docker -- push gcr.io/\u0026lt;project\u0026gt;/job-wq-2 定义一个 Job 这是 job 定义：\n. codenew file=\u0026quot;application/job/redis/job.yaml\u0026rdquo; \u0026gt;}}\n请确保将 job 模板中的 gcr.io/myproject 更改为您自己的路径。\n在这个例子中，每个 pod 处理了队列中的多个项目，直到队列中没有项目时便退出。因为是由工作程序自行检测工作队列是否为空，并且 Job 控制器不知道工作队列的存在，所以依赖于工作程序在完成工作时发出信号。工作程序以成功退出的形式发出信号表示工作队列已经为空。所以，只要有任意一个工作程序成功退出，控制器就知道工作已经完成了，所有的 Pod 将很快会退出。因此，我们将 Job 的 completion count 设置为 1 。尽管如此，Job 控制器还是会等待其它 Pod 完成。\n运行 Job 现在运行这个 Job ：\nkubectl create -f ./job.yaml 稍等片刻，然后检查这个 Job。\n$ kubectl describe jobs/job-wq-2 Name: job-wq-2 Namespace: default Selector: controller-uid=b1c7e4e3-92e1-11e7-b85e-fa163ee3c11f Labels: controller-uid=b1c7e4e3-92e1-11e7-b85e-fa163ee3c11f job-name=job-wq-2 Annotations: \u0026lt;none\u0026gt; Parallelism: 2 Completions: \u0026lt;unset\u0026gt; Start Time: Mon, 11 Jan 2016 17:07:59 -0800 Pods Statuses: 1 Running / 0 Succeeded / 0 Failed Pod Template: Labels: controller-uid=b1c7e4e3-92e1-11e7-b85e-fa163ee3c11f job-name=job-wq-2 Containers: c: Image: gcr.io/exampleproject/job-wq-2 Port: Environment: \u0026lt;none\u0026gt; Mounts: \u0026lt;none\u0026gt; Volumes: \u0026lt;none\u0026gt; Events: FirstSeen LastSeen Count From SubobjectPath Type Reason Message --------- -------- ----- ---- ------------- -------- ------ ------- 33s 33s 1 {job-controller } Normal SuccessfulCreate Created pod: job-wq-2-lglf8 $ kubectl logs pods/job-wq-2-7r7b2 Worker with sessionID: bbd72d0a-9e5c-4dd6-abf6-416cc267991f Initial queue state: empty=False Working on banana Working on date Working on lemon 您可以看到，其中的一个 pod 处理了若干个工作单元。\n其它 如果您不方便运行一个队列服务或者修改您的容器用于运行一个工作队列，您可以考虑其它的 job 模式。\n如果您有连续的后台处理业务，那么可以考虑使用 replicationController 来运行您的后台业务，和运行一个类似 https://github.com/resque/resque 的后台处理库。\n"
},
{
	"uri": "https://lijun.in/tasks/access-application-cluster/port-forward-access-application-cluster/",
	"title": "使用端口转发来访问集群中的应用",
	"tags": [],
	"description": "",
	"content": "本文展示如何使用 kubectl port-forward 连接到在 Kubernetes 集群中运行的 Redis 服务。这种类型的连接对数据库调试很有用。\n. heading \u0026ldquo;prerequisites\u0026rdquo; %}}  . include \u0026ldquo;task-tutorial-prereqs.md\u0026rdquo; \u0026gt;}} . version-check \u0026gt;}}   安装 redis-cli。  创建 Redis deployment 和服务   创建一个 Redis deployment：\n kubectl apply -f https://k8s.io/examples/application/guestbook/redis-master-deployment.yaml    查看输出是否成功，以验证是否成功创建 deployment： deployment.apps/redis-master created  查看 pod 状态，检查其是否准备就绪： kubectl get pods  输出显示创建的 pod： NAME READY STATUS RESTARTS AGE redis-master-765d459796-258hz 1/1 Running 0 50s  查看 deployment 状态： kubectl get deployment  输出显示创建的 deployment： NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE redis-master 1 1 1 1 55s  查看 replicaset 状态： kubectl get rs  输出显示创建的 replicaset： NAME DESIRED CURRENT READY AGE redis-master-765d459796 1 1 1 1m   创建一个 Redis 服务：\nkubectl apply -f https://k8s.io/examples/application/guestbook/redis-master-service.yaml    查看输出是否成功，以验证是否成功创建 service： service/redis-master created  检查 service 是否创建： kubectl get svc | grep redis  输出显示创建的 service： NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE redis-master ClusterIP 10.0.0.213 \u0026lt;none\u0026gt; 6379/TCP 27s   验证 Redis 服务是否运行在 pod 中并且监听 6379 端口：\nkubectl get pods redis-master-765d459796-258hz --template='{{(index (index .spec.containers 0).ports 0).containerPort}}{{\u0026quot;\\n\u0026quot;}}'    输出应该显示端口： 6379  转发一个本地端口到 pod 端口   从 Kubernetes v1.10 开始，kubectl port-forward 允许使用资源名称（例如 pod 名称）来选择匹配的 pod 来进行端口转发。\n kubectl port-forward redis-master-765d459796-258hz 7000:6379    这相当于 kubectl port-forward pods/redis-master-765d459796-258hz 7000:6379  或者 kubectl port-forward deployment/redis-master 7000:6379  或者 kubectl port-forward rs/redis-master 7000:6379  或者 kubectl port-forward svc/redis-master 7000:6379  以上所有命令都应该有效。输出应该类似于： I0710 14:43:38.274550 3655 portforward.go:225] Forwarding from 127.0.0.1:7000 -\u0026gt; 6379 I0710 14:43:38.274797 3655 portforward.go:225] Forwarding from [::1]:7000 -\u0026gt; 6379   启动 Redis 命令行接口：\n redis-cli -p 7000     在 Redis 命令行提示符下，输入 ping 命令：\n 127.0.0.1:7000\u0026gt;ping    成功的 ping 请求应该返回 PONG。  讨论 与本地 7000 端口建立的连接将转发到运行 Redis 服务器的 pod 的 6379 端口。通过此连接，您可以使用本地工作站来调试在 pod 中运行的数据库。\n. warning \u0026gt;}}\n警告： 由于已知的限制，目前的端口转发仅适用于 TCP 协议。 在 issue 47862 中正在跟踪对 UDP 协议的支持。 . /warning \u0026gt;}}\n. heading \u0026ldquo;whatsnext\u0026rdquo; %}} 学习更多关于 kubectl port-forward。\n"
},
{
	"uri": "https://lijun.in/tasks/manage-kubernetes-objects/imperative-config/",
	"title": "使用配置文件对 Kubernetes 对象进行命令式管理",
	"tags": [],
	"description": "",
	"content": "可以使用 kubectl 命令行工具以及用 YAML 或 JSON 编写的对象配置文件来创建、更新和删除 Kubernetes 对象。 本文档说明了如何使用配置文件定义和管理对象。\nheading \u0026ldquo;prerequisites\u0026rdquo; %}} 安装 kubectl 。\ninclude \u0026ldquo;task-tutorial-prereqs.md\u0026rdquo; \u0026gt;}} . version-check \u0026gt;}}\n权衡 kubectl 工具支持三种对象管理：\n 命令式命令 命令式对象配置 声明式对象配置  参看 Kubernetes 对象管理 讨论每种对象管理的优缺点。\n如何创建对象 您可以使用 kubectl create -f 从配置文件创建一个对象。 请参考 [kubernetes API 参考](/docs/reference/generated/kubernetes-api/. param \u0026ldquo;version\u0026rdquo; \u0026gt;}}/) 有关详细信息。\n kubectl create -f \u0026lt;filename|url\u0026gt;  如何更新对象 . warning \u0026gt;}}\n使用 replace 命令更新对象会删除所有未在配置文件中指定的规范的某些部分。 不应将其规范由集群部分管理的对象使用，比如类型为 LoadBalancer 的服务，其中 externalIPs 字段独立于配置文件进行管理。 必须将独立管理的字段复制到配置文件中，以防止 replace 删除它们。 . /warning \u0026gt;}}\n您可以使用 kubectl replace -f 根据配置文件更新活动对象。\n kubectl replace -f \u0026lt;filename|url\u0026gt;  如何删除对象 您可以使用 kubectl delete -f 删除配置文件中描述的对象。\n kubectl delete -f \u0026lt;filename|url\u0026gt;  如何查看对象 您可以使用 kubectl get -f 查看有关配置文件中描述的对象的信息。\n kubectl get -f \u0026lt;filename|url\u0026gt; -o yaml  -o yaml 标志指定打印完整的对象配置。 使用 kubectl get -h 查看选项列表。\n局限性 当完全定义每个对象的配置并将其记录在其配置文件中时，create、 replace 和delete 命令会很好的工作。 但是，当更新一个活动对象，并且更新没有合并到其配置文件中时，下一次执行 replace 时，更新将丢失。 如果控制器,例如 HorizontalPodAutoscaler ,直接对活动对象进行更新，则会发生这种情况。 这有一个例子：\n 从配置文件创建一个对象。 另一个源通过更改某些字段来更新对象。 从配置文件中替换对象。在步骤2中所做的其他源的更改将丢失。  如果需要支持同一对象的多个编写器，则可以使用 kubectl apply 来管理该对象。\n从 URL 创建和编辑对象而不保存配置 假设您具有对象配置文件的 URL。 您可以在创建对象之前使用 kubectl create --edit 对配置进行更改。 这对于指向可以由读者修改的配置文件的教程和任务特别有用。\nkubectl create -f \u0026lt;url\u0026gt; --edit 从命令式命令迁移到命令式对象配置 从命令式命令迁移到命令式对象配置涉及几个手动步骤。\n  将活动对象导出到本地对象配置文件：\nkubectl get \u0026lt;kind\u0026gt;/\u0026lt;name\u0026gt; -o yaml \u0026gt; \u0026lt;kind\u0026gt;_\u0026lt;name\u0026gt;.yaml    从对象配置文件中手动删除状态字段。    对于后续的对象管理，只能使用 replace 。\nkubectl replace -f \u0026lt;kind\u0026gt;_\u0026lt;name\u0026gt;.yaml   定义控制器选择器和 PodTemplate 标签 . warning \u0026gt;}}\n不建议在控制器上更新选择器。 . /warning \u0026gt;}}\n推荐的方法是定义单个不变的 PodTemplate 标签，该标签仅由控制器选择器使用，而没有其他语义。\n标签示例：\nselector: matchLabels: controller-selector: \u0026#34;apps/v1/deployment/nginx\u0026#34; template: metadata: labels: controller-selector: \u0026#34;apps/v1/deployment/nginx\u0026#34; . heading \u0026ldquo;whatsnext\u0026rdquo; %}}  使用命令式命令管理 Kubernetes 对象 使用对象配置管理 Kubernetes 对象 (声明式) Kubectl 命令参考 [Kubernetes API 参考](/docs/reference/generated/kubernetes-api/. param \u0026ldquo;version\u0026rdquo; \u0026gt;}}/)  "
},
{
	"uri": "https://lijun.in/tutorials/kubernetes-basics/expose/",
	"title": "公开地暴露你的应用",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lijun.in/reference/access-authn-authz/extensible-admission-controllers/",
	"title": "动态准入控制",
	"tags": [],
	"description": "",
	"content": "除了内置的 admission 插件，admission 插件可以作为扩展独立开发，并以运行时所配置的 webhook 的形式运行。 此页面描述了如何构建、配置、使用和监视 admission webhook。\n什么是 admission webhook？ Admission webhook 是一种用于接收准入请求并对其进行处理的 HTTP 回调机制。 可以定义两种类型的 admission webhook，即 validating admission webhook 和 mutating admission webhook。 Mutating admission webhook 会先被调用。它们可以更改发送到 API 服务器的对象以执行自定义的设置默认值操作。\n在完成了所有对象修改并且 API 服务器也验证了所传入的对象之后，validating admission webhook 会被调用，并通过拒绝请求的方式来强制实施自定义的策略。\n. note \u0026gt;}} 如果 admission webhook 需要保证它们所看到的是对象的最终状态以实施某种策略。则应使用 validating admission webhook，因为对象被 mutating webhook 看到之后仍然可能被修改。 . /note \u0026gt;}}\n尝试 admission webhook admission webhook 本质上是集群控制平面的一部分。您应该非常谨慎地编写和部署它们。 如果您打算编写或者部署生产级 admission webhook，请阅读用户指南以获取相关说明。 在下文中，我们将介绍如何快速试验 admission webhook。\n先决条件   确保 Kubernetes 集群版本至少为 v1.16（以便使用 admissionregistration.k8s.io/v1 API） 或者 v1.9 （以便使用 admissionregistration.k8s.io/v1beta1 API）。\n  确保启用 MutatingAdmissionWebhook 和 ValidatingAdmissionWebhook 控制器。 这里 是一组推荐的 admission 控制器，通常可以启用。\n  确保启用了 admissionregistration.k8s.io/v1beta1 API。\n  编写一个 admission webhook 服务器 请参阅 Kubernetes e2e 测试中的 admission webhook 服务器 的实现。webhook 处理由 apiserver 发送的 AdmissionReview 请求，并且将其决定作为 AdmissionReview 对象以相同版本发送回去。\n有关发送到 webhook 的数据的详细信息，请参阅 webhook 请求。\n要获取来自 webhook 的预期数据，请参阅 webhook 响应。\n示例 admission webhook 服务器置 ClientAuth 字段为空，默认为 NoClientCert 。这意味着 webhook 服务器不会验证客户端的身份，认为其是 apiservers。 如果您需要双向 TLS 或其他方式来验证客户端，请参阅如何对 apiservers 进行身份认证。\n部署 admission webhook 服务 e2e 测试中的 webhook 服务器通过 [deployment API](/docs/reference/generated/kubernetes-api/. param \u0026ldquo;version\u0026rdquo; \u0026gt;}}/#deployment-v1-apps) 部署在 Kubernetes 集群中。该测试还将创建一个 [service](/docs/reference/generated/kubernetes-api/. param \u0026ldquo;version\u0026rdquo; \u0026gt;}}/#service-v1-core) 作为 webhook 服务器的前端。参见相关代码。\n您也可以在集群外部署 webhook。这样做需要相应地更新您的 webhook 配置。\n即时配置 admission webhook 您可以通过 [ValidatingWebhookConfiguration](/docs/reference/generated/kubernetes-api/. param \u0026ldquo;version\u0026rdquo; \u0026gt;}}/#validatingwebhookconfiguration-v1-admissionregistration-k8s-io) 或者 [MutatingWebhookConfiguration](/docs/reference/generated/kubernetes-api/. param \u0026ldquo;version\u0026rdquo; \u0026gt;}}/#mutatingwebhookconfiguration-v1-admissionregistration-k8s-io) 动态配置哪些资源要被哪些 admission webhook 处理。\n以下是一个 ValidatingWebhookConfiguration 示例，mutating webhook 配置与此类似。有关每个配置字段的详细信息，请参阅 webhook 配置 部分。\n. tabs name=\u0026quot;ValidatingWebhookConfiguration_example_1\u0026rdquo; \u0026gt;}} . tab name=\u0026quot;admissionregistration.k8s.io/v1\u0026rdquo; %}}\napiVersion: admissionregistration.k8s.io/v1 kind: ValidatingWebhookConfiguration metadata: name: \u0026#34;pod-policy.example.com\u0026#34; webhooks: - name: \u0026#34;pod-policy.example.com\u0026#34; rules: - apiGroups: [\u0026#34;\u0026#34;] apiVersions: [\u0026#34;v1\u0026#34;] operations: [\u0026#34;CREATE\u0026#34;] resources: [\u0026#34;pods\u0026#34;] scope: \u0026#34;Namespaced\u0026#34; clientConfig: service: namespace: \u0026#34;example-namespace\u0026#34; name: \u0026#34;example-service\u0026#34; caBundle: \u0026#34;Ci0tLS0tQk...\u0026lt;base64-encoded PEM bundle containing the CA that signed the webhook\u0026#39;s serving certificate\u0026gt;...tLS0K\u0026#34; admissionReviewVersions: [\u0026#34;v1\u0026#34;, \u0026#34;v1beta1\u0026#34;] sideEffects: None timeoutSeconds: 5 . /tab %}} . tab name=\u0026quot;admissionregistration.k8s.io/v1beta1\u0026rdquo; %}}\n# 1.16 中被淘汰，推荐使用 admissionregistration.k8s.io/v1 apiVersion: admissionregistration.k8s.io/v1beta1 kind: ValidatingWebhookConfiguration metadata: name: \u0026#34;pod-policy.example.com\u0026#34; webhooks: - name: \u0026#34;pod-policy.example.com\u0026#34; rules: - apiGroups: [\u0026#34;\u0026#34;] apiVersions: [\u0026#34;v1\u0026#34;] operations: [\u0026#34;CREATE\u0026#34;] resources: [\u0026#34;pods\u0026#34;] scope: \u0026#34;Namespaced\u0026#34; clientConfig: service: namespace: \u0026#34;example-namespace\u0026#34; name: \u0026#34;example-service\u0026#34; caBundle: \u0026#34;Ci0tLS0tQk...\u0026lt;base64-encoded PEM bundle containing the CA that signed the webhook\u0026#39;s serving certificate\u0026gt;...tLS0K\u0026#34; admissionReviewVersions: [\u0026#34;v1beta1\u0026#34;] timeoutSeconds: 5 . /tab %}} . /tabs \u0026gt;}}\nscope 字段指定是仅集群范围的资源（Cluster）还是命名空间范围的资源资源（Namespaced）将与此规则匹配。* 表示没有范围限制。\n. note \u0026gt;}} 当使用 clientConfig.service 时，服务器证书必须对 \u0026lt;svc_name\u0026gt;.\u0026lt;svc_namespace\u0026gt;.svc 有效。 . /note \u0026gt;}}\n. note \u0026gt;}} 对于使用 admissionregistration.k8s.io/v1 创建的 webhook 而言，其 webhook 调用的默认超时是 10 秒；对于使用 admissionregistration.k8s.io/v1beta1 创建的 webhook 而言，其默认超时是 30 秒。从 kubernetes 1.14 开始，可以设置超时。建议对 webhooks 设置较短的超时时间。 如果 webhook 调用超时，则根据 webhook 的失败策略处理请求。 . /note \u0026gt;}}\n当 apiserver 收到与 rules 相匹配的请求时，apiserver 按照 clientConfig 中指定的方式向 webhook 发送一个 admissionReview 请求。\n创建 webhook 配置后，系统将花费几秒钟使新配置生效。\n对 apiservers 进行身份认证 如果您的 webhook 需要身份验证，则可以将 apiserver 配置为使用基本身份验证、持有者令牌或证书来向 webhook 提供身份证明。完成此配置需要三个步骤。\n  启动 apiserver 时，通过 --admission-control-config-file 参数指定准入控制配置文件的位置。\n  在准入控制配置文件中，指定 MutatingAdmissionWebhook 控制器和 ValidatingAdmissionWebhook 控制器应该读取凭据的位置。 凭证存储在 kubeConfig 文件中（是​​的，与 kubectl 使用的模式相同），因此字段名称为 kubeConfigFile。 以下是一个准入控制配置文件示例：\n  . tabs name=\u0026quot;admissionconfiguration_example1\u0026rdquo; \u0026gt;}} . tab name=\u0026quot;apiserver.config.k8s.io/v1\u0026rdquo; %}}\napiVersion: apiserver.config.k8s.io/v1 kind: AdmissionConfiguration plugins: - name: ValidatingAdmissionWebhook configuration: apiVersion: apiserver.config.k8s.io/v1 kind: WebhookAdmissionConfiguration kubeConfigFile: \u0026#34;\u0026lt;path-to-kubeconfig-file\u0026gt;\u0026#34; - name: MutatingAdmissionWebhook configuration: apiVersion: apiserver.config.k8s.io/v1 kind: WebhookAdmissionConfiguration kubeConfigFile: \u0026#34;\u0026lt;path-to-kubeconfig-file\u0026gt;\u0026#34; . /tab %}} . tab name=\u0026quot;apiserver.k8s.io/v1alpha1\u0026rdquo; %}}\n# 1.17 中被淘汰，推荐使用 apiserver.config.k8s.io/v1 apiVersion: apiserver.k8s.io/v1alpha1 kind: AdmissionConfiguration plugins: - name: ValidatingAdmissionWebhook configuration: # 1.17 中被淘汰，推荐使用 apiserver.config.k8s.io/v1，kind = WebhookAdmissionConfiguration apiVersion: apiserver.config.k8s.io/v1alpha1 kind: WebhookAdmission kubeConfigFile: \u0026#34;\u0026lt;path-to-kubeconfig-file\u0026gt;\u0026#34; - name: MutatingAdmissionWebhook configuration: # 1.17 中被淘汰，推荐使用 apiserver.config.k8s.io/v1，kind = WebhookAdmissionConfiguration apiVersion: apiserver.config.k8s.io/v1alpha1 kind: WebhookAdmission kubeConfigFile: \u0026#34;\u0026lt;path-to-kubeconfig-file\u0026gt;\u0026#34; . /tab %}} . /tabs \u0026gt;}}\n有关 AdmissionConfiguration 的更多信息，请参见 AdmissionConfiguration schema。 有关每个配置字段的详细信息，请参见 webhook 配置部分。\n  在 kubeConfig 文件中，提供证书凭据：\napiVersion: v1 kind: Config users: # 名称应设置为服务的 DNS 名称或配置了 Webhook 的 URL 的主机名（包括端口）。 # 如果将非 443 端口用于服务，则在配置 1.16+ API 服务器时，该端口必须包含在名称中。 # # 对于配置在默认端口（443）上与服务对话的 Webhook，请指定服务的 DNS 名称： # - name: webhook1.ns1.svc # user: ... # # 对于配置在非默认端口（例如 8443）上与服务对话的 Webhook，请在 1.16+ 中指定服务的 DNS 名称和端口： # - name: webhook1.ns1.svc:8443 # user: ... # 并可以选择仅使用服务的 DNS 名称来创建第二节，以与 1.15 API 服务器版本兼容： # - name: webhook1.ns1.svc # user: ... # # 对于配置为使用 URL 的 webhook，请匹配在 webhook 的 URL 中指定的主机（和端口）。 # 带有 `url: https://www.example.com` 的 webhook： # - name: www.example.com # user: ... # # 带有 `url: https://www.example.com:443` 的 webhook： # - name: www.example.com:443 # user: ... # # 带有 `url: https://www.example.com:8443` 的 webhook： # - name: www.example.com:8443 # user: ... # - name: \u0026#39;webhook1.ns1.svc\u0026#39; user: client-certificate-data: \u0026#34;\u0026lt;pem encoded certificate\u0026gt;\u0026#34; client-key-data: \u0026#34;\u0026lt;pem encoded key\u0026gt;\u0026#34; # `name` 支持使用 * 通配符匹配前缀段。 - name: \u0026#39;*.webhook-company.org\u0026#39; user: password: \u0026#34;\u0026lt;password\u0026gt;\u0026#34; username: \u0026#34;\u0026lt;name\u0026gt;\u0026#34; # \u0026#39;*\u0026#39; 是默认匹配项。 - name: \u0026#39;*\u0026#39; user: token: \u0026#34;\u0026lt;token\u0026gt;\u0026#34;   当然，您需要设置 webhook 服务器来处理这些身份验证。\n请求 向 Webhook 发送 POST 请求时，请设置 Content-Type: application/json 并对 admission.k8s.io API 组中的 AdmissionReview 对象进行序列化，将所得到的 JSON 作为请求的主体。\nWebhook 可以在配置中的 admissionReviewVersions 字段指定可接受的 AdmissionReview 对象版本：\n. tabs name=\u0026quot;ValidatingWebhookConfiguration_admissionReviewVersions\u0026rdquo; \u0026gt;}} . tab name=\u0026quot;admissionregistration.k8s.io/v1\u0026rdquo; %}}\napiVersion: admissionregistration.k8s.io/v1 kind: ValidatingWebhookConfiguration ... webhooks: - name: my-webhook.example.com admissionReviewVersions: [\u0026#34;v1\u0026#34;, \u0026#34;v1beta1\u0026#34;] ... 创建 admissionregistration.k8s.io/v1 webhook 配置时，admissionReviewVersions 是必填字段。 Webhook 必须支持至少一个当前和以前的 apiserver 都可以解析的 AdmissionReview 版本。 . /tab %}} . tab name=\u0026quot;admissionregistration.k8s.io/v1beta1\u0026rdquo; %}}\n# v1.16 中被淘汰，推荐使用 admissionregistration.k8s.io/v1 apiVersion: admissionregistration.k8s.io/v1beta1 kind: ValidatingWebhookConfiguration ... webhooks: - name: my-webhook.example.com admissionReviewVersions: [\u0026#34;v1beta1\u0026#34;] ... 如果未指定 admissionReviewVersions，则创建 admissionregistration.k8s.io/v1beta1 Webhook 配置时的默认值为 v1beta1。 . /tab %}} . /tabs \u0026gt;}}\nAPI 服务器将发送的是 admissionReviewVersions 列表中所支持的第一个 AdmissionReview 版本。如果 API 服务器不支持列表中的任何版本，则不允许创建配置。\n如果 API 服务器遇到以前创建的 Webhook 配置，并且不支持该 API 服务器知道如何发送的任何 AdmissionReview 版本，则调用 Webhook 的尝试将失败，并依据失败策略进行处理。\n此示例显示了 AdmissionReview 对象中包含的数据，该数据用于请求更新 apps/v1 Deployment 的 scale 子资源：\n. tabs name=\u0026quot;AdmissionReview_request\u0026rdquo; \u0026gt;}} . tab name=\u0026quot;admission.k8s.io/v1\u0026rdquo; %}}\n{ \u0026#34;apiVersion\u0026#34;: \u0026#34;admission.k8s.io/v1\u0026#34;, \u0026#34;kind\u0026#34;: \u0026#34;AdmissionReview\u0026#34;, \u0026#34;request\u0026#34;: { # 唯一标识此准入回调的随机 uid \u0026#34;uid\u0026#34;: \u0026#34;705ab4f5-6393-11e8-b7cc-42010a800002\u0026#34;, # 传入完全正确的 group/version/kind 对象 \u0026#34;kind\u0026#34;: {\u0026#34;group\u0026#34;:\u0026#34;autoscaling\u0026#34;,\u0026#34;version\u0026#34;:\u0026#34;v1\u0026#34;,\u0026#34;kind\u0026#34;:\u0026#34;Scale\u0026#34;}, # 修改 resource 的完全正确的的 group/version/kind \u0026#34;resource\u0026#34;: {\u0026#34;group\u0026#34;:\u0026#34;apps\u0026#34;,\u0026#34;version\u0026#34;:\u0026#34;v1\u0026#34;,\u0026#34;resource\u0026#34;:\u0026#34;deployments\u0026#34;}, # subResource（如果请求是针对 subResource 的） \u0026#34;subResource\u0026#34;: \u0026#34;scale\u0026#34;, # 在对 API 服务器的原始请求中，传入对象的标准 group/version/kind # 仅当 webhook 指定 `matchPolicy: Equivalent` 且将对 API 服务器的原始请求转换为 webhook 注册的版本时，这才与 `kind` 不同。 \u0026#34;requestKind\u0026#34;: {\u0026#34;group\u0026#34;:\u0026#34;autoscaling\u0026#34;,\u0026#34;version\u0026#34;:\u0026#34;v1\u0026#34;,\u0026#34;kind\u0026#34;:\u0026#34;Scale\u0026#34;}, # 在对 API 服务器的原始请求中正在修改的资源的标准 group/version/kind # 仅当 webhook 指定了 `matchPolicy：Equivalent` 并且将对 API 服务器的原始请求转换为 webhook 注册的版本时，这才与 `resource` 不同。 \u0026#34;requestResource\u0026#34;: {\u0026#34;group\u0026#34;:\u0026#34;apps\u0026#34;,\u0026#34;version\u0026#34;:\u0026#34;v1\u0026#34;,\u0026#34;resource\u0026#34;:\u0026#34;deployments\u0026#34;}, # subResource（如果请求是针对 subResource 的） # 仅当 webhook 指定了 `matchPolicy：Equivalent` 并且将对 API 服务器的原始请求转换为该 webhook 注册的版本时，这才与 `subResource` 不同。 \u0026#34;requestSubResource\u0026#34;: \u0026#34;scale\u0026#34;, # 被修改资源的名称 \u0026#34;name\u0026#34;: \u0026#34;my-deployment\u0026#34;, # 如果资源是属于命名空间（或者是命名空间对象），则这是被修改的资源的命名空间 \u0026#34;namespace\u0026#34;: \u0026#34;my-namespace\u0026#34;, # 操作可以是 CREATE、UPDATE、DELETE 或 CONNECT \u0026#34;operation\u0026#34;: \u0026#34;UPDATE\u0026#34;, \u0026#34;userInfo\u0026#34;: { # 向 API 服务器发出请求的经过身份验证的用户的用户名 \u0026#34;username\u0026#34;: \u0026#34;admin\u0026#34;, # 向 API 服务器发出请求的经过身份验证的用户的 UID \u0026#34;uid\u0026#34;: \u0026#34;014fbff9a07c\u0026#34;, # 向 API 服务器发出请求的经过身份验证的用户的组成员身份 \u0026#34;groups\u0026#34;: [\u0026#34;system:authenticated\u0026#34;,\u0026#34;my-admin-group\u0026#34;], # 向 API 服务器发出请求的用户相关的任意附加信息 # 该字段由 API 服务器身份验证层填充，并且如果 webhook 执行了任何 SubjectAccessReview 检查，则应将其包括在内。 \u0026#34;extra\u0026#34;: { \u0026#34;some-key\u0026#34;:[\u0026#34;some-value1\u0026#34;, \u0026#34;some-value2\u0026#34;] } }, # object 是被接纳的新对象。 # 对于 DELETE 操作，它为 null。 \u0026#34;object\u0026#34;: {\u0026#34;apiVersion\u0026#34;:\u0026#34;autoscaling/v1\u0026#34;,\u0026#34;kind\u0026#34;:\u0026#34;Scale\u0026#34;,...}, # oldObject 是现有对象。 # 对于 CREATE 和 CONNECT 操作，它为 null。 \u0026#34;oldObject\u0026#34;: {\u0026#34;apiVersion\u0026#34;:\u0026#34;autoscaling/v1\u0026#34;,\u0026#34;kind\u0026#34;:\u0026#34;Scale\u0026#34;,...}, # options 包含要接受的操作的选项，例如 meta.k8s.io/v CreateOptions、UpdateOptions 或 DeleteOptions。 # 对于 CONNECT 操作，它为 null。 \u0026#34;options\u0026#34;: {\u0026#34;apiVersion\u0026#34;:\u0026#34;meta.k8s.io/v1\u0026#34;,\u0026#34;kind\u0026#34;:\u0026#34;UpdateOptions\u0026#34;,...}, # dryRun 表示 API 请求正在以 `dryrun` 模式运行，并且将不会保留。 # 带有副作用的 Webhook 应该避免在 dryRun 为 true 时激活这些副作用。 # 有关更多详细信息，请参见 http://k8s.io/docs/reference/using-api/api-concepts/#make-a-dry-run-request \u0026#34;dryRun\u0026#34;: false } } . /tab %}} . tab name=\u0026quot;admission.k8s.io/v1beta1\u0026rdquo; %}}\n{ # v1.16 中被废弃，推荐使用 admission.k8s.io/v1 \u0026#34;apiVersion\u0026#34;: \u0026#34;admission.k8s.io/v1beta1\u0026#34;, \u0026#34;kind\u0026#34;: \u0026#34;AdmissionReview\u0026#34;, \u0026#34;request\u0026#34;: { # 唯一标识此准入回调的随机 uid \u0026#34;uid\u0026#34;: \u0026#34;705ab4f5-6393-11e8-b7cc-42010a800002\u0026#34;, # 传入完全正确的 group/version/kind 对象 \u0026#34;kind\u0026#34;: {\u0026#34;group\u0026#34;:\u0026#34;autoscaling\u0026#34;,\u0026#34;version\u0026#34;:\u0026#34;v1\u0026#34;,\u0026#34;kind\u0026#34;:\u0026#34;Scale\u0026#34;}, # 修改 resource 的完全正确的的 group/version/kind \u0026#34;resource\u0026#34;: {\u0026#34;group\u0026#34;:\u0026#34;apps\u0026#34;,\u0026#34;version\u0026#34;:\u0026#34;v1\u0026#34;,\u0026#34;resource\u0026#34;:\u0026#34;deployments\u0026#34;}, # subResource（如果请求是针对 subResource 的） \u0026#34;subResource\u0026#34;: \u0026#34;scale\u0026#34;, # 在对 API 服务器的原始请求中，传入对象的标准 group/version/kind。 # 仅当 Webhook 指定了 `matchPolicy：Equivalent` 并且将对 API 服务器的原始请求转换为该 Webhook 注册的版本时，这与 `kind` 不同。 # 仅由 v1.15+ API 服务器发送。 \u0026#34;requestKind\u0026#34;: {\u0026#34;group\u0026#34;:\u0026#34;autoscaling\u0026#34;,\u0026#34;version\u0026#34;:\u0026#34;v1\u0026#34;,\u0026#34;kind\u0026#34;:\u0026#34;Scale\u0026#34;}, # 在对 API 服务器的原始请求中正在修改的资源的标准 group/version/kind # 仅当 webhook 指定了 `matchPolicy：Equivalent` 并且将对 API 服务器的原始请求转换为 webhook 注册的版本时，这才与 `resource` 不同。 # 仅由 v1.15+ API 服务器发送。 \u0026#34;requestResource\u0026#34;: {\u0026#34;group\u0026#34;:\u0026#34;apps\u0026#34;,\u0026#34;version\u0026#34;:\u0026#34;v1\u0026#34;,\u0026#34;resource\u0026#34;:\u0026#34;deployments\u0026#34;}, # subResource（如果请求是针对 subResource 的） # 仅当 webhook 指定了 `matchPolicy：Equivalent` 并且将对 API 服务器的原始请求转换为该 webhook 注册的版本时，这才与 `subResource` 不同。 # 仅由 v1.15+ API 服务器发送。 \u0026#34;requestSubResource\u0026#34;: \u0026#34;scale\u0026#34;, # 被修改资源的名称 \u0026#34;name\u0026#34;: \u0026#34;my-deployment\u0026#34;, # 如果资源是属于命名空间（或者是命名空间对象），则这是被修改的资源的命名空间 \u0026#34;namespace\u0026#34;: \u0026#34;my-namespace\u0026#34;, # 操作可以是 CREATE、UPDATE、DELETE 或 CONNECT \u0026#34;operation\u0026#34;: \u0026#34;UPDATE\u0026#34;, \u0026#34;userInfo\u0026#34;: { # 向 API 服务器发出请求的经过身份验证的用户的用户名 \u0026#34;username\u0026#34;: \u0026#34;admin\u0026#34;, # 向 API 服务器发出请求的经过身份验证的用户的 UID \u0026#34;uid\u0026#34;: \u0026#34;014fbff9a07c\u0026#34;, # 向 API 服务器发出请求的经过身份验证的用户的组成员身份 \u0026#34;groups\u0026#34;: [\u0026#34;system:authenticated\u0026#34;,\u0026#34;my-admin-group\u0026#34;], # 向 API 服务器发出请求的用户相关的任意附加信息 # 该字段由 API 服务器身份验证层填充，并且如果 webhook 执行了任何 SubjectAccessReview 检查，则应将其包括在内。 \u0026#34;extra\u0026#34;: { \u0026#34;some-key\u0026#34;:[\u0026#34;some-value1\u0026#34;, \u0026#34;some-value2\u0026#34;] } }, # object 是被接纳的新对象。 # 对于 DELETE 操作，它为 null。 \u0026#34;object\u0026#34;: {\u0026#34;apiVersion\u0026#34;:\u0026#34;autoscaling/v1\u0026#34;,\u0026#34;kind\u0026#34;:\u0026#34;Scale\u0026#34;,...}, # oldObject 是现有对象。 # 对于 CREATE 和 CONNECT 操作（对于 v1.15.0 之前版本的 API 服务器中的 DELETE 操作），它为 null。 \u0026#34;oldObject\u0026#34;: {\u0026#34;apiVersion\u0026#34;:\u0026#34;autoscaling/v1\u0026#34;,\u0026#34;kind\u0026#34;:\u0026#34;Scale\u0026#34;,...}, # options 包含要接受的操作的选项，例如 meta.k8s.io/v CreateOptions、UpdateOptions 或 DeleteOptions。 # 对于 CONNECT 操作，它为 null。 # 仅由 v1.15+ API 服务器发送。 \u0026#34;options\u0026#34;: {\u0026#34;apiVersion\u0026#34;:\u0026#34;meta.k8s.io/v1\u0026#34;,\u0026#34;kind\u0026#34;:\u0026#34;UpdateOptions\u0026#34;,...}, # dryRun 表示 API 请求正在以 `dryrun` 模式运行，并且将不会保留。 # 带有副作用的 Webhook 应该避免在 dryRun 为 true 时激活这些副作用。 # 有关更多详细信息，请参见 http://k8s.io/docs/reference/using-api/api-concepts/#make-a-dry-run-request \u0026#34;dryRun\u0026#34;: false } } . /tab %}} . /tabs \u0026gt;}}\n响应 Webhook 使用 HTTP 200 状态码、Content-Type: application/json 和一个包含 AdmissionReview 对象的 JSON 序列化格式来发送响应。该 AdmissionReview 对象与发送的版本相同，且其中包含的 response 字段已被有效填充。\nresponse 至少必须包含以下字段：\n uid，从发送到 webhook 的 request.uid 中复制而来 allowed，设置为 true 或 false  Webhook 禁止请求的最简单响应示例：\n. tabs name=\u0026quot;AdmissionReview_response_allow\u0026rdquo; \u0026gt;}} . tab name=\u0026quot;admission.k8s.io/v1\u0026rdquo; %}}\n{ \u0026#34;apiVersion\u0026#34;: \u0026#34;admission.k8s.io/v1\u0026#34;, \u0026#34;kind\u0026#34;: \u0026#34;AdmissionReview\u0026#34;, \u0026#34;response\u0026#34;: { \u0026#34;uid\u0026#34;: \u0026#34;\u0026lt;value from request.uid\u0026gt;\u0026#34;, \u0026#34;allowed\u0026#34;: true } } . /tab %}} . tab name=\u0026quot;admission.k8s.io/v1beta1\u0026rdquo; %}}\n{ \u0026#34;apiVersion\u0026#34;: \u0026#34;admission.k8s.io/v1beta1\u0026#34;, \u0026#34;kind\u0026#34;: \u0026#34;AdmissionReview\u0026#34;, \u0026#34;response\u0026#34;: { \u0026#34;uid\u0026#34;: \u0026#34;\u0026lt;value from request.uid\u0026gt;\u0026#34;, \u0026#34;allowed\u0026#34;: true } } . /tab %}} . /tabs \u0026gt;}}\nWebhook 禁止请求的最简单响应示例：\n. tabs name=\u0026quot;AdmissionReview_response_forbid_minimal\u0026rdquo; \u0026gt;}} . tab name=\u0026quot;admission.k8s.io/v1\u0026rdquo; %}}\n{ \u0026#34;apiVersion\u0026#34;: \u0026#34;admission.k8s.io/v1\u0026#34;, \u0026#34;kind\u0026#34;: \u0026#34;AdmissionReview\u0026#34;, \u0026#34;response\u0026#34;: { \u0026#34;uid\u0026#34;: \u0026#34;\u0026lt;value from request.uid\u0026gt;\u0026#34;, \u0026#34;allowed\u0026#34;: false } } . /tab %}} . tab name=\u0026quot;admission.k8s.io/v1beta1\u0026rdquo; %}}\n{ \u0026#34;apiVersion\u0026#34;: \u0026#34;admission.k8s.io/v1beta1\u0026#34;, \u0026#34;kind\u0026#34;: \u0026#34;AdmissionReview\u0026#34;, \u0026#34;response\u0026#34;: { \u0026#34;uid\u0026#34;: \u0026#34;\u0026lt;value from request.uid\u0026gt;\u0026#34;, \u0026#34;allowed\u0026#34;: false } } . /tab %}} . /tabs \u0026gt;}}\n当拒绝请求时，webhook 可以使用 status 字段自定义 http 响应码和返回给用户的消息。 有关状态类型的详细信息，请参见 API 文档。 禁止请求的响应示例，它定制了向用户显示的 HTTP 状态码和消息：\n. tabs name=\u0026quot;AdmissionReview_response_forbid_details\u0026rdquo; \u0026gt;}} . tab name=\u0026quot;admission.k8s.io/v1\u0026rdquo; %}}\n{ \u0026#34;apiVersion\u0026#34;: \u0026#34;admission.k8s.io/v1\u0026#34;, \u0026#34;kind\u0026#34;: \u0026#34;AdmissionReview\u0026#34;, \u0026#34;response\u0026#34;: { \u0026#34;uid\u0026#34;: \u0026#34;\u0026lt;value from request.uid\u0026gt;\u0026#34;, \u0026#34;allowed\u0026#34;: false, \u0026#34;status\u0026#34;: { \u0026#34;code\u0026#34;: 403, \u0026#34;message\u0026#34;: \u0026#34;You cannot do this because it is Tuesday and your name starts with A\u0026#34; } } } . /tab %}} . tab name=\u0026quot;admission.k8s.io/v1beta1\u0026rdquo; %}}\n{ \u0026#34;apiVersion\u0026#34;: \u0026#34;admission.k8s.io/v1beta1\u0026#34;, \u0026#34;kind\u0026#34;: \u0026#34;AdmissionReview\u0026#34;, \u0026#34;response\u0026#34;: { \u0026#34;uid\u0026#34;: \u0026#34;\u0026lt;value from request.uid\u0026gt;\u0026#34;, \u0026#34;allowed\u0026#34;: false, \u0026#34;status\u0026#34;: { \u0026#34;code\u0026#34;: 403, \u0026#34;message\u0026#34;: \u0026#34;You cannot do this because it is Tuesday and your name starts with A\u0026#34; } } } . /tab %}} . /tabs \u0026gt;}}\n当允许请求时，mutating admission webhook 也可以选择修改传入的对象。 这是通过在响应中使用 patch 和 patchType 字段来完成的。 当前唯一支持的 patchType 是 JSONPatch。 有关更多详细信息，请参见 JSON patch。 对于 patchType: JSONPatch，patch 字段包含一个以 base64 编码的 JSON patch 操作数组。\n例如，设置 spec.replicas 的单个补丁操作将是 [{\u0026quot;op\u0026quot;: \u0026quot;add\u0026quot;, \u0026quot;path\u0026quot;: \u0026quot;/spec/replicas\u0026quot;, \u0026quot;value\u0026quot;: 3}]。\n如果以 Base64 形式编码，这将是 W3sib3AiOiAiYWRkIiwgInBhdGgiOiAiL3NwZWMvcmVwbGljYXMiLCAidmFsdWUiOiAzfV0=\n因此，添加该标签的 webhook 响应为： . tabs name=\u0026quot;AdmissionReview_response_modify\u0026rdquo; \u0026gt;}} . tab name=\u0026quot;admission.k8s.io/v1\u0026rdquo; %}}\n{ \u0026#34;apiVersion\u0026#34;: \u0026#34;admission.k8s.io/v1\u0026#34;, \u0026#34;kind\u0026#34;: \u0026#34;AdmissionReview\u0026#34;, \u0026#34;response\u0026#34;: { \u0026#34;uid\u0026#34;: \u0026#34;\u0026lt;value from request.uid\u0026gt;\u0026#34;, \u0026#34;allowed\u0026#34;: true, \u0026#34;patchType\u0026#34;: \u0026#34;JSONPatch\u0026#34;, \u0026#34;patch\u0026#34;: \u0026#34;W3sib3AiOiAiYWRkIiwgInBhdGgiOiAiL3NwZWMvcmVwbGljYXMiLCAidmFsdWUiOiAzfV0=\u0026#34; } } . /tab %}} . tab name=\u0026quot;admission.k8s.io/v1beta1\u0026rdquo; %}}\n{ \u0026#34;apiVersion\u0026#34;: \u0026#34;admission.k8s.io/v1beta1\u0026#34;, \u0026#34;kind\u0026#34;: \u0026#34;AdmissionReview\u0026#34;, \u0026#34;response\u0026#34;: { \u0026#34;uid\u0026#34;: \u0026#34;\u0026lt;value from request.uid\u0026gt;\u0026#34;, \u0026#34;allowed\u0026#34;: true, \u0026#34;patchType\u0026#34;: \u0026#34;JSONPatch\u0026#34;, \u0026#34;patch\u0026#34;: \u0026#34;W3sib3AiOiAiYWRkIiwgInBhdGgiOiAiL3NwZWMvcmVwbGljYXMiLCAidmFsdWUiOiAzfV0=\u0026#34; } } . /tab %}} . /tabs \u0026gt;}}\nwebhook 配置 要注册 admssion webhook，请创建 MutatingWebhookConfiguration 或 ValidatingWebhookConfiguration API 对象。\n每种配置可以包含一个或多个 webhook。如果在单个配置中指定了多个 webhook，则应为每个 webhook 赋予一个唯一的名称。 这在 admissionregistration.k8s.io/v1 中是必需的，但是在使用 admissionregistration.k8s.io/v1beta1 时强烈建议使用，以使生成的审核日志和指标更易于与活动配置相匹配。\n每个 webhook 定义以下内容。\n匹配请求-规则 每个 webhook 必须指定用于确定是否应将对 apiserver 的请求发送到 webhook 的规则列表。 每个规则都指定一个或多个 operations、apiGroups、apiVersions 和 resources 以及资源的 scope：\n operations 列出一个或多个要匹配的操作。可以是 CREATE、UPDATE、DELETE、CONNECT 或 * 以匹配所有内容。 apiGroups 列出了一个或多个要匹配的 API 组。\u0026quot;\u0026quot; 是核心 API 组。\u0026quot;*\u0026quot; 匹配所有 API 组。 apiVersions 列出了一个或多个要匹配的 API 版本。\u0026quot;*\u0026quot; 匹配所有 API 版本。 resources 列出了一个或多个要匹配的资源。  \u0026quot;*\u0026quot; 匹配所有资源，但不包括子资源。 \u0026quot;*/*\u0026quot; 匹配所有资源，包括子资源。 \u0026quot;pods/*\u0026quot; 匹配 pod 的所有子资源。 \u0026quot;*/status\u0026quot; 匹配所有 status 子资源。   scope 指定要匹配的范围。有效值为 \u0026quot;Cluster\u0026quot;、\u0026quot;Namespaced\u0026quot; 和 \u0026quot;*\u0026quot;。子资源匹配其父资源的范围。在 Kubernetes v1.14+ 版本中才被支持。默认值为 \u0026quot;*\u0026quot;，对应 1.14 版本之前的行为。  \u0026quot;Cluster\u0026quot; 表示只有集群作用域的资源才能匹配此规则（API 对象 Namespace 是集群作用域的）。 \u0026quot;Namespaced\u0026quot; 意味着仅具有命名空间的资源才符合此规则。 \u0026quot;*\u0026quot; 表示没有范围限制。    如果传入请求与任何 Webhook 规则的指定操作、组、版本、资源和范围匹配，则该请求将发送到 Webhook。\n以下是可用于指定应拦截哪些资源的规则的其他示例。\n匹配针对 apps/v1 和 apps/v1beta1 组中 deployments 和 replicasets 资源的 CREATE 或 UPDATE 请求：\n. tabs name=\u0026quot;ValidatingWebhookConfiguration_rules_1\u0026rdquo; \u0026gt;}} . tab name=\u0026quot;admissionregistration.k8s.io/v1\u0026rdquo; %}}\napiVersion: admissionregistration.k8s.io/v1 kind: ValidatingWebhookConfiguration ... webhooks: - name: my-webhook.example.com rules: - operations: [\u0026#34;CREATE\u0026#34;, \u0026#34;UPDATE\u0026#34;] apiGroups: [\u0026#34;apps\u0026#34;] apiVersions: [\u0026#34;v1\u0026#34;, \u0026#34;v1beta1\u0026#34;] resources: [\u0026#34;deployments\u0026#34;, \u0026#34;replicasets\u0026#34;] scope: \u0026#34;Namespaced\u0026#34; ... . /tab %}} . tab name=\u0026quot;admissionregistration.k8s.io/v1beta1\u0026rdquo; %}}\n# v1.16 中被废弃，推荐使用 admissionregistration.k8s.io/v1 apiVersion: admissionregistration.k8s.io/v1beta1 kind: ValidatingWebhookConfiguration ... webhooks: - name: my-webhook.example.com rules: - operations: [\u0026#34;CREATE\u0026#34;, \u0026#34;UPDATE\u0026#34;] apiGroups: [\u0026#34;apps\u0026#34;] apiVersions: [\u0026#34;v1\u0026#34;, \u0026#34;v1beta1\u0026#34;] resources: [\u0026#34;deployments\u0026#34;, \u0026#34;replicasets\u0026#34;] scope: \u0026#34;Namespaced\u0026#34; ... . /tab %}} . /tabs \u0026gt;}}\n匹配所有 API 组和版本中的所有资源（但不包括子资源）的创建请求：\n. tabs name=\u0026quot;ValidatingWebhookConfiguration_rules_2\u0026rdquo; \u0026gt;}} . tab name=\u0026quot;admissionregistration.k8s.io/v1\u0026rdquo; %}}\napiVersion: admissionregistration.k8s.io/v1 kind: ValidatingWebhookConfiguration ... webhooks: - name: my-webhook.example.com rules: - operations: [\u0026#34;CREATE\u0026#34;] apiGroups: [\u0026#34;*\u0026#34;] apiVersions: [\u0026#34;*\u0026#34;] resources: [\u0026#34;*\u0026#34;] scope: \u0026#34;*\u0026#34; ... . /tab %}} . tab name=\u0026quot;admissionregistration.k8s.io/v1beta1\u0026rdquo; %}}\n# v1.16 中被废弃，推荐使用 admissionregistration.k8s.io/v1 apiVersion: admissionregistration.k8s.io/v1beta1 kind: ValidatingWebhookConfiguration ... webhooks: - name: my-webhook.example.com rules: - operations: [\u0026#34;CREATE\u0026#34;] apiGroups: [\u0026#34;*\u0026#34;] apiVersions: [\u0026#34;*\u0026#34;] resources: [\u0026#34;*\u0026#34;] scope: \u0026#34;*\u0026#34; ... . /tab %}} . /tabs \u0026gt;}}\n匹配所有 API 组和版本中所有 status 子资源的更新请求：\n. tabs name=\u0026quot;ValidatingWebhookConfiguration_rules_2\u0026rdquo; \u0026gt;}} . tab name=\u0026quot;admissionregistration.k8s.io/v1\u0026rdquo; %}}\napiVersion: admissionregistration.k8s.io/v1 kind: ValidatingWebhookConfiguration ... webhooks: - name: my-webhook.example.com rules: - operations: [\u0026#34;CREATE\u0026#34;] apiGroups: [\u0026#34;*\u0026#34;] apiVersions: [\u0026#34;*\u0026#34;] resources: [\u0026#34;*\u0026#34;] scope: \u0026#34;*\u0026#34; ... . /tab %}} . tab name=\u0026quot;admissionregistration.k8s.io/v1beta1\u0026rdquo; %}}\n# v1.16 中被废弃，推荐使用 admissionregistration.k8s.io/v1 apiVersion: admissionregistration.k8s.io/v1beta1 kind: ValidatingWebhookConfiguration ... webhooks: - name: my-webhook.example.com rules: - operations: [\u0026#34;CREATE\u0026#34;] apiGroups: [\u0026#34;*\u0026#34;] apiVersions: [\u0026#34;*\u0026#34;] resources: [\u0026#34;*\u0026#34;] scope: \u0026#34;*\u0026#34; ... . /tab %}} . /tabs \u0026gt;}}\n匹配请求：objectSelector 在版本 v1.15+ 中, 通过指定 objectSelector，webhook 能够根据可能发送的对象的标签来限制哪些请求被拦截。如果指定，则将对 objectSelector 和可能发送到 webhook 的 object 和 oldObject 进行评估。如果两个对象之一与选择器匹配，则认为该请求已匹配。\n空对象（对于创建操作而言为 oldObject，对于删除操作而言为 newObject），或不能带标签的对象（例如 DeploymentRollback 或 PodProxyOptions 对象）被认为不匹配。\n仅当选择使用 webhook 时才使用对象选择器，因为最终用户可以通过设置标签来跳过 admission webhook。\n这个例子展示了一个 mutating webhook，它将匹配带有标签 foo:bar 的任何资源的 CREATE 的操作：\n. tabs name=\u0026quot;objectSelector_example\u0026rdquo; \u0026gt;}} . tab name=\u0026quot;admissionregistration.k8s.io/v1\u0026rdquo; %}}\napiVersion: admissionregistration.k8s.io/v1 kind: MutatingWebhookConfiguration ... webhooks: - name: my-webhook.example.com objectSelector: matchLabels: foo: bar rules: - operations: [\u0026#34;CREATE\u0026#34;] apiGroups: [\u0026#34;*\u0026#34;] apiVersions: [\u0026#34;*\u0026#34;] resources: [\u0026#34;*\u0026#34;] scope: \u0026#34;*\u0026#34; ... . /tab %}} . tab name=\u0026quot;admissionregistration.k8s.io/v1beta1\u0026rdquo; %}}\n# v1.16 中被废弃，推荐使用 admissionregistration.k8s.io/v1 apiVersion: admissionregistration.k8s.io/v1beta1 kind: MutatingWebhookConfiguration ... webhooks: - name: my-webhook.example.com objectSelector: matchLabels: foo: bar rules: - operations: [\u0026#34;CREATE\u0026#34;] apiGroups: [\u0026#34;*\u0026#34;] apiVersions: [\u0026#34;*\u0026#34;] resources: [\u0026#34;*\u0026#34;] scope: \u0026#34;*\u0026#34; ... . /tab %}} . /tabs \u0026gt;}}\n有关标签选择器的更多示例，请参见标签。\n匹配请求：namespaceSelector 通过指定 namespaceSelector，Webhook 可以根据具有命名空间的资源所处的命名空间的标签来选择拦截哪些资源的操作。\nnamespaceSelector 根据命名空间的标签是否匹配选择器，决定是否针对具命名空间的资源（或 Namespace 对象）的请求运行 webhook。 如果对象是除 Namespace 以外的集群范围的资源，则 namespaceSelector 标签无效。\n本例给出的 mutating webhook 将匹配到对命名空间中具命名空间的资源的 CREATE 请求，前提是这些资源不含值为 \u0026ldquo;0\u0026rdquo; 或 \u0026ldquo;1\u0026rdquo; 的 \u0026ldquo;runlevel\u0026rdquo; 标签：\n. tabs name=\u0026quot;MutatingWebhookConfiguration_namespaceSelector_1\u0026rdquo; \u0026gt;}} . tab name=\u0026quot;admissionregistration.k8s.io/v1\u0026rdquo; %}}\napiVersion: admissionregistration.k8s.io/v1 kind: MutatingWebhookConfiguration ... webhooks: - name: my-webhook.example.com namespaceSelector: matchExpressions: - key: runlevel operator: NotIn values: [\u0026#34;0\u0026#34;,\u0026#34;1\u0026#34;] rules: - operations: [\u0026#34;CREATE\u0026#34;] apiGroups: [\u0026#34;*\u0026#34;] apiVersions: [\u0026#34;*\u0026#34;] resources: [\u0026#34;*\u0026#34;] scope: \u0026#34;Namespaced\u0026#34; ... . /tab %}} . tab name=\u0026quot;admissionregistration.k8s.io/v1beta1\u0026rdquo; %}}\n# v1.16 中被废弃，推荐使用 admissionregistration.k8s.io/v1 apiVersion: admissionregistration.k8s.io/v1beta1 kind: MutatingWebhookConfiguration ... webhooks: - name: my-webhook.example.com namespaceSelector: matchExpressions: - key: runlevel operator: NotIn values: [\u0026#34;0\u0026#34;,\u0026#34;1\u0026#34;] rules: - operations: [\u0026#34;CREATE\u0026#34;] apiGroups: [\u0026#34;*\u0026#34;] apiVersions: [\u0026#34;*\u0026#34;] resources: [\u0026#34;*\u0026#34;] scope: \u0026#34;Namespaced\u0026#34; ... . /tab %}} . /tabs \u0026gt;}}\n此示例显示了一个 validating webhook，它将匹配到对某命名空间中的任何具命名空间的资源的 CREATE 请求，前提是该命名空间具有值为 \u0026ldquo;prod\u0026rdquo; 或 \u0026ldquo;staging\u0026rdquo; 的 \u0026ldquo;environment\u0026rdquo; 标签： . tabs name=\u0026quot;ValidatingWebhookConfiguration_namespaceSelector_2\u0026rdquo; \u0026gt;}} . tab name=\u0026quot;admissionregistration.k8s.io/v1\u0026rdquo; %}}\napiVersion: admissionregistration.k8s.io/v1 kind: ValidatingWebhookConfiguration ... webhooks: - name: my-webhook.example.com namespaceSelector: matchExpressions: - key: environment operator: In values: [\u0026#34;prod\u0026#34;,\u0026#34;staging\u0026#34;] rules: - operations: [\u0026#34;CREATE\u0026#34;] apiGroups: [\u0026#34;*\u0026#34;] apiVersions: [\u0026#34;*\u0026#34;] resources: [\u0026#34;*\u0026#34;] scope: \u0026#34;Namespaced\u0026#34; ... . /tab %}} . tab name=\u0026quot;admissionregistration.k8s.io/v1beta1\u0026rdquo; %}}\n# v1.16 中被废弃，推荐使用 admissionregistration.k8s.io/v1 apiVersion: admissionregistration.k8s.io/v1beta1 kind: ValidatingWebhookConfiguration ... webhooks: - name: my-webhook.example.com namespaceSelector: matchExpressions: - key: environment operator: In values: [\u0026#34;prod\u0026#34;,\u0026#34;staging\u0026#34;] rules: - operations: [\u0026#34;CREATE\u0026#34;] apiGroups: [\u0026#34;*\u0026#34;] apiVersions: [\u0026#34;*\u0026#34;] resources: [\u0026#34;*\u0026#34;] scope: \u0026#34;Namespaced\u0026#34; ... . /tab %}} . /tabs \u0026gt;}}\n有关标签选择器的更多示例，请参见标签。\n匹配请求：matchPolicy API 服务器可以通过多个 API 组或版本来提供对象。 例如，Kubernetes API 服务器允许通过 extensions/v1beta1、apps/v1beta1、apps/v1beta2 和 apps/v1 API 创建和修改 Deployment 对象。\n例如，如果一个 webhook 仅为某些 API 组/版本指定了规则（例如 apiGroups:[\u0026quot;apps\u0026quot;], apiVersions:[\u0026quot;v1\u0026quot;,\u0026quot;v1beta1\u0026quot;]），而修改资源的请求是通过另一个 API 组/版本（例如 extensions/v1beta1）发出的，该请求将不会被发送到 Webhook。\n在 v1.15+ 中，matchPolicy 允许 webhook 定义如何使用其 rules 匹配传入的请求。 允许的值为 Exact 或 Equivalent。\n Exact 表示仅当请求与指定规则完全匹配时才应拦截该请求。 Equivalent 表示如果某个请求意在修改 rules 中列出的资源，即使该请求是通过其他 API 组或版本发起，也应拦截该请求。  在上面给出的示例中，仅为 apps/v1 注册的 webhook 可以使用 matchPolicy：\n matchPolicy: Exact 表示不会将 extensions/v1beta1 请求发送到 Webhook matchPolicy:Equivalent 表示将 extensions/v1beta1 请求发送到 webhook（将对象转换为 webhook 指定的版本：apps/v1）  建议指定 Equivalent，确保升级后启用 API 服务器中资源的新版本时，webhook 继续拦截他们期望的资源。\n当 API 服务器停止提供某资源时，该资源不再被视为等同于该资源的其他仍在提供服务的版本。 例如，extensions/v1beta1 中的 Deployment 已被废弃，计划在 v1.16 中默认停止使用。 在这种情况下，带有 apiGroups:[\u0026quot;extensions\u0026quot;], apiVersions:[\u0026quot;v1beta1\u0026quot;], resources: [\u0026quot;deployments\u0026quot;] 规则的 webhook 将不再拦截通过 apps/v1 API 来创建 Deployment 的请求。 [\u0026ldquo;deployments\u0026rdquo;] 规则将不再拦截通过 apps/v1 API 创建的部署。\n此示例显示了一个 validating webhook，该 Webhook 拦截对 Deployment 的修改（无论 API 组或版本是什么）， 始终会发送一个 apps/v1 版本的 Deployment 对象：\n. tabs name=\u0026quot;ValidatingWebhookConfiguration_matchPolicy\u0026rdquo; \u0026gt;}} . tab name=\u0026quot;admissionregistration.k8s.io/v1\u0026rdquo; %}}\napiVersion: admissionregistration.k8s.io/v1 kind: ValidatingWebhookConfiguration ... webhooks: - name: my-webhook.example.com matchPolicy: Equivalent rules: - operations: [\u0026#34;CREATE\u0026#34;,\u0026#34;UPDATE\u0026#34;,\u0026#34;DELETE\u0026#34;] apiGroups: [\u0026#34;apps\u0026#34;] apiVersions: [\u0026#34;v1\u0026#34;] resources: [\u0026#34;deployments\u0026#34;] scope: \u0026#34;Namespaced\u0026#34; ... 使用 admissionregistration.k8s.io/v1 创建的 admission webhhok 默认为 Equivalent。\n. /tab %}} . tab name=\u0026quot;admissionregistration.k8s.io/v1beta1\u0026rdquo; %}}\n# v1.16 中被废弃，推荐使用 admissionregistration.k8s.io/v1 apiVersion: admissionregistration.k8s.io/v1beta1 kind: ValidatingWebhookConfiguration ... webhooks: - name: my-webhook.example.com matchPolicy: Equivalent rules: - operations: [\u0026#34;CREATE\u0026#34;,\u0026#34;UPDATE\u0026#34;,\u0026#34;DELETE\u0026#34;] apiGroups: [\u0026#34;apps\u0026#34;] apiVersions: [\u0026#34;v1\u0026#34;] resources: [\u0026#34;deployments\u0026#34;] scope: \u0026#34;Namespaced\u0026#34; ... 使用 admissionregistration.k8s.io/v1beta1 创建的 admission webhhok 默认为 Exact。 . /tab %}} . /tabs \u0026gt;}}\n调用 Webhook API 服务器确定请求应发送到 webhook 后，它需要知道如何调用 webhook。此信息在 webhook 配置的 clientConfig 节中指定。\nWebhook 可以通过 URL 或服务引用来调用，并且可以选择包含自定义 CA 包，以用于验证 TLS 连接。\nURL url 以标准 URL 形式给出 webhook 的位置（scheme://host:port/path）。\nhost 不应引用集群中运行的服务；通过指定 service 字段来使用服务引用。 主机可以通过某些 apiserver 中的外部 DNS 进行解析。 （例如，kube-apiserver 无法解析集群内 DNS，因为这将违反分层规则）。host 也可以是 IP 地址。\n请注意，将 localhost 或 127.0.0.1 用作 host 是有风险的，除非您非常小心地在所有运行 apiserver 的、可能需要对此 webhook 进行调用的主机上运行。这样的安装可能不具有可移植性，即很难在新集群中启用。\nscheme 必须为 \u0026ldquo;https\u0026rdquo;；URL 必须以 \u0026ldquo;https://\u0026rdquo; 开头。\n使用用户或基本身份验证（例如：\u0026ldquo;user:password@\u0026quot;）是不允许的。 使用片段（\u0026quot;#\u0026hellip;\u0026quot;）和查询参数（\u0026rdquo;?\u0026hellip;\u0026quot;）也是不允许的。\n这是配置为调用 URL 的 mutating Webhook 的示例（并且期望使用系统信任根证书来验证 TLS 证书，因此不指定 caBundle）：\n. tabs name=\u0026quot;MutatingWebhookConfiguration_url\u0026rdquo; \u0026gt;}} . tab name=\u0026quot;admissionregistration.k8s.io/v1\u0026rdquo; %}}\napiVersion: admissionregistration.k8s.io/v1 kind: MutatingWebhookConfiguration ... webhooks: - name: my-webhook.example.com clientConfig: url: \u0026#34;https://my-webhook.example.com:9443/my-webhook-path\u0026#34; ... . /tab %}} . tab name=\u0026quot;admissionregistration.k8s.io/v1beta1\u0026rdquo; %}}\n# v1.16 中被废弃，推荐使用 admissionregistration.k8s.io/v1 apiVersion: admissionregistration.k8s.io/v1beta1 kind: MutatingWebhookConfiguration ... webhooks: - name: my-webhook.example.com clientConfig: url: \u0026#34;https://my-webhook.example.com:9443/my-webhook-path\u0026#34; ... . /tab %}} . /tabs \u0026gt;}}\n服务引用  clientConfig 内部的 Service 是对该 Webhook 服务的引用。如果 Webhook 在集群中运行，则应使用 service 而不是 url。服务的 namespace 和 name 是必需的。port 是可选的，默认值为 443。path 是可选的，默认为 \u0026ldquo;/\u0026quot;。\n这是一个 mutating Webhook 的示例，该 mutating Webhook 配置为在子路径 \u0026ldquo;/my-path\u0026rdquo; 端口 \u0026ldquo;1234\u0026rdquo; 上调用服务，并使用自定义 CA 包针对 ServerName my-service-name.my-service-namespace.svc 验证 TLS 连接：\n. tabs name=\u0026quot;MutatingWebhookConfiguration_service\u0026rdquo; \u0026gt;}} . tab name=\u0026quot;admissionregistration.k8s.io/v1\u0026rdquo; %}}\napiVersion: admissionregistration.k8s.io/v1 kind: MutatingWebhookConfiguration ... webhooks: - name: my-webhook.example.com clientConfig: caBundle: \u0026#34;Ci0tLS0tQk...\u0026lt;base64-encoded PEM bundle containing the CA that signed the webhook\u0026#39;s serving certificate\u0026gt;...tLS0K\u0026#34; service: namespace: my-service-namespace name: my-service-name path: /my-path port: 1234 ... . /tab %}} . tab name=\u0026quot;admissionregistration.k8s.io/v1beta1\u0026rdquo; %}}\n# v1.16 中被废弃，推荐使用 admissionregistration.k8s.io/v1 apiVersion: admissionregistration.k8s.io/v1beta1 kind: MutatingWebhookConfiguration ... webhooks: - name: my-webhook.example.com clientConfig: caBundle: \u0026#34;Ci0tLS0tQk...\u0026lt;base64-encoded PEM bundle containing the CA that signed the webhook\u0026#39;s serving certificate\u0026gt;...tLS0K\u0026#34; service: namespace: my-service-namespace name: my-service-name path: /my-path port: 1234 ... . /tab %}} . /tabs \u0026gt;}}\n副作用 Webhook 通常仅对发送给他们的 AdmissionReview 内容进行操作。但是，某些 Webhook 在处理 admission 请求时会进行带外更改。\n进行带外更改的（产生“副作用”的） Webhook 必须具有协调机制（如控制器），该机制定期确定事物的实际状态，并调整由 admission Webhook 修改的带外数据以反映现实情况。 这是因为对 admission Webhook 的调用不能保证所准入的对象将原样保留，或根本不保留。 以后，webhook 可以修改对象的内容，在写入存储时可能会发生冲突，或者服务器可以在持久保存对象之前关闭电源。\n此外，处理 dryRun: true admission 请求时，具有副作用的 webhook 必须避免产生副作用。 一个 webhook 必须明确指出在使用 dryRun 运行时不会有副作用，否则 dry-run 请求将不会发送到该 webhook，而 API 请求将会失败。\nWebhook 使用 webhook 配置中的 sideEffects 字段显示它们是否有副作用：\n Unknown：有关调用 Webhook 的副作用的信息是不可知的。 如果带有 dryRun：true 的请求将触发对该 Webhook 的调用，则该请求将失败，并且不会调用该 Webhook。 None：调用 webhook 没有副作用。 Some：调用 webhook 可能会有副作用。 如果请求具有 dry-run 属性将触发对此 Webhook 的调用，则该请求将会失败，并且不会调用该 Webhook。 NoneOnDryRun：调用 webhook 可能会有副作用，但是如果将带有 dryRun: true 属性的请求发送到 webhook，则 webhook 将抑制副作用（该 webhook 可识别 dryRun）。  允许值：\n 在 admissionregistration.k8s.io/v1beta1 中，sideEffects 可以设置为 Unknown、None、Some 或者 NoneOnDryRun，并且默认值为 Unknown。 在 admissionregistration.k8s.io/v1 中, sideEffects 必须设置为 None 或者 NoneOnDryRun。  这是一个 validating webhook 的示例，表明它对 dryRun: true 请求没有副作用：\n. tabs name=\u0026quot;ValidatingWebhookConfiguration_sideEffects\u0026rdquo; \u0026gt;}} . tab name=\u0026quot;admissionregistration.k8s.io/v1\u0026rdquo; %}}\napiVersion: admissionregistration.k8s.io/v1 kind: ValidatingWebhookConfiguration ... webhooks: - name: my-webhook.example.com sideEffects: NoneOnDryRun ... . /tab %}} . tab name=\u0026quot;admissionregistration.k8s.io/v1beta1\u0026rdquo; %}}\n# v1.16 中被废弃，推荐使用 admissionregistration.k8s.io/v1 apiVersion: admissionregistration.k8s.io/v1beta1 kind: ValidatingWebhookConfiguration ... webhooks: - name: my-webhook.example.com sideEffects: NoneOnDryRun ... . /tab %}} . /tabs \u0026gt;}}\n超时 由于 Webhook 会增加 API 请求的延迟，因此应尽快完成自身的操作。 timeoutSeconds 用来配置在将调用视为失败之前，允许 API 服务器等待 Webhook 响应的时间长度。\n如果超时在 Webhook 响应之前被触发，则基于失败策略，将忽略 Webhook 调用或拒绝 API 调用。\n超时值必须设置在 1 到 30 秒之间。\n这是一个自定义超时设置为 2 秒的 validating Webhook 的示例：\n. tabs name=\u0026quot;ValidatingWebhookConfiguration_timeoutSeconds\u0026rdquo; \u0026gt;}} . tab name=\u0026quot;admissionregistration.k8s.io/v1\u0026rdquo; %}}\napiVersion: admissionregistration.k8s.io/v1 kind: ValidatingWebhookConfiguration ... webhooks: - name: my-webhook.example.com timeoutSeconds: 2 ... 使用 admissionregistration.k8s.io/v1 创建的 admission webhook 默认超时为 10 秒。 . /tab %}} . tab name=\u0026quot;admissionregistration.k8s.io/v1beta1\u0026rdquo; %}}\n# v1.16 中被废弃，推荐使用 admissionregistration.k8s.io/v1 apiVersion: admissionregistration.k8s.io/v1beta1 kind: ValidatingWebhookConfiguration ... webhooks: - name: my-webhook.example.com timeoutSeconds: 2 ... 使用 admissionregistration.k8s.io/v1beta1 创建的 admission webhook 默认超时为 30 秒。 . /tab %}} . /tabs \u0026gt;}}\n再调用策略  mutating 准入插件（包括 Webhook）的任何一种排序方式都不会适用于所有情况。(参见 https://issue.k8s.io/64333 示例)。mutating Webhook 可以向对象中添加新的子结构（例如向 pod 中添加 container），已经运行的其他 mutating 插件可能会对这些新结构有影响（就像在所有容器上设置 imagePullPolicy 一样）。\n在 v1.15+ 中，允许 mutating 准入插件感应到其他插件所做的更改，如果 mutating webhook 修改了一个对象，则会重新运行内置的 mutating 准入插件，并且 mutating webhook 可以指定 reinvocationPolicy 来控制是否也重新调用它们。\n可以将 reinvocationPolicy 设置为 Never 或 IfNeeded。 默认为 Never。\n Never: 在一次准入测试中，不得多次调用 Webhook。 IfNeeded: 如果在最初的 webhook 调用之后被其他对象的插件修改了被接纳的对象，则可以作为准入测试的一部分再次调用该 webhook。  要注意的重要因素有：\n 不能保证附加调用的次数恰好是一。 如果其他调用导致对该对象的进一步修改，则不能保证再次调用 webhook。 使用此选项的 Webhook 可能会重新排序，以最大程度地减少额外调用的次数。 要在确保所有 mutation 都完成后验证对象，请改用 validating admission webhook（推荐用于有副作用的 webhook）。  这是一个 mutating webhook 的示例，该 Webhook 在以后的准入插件修改对象时被重新调用：\n. tabs name=\u0026quot;MutatingWebhookConfiguration_reinvocationPolicy\u0026rdquo; \u0026gt;}} . tab name=\u0026quot;admissionregistration.k8s.io/v1\u0026rdquo; %}}\napiVersion: admissionregistration.k8s.io/v1 kind: MutatingWebhookConfiguration ... webhooks: - name: my-webhook.example.com reinvocationPolicy: IfNeeded ... . /tab %}} . tab name=\u0026quot;admissionregistration.k8s.io/v1beta1\u0026rdquo; %}}\n# v1.16 中被废弃，推荐使用 admissionregistration.k8s.io/v1 apiVersion: admissionregistration.k8s.io/v1beta1 kind: MutatingWebhookConfiguration ... webhooks: - name: my-webhook.example.com reinvocationPolicy: IfNeeded ... . /tab %}} . /tabs \u0026gt;}}\nmutating webhook 必须具有 幂等 性，并且能够成功处理已被接纳并可能被修改的对象的 mutating Web 钩子。对于所有 mutating admission webhook 都是如此，因为它们可以在对象中进行的任何更改可能已经存在于用户提供的对象中，但是对于选择重新调用的 webhook 来说是必不可少的。\n失败策略 failurePolicy 定义了如何处理 admission webhook 中无法识别的错误和超时错误。允许的值为 Ignore 或 Fail。\n Ignore 表示调用 webhook 的错误将被忽略并且允许 API 请求继续。 Fail 表示调用 webhook 的错误导致准入失败并且 API 请求被拒绝。  这是一个 mutating webhook，配置为在调用准入 Webhook 遇到错误时拒绝 API 请求：\n. tabs name=\u0026quot;MutatingWebhookConfiguration_failurePolicy\u0026rdquo; \u0026gt;}} . tab name=\u0026quot;admissionregistration.k8s.io/v1\u0026rdquo; %}}\napiVersion: admissionregistration.k8s.io/v1 kind: MutatingWebhookConfiguration ... webhooks: - name: my-webhook.example.com failurePolicy: Fail ... 使用 admissionregistration.k8s.io/v1beta1 创建的 admission webhook 将 failurePolicy 默认设置为 Ignore。\n. /tab %}} . tab name=\u0026quot;admissionregistration.k8s.io/v1beta1\u0026rdquo; %}}\n# v1.16 中被废弃，推荐使用 admissionregistration.k8s.io/v1 apiVersion: admissionregistration.k8s.io/v1beta1 kind: MutatingWebhookConfiguration ... webhooks: - name: my-webhook.example.com failurePolicy: Fail ... 使用 admissionregistration.k8s.io/v1beta1 创建的 admission webhook 将 failurePolicy 默认设置为 Ignore。 . /tab %}} . /tabs \u0026gt;}}\n监控 Admission Webhook  API 服务器提供了监视 admission Webhook 行为的方法。这些监视机制可帮助集群管理员回答以下问题：\n  哪个 mutating webhook 改变了 API 请求中的对象？\n  mutating webhook 对对象做了哪些更改？\n  哪些 webhook 经常拒绝 API 请求？是什么原因拒绝？\n  Mutating Webhook 审计注解 有时，了解 API 请求中的哪个 mutating Webhook 使对象改变以及该 Webhook 应用了哪些更改很有用。\n在 v1.16+ 中，kube-apiserver 针对每个 mutating webhook 调用执行审计操作。 每个调用都会生成一个审计注解，记述请求对象是否发生改变，可选地还可以根据 webhook 的准入响应生成一个注解，记述所应用的修补。 针对给定请求的给定执行阶段，注解被添加到审计事件中，然后根据特定策略进行预处理并写入后端。\n事件的审计级别决定了要记录哪些注解：\n在 Metadata 或更高审计级别上，将使用 JSON 负载记录带有键名 mutation.webhook.admission.k8s.io/round_{round idx}_index_{order idx} 的注解， 该注解表示针对给定请求调用了 Webhook，以及该 Webhook 是否更改了对象。\n例如，对于正在被重新调用的某 Webhook，所记录的注解如下。 Webhook 在 mutating Webhook 链中排在第三个位置，并且在调用期间未改变请求对象。\n# 审计事件相关记录 { \u0026#34;kind\u0026#34;: \u0026#34;Event\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;audit.k8s.io/v1\u0026#34;, \u0026#34;annotations\u0026#34;: { \u0026#34;mutation.webhook.admission.k8s.io/round_1_index_2\u0026#34;: \u0026#34;{\\\u0026#34;configuration\\\u0026#34;:\\\u0026#34;my-mutating-webhook-configuration.example.com\\\u0026#34;,\\\u0026#34;webhook\\\u0026#34;:\\\u0026#34;my-webhook.example.com\\\u0026#34;,\\\u0026#34;mutated\\\u0026#34;: false}\u0026#34; # 其他注解 ... } # 其他字段 ... } # 反序列化的注解值 { \u0026#34;configuration\u0026#34;: \u0026#34;my-mutating-webhook-configuration.example.com\u0026#34;, \u0026#34;webhook\u0026#34;: \u0026#34;my-webhook.example.com\u0026#34;, \u0026#34;mutated\u0026#34;: false } 对于在第一轮中调用的 Webhook，所记录的注解如下。 Webhook 在 mutating Webhook 链中排在第一位，并在调用期间改变了请求对象。\n# 审计事件相关记录 { \u0026#34;kind\u0026#34;: \u0026#34;Event\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;audit.k8s.io/v1\u0026#34;, \u0026#34;annotations\u0026#34;: { \u0026#34;mutation.webhook.admission.k8s.io/round_0_index_0\u0026#34;: \u0026#34;{\\\u0026#34;configuration\\\u0026#34;:\\\u0026#34;my-mutating-webhook-configuration.example.com\\\u0026#34;,\\\u0026#34;webhook\\\u0026#34;:\\\u0026#34;my-webhook-always-mutate.example.com\\\u0026#34;,\\\u0026#34;mutated\\\u0026#34;: true}\u0026#34; # 其他注解 ... } # 其他字段 ... } # 反序列化的注解值 { \u0026#34;configuration\u0026#34;: \u0026#34;my-mutating-webhook-configuration.example.com\u0026#34;, \u0026#34;webhook\u0026#34;: \u0026#34;my-webhook-always-mutate.example.com\u0026#34;, \u0026#34;mutated\u0026#34;: true } 在 Request 或更高审计级别上，将使用 JSON 负载记录带有键名为 patch.webhook.admission.k8s.io/round_{round idx}_index_{order idx} 的注解， 该注解表明针对给定请求调用了 Webhook 以及应用于请求对象之上的修改。\n例如，以下是针对正在被重新调用的某 Webhook 所记录的注解。 Webhook 在 mutating Webhook 链中排在第四，并在其响应中包含一个 JSON 补丁，该补丁已被应用于请求对象。\n# 审计事件相关记录 { \u0026#34;kind\u0026#34;: \u0026#34;Event\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;audit.k8s.io/v1\u0026#34;, \u0026#34;annotations\u0026#34;: { \u0026#34;patch.webhook.admission.k8s.io/round_1_index_3\u0026#34;: \u0026#34;{\\\u0026#34;configuration\\\u0026#34;:\\\u0026#34;my-other-mutating-webhook-configuration.example.com\\\u0026#34;,\\\u0026#34;webhook\\\u0026#34;:\\\u0026#34;my-webhook-always-mutate.example.com\\\u0026#34;,\\\u0026#34;patch\\\u0026#34;:[{\\\u0026#34;op\\\u0026#34;:\\\u0026#34;add\\\u0026#34;,\\\u0026#34;path\\\u0026#34;:\\\u0026#34;/data/mutation-stage\\\u0026#34;,\\\u0026#34;value\\\u0026#34;:\\\u0026#34;yes\\\u0026#34;}],\\\u0026#34;patchType\\\u0026#34;:\\\u0026#34;JSONPatch\\\u0026#34;}\u0026#34; # 其他注解 ... } # 其他字段 ... } # 反序列化的注解值 { \u0026#34;configuration\u0026#34;: \u0026#34;my-other-mutating-webhook-configuration.example.com\u0026#34;, \u0026#34;webhook\u0026#34;: \u0026#34;my-webhook-always-mutate.example.com\u0026#34;, \u0026#34;patchType\u0026#34;: \u0026#34;JSONPatch\u0026#34;, \u0026#34;patch\u0026#34;: [ { \u0026#34;op\u0026#34;: \u0026#34;add\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/data/mutation-stage\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;yes\u0026#34; } ] } Amission webhook metrics Kube-apiserver 从 /metrics 端点公开 Prometheus 指标，这些指标可用于监控和诊断 apiserver 状态。以下指标记录了与 admission Webhook 相关的状态。\napiserver Admission Webhook rejection count 有时，了解哪些 admission webhook 经常拒绝 API 请求以及拒绝的原因是很有用的。\n在 v1.16+ 中，kube-apiserver 提供了 Prometheus 计数器度量值，记录了 admission Webhook 的拒绝次数。 度量值的标签给出了 Webhook 拒绝该请求的原因：\n name：拒绝请求 Webhook 的名称。 operation：请求的操作类型可以是 CREATE、UPDATE、DELETE 和 CONNECT 其中之一。 type：Admission webhook 类型，可以是 admit 和 validating 其中之一。 error_type：标识在 webhook 调用期间是否发生了错误并且导致了拒绝。其值可以是以下之一：  calling_webhook_error：发生了来自 Admission Webhook 的无法识别的错误或超时错误，并且 webhook 的 失败策略 设置为 Fail。 no_error：未发生错误。Webhook 在准入响应中以 allowed: false 值拒绝了请求。度量标签 rejection_code 记录了在准入响应中设置的 .status.code。 apiserver_internal_error：apiserver 发生内部错误。   rejection_code：当 Webhook 拒绝请求时，在准入响应中设置的 HTTP 状态码。  拒绝计数指标示例：\n# HELP apiserver_admission_webhook_rejection_count [ALPHA] Admission webhook rejection count, identified by name and broken out for each admission type (validating or admit) and operation. Additional labels specify an error type (calling_webhook_error or apiserver_internal_error if an error occurred; no_error otherwise) and optionally a non-zero rejection code if the webhook rejects the request with an HTTP status code (honored by the apiserver when the code is greater or equal to 400). Codes greater than 600 are truncated to 600, to keep the metrics cardinality bounded. # TYPE apiserver_admission_webhook_rejection_count counter apiserver_admission_webhook_rejection_count{error_type=\u0026quot;calling_webhook_error\u0026quot;,name=\u0026quot;always-timeout-webhook.example.com\u0026quot;,operation=\u0026quot;CREATE\u0026quot;,rejection_code=\u0026quot;0\u0026quot;,type=\u0026quot;validating\u0026quot;} 1 apiserver_admission_webhook_rejection_count{error_type=\u0026quot;calling_webhook_error\u0026quot;,name=\u0026quot;invalid-admission-response-webhook.example.com\u0026quot;,operation=\u0026quot;CREATE\u0026quot;,rejection_code=\u0026quot;0\u0026quot;,type=\u0026quot;validating\u0026quot;} 1 apiserver_admission_webhook_rejection_count{error_type=\u0026quot;no_error\u0026quot;,name=\u0026quot;deny-unwanted-configmap-data.example.com\u0026quot;,operation=\u0026quot;CREATE\u0026quot;,rejection_code=\u0026quot;400\u0026quot;,type=\u0026quot;validating\u0026quot;} 13 最佳实践和警告 幂等性  幂等的 mutating admission Webhook 能够成功处理已经被它接纳甚或修改的对象。即使多次执行该准入测试，也不会产生与初次执行结果相异的结果。\n幂等 mutating admission Webhook 的示例：   对于 CREATE Pod 请求，将 Pod 的字段 .spec.securityContext.runAsNonRoot 设置为 true，以实施安全最佳实践。\n  对于 CREATE Pod 请求，如果未设置容器的字段 .spec.containers[].resources.limits，设置默认资源限制值。\n  对于 CREATE pod 请求，如果 Pod 中不存在名为 foo-sidecar 的 sidecar 容器，向 Pod 注入一个 foo-sidecar 容器。\n  在上述情况下，可以安全地重新调用 Webhook，或接受已经设置了字段的对象。\n非幂等 mutating admission Webhook 的示例：   对于 CREATE pod 请求，注入名称为 foo-sidecar 并带有当前时间戳的 sidecar 容器（例如 foo-sidecar-19700101-000000）。\n  对于 CREATE/UPDATE pod 请求，如果容器已设置标签 \u0026quot;env\u0026quot; 则拒绝，否则将 \u0026quot;env\u0026quot;: \u0026quot;prod\u0026quot; 标签添加到容器。\n  对于 CREATE pod 请求，盲目地添加一个名为 foo-sidecar 的 sidecar 容器，而未查看 Pod 中是否已经有 foo-sidecar 容器。\n  在上述第一种情况下，重新调用该 Webhook 可能导致同一个 Sidecar 容器多次注入到 Pod 中，而且每次使用不同的容器名称。 类似地，如果 Sidecar 已存在于用户提供的 Pod 中，则 Webhook 可能注入重复的容器。\n在上述第二种情况下，重新调用 Webhook 将导致 Webhook 自身输出失败。\n在上述第三种情况下，重新调用 Webhook 将导致 Pod 规范中的容器重复，从而使请求无效并被 API 服务器拒绝。\n拦截对象的所有版本 建议通过将 .webhooks[].matchPolicy 设置为 Equivalent，以确保 admission webhooks 始终拦截对象的所有版本。 建议 admission webhooks 应该更偏向注册资源的稳定版本。如果无法拦截对象的所有版本，可能会导致准入策略未再某些版本的请求上执行。 有关示例，请参见匹配请求：matchPolicy。\n可用性  建议 admission webhook 尽快完成执行（时长通常是毫秒级），因为它们会增加 API 请求的延迟。 建议对 Webhook 使用较小的超时值。有关更多详细信息，请参见超时。\n建议 Admission Webhook 应该采用某种形式的负载均衡机制，以提供高可用性和高性能。 如果集群中正在运行 Webhook，则可以在服务后面运行多个 Webhook 后端，以利用该服务支持的负载均衡。\n确保看到对象的最终状态 如果某 Admission Webhook 需要保证自己能够看到对象的最终状态以实施策略，则应该使用一个 validating admission webhook， 因为可以通过 mutating Webhook 看到对象后对其进行修改。\n例如，一个 mutating admission webhook 被配置为在每个 CREATE Pod 请求中注入一个名称为 \u0026ldquo;foo-sidecar\u0026rdquo; 的 sidecar 容器。\n例如，一个 mutating admission webhook 配置为在每个容器上注入一个名称为foo-sidecar的边车容器。如果必须存在边车容器， 则还应配置一个 validating admisson webhook 以拦截 CREATE Pod 请求，并验证要创建的对象中是否存在具有预期配置的名称为 \u0026ldquo;foo-sidecar\u0026rdquo; 的容器。\n避免自托管的 Webhooks 中出现死锁 如果集群内的 Webhook 配置能够拦截启动其自己的 Pod 所需的资源，则该 Webhook 可能导致其自身部署时发生死锁。\n例如，某 mutating admission webhook 配置为仅当 Pod 中设置了某个标签（例如 \u0026quot;env\u0026quot;: \u0026quot;prod\u0026quot;）时，才接受 CREATE Pod 请求。 Webhook 服务器在未设置 \u0026quot;env\u0026quot; 标签的 Deployment 中运行。当运行 Webhook 服务器的容器的节点运行不正常时，Webhook 部署尝试将容器重新调度到另一个节点。 但是，由于未设置 \u0026quot;env\u0026quot; 标签，因此请求将被现有的 webhook 服务器拒绝，并且调度迁移不会发生。\n建议使用 namespaceSelector 排除 Webhook 所在的命名空间。\nSide Effects 建议 admission webhook 应尽可能避免副作用，这意味着该 admission webhook 仅对发送给他们的 AdmissionReview 的内容起作用，并且不要进行额外更改。 如果 Webhook 没有任何副作用，则 .webhooks[].sideEffects 字段应设置为 None。\n如果在 admission 执行期间存在副作用，则应在处理 dryRun 为 true 的 AdmissionReview 对象时避免产生副作用，并且其 .webhooks[].sideEffects 字段应设置为 NoneOnDryRun。 有关更多详细信息，请参见副作用。\n避免对 kube-system 命名空间进行操作 kube-system 命名空间包含由 Kubernetes 系统创建的对象，例如用于控制平面组件的服务账号，诸如 kube-dns 之类的 Pod 等。 意外更改或拒绝 kube-system 命名空间中的请求可能会导致控制平面组件停止运行或者导致未知行为发生。 如果您的 admission webhook 不想修改 Kubernetes 控制平面的行为，请使用 namespaceSelector 避免拦截 kube-system 命名空间。\n"
},
{
	"uri": "https://lijun.in/concepts/storage/dynamic-provisioning/",
	"title": "动态卷供应",
	"tags": [],
	"description": "",
	"content": "动态卷供应允许按需创建存储卷。 如果没有动态供应，集群管理员必须手动地联系他们的云或存储提供商来创建新的存储卷， 然后在 Kubernetes 集群创建 PersistentVolume 对象来表示这些卷。 动态供应功能消除了集群管理员预先配置存储的需要。 相反，它在用户请求时自动供应存储。\n背景 动态卷供应的实现基于 storage.k8s.io API 组中的 StorageClass API 对象。 集群管理员可以根据需要定义多个 StorageClass 对象，每个对象指定一个卷插件（又名 provisioner）， 卷插件向卷供应商提供在创建卷时需要的数据卷信息及相关参数。\n集群管理员可以在集群中定义和公开多种存储（来自相同或不同的存储系统），每种都具有自定义参数集。 该设计也确保终端用户不必担心存储供应的复杂性和细微差别，但仍然能够从多个存储选项中进行选择。\n点击这里查阅有关存储类的更多信息。\n启用动态卷供应 要启用动态供应功能，集群管理员需要为用户预先创建一个或多个 StorageClass 对象。 StorageClass 对象定义当动态供应被调用时，哪一个驱动将被使用和哪些参数将被传递给驱动。 以下清单创建了一个 StorageClass 存储类 \u0026ldquo;slow\u0026rdquo;，它提供类似标准磁盘的永久磁盘。\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: slow provisioner: kubernetes.io/gce-pd parameters: type: pd-standard 以下清单创建了一个 \u0026ldquo;fast\u0026rdquo; 存储类，它提供类似 SSD 的永久磁盘。\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: fast provisioner: kubernetes.io/gce-pd parameters: type: pd-ssd 使用动态卷供应 用户通过在 PersistentVolumeClaim 中包含存储类来请求动态供应的存储。 在 Kubernetes v1.6 之前，这通过 volume.beta.kubernetes.io/storage-class 注解实现。然而，这个注解自 v1.6 起就不被推荐使用了。 用户现在能够而且应该使用 PersistentVolumeClaim 对象的 storageClassName 字段。 这个字段的值必须能够匹配到集群管理员配置的 StorageClass 名称（见下面）。\n例如，要选择 \u0026ldquo;fast\u0026rdquo; 存储类，用户将创建如下的 PersistentVolumeClaim：\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: claim1 spec: accessModes: - ReadWriteOnce storageClassName: fast resources: requests: storage: 30Gi 该声明会自动供应一块类似 SSD 的永久磁盘。 在删除该声明后，这个卷也会被销毁。\n默认行为 可以在群集上启用动态卷供应，以便在未指定存储类的情况下动态设置所有声明。 集群管理员可以通过以下方式启用此行为：\n 标记一个 StorageClass 为 默认； 确保 DefaultStorageClass 准入控制器在 API 服务端被启用。  管理员可以通过向其添加 storageclass.kubernetes.io/is-default-class 注解来将特定的 StorageClass 标记为默认。 当集群中存在默认的 StorageClass 并且用户创建了一个未指定 storageClassName 的 PersistentVolumeClaim 时， DefaultStorageClass 准入控制器会自动向其中添加指向默认存储类的 storageClassName 字段。\n请注意，群集上最多只能有一个 默认 存储类，否则无法创建没有明确指定 storageClassName 的 PersistentVolumeClaim。\n拓扑感知 在多区域集群中，Pod 可以被分散到多个区域。 单区域存储后端应该被供应到 Pod 被调度到的区域。 这可以通过设置卷绑定模式来实现。\n"
},
{
	"uri": "https://lijun.in/concepts/extend-kubernetes/service-catalog/",
	"title": "服务目录",
	"tags": [],
	"description": "",
	"content": "term_id=\u0026quot;service-catalog\u0026rdquo; length=\u0026quot;all\u0026rdquo; prepend=\u0026rdquo;\u0026rdquo; \u0026gt;}}\n服务代理是由开放服务代理 API 规范定义的一组托管服务的终结点，由第三方提供并维护，其中的第三方可以是 AWS，GCP 或 Azure 等云服务提供商。 托管服务的一些示例是 Microsoft Azure Cloud Queue，Amazon Simple Queue Service 和 Google Cloud Pub/Sub，但它们是可以使用应用程序的任何软件产品。\n使用服务目录，集群操作者可以浏览其提供的托管服务列表，提供托管服务实例并与之绑定，以使其可以被 Kubernetes 集群中的应用程序使用。\n示例用例 应用开发者希望使用消息队列作为其在 Kubernetes 集群中运行的应用程序的一部分。 但是，它们不想承受建立这种服务的开销，也不想自行管理。幸运的是，有一家云服务提供商通过它们的服务代理将消息队列作为托管服务提供。\n集群运维人员可以设置服务目录并使用它与云服务提供商的服务代理 通信，以此提供消息队列服务的实例并使其对 Kubernetes 中的应用程序可用。 因此，应用开发者可以不用关心消息队列的实现细节，也不用对其进行管理。它们的应用程序可以简单的将其作为服务使用。\n架构 服务目录使用开放服务代理 API 与服务代理进行通信，并作为 Kubernetes API Server 的中介，以便协商首要规定并获取应用程序使用托管服务的必要凭据。\n它被实现为一个扩展 API 服务和一个控制器管理器，使用 Etcd 作为存储。它还使用了 Kubernetes 1.7+ 版本中提供的 aggregation layer 来呈现其 API。\nAPI 资源 服务目录安装 servicecatalog.k8s.io API 并提供以下 Kubernetes 资源：\n ClusterServiceBroker：服务目录的集群内代表，封装了它的服务连接细节。集群运维人员创建和管理这些资源，并希望使用该代理服务在集群中提供新类型的托管服务。 ClusterServiceClass：由特定服务代理提供的托管服务。当新的 ClusterServiceBroker 资源被添加到集群时，服务目录控制器将连接到服务代理以获取可用的托管服务列表。然后为每个托管服务创建对应的新 ClusterServiceClass 资源。 ClusterServicePlan：托管服务的特定产品。例如托管服务可能有不同的计划可用，如免费版本和付费版本，或者可能有不同的配置选项，例如使用 SSD 存储或拥有更多资源。与 ClusterServiceClass 类似，当一个新的 ClusterServiceBroker 被添加到集群时，服务目录会为每个托管服务的每个可用服务计划创建对应的新 ClusterServicePlan 资源。 ServiceInstance：ClusterServiceClass 提供的示例。由集群运维人员创建，以使托管服务的特定实例可供一个或多个集群内应用程序使用。当创建一个新的 ServiceInstance 资源时，服务目录控制器将连接到相应的服务代理并指示它调配服务实例。 ServiceBinding：ServiceInstance 的访问凭据。由希望其应用程序使用服务 ServiceInstance 的集群运维人员创建。创建之后，服务目录控制器将创建一个 Kubernetes Secret，其中包含服务实例的连接细节和凭据，可以挂载到 Pod 中。  认证 服务目录支持这些认证方法：\n 基础认证（用户名/密码） OAuth 2.0 不记名令牌  使用方式 集群运维人员可以使用服务目录 API 资源来提供托管服务并使其在 Kubernetes 集群内可用。涉及的步骤有：\n 列出服务代理提供的托管服务和服务计划。 配置托管服务的新实例。 绑定到托管服务，它将返回连接凭证。 将连接凭证映射到应用程序中。  列出托管服务和服务计划 首先，集群运维人员在 servicecatalog.k8s.io 组内创建一个 ClusterServiceBroker 资源。此资源包含访问服务代理终结点所需的 URL 和连接详细信息。\n这是一个 ClusterServiceBroker 资源的例子：\napiVersion: servicecatalog.k8s.io/v1beta1 kind: ClusterServiceBroker metadata: name: cloud-broker spec: # Points to the endpoint of a service broker. (This example is not a working URL.) url: https://servicebroker.somecloudprovider.com/v1alpha1/projects/service-catalog/brokers/default ##### # Additional values can be added here, which may be used to communicate # with the service broker, such as bearer token info or a caBundle for TLS. ##### 下面的顺序图展示了从一个服务代理列出可用托管服务和计划所有涉及的步骤：\n  一旦 ClusterServiceBroker 资源被添加到了服务目录之后，将会触发一个到外部服务代理的 List Services 调用。\n  服务代理返回可用的托管服务和服务计划列表，这些列表将本地缓存在 ClusterServiceClass 和 ClusterServicePlan 资源中。\n  然后集群运维人员可以使用以下命令获取可用托管服务的列表：\n kubectl get clusterserviceclasses -o=custom-columns=SERVICE\\ NAME:.metadata.name,EXTERNAL\\ NAME:.spec.externalName  它应该输出一个和以下格式类似的服务名称列表：\n SERVICE NAME EXTERNAL NAME 4f6e6cf6-ffdd-425f-a2c7-3c9258ad2468 cloud-provider-service ... ...  他们还可以使用以下命令查看可用的服务计划：\n kubectl get clusterserviceplans -o=custom-columns=PLAN\\ NAME:.metadata.name,EXTERNAL\\ NAME:.spec.externalName  它应该输出一个和以下格式类似的服务计划列表：\n PLAN NAME EXTERNAL NAME 86064792-7ea2-467b-af93-ac9694d96d52 service-plan-name ... ...    配置一个新实例 集群运维人员 可以通过创建一个 ServiceInstance 资源来启动一个新实例的配置。\n这是一个 ServiceInstance 资源的例子：\napiVersion: servicecatalog.k8s.io/v1beta1 kind: ServiceInstance metadata: name: cloud-queue-instance namespace: cloud-apps spec: # References one of the previously returned services clusterServiceClassExternalName: cloud-provider-service clusterServicePlanExternalName: service-plan-name ##### # Additional parameters can be added here, # which may be used by the service broker. ##### 以下顺序图展示了配置托管服务新实例所涉及的步骤：\n 当创建 ServiceInstance 资源时，服务目录将启动一个到外部服务代理的配置实例调用。 服务代理创建一个托管服务的新实例并返回 HTTP 响应。 然后集群运维人员可以检查实例的状态是否就绪。  绑定到托管服务 在设置新实例之后，集群运维人员必须绑定到托管服务才能获取应用程序使用服务所需的连接凭据和服务账户的详细信息。该操作通过创建一个 ServiceBinding 资源完成。\n以下是 ServiceBinding 资源的示例：\napiVersion: servicecatalog.k8s.io/v1beta1 kind: ServiceBinding metadata: name: cloud-queue-binding namespace: cloud-apps spec: instanceRef: name: cloud-queue-instance ##### # Additional information can be added here, such as a secretName or # service account parameters, which may be used by the service broker. ##### 以下顺序图展示了绑定到托管服务实例的步骤：\n 在创建 ServiceBinding 之后，服务目录调用外部服务代理，请求绑定服务实例所需的信息。 服务代理为相应服务账户启用应用权限/角色。 服务代理返回连接和访问托管服务示例所需的信息。这是由提供商和服务特定的，故返回的信息可能因服务提供商和其托管服务而有所不同。  映射连接凭据 完成绑定之后的最后一步就是将连接凭据和服务特定的信息映射到应用程序中。这些信息存储在 secret 中，集群中的应用程序可以访问并使用它们直接与托管服务进行连接。\nPod 配置文件 执行此映射的一种方法是使用声明式 Pod 配置。\n以下示例描述了如何将服务账户凭据映射到应用程序中。名为 sa-key 的密钥保存在一个名为 provider-cloud-key 的卷中，应用程序会将该卷挂载在 /var/secrets/provider/key.json 路径下。环境变量 PROVIDER_APPLICATION_CREDENTIALS 将映射为挂载文件的路径。\n... spec: volumes: - name: provider-cloud-key secret: secretName: sa-key containers: ... volumeMounts: - name: provider-cloud-key mountPath: /var/secrets/provider env: - name: PROVIDER_APPLICATION_CREDENTIALS value: \u0026#34;/var/secrets/provider/key.json\u0026#34; 以下示例描述了如何将 secret 值映射为应用程序的环境变量。在这个示例中，消息队列的主题名从 secret provider-queue-credentials 中名为 topic 的 key 项映射到环境变量 TOPIC 中。\n... env: - name: \u0026#34;TOPIC\u0026#34; valueFrom: secretKeyRef: name: provider-queue-credentials key: topic   如果您熟悉Helm Charts，您可以使用 Helm 将服务目录安装到 Kubernetes 集群中。或者，您可以使用 SC 工具安装服务目录。 查看服务代理示例。 浏览 kubernetes-incubator/service-catalog 项目。 查看 svc-cat.io。  "
},
{
	"uri": "https://lijun.in/setup/production-environment/on-premises-vm/",
	"title": "本地 VMs",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lijun.in/concepts/overview/working-with-objects/labels/",
	"title": "标签和选择器",
	"tags": [],
	"description": "",
	"content": "标签 是附加到 Kubernetes 对象（比如 Pods）上的键值对。 标签旨在用于指定对用户有意义且相关的对象的标识属性，但不直接对核心系统有语义含义。 标签可以用于组织和选择对象的子集。标签可以在创建时附加到对象，随后可以随时添加和修改。 每个对象都可以定义一组键/值标签。每个键对于给定对象必须是唯一的。\n\u0026#34;metadata\u0026#34;: { \u0026#34;labels\u0026#34;: { \u0026#34;key1\u0026#34; : \u0026#34;value1\u0026#34;, \u0026#34;key2\u0026#34; : \u0026#34;value2\u0026#34; } } 我们最终将标签索引和反向索引，用于高效查询和监视，使用它们在 UI 和 CLI 中进行排序和分组等。我们不希望将非标识性的、尤其是大型或结构化数据用作标签，给后者带来污染。应使用 注解 记录非识别信息\n动机 标签使用户能够以松散耦合的方式将他们自己的组织结构映射到系统对象，而无需客户端存储这些映射。\n服务部署和批处理流水线通常是多维实体（例如，多个分区或部署、多个发行序列、多个层，每层多个微服务）。管理通常需要交叉操作，这打破了严格的层次表示的封装，特别是由基础设施而不是用户确定的严格的层次结构。\n示例标签：\n \u0026quot;release\u0026quot; : \u0026quot;stable\u0026quot;, \u0026quot;release\u0026quot; : \u0026quot;canary\u0026quot; \u0026quot;environment\u0026quot; : \u0026quot;dev\u0026quot;, \u0026quot;environment\u0026quot; : \u0026quot;qa\u0026quot;, \u0026quot;environment\u0026quot; : \u0026quot;production\u0026quot; \u0026quot;tier\u0026quot; : \u0026quot;frontend\u0026quot;, \u0026quot;tier\u0026quot; : \u0026quot;backend\u0026quot;, \u0026quot;tier\u0026quot; : \u0026quot;cache\u0026quot; \u0026quot;partition\u0026quot; : \u0026quot;customerA\u0026quot;, \u0026quot;partition\u0026quot; : \u0026quot;customerB\u0026quot; \u0026quot;track\u0026quot; : \u0026quot;daily\u0026quot;, \u0026quot;track\u0026quot; : \u0026quot;weekly\u0026quot;  这些只是常用标签的例子; 您可以任意制定自己的约定。请记住，对于给定对象标签的键必须是唯一的。\n语法和字符集 标签 是键值对。有效的标签键有两个段：可选的前缀和名称，用斜杠（/）分隔。名称段是必需的，必须小于等于 63 个字符，以字母数字字符（[a-z0-9A-Z]）开头和结尾，带有破折号（-），下划线（_），点（ .）和之间的字母数字。前缀是可选的。如果指定，前缀必须是 DNS 子域：由点（.）分隔的一系列 DNS 标签，总共不超过 253 个字符，后跟斜杠（/）。 如果省略前缀，则假定标签键对用户是私有的。 向最终用户对象添加标签的自动系统组件（例如 kube-scheduler，kube-controller-manager，kube-apiserver，kubectl 或其他第三方自动化）必须指定前缀。kubernetes.io/ 前缀是为 Kubernetes 核心组件保留的。\n有效标签值必须为 63 个字符或更少，并且必须为空或以字母数字字符（[a-z0-9A-Z]）开头和结尾，中间可以包含破折号（-）、下划线（_）、点（.）和字母或数字。\n标签选择器 与 名称和 UID 不同，标签不提供唯一性。通常，我们希望许多对象携带相同的标签。\n通过 标签选择器，客户端/用户可以识别一组对象。标签选择器是 Kubernetes 中的核心分组原语。\nAPI 目前支持两种类型的选择器：基于相等性的 和 基于集合的。 标签选择器可以由逗号分隔的多个 需求 组成。在多个需求的情况下，必须满足所有要求，因此逗号分隔符充当逻辑 与（\u0026amp;\u0026amp;）运算符。\n空标签选择器（即，需求为零的选择器）选择集合中的每个对象。\nnull 值的标签选择器（仅可用于可选选择器字段）不选择任何对象\n注意：两个控制器的标签选择器不得在命名空间内重叠，否则它们将互相冲突。\n基于相等性的 需求 基于相等性 或 不相等 的需求允许按标签键和值进行过滤。匹配对象必须满足所有指定的标签约束，尽管它们也可能具有其他标签。 可接受的运算符有=、== 和 ！= 三种。 前两个表示 相等（并且只是同义词），而后者表示 不相等。 例如：\nenvironment = production tier != frontend 前者选择所有资源，其键名等于 environment，值等于 production。 后者选择所有资源，其键名等于 tier，值不同于 frontend，所有资源都没有带有 tier 键的标签。 可以使用逗号运算符来过滤 production 环境中的非 frontend 层资源：environment=production,tier!=frontend。\n基于相等性的标签要求的一种使用场景是 Pods 要指定节点选择标准。例如，下面的示例 Pod 选择带有标签 \u0026ldquo;accelerator=nvidia-tesla-p100\u0026rdquo;。\napiVersion: v1 kind: Pod metadata: name: cuda-test spec: containers: - name: cuda-test image: \u0026#34;k8s.gcr.io/cuda-vector-add:v0.1\u0026#34; resources: limits: nvidia.com/gpu: 1 nodeSelector: accelerator: nvidia-tesla-p100 基于集合 的需求 基于集合 的标签需求允许您通过一组值来过滤键。支持三种操作符：in,notin and exists (只可以用在键标识符上)。例如：\nenvironment in (production, qa) tier notin (frontend, backend) partition !partition 第一个示例选择了所有键等于 environment 并且值等于 production 或者 qa 的资源。\n第二个示例选择了所有键等于 tier 并且值不等于 frontend 或者 backend 的资源，以及所有没有 tier 键标签的资源。\n第三个示例选择了所有包含了有 partition 标签的资源；没有校验它的值。\n第四个示例选择了所有没有 partition 标签的资源；没有校验它的值。\n类似地，逗号分隔符充当 AND 运算符。因此，使用 partition 键（无论为何值）和 environment 不同于 qa 来过滤资源可以使用 partition，environment notin（qa) 来实现。\n基于集合 的标签选择器是相等标签选择器的一般形式，因为 environment = production 等同于 environment in（production）;！= 和 notin 也是类似的。\n基于集合 的要求可以与基于 相等 的要求混合使用。例如：partition in (customerA, customerB),environment!=qa。\nAPI LIST 和 WATCH 过滤 LIST and WATCH 操作可以使用查询参数指定标签选择器过滤一组对象。两种需求都是允许的。（这里显示的是它们出现在 URL 查询字符串中）\n 基于相等性 的需求: ?labelSelector=environment%3Dproduction,tier%3Dfrontend 基于集合 的需求: ?labelSelector=environment+in+%28production%2Cqa%29%2Ctier+in+%28frontend%29  两种标签选择器都可以通过 REST 客户端用于 list 或者 watch 资源。例如，使用 kubectl 定位 apiserver，可以使用 基于相等性 的标签选择器可以这么写：\n$ kubectl get pods -l environment=production,tier=frontend 或者使用 基于集合的 需求：\n$ kubectl get pods -l \u0026#39;environment in (production),tier in (frontend)\u0026#39; 正如刚才提到的，基于集合 的需求更具有表达力。例如，它们可以实现值的 或 操作：\n$ kubectl get pods -l \u0026#39;environment in (production, qa)\u0026#39; 或者通过 exists 运算符限制不匹配：\n$ kubectl get pods -l \u0026#39;environment,environment notin (frontend)\u0026#39; 在 API 对象上设置引用 一些 Kubernetes 对象，例如 services 和 replicationcontrollers ，也使用了标签选择器去指定了其他资源的集合，例如 pods。\nService 和 ReplicationController 一个 Service 指向的一组 pods 是由标签选择器定义的。同样，一个 ReplicationController 应该管理的 pods 的数量也是由标签选择器定义的。\n两个对象的标签选择器都是在 json 或者 yaml 文件中使用映射定义的，并且只支持 基于相等性 需求的选择器：\n\u0026#34;selector\u0026#34;: { \u0026#34;component\u0026#34; : \u0026#34;redis\u0026#34;, } 或者\nselector: component: redis 这个选择器(分别在 json 或者 yaml 格式中) 等价于 component=redis 或 component in (redis) 。\n支持基于集合需求的资源 比较新的资源，例如 Job、Deployment、Replica Set 和Daemon Set ,也支持 基于集合的 需求。\nselector: matchLabels: component: redis matchExpressions: - {key: tier, operator: In, values: [cache]} - {key: environment, operator: NotIn, values: [dev]} matchLabels 是由 {key，value} 对组成的映射。matchLabels 映射中的单个 {key，value } 等同于 matchExpressions 的元素，其 key字段为 \u0026ldquo;key\u0026rdquo;，operator 为 \u0026ldquo;In\u0026rdquo;，而 values 数组仅包含 \u0026ldquo;value\u0026rdquo;。matchExpressions 是 pod 选择器要求的列表。有效的运算符包括 In，NotIn，Exists 和 DoesNotExist。在 In 和 NotIn 的情况下，设置的值必须是非空的。来自 matchLabels 和 matchExpressions 的所有要求都是合在一起 \u0026ndash; 它们必须都满足才能匹配。\n选择节点集 通过标签进行选择的一个用例是确定节点集，方便 pod 调度。 有关更多信息，请参阅 选择节点 上的文档。\n"
},
{
	"uri": "https://lijun.in/concepts/cluster-administration/manage-deployment/",
	"title": "管理资源",
	"tags": [],
	"description": "",
	"content": "您已经部署了应用并通过服务暴露它。然后呢？Kubernetes 提供了一些工具来帮助管理您的应用部署，包括缩扩容和更新。我们将更深入讨论的特性包括配置文件和标签。\n组织资源配置 许多应用需要创建多个资源，例如 Deployment 和 Service。可以通过将多个资源组合在同一个文件中（在 YAML 中以 --- 分隔）来简化对它们的管理。例如：\nfile=\u0026quot;application/nginx-app.yaml\u0026rdquo; \u0026gt;}}\n可以用创建单个资源相同的方式来创建多个资源：\nkubectl apply -f https://k8s.io/examples/application/nginx-app.yaml service/my-nginx-svc created deployment.apps/my-nginx created 资源将按照它们在文件中的顺序创建。因此，最好先指定服务，这样在控制器（例如 Deployment）创建 Pod 时能够确保调度器可以将与服务关联的多个 Pod 分散到不同节点。\nkubectl create 也接受多个 -f 参数:\nkubectl apply -f https://k8s.io/examples/application/nginx/nginx-svc.yaml -f https://k8s.io/examples/application/nginx/nginx-deployment.yaml 还可以指定目录路径，而不用添加多个单独的文件：\nkubectl apply -f https://k8s.io/examples/application/nginx/ kubectl 将读取任何后缀为 .yaml，.yml 或者 .json 的文件。\n建议的做法是，将同一个微服务或同一应用层相关的资源放到同一个文件中，将同一个应用相关的所有文件按组存放到同一个目录中。如果应用的各层使用 DNS 相互绑定，那么您可以简单地将堆栈的所有组件一起部署。\n还可以使用 URL 作为配置源，便于直接使用已经提交到 Github 上的配置文件进行部署：\nkubectl apply -f https://raw.githubusercontent.com/kubernetes/website/master/content/zh/examples/application/nginx/nginx-deployment.yaml deployment.apps/my-nginx created kubectl 中的批量操作 资源创建并不是 kubectl 可以批量执行的唯一操作。kubectl 还可以从配置文件中提取资源名，以便执行其他操作，特别是删除您之前创建的资源：\nkubectl delete -f https://k8s.io/examples/application/nginx-app.yaml deployment.apps \u0026#34;my-nginx\u0026#34; deleted service \u0026#34;my-nginx-svc\u0026#34; deleted 在仅有两种资源的情况下，可以使用\u0026quot;资源类型/资源名\u0026quot;的语法在命令行中同时指定这两个资源：\nkubectl delete deployments/my-nginx services/my-nginx-svc 对于资源数目较大的情况，您会发现使用 -l 或 --selector 指定的筛选器（标签查询）能很容易根据标签筛选资源：\nkubectl delete deployment,services -l app=nginx deployment.apps \u0026#34;my-nginx\u0026#34; deleted service \u0026#34;my-nginx-svc\u0026#34; deleted 由于 kubectl 用来输出资源名称的语法与其所接受的资源名称语法相同，所以很容易使用 $() 或 xargs 进行链式操作：\nkubectl get $(kubectl create -f docs/concepts/cluster-administration/nginx/ -o name | grep service) NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE my-nginx-svc LoadBalancer 10.0.0.208 \u0026lt;pending\u0026gt; 80/TCP 0s 上面的命令中，我们首先使用 examples/application/nginx/ 下的配置文件创建资源，并使用 -o name 的输出格式（以\u0026quot;资源/名称\u0026quot;的形式打印每个资源）打印所创建的资源。然后，我们通过 grep 来过滤 \u0026ldquo;service\u0026rdquo;，最后再打印 kubectl get 的内容。\n如果您碰巧在某个路径下的多个子路径中组织资源，那么也可以递归地在所有子路径上执行操作，方法是在 --filename,-f 后面指定 --recursive 或者 -R。\n例如，假设有一个目录路径为 project/k8s/development，它保存开发环境所需的所有清单，并按资源类型组织：\nproject/k8s/development ├── configmap │ └── my-configmap.yaml ├── deployment │ └── my-deployment.yaml └── pvc └── my-pvc.yaml 默认情况下，对 project/k8s/development 执行的批量操作将停止在目录的第一级，而不是处理所有子目录。 如果我们试图使用以下命令在此目录中创建资源，则会遇到一个错误：\nkubectl apply -f project/k8s/development error: you must provide one or more resources by argument or filename (.json|.yaml|.yml|stdin) 然而，在 --filename,-f 后面标明 --recursive 或者 -R 之后：\nkubectl apply -f project/k8s/development --recursive configmap/my-config created deployment.apps/my-deployment created persistentvolumeclaim/my-pvc created --recursive 可以用于接受 --filename,-f 参数的任何操作，例如：kubectl {create,get,delete,describe,rollout} 等。\n有多个 -f 参数出现的时候，--recursive 参数也能正常工作：\nkubectl apply -f project/k8s/namespaces -f project/k8s/development --recursive namespace/development created namespace/staging created configmap/my-config created deployment.apps/my-deployment created persistentvolumeclaim/my-pvc created 如果您有兴趣学习更多关于 kubectl 的内容，请阅读 kubectl 概述。\n有效地使用标签 到目前为止我们使用的示例中的资源最多使用了一个标签。在许多情况下，应使用多个标签来区分集合。\n例如，不同的应用可能会为 app 标签设置不同的值。 但是，类似 [guestbook 示例](https://github.com/kubernetes/examples/tree/\u0026lt; param \u0026ldquo;githubbranch\u0026rdquo; \u0026gt;}}/guestbook/) 这样的多层应用，还需要区分每一层。前端可以带以下标签：\nlabels: app: guestbook tier: frontend Redis 的主节点和从节点会有不同的 tier 标签，甚至还有一个额外的 role 标签：\nlabels: app: guestbook tier: backend role: master 以及\nlabels: app: guestbook tier: backend role: slave 标签允许我们按照标签指定的任何维度对我们的资源进行切片和切块：\nkubectl apply -f examples/guestbook/all-in-one/guestbook-all-in-one.yaml kubectl get pods -Lapp -Ltier -Lrole NAME READY STATUS RESTARTS AGE APP TIER ROLE guestbook-fe-4nlpb 1/1 Running 0 1m guestbook frontend \u0026lt;none\u0026gt; guestbook-fe-ght6d 1/1 Running 0 1m guestbook frontend \u0026lt;none\u0026gt; guestbook-fe-jpy62 1/1 Running 0 1m guestbook frontend \u0026lt;none\u0026gt; guestbook-redis-master-5pg3b 1/1 Running 0 1m guestbook backend master guestbook-redis-slave-2q2yf 1/1 Running 0 1m guestbook backend slave guestbook-redis-slave-qgazl 1/1 Running 0 1m guestbook backend slave my-nginx-divi2 1/1 Running 0 29m nginx \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; my-nginx-o0ef1 1/1 Running 0 29m nginx \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kubectl get pods -lapp=guestbook,role=slave NAME READY STATUS RESTARTS AGE guestbook-redis-slave-2q2yf 1/1 Running 0 3m guestbook-redis-slave-qgazl 1/1 Running 0 3m 金丝雀部署 另一个需要多标签的场景是用来区分同一组件的不同版本或者不同配置的多个部署。常见的做法是部署一个使用金丝雀发布来部署新应用版本（在 pod 模板中通过镜像标签指定），保持新旧版本应用同时运行，这样，新版本在完全发布之前也可以接收实时的生产流量。\n例如，您可以使用 track 标签来区分不同的版本。\n主要稳定的发行版将有一个 track 标签，其值为 stable：\nname: frontend replicas: 3 ... labels: app: guestbook tier: frontend track: stable ... image: gb-frontend:v3 然后，您可以创建 guestbook 前端的新版本，让这些版本的 track 标签带有不同的值（即 canary），以便两组 pod 不会重叠：\nname: frontend-canary replicas: 1 ... labels: app: guestbook tier: frontend track: canary ... image: gb-frontend:v4 前端服务通过选择标签的公共子集（即忽略 track 标签）来覆盖两组副本，以便流量可以转发到两个应用：\nselector: app: guestbook tier: frontend 您可以调整 stable 和 canary 版本的副本数量，以确定每个版本将接收实时生产流量的比例(在本例中为 3:1)。一旦有信心，您就可以将新版本应用的 track 标签的值从 canary 替换为 stable，并且将老版本应用删除。\n想要了解更具体的示例，请查看 Ghost 部署教程。\n更新标签 有时，现有的 pod 和其它资源需要在创建新资源之前重新标记。这可以用 kubectl label 完成。 例如，如果想要将所有 nginx pod 标记为前端层，只需运行：\nkubectl label pods -l app=nginx tier=fe pod/my-nginx-2035384211-j5fhi labeled pod/my-nginx-2035384211-u2c7e labeled pod/my-nginx-2035384211-u3t6x labeled 首先用标签 \u0026ldquo;app=nginx\u0026rdquo; 过滤所有的 pod，然后用 \u0026ldquo;tier=fe\u0026rdquo; 标记它们。想要查看您刚才标记的 pod，请运行：\nkubectl get pods -l app=nginx -L tier NAME READY STATUS RESTARTS AGE TIER my-nginx-2035384211-j5fhi 1/1 Running 0 23m fe my-nginx-2035384211-u2c7e 1/1 Running 0 23m fe my-nginx-2035384211-u3t6x 1/1 Running 0 23m fe 这将输出所有 \u0026ldquo;app=nginx\u0026rdquo; 的 pod，并有一个额外的描述 pod 的 tier 的标签列（用参数 -L 或者 --label-columns 标明）。\n想要了解更多信息，请参考 标签 和 kubectl label。\n更新注解 有时，您可能希望将注解附加到资源中。注解是 API 客户端（如工具、库等）用于检索的任意非标识元数据。这可以通过 kubectl annotate 来完成。例如：\nkubectl annotate pods my-nginx-v4-9gw19 description=\u0026#39;my frontend running nginx\u0026#39; kubectl get pods my-nginx-v4-9gw19 -o yaml apiVersion: v1 kind: pod metadata: annotations: description: my frontend running nginx ... 想要了解更多信息，请参考 注解 和 kubectl annotate 文档。\n缩扩您的应用 当应用上的负载增长或收缩时，使用 kubectl 能够轻松实现规模的缩扩。例如，要将 nginx 副本的数量从 3 减少到 1，请执行以下操作：\nkubectl scale deployment/my-nginx --replicas=1 deployment.extensions/my-nginx scaled 现在，您的 deployment 管理的 pod 只有一个了。\nkubectl get pods -l app=nginx NAME READY STATUS RESTARTS AGE my-nginx-2035384211-j5fhi 1/1 Running 0 30m 想要让系统自动选择需要 nginx 副本的数量，范围从 1 到 3，请执行以下操作：\nkubectl autoscale deployment/my-nginx --min=1 --max=3 horizontalpodautoscaler.autoscaling/my-nginx autoscaled 现在，您的 nginx 副本将根据需要自动地增加或者减少。\n想要了解更多信息，请参考 kubectl scale, kubectl autoscale 和 pod 水平自动伸缩 文档。\n就地更新资源 有时，有必要对您所创建的资源进行小范围、无干扰地更新。\nkubectl apply 建议在源代码管理中维护一组配置文件（参见配置即代码），这样，它们就可以和应用代码一样进行维护和版本管理。然后，您可以用 kubectl apply 将配置变更应用到集群中。\n这个命令将会把推送的版本与以前的版本进行比较，并应用您所做的更改，但是不会自动覆盖任何你没有指定更改的属性。\nkubectl apply -f https://k8s.io/examples/application/nginx/nginx-deployment.yaml deployment.apps/my-nginx configured 注意，kubectl apply 将为资源增加一个额外的注解，以确定自上次调用以来对配置的更改。当调用它时，kubectl apply 会在以前的配置、提供的输入和资源的当前配置之间找出三方差异，以确定如何修改资源。\n目前，新创建的资源是没有这个注解的，所以，第一次调用 kubectl apply 将使用提供的输入和资源的当前配置双方之间差异进行比较。在第一次调用期间，它无法检测资源创建时属性集的删除情况。因此，不会删除它们。\n所有后续调用 kubectl apply 以及其它修改配置的命令，如 kubectl replace 和 kubectl edit，都将更新注解，并允许随后调用的 kubectl apply 使用三方差异进行检查和执行删除。\n想要使用 apply，请始终使用 kubectl apply 或 kubectl create --save-config 创建资源。\nkubectl edit 或者，您也可以使用 kubectl edit 更新资源：\nkubectl edit deployment/my-nginx 这相当于首先 get 资源，在文本编辑器中编辑它，然后用更新的版本 apply 资源：\nkubectl get deployment my-nginx -o yaml \u0026gt; /tmp/nginx.yaml vi /tmp/nginx.yaml # do some edit, and then save the file kubectl apply -f /tmp/nginx.yaml deployment.apps/my-nginx configured rm /tmp/nginx.yaml 这使您可以更加容易地进行更重大的更改。请注意，可以使用 EDITOR 或 KUBE_EDITOR 环境变量来指定编辑器。\n想要了解更多信息，请参考 kubectl edit 文档。\nkubectl patch 您可以使用 kubectl patch 来更新 API 对象。此命令支持 JSON patch，JSON merge patch，以及 strategic merge patch。 请参考 使用 kubectl patch 更新 API 对象 和 kubectl patch.\n破坏性的更新 在某些情况下，您可能需要更新某些初始化后无法更新的资源字段，或者您可能只想立即进行递归更改，例如修复 Deployment 创建的不正常的 Pod。若要更改这些字段，请使用 replace --force，它将删除并重新创建资源。在这种情况下，您可以简单地修改原始配置文件：\nkubectl replace -f https://k8s.io/examples/application/nginx/nginx-deployment.yaml --force deployment.apps/my-nginx deleted deployment.apps/my-nginx replaced 在不中断服务的情况下更新应用 在某些时候，您最终需要更新已部署的应用，通常都是通过指定新的镜像或镜像标签，如上面的金丝雀发布的场景中所示。kubectl 支持几种更新操作，每种更新操作都适用于不同的场景。\n我们将指导您通过 Deployment 如何创建和更新应用。\n假设您正运行的是 1.7.9 版本的 nginx：\nkubectl run my-nginx --image=nginx:1.7.9 --replicas=3 deployment.apps/my-nginx created 要更新到 1.9.1 版本，只需使用我们前面学到的 kubectl 命令将 .spec.template.spec.containers[0].image 从 nginx:1.7.9 修改为 nginx:1.9.1。\nkubectl edit deployment/my-nginx 没错，就是这样！Deployment 将在后台逐步更新已经部署的 nginx 应用。它确保在更新过程中，只有一定数量的旧副本被开闭，并且只有一定基于所需 pod 数量的新副本被创建。想要了解更多细节，请参考 Deployment。\n  学习怎么样使用 kubectl 观察和调试应用 配置最佳实践和技巧  "
},
{
	"uri": "https://lijun.in/setup/best-practices/",
	"title": "💖 - 最佳实践",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lijun.in/concepts/",
	"title": "😊 - Kubernetes Concepts",
	"tags": [],
	"description": "",
	"content": "概念部分可以帮助你了解 Kubernetes 的各个组成部分以及 Kubernetes 用来表示集群的一些抽象概念，并帮助你更加深入的理解 Kubernetes 是如何工作的。\n概述 要使用 Kubernetes，你需要用 Kubernetes API 对象 来描述集群的 预期状态（desired state） ：包括你需要运行的应用或者负载，它们使用的镜像、副本数，以及所需网络和磁盘资源等等。你可以使用命令行工具 kubectl 来调用 Kubernetes API 创建对象，通过所创建的这些对象来配置预期状态。你也可以直接调用 Kubernetes API 和集群进行交互，设置或者修改预期状态。\n一旦你设置了你所需的目标状态，Kubernetes 控制面（control plane） 会通过 Pod 生命周期事件生成器(PLEG)，促成集群的当前状态符合其预期状态。为此，Kubernetes 会自动执行各类任务，比如运行或者重启容器、调整给定应用的副本数等等。Kubernetes 控制面由一组运行在集群上的进程组成：\n Kubernetes 主控组件（Master） 包含三个进程，都运行在集群中的某个节点上，主控组件通常这个节点被称为 master 节点。这些进程包括：kube-apiserver、kube-controller-manager 和 kube-scheduler。 集群中的每个非 master 节点都运行两个进程：  kubelet，和 master 节点进行通信。 kube-proxy，一种网络代理，将 Kubernetes 的网络服务代理到每个节点上。    Kubernetes 对象 Kubernetes 包含若干用来表示系统状态的抽象层，包括：已部署的容器化应用和负载、与它们相关的网络和磁盘资源以及有关集群正在运行的其他操作的信息。这些抽象使用 Kubernetes API 对象来表示。有关更多详细信息，请参阅了解 Kubernetes 对象。\n基本的 Kubernetes 对象包括：\n Pod Service Volume Namespace  Kubernetes 也包含大量的被称作 Controller 的高级抽象。控制器基于基本对象构建并提供额外的功能和方便使用的特性。具体包括：\n Deployment DaemonSet StatefulSet ReplicaSet Job  Kubernetes 控制面 关于 Kubernetes 控制平面的各个部分，（如 Kubernetes 主控组件和 kubelet 进程），管理着 Kubernetes 如何与你的集群进行通信。控制平面维护着系统中所有的 Kubernetes 对象的状态记录，并且通过连续的控制循环来管理这些对象的状态。在任意的给定时间点，控制面的控制环都能响应集群中的变化，并且让系统中所有对象的实际状态与你提供的预期状态相匹配。\n比如， 当你通过 Kubernetes API 创建一个 Deployment 对象，你就为系统增加了一个新的目标状态。Kubernetes 控制平面记录着对象的创建，并启动必要的应用然后将它们调度至集群某个节点上来执行你的指令，以此来保持集群的实际状态和目标状态的匹配。\nKubernetes Master 节点 Kubernetes master 节点负责维护集群的目标状态。当你要与 Kubernetes 通信时，使用如 kubectl 的命令行工具，就可以直接与 Kubernetes master 节点进行通信。\n \u0026ldquo;master\u0026rdquo; 是指管理集群状态的一组进程的集合。通常这些进程都跑在集群中一个单独的节点上，并且这个节点被称为 master 节点。master 节点也可以扩展副本数，来获取更好的可用性及冗余。\n Kubernetes Node 节点 集群中的 node 节点（虚拟机、物理机等等）都是用来运行你的应用和云工作流的机器。Kubernetes master 节点控制所有 node 节点；你很少需要和 node 节点进行直接通信。\n如果你想编写一个概念页面，请参阅使用页面模板获取更多有关概念页面类型和概念模板的信息。\n"
},
{
	"uri": "https://lijun.in/concepts/extend-kubernetes/",
	"title": "😊 - 扩展 Kubernetes",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lijun.in/tutorials/stateless-application/",
	"title": "😎 - 无状态应用程序",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lijun.in/tasks/run-application/",
	"title": "😝 - 运行应用",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lijun.in/concepts/workloads/controllers/daemonset/",
	"title": "DaemonSet",
	"tags": [],
	"description": "",
	"content": "DaemonSet 确保全部（或者某些）节点上运行一个 Pod 的副本。当有节点加入集群时， 也会为他们新增一个 Pod 。当有节点从集群移除时，这些 Pod 也会被回收。删除 DaemonSet 将会删除它创建的所有 Pod。\nDaemonSet 的一些典型用法：\n 在每个节点上运行集群存储 DaemonSet，例如 glusterd、ceph。 在每个节点上运行日志收集 DaemonSet，例如 fluentd、logstash。 在每个节点上运行监控 DaemonSet，例如 Prometheus Node Exporter、Flowmill、Sysdig 代理、collectd、Dynatrace OneAgent、AppDynamics 代理、Datadog 代理、New Relic 代理、Ganglia gmond 或者 Instana 代理。  一个简单的用法是在所有的节点上都启动一个 DaemonSet，将被作为每种类型的 daemon 使用。\n一个稍微复杂的用法是单独对每种 daemon 类型使用多个 DaemonSet，但具有不同的标志， 并且对不同硬件类型具有不同的内存、CPU 要求。\n编写 DaemonSet Spec 创建 DaemonSet 您可以在 YAML 文件中描述 DaemonSet。例如，下面的 daemonset.yaml 文件描述了一个运行 fluentd-elasticsearch Docker 镜像的 DaemonSet：\nodenew file=\u0026quot;controllers/daemonset.yaml\u0026rdquo; \u0026gt;}}\n 基于 YAML 文件创建 DaemonSet:  kubectl apply -f https://k8s.io/examples/controllers/daemonset.yaml 必需字段 和其它所有 Kubernetes 配置一样，DaemonSet 需要 apiVersion、kind 和 metadata 字段。有关配置文件的基本信息，详见文档 部署应用、配置容器 和 使用 kubectl 进行对象管理。\nDaemonSet 也需要一个 .spec 配置段。\nPod 模板 .spec 中唯一必需的字段是 .spec.template。\n.spec.template 是一个 Pod 模板。除了它是嵌套的，而且不具有 apiVersion 或 kind 字段，它与 Pod 具有相同的 schema。\n除了 Pod 必需字段外，在 DaemonSet 中的 Pod 模板必须指定合理的标签（查看 Pod Selector）。\n在 DaemonSet 中的 Pod 模板必须具有一个值为 Always 的 RestartPolicy，或者未指定它的值，默认是 Always。\nPod Selector .spec.selector 字段表示 Pod Selector，它与 Job 的 .spec.selector 的作用是相同的。\n从 Kubernetes 1.8 开始，您必须指定与 .spec.template 的标签匹配的 pod selector。当不配置时，pod selector 将不再有默认值。selector 默认与 kubectl apply 不兼容。 此外，一旦创建了 DaemonSet，它的 .spec.selector 就不能修改。修改 pod selector 可能导致 Pod 意外悬浮，并且这对用户来说是困惑的。\nspec.selector 表示一个对象，它由如下两个字段组成：\n matchLabels - 与 ReplicationController 的 .spec.selector 的作用相同。 matchExpressions - 允许构建更加复杂的 Selector，可以通过指定 key、value 列表 ，以及与 key 和 value 列表相关的操作符。  当上述两个字段都指定时，结果表示的是 AND 关系。\n如果指定了 .spec.selector，必须与 .spec.template.metadata.labels 相匹配。如果与它们配置的不匹配，则会被 API 拒绝。\n另外，通常不应直接通过另一个 DaemonSet 或另一个工作负载资源（例如 ReplicaSet）来创建其标签与该选择器匹配的任何 Pod。否则，DaemonSet 控制器会认为这些 Pod 是由它创建的。Kubernetes 不会阻止你这样做。您可能要执行此操作的一种情况是，手动在节点上创建具有不同值的 Pod 进行测试。\n仅在某些节点上运行 Pod 如果指定了 .spec.template.spec.nodeSelector，DaemonSet Controller 将在能够与 Node Selector 匹配的节点上创建 Pod。类似这种情况，可以指定 .spec.template.spec.affinity，然后 DaemonSet Controller 将在能够与 node Affinity 匹配的节点上创建 Pod。 如果根本就没有指定，则 DaemonSet Controller 将在所有节点上创建 Pod。\n如何调度 Daemon Pods 通过默认 scheduler 调度 feature-state state=\u0026quot;stable\u0026rdquo; for-kubernetes-version=\u0026quot;1.17\u0026rdquo; \u0026gt;}}\nDaemonSet 确保所有符合条件的节点都运行该 Pod 的一个副本。通常，运行 Pod 的节点由 Kubernetes 调度器抉择。不过，DaemonSet pods 由 DaemonSet 控制器创建和调度。这将引入以下问题：\n Pod 行为的不一致性：等待调度的正常 Pod 已被创建并处于 Pending 状态，但 DaemonSet pods 未在 Pending 状态下创建。 这使用户感到困惑。 Pod preemption由默认 scheduler 处理。 启用抢占后，DaemonSet 控制器将在不考虑 pod 优先级和抢占的情况下制定调度决策。  ScheduleDaemonSetPods 允许您使用默认调度器而不是 DaemonSet 控制器来调度 DaemonSets，方法是将 NodeAffinity 添加到 DaemonSet pods，而不是 .spec.nodeName。 然后使用默认调度器将 pod 绑定到目标主机。 如果 DaemonSet pod 的亲和节点已存在，则替换它。 DaemonSet 控制器仅在创建或修改 DaemonSet pods 时执行这些操作，并且不对 DaemonSet 的 spec.template 进行任何更改。\nnodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchFields: - key: metadata.name operator: In values: - target-host-name 此外，系统会自动添加 node.kubernetes.io/unschedulable：NoSchedule 容忍度到 DaemonSet Pods。 在调度 DaemonSet Pod 时，默认调度器会忽略 unschedulable 节点。\n污点和容忍度 尽管 Daemon Pods 遵循污点和容忍度 规则，根据相关特性，会自动将以下容忍度添加到 DaemonSet Pods 中。\n   容忍度关键词 影响 版本 描述     node.kubernetes.io/not-ready NoExecute 1.13+ DaemonSet pods will not be evicted when there are node problems such as a network partition.   node.kubernetes.io/unreachable NoExecute 1.13+ DaemonSet pods will not be evicted when there are node problems such as a network partition.   node.kubernetes.io/disk-pressure NoSchedule 1.8+    node.kubernetes.io/memory-pressure NoSchedule 1.8+    node.kubernetes.io/unschedulable NoSchedule 1.12+ DaemonSet pods tolerate unschedulable attributes by default scheduler.   node.kubernetes.io/network-unavailable NoSchedule 1.12+ DaemonSet pods, who uses host network, tolerate network-unavailable attributes by default scheduler.    与 Daemon Pods 通信 与 DaemonSet 中的 Pod 进行通信的几种可能模式如下：\n Push：将 DaemonSet 中的 Pod 配置为将更新发送到另一个 Service，例如统计数据库。 NodeIP 和已知端口：DaemonSet 中的 Pod 可以使用 hostPort，从而可以通过节点 IP 访问到 Pod。客户端能通过某种方法获取节点 IP 列表，并且基于此也可以获取到相应的端口。 DNS：创建具有相同 Pod Selector 的 Headless Service，然后通过使用 endpoints 资源或从 DNS 中检索到多个 A 记录来发现 DaemonSet。 Service：创建具有相同 Pod Selector 的 Service，并使用该 Service 随机访问到某个节点上的 daemon（没有办法访问到特定节点）。  更新 DaemonSet 如果修改了节点标签，DaemonSet 将立刻向新匹配上的节点添加 Pod，同时删除不能够匹配的节点上的 Pod。\n您可以修改 DaemonSet 创建的 Pod。然而，不允许对 Pod 的所有字段进行更新。当下次 节点（即使具有相同的名称）被创建时，DaemonSet Controller 还会使用最初的模板。\n您可以删除一个 DaemonSet。如果使用 kubectl 并指定 --cascade=false 选项，则 Pod 将被保留在节点上。然后可以创建具有不同模板的新 DaemonSet。具有不同模板的新 DaemonSet 将能够通过标签匹配并识别所有已经存在的 Pod。 如果有任何 Pod 需要替换，则 DaemonSet 根据它的 updateStrategy 来替换。\nDaemonSet 的可替代选择 init 脚本 我们很可能希望直接在一个节点上启动 daemon 进程（例如，使用 init、upstartd、或 systemd）。这非常好，但基于 DaemonSet 来运行这些进程有如下一些好处：\n  像对待应用程序一样，具备为 daemon 提供监控和管理日志的能力。\n  为 daemon 和应用程序使用相同的配置语言和工具（如 Pod 模板、kubectl）。\n  在资源受限的容器中运行 daemon，能够增加 daemon 和应用容器的隔离性。然而，这也实现了在容器中运行 daemon，但却不能在 Pod 中运行（例如，直接基于 Docker 启动）。\n  裸 Pod 可能要直接创建 Pod，同时指定其运行在特定的节点上。然而，DaemonSet 替换了由于任何原因被删除或终止的 Pod，例如节点失败、例行节点维护、内核升级。由于这个原因，我们应该使用 DaemonSet 而不是单独创建 Pod。\n静态 Pod 可能需要通过在一个指定目录下编写文件来创建 Pod，该目录受 Kubelet 所监视。这些 Pod 被称为 静态 Pod。 不像 DaemonSet，静态 Pod 不受 kubectl 和其它 Kubernetes API 客户端管理。静态 Pod 不依赖于 apiserver，这使得它们在集群启动的情况下非常有用。而且，未来静态 Pod 可能会被废弃掉。\nDeployments DaemonSet 与 Deployments 非常类似，它们都能创建 Pod，这些 Pod 对应的进程都不希望被终止掉（例如，Web 服务器、存储服务器）。 为无状态的 Service 使用 Deployments，比如前端 Frontend 服务，实现对副本的数量进行扩缩容、平滑升级，比基于精确控制 Pod 运行在某个主机上要重要得多。 需要 Pod 副本总是运行在全部或特定主机上，并需要先于其他 Pod 启动，当这被认为非常重要时，应该使用 Daemon Controller。\n"
},
{
	"uri": "https://lijun.in/reference/setup-tools/kubeadm/kubeadm-config/",
	"title": "kubeadm config",
	"tags": [],
	"description": "",
	"content": "从 v1.8.0 开始，kubeadm 将集群的配置上传到名为 kube-system 的 ConfigMap 对象中，对象位于 kube-system 命名空间内。并在以后的升级中读取这个 ConfigMap 配置对象。 这样可以保证系统组件的正确配置，提供无缝的用户体验。\n您可以执行 kubeadm config view 来查看 ConfigMap。如果使用 kubeadm v1.7.x 或更低版本来初始化群集，必须先使用 kubeadm config upload 创建 ConfigMap，然后才能使用 kubeadm upgrade。\n在 Kubernetes v1.11.0 中，添加了一些新命令。你可以使用 kubeadm config print-default 打印默认配置，可以用 kubeadm config migrate 来将旧的配置文件转换到较新的版本，还可以使用 kubeadm config images list 和 kubeadm config images pull 列出并拉取 kubeadm 所需的镜像。\nkubeadm config upload from-file kubeadm config view . include \u0026ldquo;generated/kubeadm_config_view.md\u0026rdquo; \u0026gt;}}\nkubeadm config print init-defaults . include \u0026ldquo;generated/kubeadm_config_print_init-defaults.md\u0026rdquo; \u0026gt;}}\nkubeadm config print join-defaults . include \u0026ldquo;generated/kubeadm_config_print_join-defaults.md\u0026rdquo; \u0026gt;}}\nkubeadm config migrate . include \u0026ldquo;generated/kubeadm_config_migrate.md\u0026rdquo; \u0026gt;}}\nkubeadm config images list . include \u0026ldquo;generated/kubeadm_config_images_list.md\u0026rdquo; \u0026gt;}}\nkubeadm config images pull . include \u0026ldquo;generated/kubeadm_config_images_pull.md\u0026rdquo; \u0026gt;}}\n. heading \u0026ldquo;whatsnext\u0026rdquo; %}}  kubeadm upgrade 将 Kubernetes 集群升级到更新版本 [kubeadm upgrade]  "
},
{
	"uri": "https://lijun.in/concepts/scheduling-eviction/kube-scheduler/",
	"title": "Kubernetes 调度器",
	"tags": [],
	"description": "",
	"content": "在 Kubernetes 中，调度 是指将 text=\u0026quot;Pod\u0026rdquo; term_id=\u0026quot;pod\u0026rdquo; \u0026gt;}} 放置到合适的 text=\u0026quot;Node\u0026rdquo; term_id=\u0026quot;node\u0026rdquo; \u0026gt;}} 上，然后对应 Node 上的glossary_tooltip term_id=\u0026quot;kubelet\u0026rdquo; \u0026gt;}} 才能够运行这些 pod。\n调度概览 调度器通过 kubernetes 的 watch 机制来发现集群中新创建且尚未被调度到 Node 上的 Pod。调度器会将发现的每一个未调度的 Pod 调度到一个合适的 Node 上来运行。调度器会依据下文的调度原则来做出调度选择。\n如果你想要理解 Pod 为什么会被调度到特定的 Node 上，或者你想要尝试实现一个自定义的调度器，这篇文章将帮助你了解调度。\nkube-scheduler kube-scheduler 是 Kubernetes 集群的默认调度器，并且是集群 text=\u0026quot;控制面\u0026rdquo; term_id=\u0026quot;control-plane\u0026rdquo; \u0026gt;}} 的一部分。如果你真的希望或者有这方面的需求，kube-scheduler 在设计上是允许你自己写一个调度组件并替换原有的 kube-scheduler。\n对每一个新创建的 Pod 或者是未被调度的 Pod，kube-scheduler 会选择一个最优的 Node 去运行这个 Pod。然而，Pod 内的每一个容器对资源都有不同的需求，而且 Pod 本身也有不同的资源需求。因此，Pod 在被调度到 Node 上之前，根据这些特定的资源调度需求，需要对集群中的 Node 进行一次过滤。\n在一个集群中，满足一个 Pod 调度请求的所有 Node 称之为 可调度节点。如果没有任何一个 Node 能满足 Pod 的资源请求，那么这个 Pod 将一直停留在未调度状态直到调度器能够找到合适的 Node。\n调度器先在集群中找到一个 Pod 的所有可调度节点，然后根据一系列函数对这些可调度节点打分，然后选出其中得分最高的 Node 来运行 Pod。之后，调度器将这个调度决定通知给 kube-apiserver，这个过程叫做 绑定。\n在做调度决定时需要考虑的因素包括：单独和整体的资源请求、硬件/软件/策略限制、亲和以及反亲和要求、数据局域性、负载间的干扰等等。\nkube-scheduler 调度流程 kube-scheduler 给一个 pod 做调度选择包含两个步骤：\n 过滤 打分  过滤阶段会将所有满足 Pod 调度需求的 Node 选出来。例如，PodFitsResources 过滤函数会检查候选 Node 的可用资源能否满足 Pod 的资源请求。在过滤之后，得出一个 Node 列表，里面包含了所有可调度节点；通常情况下，这个 Node 列表包含不止一个 Node。如果这个列表是空的，代表这个 Pod 不可调度。\n在打分阶段，调度器会为 Pod 从所有可调度节点中选取一个最合适的 Node。根据当前启用的打分规则，调度器会给每一个可调度节点进行打分。\n最后，kube-scheduler 会将 Pod 调度到得分最高的 Node 上。如果存在多个得分最高的 Node，kube-scheduler 会从中随机选取一个。\n支持以下两种方式配置调度器的过滤和打分行为：\n 调度策略 允许你配置过滤的 谓词(Predicates) 和打分的 优先级(Priorities) 。 调度配置 允许你配置实现不同调度阶段的插件，包括：QueueSort, Filter, Score, Bind, Reserve, Permit 等等。你也可以配置 kube-scheduler 运行不同的配置文件。    阅读关于 调度器性能调优 阅读关于 Pod 拓扑分布约束 阅读关于 kube-scheduler 的 参考文档 了解关于 配置多个调度器 的方式 了解关于 拓扑结构管理策略 了解关于 Pod 额外开销  "
},
{
	"uri": "https://lijun.in/concepts/workloads/pods/podpreset/",
	"title": "Pod Preset",
	"tags": [],
	"description": "",
	"content": "本文提供了 PodPreset 的概述。 在 Pod 创建时，用户可以使用 PodPreset 对象将特定信息注入 Pod 中，这些信息可以包括 secret、 卷、卷挂载和环境变量。\n理解 Pod Preset Pod Preset 是一种 API 资源，在 Pod 创建时，用户可以用它将额外的运行时需求信息注入 Pod。 使用标签选择器（label selector）来指定 Pod Preset 所适用的 Pod。\n使用 Pod Preset 使得 Pod 模板编写者不必显式地为每个 Pod 设置信息。 这样，使用特定服务的 Pod 模板编写者不需要了解该服务的所有细节。\n了解更多的相关背景信息，请参考 PodPreset 设计提案。\nPodPreset 如何工作 Kubernetes 提供了准入控制器 (PodPreset)，该控制器被启用时，会将 Pod Preset 应用于接收到的 Pod 创建请求中。 当出现 Pod 创建请求时，系统会执行以下操作：\n 检索所有可用 PodPresets 。 检查 PodPreset 的标签选择器与要创建的 Pod 的标签是否匹配。 尝试合并 PodPreset 中定义的各种资源，并注入要创建的 Pod。 发生错误时抛出事件，该事件记录了 pod 信息合并错误，同时在 不注入 PodPreset 信息的情况下创建 Pod。 为改动的 Pod spec 添加注解，来表明它被 PodPreset 所修改。 注解形如： podpreset.admission.kubernetes.io/podpreset-\u0026lt;pod-preset name\u0026gt;\u0026quot;: \u0026quot;\u0026lt;resource version\u0026gt;\u0026quot;。  一个 Pod 可能不与任何 Pod Preset 匹配，也可能匹配多个 Pod Preset。 同时，一个 PodPreset 可能不应用于任何 Pod，也可能应用于多个 Pod。 当 PodPreset 应用于一个或多个 Pod 时，Kubernetes 修改 pod spec。 对于 Env、 EnvFrom 和 VolumeMounts 的改动， Kubernetes 修改 pod 中所有容器的规格，对于卷的改动，Kubernetes 修改 Pod spec。\n适当时候，Pod Preset 可以修改 Pod 规范中的以下字段：\n .spec.containers 字段 initContainers 字段 (需要 Kubernetes 1.14.0 或更高版本)。  为特定 Pod 禁用 Pod Preset 在一些情况下，用户不希望 Pod 被 Pod Preset 所改动，这时，用户可以在 Pod spec 中添加形如 podpreset.admission.kubernetes.io/exclude: \u0026quot;true\u0026quot; 的注解。\n启用 Pod Preset 为了在集群中使用 Pod Preset，必须确保以下几点：\n  已启用 API 类型 settings.k8s.io/v1alpha1/podpreset。 例如，这可以通过在 API 服务器的 --runtime-config 配置项中包含 settings.k8s.io/v1alpha1=true 来实现。在 minikube 部署的集群中，启动集群时添加此参数 --extra-config=apiserver.runtime-config=settings.k8s.io/v1alpha1=true。\n  已启用准入控制器 PodPreset。 启用的一种方式是在 API 服务器的 --enable-admission-plugins 配置项中包含 PodPreset 。在 minikube 部署的集群中，启动集群时添加以下参数：\n--extra-config=apiserver.enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota,PodPreset   已经通过在相应的命名空间中创建 PodPreset 对象，定义了 Pod Preset。\n  eading \u0026ldquo;whatsnext\u0026rdquo;  使用 PodPreset 将信息注入 Pod  "
},
{
	"uri": "https://lijun.in/concepts/workloads/pods/pod-topology-spread-constraints/",
	"title": "Pod 拓扑扩展约束",
	"tags": [],
	"description": "",
	"content": "feature-state for_k8s_version=\u0026quot;v1.16\u0026rdquo; state=\u0026quot;alpha\u0026rdquo; \u0026gt;}}\n可以使用拓扑扩展约束来控制glossary_tooltip text=\u0026quot;Pods\u0026rdquo; term_id=\u0026quot;Pod\u0026rdquo; \u0026gt;}} 在集群内故障域（例如地区，区域，节点和其他用户自定义拓扑域）之间的分布。这可以帮助实现高可用以及提升资源利用率。\n先决条件 启用功能 确保 EvenPodsSpread 功能已开启（在 1.16 版本中该功能默认关闭）。阅读功能选项了解如何开启该功能。EvenPodsSpread 必须在 glossary_tooltip text=\u0026quot;API Server\u0026rdquo; term_id=\u0026quot;kube-apiserver\u0026rdquo; \u0026gt;}} 和 glossary_tooltip text=\u0026quot;scheduler\u0026rdquo; term_id=\u0026quot;kube-scheduler\u0026rdquo; \u0026gt;}} 中都要开启。\n节点标签 拓扑扩展约束依赖于节点标签来标识每个节点所在的拓扑域。例如，一个节点可能具有标签：node=node1,zone=us-east-1a,region=us-east-1\n假设你拥有一个具有以下标签的 4 节点集群：\nNAME STATUS ROLES AGE VERSION LABELS node1 Ready \u0026lt;none\u0026gt; 4m26s v1.16.0 node=node1,zone=zoneA node2 Ready \u0026lt;none\u0026gt; 3m58s v1.16.0 node=node2,zone=zoneA node3 Ready \u0026lt;none\u0026gt; 3m17s v1.16.0 node=node3,zone=zoneB node4 Ready \u0026lt;none\u0026gt; 2m43s v1.16.0 node=node4,zone=zoneB 然后从逻辑上看集群如下：\n+---------------+---------------+ | zoneA | zoneB | +-------+-------+-------+-------+ | node1 | node2 | node3 | node4 | +-------+-------+-------+-------+ 可以复用在大多数集群上自动创建和填充的知名标签，而不是手动添加标签。\nPod 的拓扑约束 API pod.spec.topologySpreadConstraints 字段定义如下所示：\napiVersion: v1 kind: Pod metadata: name: mypod spec: topologySpreadConstraints: - maxSkew: \u0026lt;integer\u0026gt; topologyKey: \u0026lt;string\u0026gt; whenUnsatisfiable: \u0026lt;string\u0026gt; labelSelector: \u0026lt;object\u0026gt; 可以定义一个或多个 topologySpreadConstraint 来指示 kube-scheduler 如何将每个传入的 Pod 根据与现有的 Pod 的关联关系在集群中部署。字段包括：\n maxSkew 描述 pod 分布不均的程度。这是给定拓扑类型中任意两个拓扑域中匹配的 pod 之间的最大允许差值。它必须大于零。 topologyKey 是节点标签的键。如果两个节点使用此键标记并且具有相同的标签值，则调度器会将这两个节点视为处于同一拓扑中。调度器试图在每个拓扑域中放置数量均衡的 pod。 whenUnsatisfiable 指示如果 pod 不满足扩展约束时如何处理：  DoNotSchedule（默认）告诉调度器不用进行调度。 ScheduleAnyway 告诉调度器在对最小化倾斜的节点进行优先级排序时仍对其进行调度。   labelSelector 用于查找匹配的 pod。匹配此标签的 pod 将被统计，以确定相应拓扑域中 pod 的数量。有关详细信息，请参考标签选择器。  执行 kubectl explain Pod.spec.topologySpreadConstraints 命令了解更多关于 topologySpreadConstraints 的信息。\n例子：单个拓扑扩展约束 假设你拥有一个 4 节点集群，其中标记为 foo:bar 的 3 个 pod 分别位于 node1，node2 和 node3 中（P 表示 pod）：\n+---------------+---------------+ | zoneA | zoneB | +-------+-------+-------+-------+ | node1 | node2 | node3 | node4 | +-------+-------+-------+-------+ | P | P | P | | +-------+-------+-------+-------+ 如果希望传入的 pod 均匀散布在现有的 pod 区域，则可以指定字段如下：\ncodenew file=\u0026quot;pods/topology-spread-constraints/one-constraint.yaml\u0026rdquo; \u0026gt;}}\ntopologyKey: zone 意味着均匀分布将只应用于存在标签对为 \u0026ldquo;zone:\u0026lt;any value\u0026gt;\u0026rdquo; 的节点上。whenUnsatisfiable: DoNotSchedule 告诉调度器，如果传入的 pod 不满足约束，则让它保持挂起状态。\n如果调度器将传入的 pod 放入 \u0026ldquo;zoneA\u0026rdquo;，pod 分布将变为 [3, 1]，因此实际的倾斜为 2（3 - 1）。这违反了 maxSkew: 1。此示例中，传入的 pod 只能放置在 \u0026ldquo;zoneB\u0026rdquo; 上：\n+---------------+---------------+ +---------------+---------------+ | zoneA | zoneB | | zoneA | zoneB | +-------+-------+-------+-------+ +-------+-------+-------+-------+ | node1 | node2 | node3 | node4 | OR | node1 | node2 | node3 | node4 | +-------+-------+-------+-------+ +-------+-------+-------+-------+ | P | P | P | P | | P | P | P P | | +-------+-------+-------+-------+ +-------+-------+-------+-------+ 可以调整 pod 规格以满足各种要求：\n 将 maxSkew 更改为更大的值，比如 \u0026ldquo;2\u0026rdquo;，这样传入的 pod 也可以放在 \u0026ldquo;zoneA\u0026rdquo; 上。 将 topologyKey 更改为 \u0026ldquo;node\u0026rdquo;，以便将 pod 均匀分布在节点上而不是区域中。在上面的例子中，如果 maxSkew 保持为 \u0026ldquo;1\u0026rdquo;，那么传入的 pod 只能放在 \u0026ldquo;node4\u0026rdquo; 上。 将 whenUnsatisfiable: DoNotSchedule 更改为 whenUnsatisfiable: ScheduleAnyway，以确保传入的 pod 始终可以调度（假设满足其他的调度 API）。但是，最好将其放置在具有较少匹配 pod 的拓扑域中。（请注意，此优先性与其他内部调度优先级（如资源使用率等）一起进行标准化。）  例子：多个拓扑扩展约束 下面的例子建立在前面例子的基础上。假设你拥有一个 4 节点集群，其中 3 个标记为 foo:bar 的 pod 分别位于 node1，node2 和 node3 上（P 表示 pod）：\n+---------------+---------------+ | zoneA | zoneB | +-------+-------+-------+-------+ | node1 | node2 | node3 | node4 | +-------+-------+-------+-------+ | P | P | P | | +-------+-------+-------+-------+ 可以使用 2 个拓扑扩展约束来控制 pod 在 区域和节点两个维度上进行分布：\ncodenew file=\u0026quot;pods/topology-spread-constraints/two-constraints.yaml\u0026rdquo; \u0026gt;}}\n在这种情况下，为了匹配第一个约束，传入的 pod 只能放置在 \u0026ldquo;zoneB\u0026rdquo; 中；而在第二个约束中，传入的 pod 只能放置在 \u0026ldquo;node4\u0026rdquo; 上。然后两个约束的结果加在一起，因此唯一可行的选择是放置在 \u0026ldquo;node4\u0026rdquo; 上。\n多个约束可能导致冲突。假设有一个跨越 2 个区域的 3 节点集群：\n+---------------+-------+ | zoneA | zoneB | +-------+-------+-------+ | node1 | node2 | nod3 | +-------+-------+-------+ | P P | P | P P | +-------+-------+-------+ 如果对集群应用 \u0026ldquo;two-constraints.yaml\u0026rdquo;，会发现 \u0026ldquo;mypod\u0026rdquo; 处于 Pending 状态。这是因为：为了满足第一个约束，\u0026ldquo;mypod\u0026rdquo; 只能放在 \u0026ldquo;zoneB\u0026rdquo; 中，而第二个约束要求 \u0026ldquo;mypod\u0026rdquo; 只能放在 \u0026ldquo;node2\u0026rdquo; 上。pod 调度无法满足两种约束。\n为了克服这种情况，可以增加 maxSkew 或修改其中一个约束，让其使用 whenUnsatisfiable: ScheduleAnyway。\n约定 这里有一些值得注意的隐式约定：\n 只有与传入 pod 具有相同命名空间的 pod 才能作为匹配候选者。   没有 topologySpreadConstraints[*].topologyKey 的节点将被忽略。这意味着：  位于这些节点上的 pod 不影响 maxSkew 的计算。在上面的例子中，假设 \u0026ldquo;node1\u0026rdquo; 没有标签 \u0026ldquo;zone\u0026rdquo;，那么 2 个 pod 将被忽略，因此传入的 pod 将被调度到 \u0026ldquo;zoneA\u0026rdquo; 中。 传入的 pod 没有机会被调度到这类节点上。在上面的例子中，假设一个带有标签 {zone-typo: zoneC} 的 \u0026ldquo;node5\u0026rdquo; 加入到集群，它将由于没有标签键 \u0026ldquo;zone\u0026rdquo; 而被忽略。    注意，如果传入 pod 的 topologySpreadConstraints[*].labelSelector 与自身的标签不匹配，将会发生什么。在上面的例子中，如果移除传入 pod 的标签，pod 仍然可以调度到 \u0026ldquo;zoneB\u0026rdquo;，因为约束仍然满足。然而，在调度之后，集群的不平衡程度保持不变。zoneA 仍然有 2 个带有 {foo:bar} 标签的 pod，zoneB 有 1 个带有 {foo:bar} 标签的 pod。因此，如果这不是你所期望的，建议工作负载的 topologySpreadConstraints[*].labelSelector 与其自身的标签匹配。\n  如果传入的 pod 定义了 spec.nodeSelector 或 spec.affinity.nodeAffinity，则将忽略不匹配的节点。\n假设有一个从 zonea 到 zonec 的 5 节点集群：\n+---------------+---------------+-------+ | zoneA | zoneB | zoneC | +-------+-------+-------+-------+-------+ | node1 | node2 | node3 | node4 | node5 | +-------+-------+-------+-------+-------+ | P | P | P | | | +-------+-------+-------+-------+-------+ 你知道 \u0026ldquo;zoneC\u0026rdquo; 必须被排除在外。在这种情况下，可以按如下方式编写 yaml，以便将 \u0026ldquo;mypod\u0026rdquo; 放置在 \u0026ldquo;zoneB\u0026rdquo; 上，而不是 \u0026ldquo;zoneC\u0026rdquo; 上。同样，spec.nodeSelector 也要一样处理。\ncodenew file=\u0026quot;pods/topology-spread-constraints/one-constraint-with-nodeaffinity.yaml\u0026rdquo; \u0026gt;}}\n  与 PodAffinity/PodAntiAffinity 相比较 在 Kubernetes 中，与 \u0026ldquo;Affinity\u0026rdquo; 相关的指令控制 pod 的调度方式（更密集或更分散）。\n 对于 PodAffinity，可以尝试将任意数量的 pod 打包到符合条件的拓扑域中。 对于 PodAntiAffinity，只能将一个 pod 调度到单个拓扑域中。  \u0026ldquo;EvenPodsSpread\u0026rdquo; 功能提供灵活的选项来将 pod 均匀分布到不同的拓扑域中，以实现高可用性或节省成本。这也有助于滚动更新工作负载和平滑扩展副本。有关详细信息，请参考动机。\n已知局限性 1.16 版本（此功能为 alpha）存在下面的一些限制：\n Deployment 的缩容可能导致 pod 分布不平衡。 pod 匹配到污点节点是允许的。参考 Issue 80921。  "
},
{
	"uri": "https://lijun.in/concepts/configuration/secret/",
	"title": "Secret",
	"tags": [],
	"description": "",
	"content": "Secret 对象类型用来保存敏感信息，例如密码、OAuth 令牌和 ssh key。 将这些信息放在 secret 中比放在 term_id=\u0026quot;pod\u0026rdquo; \u0026gt;}} 的定义或者 text=\u0026quot;容器镜像\u0026rdquo; term_id=\u0026quot;image\u0026rdquo; \u0026gt;}} 中来说更加安全和灵活。 参阅 Secret 设计文档 获取更多详细信息。\nSecret 概览 Secret 是一种包含少量敏感信息例如密码、token 或 key 的对象。这样的信息可能会被放在 Pod spec 中或者镜像中；将其放在一个 secret 对象中可以更好地控制它的用途，并降低意外暴露的风险。\n用户可以创建 secret，同时系统也创建了一些 secret。\n要使用 secret，pod 需要引用 secret。Pod 可以用两种方式使用 secret：作为 text=\u0026quot;volume\u0026rdquo; term_id=\u0026quot;volume\u0026rdquo; \u0026gt;}} 中的文件被挂载到 pod 中的一个或者多个容器里，或者当 kubelet 为 pod 拉取镜像时使用。\n内置 secret Service Account 使用 API 凭证自动创建和附加 secret Kubernetes 自动创建包含访问 API 凭据的 secret，并自动修改您的 pod 以使用此类型的 secret。\n如果需要，可以禁用或覆盖自动创建和使用 API 凭据。但是，如果您需要的只是安全地访问 apiserver，我们推荐这样的工作流程。\n参阅 Service Account 文档获取关于 Service Account 如何工作的更多信息。\n创建您自己的 Secret 使用 kubectl 创建 Secret 假设有些 pod 需要访问数据库。这些 pod 需要使用的用户名和密码在您本地机器的 ./username.txt 和 ./password.txt 文件里。\n# Create files needed for rest of example. echo -n \u0026#39;admin\u0026#39; \u0026gt; ./username.txt echo -n \u0026#39;1f2d1e2e67df\u0026#39; \u0026gt; ./password.txt kubectl create secret 命令将这些文件打包到一个 Secret 中并在 API server 中创建了一个对象。\nkubectl create secret generic db-user-pass --from-file=./username.txt --from-file=./password.txt secret \u0026quot;db-user-pass\u0026quot; created 特殊字符（例如 $, \\* 和 ! ）需要转义。 如果您使用的密码具有特殊字符，则需要使用 \\\\ 字符对其进行转义。 例如，如果您的实际密码是 S!B\\*d$zDsb ，则应通过以下方式执行命令： kubectl create secret generic dev-db-secret \u0026ndash;from-literal=username=devuser \u0026ndash;from-literal=password=S\\!B\\\\*d\\$zDsb 您无需从文件中转义密码中的特殊字符（ --from-file ）。\n您可以这样检查刚创建的 secret：\nkubectl get secrets NAME TYPE DATA AGE db-user-pass Opaque 2 51s kubectl describe secrets/db-user-pass Name: db-user-pass Namespace: default Labels: \u0026lt;none\u0026gt; Annotations: \u0026lt;none\u0026gt; Type: Opaque Data ==== password.txt: 12 bytes username.txt: 5 bytes 默认情况下，kubectl get 和 kubectl describe 避免显示密码的内容。 这是为了防止机密被意外地暴露给旁观者或存储在终端日志中。\n请参阅 解码 secret 了解如何查看它们的内容。\n手动创建 Secret 您也可以先以 json 或 yaml 格式在文件中创建一个 secret 对象，然后创建该对象。 密码包含两种类型，数据和字符串数据。 数据字段用于存储使用 base64 编码的任意数据。 提供 stringData 字段是为了方便起见，它允许您将机密数据作为未编码的字符串提供。\n例如，要使用数据字段将两个字符串存储在 Secret 中，请按如下所示将它们转换为 base64：\necho -n \u0026#39;admin\u0026#39; | base64 YWRtaW4= echo -n \u0026#39;1f2d1e2e67df\u0026#39; | base64 MWYyZDFlMmU2N2Rm 现在可以像这样写一个 secret 对象：\napiVersion: v1 kind: Secret metadata: name: mysecret type: Opaque data: username: YWRtaW4= password: MWYyZDFlMmU2N2Rm 使用 kubectl apply 创建 secret：\nkubectl apply -f ./secret.yaml secret \u0026quot;mysecret\u0026quot; created 对于某些情况，您可能希望改用 stringData 字段。此字段允许您将非 base64 编码的字符串直接放入 Secret 中， 并且在创建或更新 Secret 时将为您编码该字符串。\n下面的一个实践示例提供了一个参考，您正在部署使用密钥存储配置文件的应用程序，并希望在部署过程中填补齐配置文件的部分内容。\n如果您的应用程序使用以下配置文件：\napiUrl: \u0026#34;https://my.api.com/api/v1\u0026#34; username: \u0026#34;user\u0026#34; password: \u0026#34;password\u0026#34; 您可以使用以下方法将其存储在Secret中：\napiVersion: v1 kind: Secret metadata: name: mysecret type: Opaque stringData: config.yaml: |- apiUrl: \u0026#34;https://my.api.com/api/v1\u0026#34; username: {{username}} password: {{password}} 然后，您的部署工具可以在执行 kubectl apply 之前替换模板的 {{username}} 和 {{password}} 变量。 stringData 是只写的便利字段。检索 Secrets 时永远不会被输出。例如，如果您运行以下命令：\nkubectl get secret mysecret -o yaml 输出将类似于：\napiVersion: v1 kind: Secret metadata: creationTimestamp: 2018-11-15T20:40:59Z name: mysecret namespace: default resourceVersion: \u0026#34;7225\u0026#34; uid: c280ad2e-e916-11e8-98f2-025000000001 type: Opaque data: config.yaml: YXBpVXJsOiAiaHR0cHM6Ly9teS5hcGkuY29tL2FwaS92MSIKdXNlcm5hbWU6IHt7dXNlcm5hbWV9fQpwYXNzd29yZDoge3twYXNzd29yZH19 如果在 data 和 stringData 中都指定了字段，则使用 stringData 中的值。例如，以下是 Secret 定义：\napiVersion: v1 kind: Secret metadata: name: mysecret type: Opaque data: username: YWRtaW4= stringData: username: administrator secret 中的生成结果：\napiVersion: v1 kind: Secret metadata: creationTimestamp: 2018-11-15T20:46:46Z name: mysecret namespace: default resourceVersion: \u0026#34;7579\u0026#34; uid: 91460ecb-e917-11e8-98f2-025000000001 type: Opaque data: username: YWRtaW5pc3RyYXRvcg== YWRtaW5pc3RyYXRvcg== 转换成了 administrator。\ndata 和 stringData 的键必须由字母数字字符 \u0026lsquo;-\u0026rsquo;, \u0026lsquo;_\u0026rsquo; 或者 \u0026lsquo;.\u0026rsquo; 组成。\n** 编码注意：** 秘密数据的序列化 JSON 和 YAML 值被编码为 base64 字符串。换行符在这些字符串中无效，因此必须省略。在 Darwin/macOS 上使用 base64 实用程序时，用户应避免使用 -b 选项来分隔长行。相反，Linux用户 *应该* 在 base64 命令中添加选项 -w 0 ，或者，如果 -w 选项不可用的情况下，执行 base64 | tr -d '\\n'。\n从生成器创建 Secret Kubectl 从 1.14 版本开始支持 使用 Kustomize 管理对象 使用此新功能，您还可以从生成器创建一个 Secret，然后将其应用于在 Apiserver 上创建对象。 生成器应在目录内的 kustomization.yaml 中指定。\n例如，从文件 ./username.txt 和 ./password.txt 生成一个 Secret。\n# Create a kustomization.yaml file with SecretGenerator cat \u0026lt;\u0026lt;EOF \u0026gt;./kustomization.yaml secretGenerator: - name: db-user-pass files: - username.txt - password.txt EOF 应用 kustomization 目录创建 Secret 对象。\n$ kubectl apply -k . secret/db-user-pass-96mffmfh4k created 您可以检查 secret 是否是这样创建的：\n$ kubectl get secrets NAME TYPE DATA AGE db-user-pass-96mffmfh4k Opaque 2 51s $ kubectl describe secrets/db-user-pass-96mffmfh4k Name: db-user-pass Namespace: default Labels: \u0026lt;none\u0026gt; Annotations: \u0026lt;none\u0026gt; Type: Opaque Data ==== password.txt: 12 bytes username.txt: 5 bytes 例如，要从文字 username=admin 和 password=secret 生成秘密，可以在 kustomization.yaml 中将秘密生成器指定为\n# Create a kustomization.yaml file with SecretGenerator $ cat \u0026lt;\u0026lt;EOF \u0026gt;./kustomization.yaml secretGenerator: - name: db-user-pass literals: - username=admin - password=secret EOF Apply the kustomization directory to create the Secret object.\n$ kubectl apply -k . secret/db-user-pass-dddghtt9b5 created 通过对内容进行序列化后，生成一个后缀作为 Secrets 的名称。这样可以确保每次修改内容时都会生成一个新的 Secret。\n解码 Secret 可以使用 kubectl get secret 命令获取 secret。例如，获取在上一节中创建的 secret：\nkubectl get secret mysecret -o yaml apiVersion: v1 kind: Secret metadata: creationTimestamp: 2016-01-22T18:41:56Z name: mysecret namespace: default resourceVersion: \u0026quot;164619\u0026quot; uid: cfee02d6-c137-11e5-8d73-42010af00002 type: Opaque data: username: YWRtaW4= password: MWYyZDFlMmU2N2Rm 解码密码字段：\necho \u0026#39;MWYyZDFlMmU2N2Rm\u0026#39; | base64 --decode 1f2d1e2e67df 编辑 Secret 可以通过下面的命令编辑一个已经存在的 secret 。\nkubectl edit secrets mysecret 这将打开默认配置的编辑器，并允许更新 data 字段中的 base64 编码的 secret：\n# Please edit the object below. Lines beginning with a '#' will be ignored, # and an empty file will abort the edit. If an error occurs while saving this file will be # reopened with the relevant failures. # apiVersion: v1 data: username: YWRtaW4= password: MWYyZDFlMmU2N2Rm kind: Secret metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: { ... } creationTimestamp: 2016-01-22T18:41:56Z name: mysecret namespace: default resourceVersion: \u0026quot;164619\u0026quot; uid: cfee02d6-c137-11e5-8d73-42010af00002 type: Opaque 使用 Secret Secret 可以作为数据卷被挂载，或作为环境变量 暴露出来以供 pod 中的容器使用。它们也可以被系统的其他部分使用，而不直接暴露在 pod 内。 例如，它们可以保存凭据，系统的其他部分应该用它来代表您与外部系统进行交互。\n在 Pod 中使用 Secret 文件 在 Pod 中的 volume 里使用 Secret：\n 创建一个 secret 或者使用已有的 secret。多个 pod 可以引用同一个 secret。 修改您的 pod 的定义在 spec.volumes[] 下增加一个 volume。可以给这个 volume 随意命名，它的 spec.volumes[].secret.secretName 必须等于 secret 对象的名字。 将 spec.containers[].volumeMounts[] 加到需要用到该 secret 的容器中。指定 spec.containers[].volumeMounts[].readOnly = true 和 spec.containers[].volumeMounts[].mountPath 为您想要该 secret 出现的尚未使用的目录。 修改您的镜像并且／或者命令行让程序从该目录下寻找文件。Secret 的 data 映射中的每一个键都成为了 mountPath 下的一个文件名。  这是一个在 pod 中使用 volume 挂在 secret 的例子：\napiVersion: v1 kind: Pod metadata: name: mypod spec: containers: - name: mypod image: redis volumeMounts: - name: foo mountPath: \u0026#34;/etc/foo\u0026#34; readOnly: true volumes: - name: foo secret: secretName: mysecret 您想要用的每个 secret 都需要在 spec.volumes 中指明。\n如果 pod 中有多个容器，每个容器都需要自己的 volumeMounts 配置块，但是每个 secret 只需要一个 spec.volumes。\n您可以打包多个文件到一个 secret 中，或者使用的多个 secret，怎样方便就怎样来。\n向特性路径映射 secret 密钥\n我们还可以控制 Secret key 映射在 volume 中的路径。您可以使用 spec.volumes[].secret.items 字段修改每个 key 的目标路径：\napiVersion: v1 kind: Pod metadata: name: mypod spec: containers: - name: mypod image: redis volumeMounts: - name: foo mountPath: \u0026#34;/etc/foo\u0026#34; readOnly: true volumes: - name: foo secret: secretName: mysecret items: - key: username path: my-group/my-username 将会发生什么呢：\n username secret 存储在 /etc/foo/my-group/my-username 文件中而不是 /etc/foo/username 中。 password secret 没有被映射  如果使用了 spec.volumes[].secret.items，只有在 items 中指定的 key 被映射。要使用 secret 中所有的 key，所有这些都必须列在 items 字段中。所有列出的密钥必须存在于相应的 secret 中。否则，不会创建卷。\nSecret 文件权限\n您还可以指定 secret 将拥有的权限模式位文件。如果不指定，默认使用 0644。您可以为整个保密卷指定默认模式，如果需要，可以覆盖每个密钥。\n例如，您可以指定如下默认模式：\napiVersion: v1 kind: Pod metadata: name: mypod spec: containers: - name: mypod image: redis volumeMounts: - name: foo mountPath: \u0026#34;/etc/foo\u0026#34; volumes: - name: foo secret: secretName: mysecret defaultMode: 256 然后，secret 将被挂载到 /etc/foo 目录，所有通过该 secret volume 挂载创建的文件的权限都是 0400。\n请注意，JSON 规范不支持八进制符号，因此使用 256 值作为 0400 权限。如果您使用 yaml 而不是 json 作为 pod，则可以使用八进制符号以更自然的方式指定权限。\n您还可以使用映射，如上一个示例，并为不同的文件指定不同的权限，如下所示：\napiVersion: v1 kind: Pod metadata: name: mypod spec: containers: - name: mypod image: redis volumeMounts: - name: foo mountPath: \u0026#34;/etc/foo\u0026#34; volumes: - name: foo secret: secretName: mysecret items: - key: username path: my-group/my-username mode: 511 在这种情况下，导致 /etc/foo/my-group/my-username 的文件的权限值为 0777。由于 JSON 限制，必须以十进制格式指定模式。\n请注意，如果稍后阅读此权限值可能会以十进制格式显示。\n从 Volume 中消费 secret 值\n在挂载的 secret volume 的容器内，secret key 将作为文件，并且 secret 的值使用 base-64 解码并存储在这些文件中。这是在上面的示例容器内执行的命令的结果：\nls /etc/foo/ username password cat /etc/foo/username admin cat /etc/foo/password 1f2d1e2e67df 容器中的程序负责从文件中读取 secret。\n挂载的 secret 被自动更新\n当已经在 volume 中被消费的 secret 被更新时，被映射的 key 也将被更新。Kubelet 在周期性同步时检查被挂载的 secret 是不是最新的。但是，它正在使用其本地缓存来获取 Secret 的当前值。\n缓存的类型可以使用 (ConfigMapAndSecretChangeDetectionStrategy 中的 [KubeletConfiguration 结构](https://github.com/kubernetes/kubernetes/blob/ param \u0026ldquo;docsbranch\u0026rdquo; \u0026gt;}}/staging/src/k8s.io/kubelet/config/v1beta1/types.go)). 它可以通过基于 ttl 的 watch(默认)传播，也可以将所有请求直接重定向到直接kube-apiserver。 结果，从更新密钥到将新密钥投射到 Pod 的那一刻的总延迟可能与 kubelet 同步周期 + 缓存传播延迟一样长，其中缓存传播延迟取决于所选的缓存类型。 (它等于观察传播延迟，缓存的 ttl 或相应为 0)\n使用 Secret 作为子路径卷安装的容器将不会收到 Secret 更新。\nfeature-state for_k8s_version=\u0026quot;v1.18\u0026rdquo; state=\u0026quot;alpha\u0026rdquo;\nKubernetes 的 alpha 特性 不可变的 Secret 和 ConfigMap 提供了一个设置各个 Secret 和 ConfigMap 为不可变的选项。 对于大量使用 Secret 的集群（至少有成千上万各不相同的 Secret 供 Pod 挂载），禁止变更它们的数据有下列好处：\n 防止意外（或非预期的）更新导致应用程序中断 通过将 Secret 标记为不可变来关闭 kube-apiserver 对其的监视，以显著地降低 kube-apiserver 的负载来提升集群性能。  使用这个特性需要启用 ImmutableEmphemeralVolumes 特性开关 并将 Secret 或 ConfigMap 的 immutable 字段设置为 true. 例如：\napiVersion: v1 kind: Secret metadata: ... data: ... immutable: true 一旦一个 Secret 或 ConfigMap 被标记为不可变，撤销此操作或者更改 data 字段的内容都是 不 可能的。 只能删除并重新创建这个 Secret. 现有的 Pod 将维持对已删除 Secret 的挂载点 - 建议重新创建这些 pod.\nSecret 作为环境变量 将 secret 作为 pod 中的 环境变量使用：\n 创建一个 secret 或者使用一个已存在的 secret。多个 pod 可以引用同一个 secret。 修改 Pod 定义，为每个要使用 secret 的容器添加对应 secret key 的环境变量。消费secret key 的环境变量应填充 secret 的名称，并键入 env[x].valueFrom.secretKeyRef。 修改镜像并／或者命令行，以便程序在指定的环境变量中查找值。  这是一个使用 Secret 作为环境变量的示例：\napiVersion: v1 kind: Pod metadata: name: secret-env-pod spec: containers: - name: mycontainer image: redis env: - name: SECRET_USERNAME valueFrom: secretKeyRef: name: mysecret key: username - name: SECRET_PASSWORD valueFrom: secretKeyRef: name: mysecret key: password restartPolicy: Never 消费环境变量里的 Secret 值\n在一个消耗环境变量 secret 的容器中，secret key 作为包含 secret 数据的 base-64 解码值的常规环境变量。这是从上面的示例在容器内执行的命令的结果：\necho $SECRET_USERNAME admin echo $SECRET_PASSWORD 1f2d1e2e67df 使用 imagePullSecret imagePullSecret 是将包含 Docker（或其他）镜像注册表密码的 secret 传递给 Kubelet 的一种方式，因此可以代表您的 pod 拉取私有镜像。\n手动指定 imagePullSecret\nimagePullSecret 的使用在 镜像文档 中说明。\n安排 imagePullSecrets 自动附加 您可以手动创建 imagePullSecret，并从 serviceAccount 引用它。使用该 serviceAccount 创建的任何 pod 和默认使用该 serviceAccount 的 pod 将会将其的 imagePullSecret 字段设置为服务帐户的 imagePullSecret 字段。有关该过程的详细说明，请参阅 将 ImagePullSecrets 添加到服务帐户。\n自动挂载手动创建的 Secret 手动创建的 secret（例如包含用于访问 github 帐户的令牌）可以根据其服务帐户自动附加到 pod。请参阅 使用 PodPreset 向 Pod 中注入信息 以获取该进程的详细说明。\n详细 限制 验证 secret volume 来源确保指定的对象引用实际上指向一个类型为 Secret 的对象。因此，需要在依赖于它的任何 pod 之前创建一个 secret。\nSecret API 对象驻留在命名空间中。它们只能由同一命名空间中的 pod 引用。\n每个 secret 的大小限制为 1MB。这是为了防止创建非常大的 secret 会耗尽 apiserver 和 kubelet 的内存。然而，创建许多较小的 secret 也可能耗尽内存。更全面得限制 secret 对内存使用的功能还在计划中。\nKubelet 仅支持从 API server 获取的 Pod 使用 secret。这包括使用 kubectl 创建的任何 pod，或间接通过 replication controller 创建的 pod。它不包括通过 kubelet --manifest-url 标志，其 --config 标志或其 REST API 创建的 pod（这些不是创建 pod 的常用方法）。\n必须先创建 secret，除非将它们标记为可选项，否则必须在将其作为环境变量在 pod 中使用之前创建 secret。对不存在的 secret 的引用将阻止其启动。\n使用 secretKeyRef ，引用指定的 secret 中的不存在的 key ，这会阻止 pod 的启动。\n对于通过 envFrom 填充环境变量的 secret，这些环境变量具有被认为是无效环境变量名称的 key 将跳过这些键。该 pod 将被允许启动。将会有一个事件，其原因是 InvalidVariableNames，该消息将包含被跳过的无效键的列表。该示例显示一个 pod，它指的是包含2个无效键，1badkey 和 2alsobad 的默认/mysecret ConfigMap。\nkubectl get events LASTSEEN FIRSTSEEN COUNT NAME KIND SUBOBJECT TYPE REASON 0s 0s 1 dapi-test-pod Pod Warning InvalidEnvironmentVariableNames kubelet, 127.0.0.1 Keys [1badkey, 2alsobad] from the EnvFrom secret default/mysecret were skipped since they are considered invalid environment variable names. Secret 与 Pod 生命周期的联系 通过 API 创建 Pod 时，不会检查应用的 secret 是否存在。一旦 Pod 被调度，kubelet 就会尝试获取该 secret 的值。如果获取不到该 secret，或者暂时无法与 API server 建立连接，kubelet 将会定期重试。Kubelet 将会报告关于 pod 的事件，并解释它无法启动的原因。一旦获取到 secret，kubelet 将创建并装载一个包含它的卷。在所有 pod 的卷被挂载之前，都不会启动 pod 的容器。\n使用案例 使用案例：包含 ssh 密钥的 pod 创建一个包含 ssh key 的 secret：\nkubectl create secret generic ssh-key-secret --from-file=ssh-privatekey=/path/to/.ssh/id_rsa --from-file=ssh-publickey=/path/to/.ssh/id_rsa.pub secret \u0026quot;ssh-key-secret\u0026quot; created 发送自己的 ssh 密钥之前要仔细思考：集群的其他用户可能有权访问该密钥。使用您想要共享 Kubernetes 群集的所有用户可以访问的服务帐户，如果它们遭到入侵，可以撤销。\n现在我们可以创建一个使用 ssh 密钥引用 secret 的 pod，并在一个卷中使用它：\napiVersion: v1 kind: Pod metadata: name: secret-test-pod labels: name: secret-test spec: volumes: - name: secret-volume secret: secretName: ssh-key-secret containers: - name: ssh-test-container image: mySshImage volumeMounts: - name: secret-volume readOnly: true mountPath: \u0026#34;/etc/secret-volume\u0026#34; 当容器中的命令运行时，密钥的片段将可在以下目录：\n/etc/secret-volume/ssh-publickey /etc/secret-volume/ssh-privatekey 然后容器可以自由使用密钥数据建立一个 ssh 连接。\n使用案例：包含 prod/test 凭据的 pod 下面的例子说明一个 pod 消费一个包含 prod 凭据的 secret，另一个 pod 使用测试环境凭据消费 secret。\n通过秘钥生成器制作 kustomization.yaml\nkubectl create secret generic prod-db-secret --from-literal=username=produser --from-literal=password=Y4nys7f11 secret \u0026quot;prod-db-secret\u0026quot; created kubectl create secret generic test-db-secret --from-literal=username=testuser --from-literal=password=iluvtests secret \u0026quot;test-db-secret\u0026quot; created 特殊字符（例如 $, \\*, 和 !）需要转义。 如果您使用的密码具有特殊字符，则需要使用 \\\\ 字符对其进行转义。 例如，如果您的实际密码是 S!B\\*d$zDsb，则应通过以下方式执行命令：\nkubectl create secret generic dev-db-secret --from-literal=username=devuser --from-literal=password=S\\\\!B\\\\\\*d\\\\$zDsb 您无需从文件中转义密码中的特殊字符（ --from-file ）。\n创建 pod ：\n$ cat \u0026lt;\u0026lt;EOF \u0026gt; pod.yaml apiVersion: v1 kind: List items: - kind: Pod apiVersion: v1 metadata: name: prod-db-client-pod labels: name: prod-db-client spec: volumes: - name: secret-volume secret: secretName: prod-db-secret containers: - name: db-client-container image: myClientImage volumeMounts: - name: secret-volume readOnly: true mountPath: \u0026#34;/etc/secret-volume\u0026#34; - kind: Pod apiVersion: v1 metadata: name: test-db-client-pod labels: name: test-db-client spec: volumes: - name: secret-volume secret: secretName: test-db-secret containers: - name: db-client-container image: myClientImage volumeMounts: - name: secret-volume readOnly: true mountPath: \u0026#34;/etc/secret-volume\u0026#34; EOF 加入 Pod 到同样的 kustomization.yaml 文件\n$ cat \u0026lt;\u0026lt;EOF \u0026gt;\u0026gt; kustomization.yaml resources: - pod.yaml EOF 部署所有的对象通过下面的命令\nkubectl apply -k . 这两个容器将在其文件系统上显示以下文件，其中包含每个容器环境的值：\n/etc/secret-volume/username /etc/secret-volume/password 请注意，两个 pod 的 spec 配置中仅有一个字段有所不同；这有助于使用普通的 pod 配置模板创建具有不同功能的 pod。\n您可以使用两个 service account 进一步简化基本 pod spec：一个名为 prod-user 拥有 prod-db-secret ，另一个称为 test-user 拥有 test-db-secret 。然后，pod spec 可以缩短为，例如：\napiVersion: v1 kind: Pod metadata: name: prod-db-client-pod labels: name: prod-db-client spec: serviceAccount: prod-db-client containers: - name: db-client-container image: myClientImage 使用案例：Secret 卷中以点号开头的文件 为了将数据“隐藏”起来（即文件名以点号开头的文件），简单地说让该键以一个点开始。例如，当如下 secret 被挂载到卷中：\napiVersion: v1 kind: Secret metadata: name: dotfile-secret data: .secret-file: dmFsdWUtMg0KDQo= --- apiVersion: v1 kind: Pod metadata: name: secret-dotfiles-pod spec: volumes: - name: secret-volume secret: secretName: dotfile-secret containers: - name: dotfile-test-container image: k8s.gcr.io/busybox command: - ls - \u0026#34;-l\u0026#34; - \u0026#34;/etc/secret-volume\u0026#34; volumeMounts: - name: secret-volume readOnly: true mountPath: \u0026#34;/etc/secret-volume\u0026#34; Secret-volume 将包含一个单独的文件，叫做 .secret-file，dotfile-test-container 的 /etc/secret-volume/.secret-file 路径下将有该文件。\n以点号开头的文件在 ls -l 的输出中被隐藏起来了；列出目录内容时，必须使用 ls -la 才能查看它们。\n使用案例：Secret 仅对 pod 中的一个容器可见 考虑以下一个需要处理 HTTP 请求的程序，执行一些复杂的业务逻辑，然后使用 HMAC 签署一些消息。因为它具有复杂的应用程序逻辑，所以在服务器中可能会出现一个未被注意的远程文件读取漏洞，这可能会将私钥暴露给攻击者。\n这可以在两个容器中分为两个进程：前端容器，用于处理用户交互和业务逻辑，但无法看到私钥；以及可以看到私钥的签名者容器，并且响应来自前端的简单签名请求（例如通过本地主机网络）。\n使用这种分割方法，攻击者现在必须欺骗应用程序服务器才能进行任意的操作，这可能比使其读取文件更难。\n最佳实践 客户端使用 Secret API 当部署与 secret API 交互的应用程序时，应使用 授权策略， 例如 RBAC 来限制访问。\nSecret 中的值对于不同的环境来说重要性可能不同，例如对于 Kubernetes 集群内部（例如 service account 令牌）和集群外部来说就不一样。即使一个应用程序可以理解其期望的与之交互的 secret 有多大的能力，但是同一命名空间中的其他应用程序却可能不这样认为。\n由于这些原因，在命名空间中 watch 和 list secret 的请求是非常强大的功能，应该避免这样的行为，因为列出 secret 可以让客户端检查所有 secret 是否在该命名空间中。在群集中 watch 和 list 所有 secret 的能力应该只保留给最有特权的系统级组件。\n需要访问 secrets API 的应用程序应该根据他们需要的 secret 执行 get 请求。这允许管理员限制对所有 secret 的访问， 同时设置 白名单访问 应用程序需要的各个实例。\n为了提高循环获取的性能，客户端可以设计引用 secret 的资源，然后 watch 资源，在引用更改时重新请求 secret。 此外，还提出了一种 ”批量监控“ API 来让客户端 watch 每个资源，该功能可能会在将来的 Kubernetes 版本中提供。\n安全属性 保护 因为 secret 对象可以独立于使用它们的 pod 而创建，所以在创建、查看和编辑 pod 的流程中 secret 被暴露的风险较小。系统还可以对 secret 对象采取额外的预防措施，例如避免将其写入到磁盘中可能的位置。\n只有当节点上的 pod 需要用到该 secret 时，该 secret 才会被发送到该节点上。它不会被写入磁盘，而是存储在 tmpfs 中。一旦依赖于它的 pod 被删除，它就被删除。\n同一节点上的很多个 pod 可能拥有多个 secret。但是，只有 pod 请求的 secret 在其容器中才是可见的。因此，一个 pod 不能访问另一个 Pod 的 secret。\nPod 中有多个容器。但是，pod 中的每个容器必须请求其挂载卷中的 secret 卷才能在容器内可见。 这可以用于 在 Pod 级别构建安全分区。\n在大多数 Kubernetes 项目维护的发行版中，用户与 API server 之间的通信以及从 API server 到 kubelet 的通信都受到 SSL/TLS 的保护。通过这些通道传输时，secret 受到保护。\nfeature-state for_k8s_version=\u0026quot;v1.13\u0026rdquo; state=\u0026quot;beta\u0026rdquo;\n你可以为 secret 数据开启静态加密，这样秘密信息就不会以明文形式存储到etcd。\n风险  API server 的 secret 数据以纯文本的方式存储在 etcd 中，因此：  管理员应该为集群数据开启静态加密(需求 v1.13 或者更新)。 管理员应该限制 admin 用户访问 etcd； API server 中的 secret 数据位于 etcd 使用的磁盘上；管理员可能希望在不再使用时擦除/粉碎 etcd 使用的磁盘 如果 etcd 运行在集群内，管理员应该确保 etcd 之间的通信使用 SSL/TLS 进行加密。   如果您将 secret 数据编码为 base64 的清单（JSON 或 YAML）文件，共享该文件或将其检入代码库，这样的话该密码将会被泄露。 Base64 编码不是一种加密方式，一样也是纯文本。 应用程序在从卷中读取 secret 后仍然需要保护 secret 的值，例如不会意外记录或发送给不信任方。 可以创建和使用 secret 的 pod 的用户也可以看到该 secret 的值。即使 API server 策略不允许用户读取 secret 对象，用户也可以运行暴露 secret 的 pod。 目前，任何节点的 root 用户都可以通过模拟 kubelet 来读取 API server 中的任何 secret。只有向实际需要它们的节点发送 secret 才能限制单个节点的根漏洞的影响，该功能还在计划中。  "
},
{
	"uri": "https://lijun.in/reference/kubernetes-api/api-index/",
	"title": "v1.17",
	"tags": [],
	"description": "",
	"content": "Kubernetes API v1.17\n"
},
{
	"uri": "https://lijun.in/setup/production-environment/windows/",
	"title": "Windows Kubernetes",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lijun.in/tasks/administer-cluster/manage-resources/quota-memory-cpu-namespace/",
	"title": "为命名空间配置内存和 CPU 配额",
	"tags": [],
	"description": "",
	"content": "本文介绍怎样为命名空间设置容器可用的内存和 CPU 总量。你可以通过 [ResourceQuota](/docs/reference/generated/kubernetes-api/. param \u0026ldquo;version\u0026rdquo; \u0026gt;}}/#resourcequota-v1-core) 对象设置配额.\n. heading \u0026ldquo;prerequisites\u0026rdquo; %}} . include \u0026ldquo;task-tutorial-prereqs.md\u0026rdquo; \u0026gt;}} . version-check \u0026gt;}}\n集群中每个节点至少有1 GiB的内存。\n创建命名空间 创建一个命名空间，以便本练习中创建的资源和集群的其余部分相隔离。\nkubectl create namespace quota-mem-cpu-example 创建 ResourceQuota 这里给出一个 ResourceQuota 对象的配置文件：\n. codenew file=\u0026quot;admin/resource/quota-mem-cpu.yaml\u0026rdquo; \u0026gt;}}\n创建 ResourceQuota\nkubectl create -f https://k8s.io/examples/admin/resource/quota-mem-cpu.yaml --namespace=quota-mem-cpu-example 查看 ResourceQuota 详情：\nkubectl get resourcequota mem-cpu-demo --namespace=quota-mem-cpu-example --output=yaml ResourceQuota 在 quota-mem-cpu-example 命名空间中设置了如下要求：\n 每个容器必须有内存请求和限制，以及 CPU 请求和限制。 所有容器的内存请求总和不能超过1 GiB。 所有容器的内存限制总和不能超过2 GiB。 所有容器的 CPU 请求总和不能超过1 cpu。 所有容器的 CPU 限制总和不能超过2 cpu。  创建 Pod 这里给出 Pod 的配置文件：\n. codenew file=\u0026quot;admin/resource/quota-mem-cpu-pod.yaml\u0026rdquo; \u0026gt;}}\n创建 Pod：\nkubectl create -f https://k8s.io/examples/admin/resource/quota-mem-cpu-pod.yaml --namespace=quota-mem-cpu-example 检查下 Pod 中的容器在运行：\nkubectl get pod quota-mem-cpu-demo --namespace=quota-mem-cpu-example 再查看 ResourceQuota 的详情：\nkubectl get resourcequota mem-cpu-demo --namespace=quota-mem-cpu-example --output=yaml 输出结果显示了配额以及有多少配额已经被使用。你可以看到 Pod 的内存和 CPU 请求值及限制值没有超过配额。\nstatus: hard: limits.cpu: \u0026quot;2\u0026quot; limits.memory: 2Gi requests.cpu: \u0026quot;1\u0026quot; requests.memory: 1Gi used: limits.cpu: 800m limits.memory: 800Mi requests.cpu: 400m requests.memory: 600Mi 尝试创建第二个 Pod 这里给出了第二个 Pod 的配置文件：\n. codenew file=\u0026quot;admin/resource/quota-mem-cpu-pod-2.yaml\u0026rdquo; \u0026gt;}}\n配置文件中，你可以看到 Pod 的内存请求为700 MiB。请注意新的内存请求与已经使用的内存请求只和超过了内存请求的配额。600 MiB + 700 MiB \u0026gt; 1 GiB。\n尝试创建 Pod：\nkubectl create -f https://k8s.io/examples/admin/resource/quota-mem-cpu-pod-2.yaml --namespace=quota-mem-cpu-example 第二个 Pod 不能被创建成功。输出结果显示创建第二个 Pod 会导致内存请求总量超过内存请求配额。\nError from server (Forbidden): error when creating \u0026quot;examples/admin/resource/quota-mem-cpu-pod-2.yaml\u0026quot;: pods \u0026quot;quota-mem-cpu-demo-2\u0026quot; is forbidden: exceeded quota: mem-cpu-demo, requested: requests.memory=700Mi,used: requests.memory=600Mi, limited: requests.memory=1Gi 讨论 如你在本练习中所见，你可以用 ResourceQuota 限制命名空间中所有容器的内存请求总量。同样你也可以限制内存限制总量、CPU 请求总量、CPU 限制总量。\n如果你想对单个容器而不是所有容器进行限制，就请使用 LimitRange。\n数据清理 删除你的命名空间：\nkubectl delete namespace quota-mem-cpu-example . heading \u0026ldquo;whatsnext\u0026rdquo; %}} 集群管理员参考   为命名空间配置默认内存请求和限制\n  为命名空间配置内存限制的最小值和最大值\n  为命名空间配置 CPU 限制的最小值和最大值\n  为命名空间配置内存和 CPU 配额\n  为命名空间配置 Pod 配额\n  为 API 对象配置配额\n  应用开发者参考   为容器和 Pod 分配内存资源\n  为容器和 Pod 分配CPU资源\n  为 Pod 配置 Service 数量\n  "
},
{
	"uri": "https://lijun.in/tutorials/kubernetes-basics/scale/",
	"title": "伸缩您的应用",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lijun.in/tasks/administer-cluster/network-policy-provider/weave-network-policy/",
	"title": "使用 Weave Net 作为 NetworkPolicy",
	"tags": [],
	"description": "",
	"content": "本页展示了如何使用使用 Weave Net 作为 NetworkPolicy。\n. heading \u0026ldquo;prerequisites\u0026rdquo; %}} 您需要拥有一个 Kubernetes 集群。按照kubeadm 入门指南来引导一个。\n安装 Weave Net 插件 按照通过插件集成Kubernetes指南。\nKubernetes 的 Weave Net 插件带有网络策略控制器，可自动监控 Kubernetes 所有名称空间中的任何 NetworkPolicy 注释。 配置iptables规则以允许或阻止策略指示的流量。\n测试安装 验证 weave 是否有效。\n输入以下命令：\nkubectl get po -n kube-system -o wide 输出类似这样：\nNAME READY STATUS RESTARTS AGE IP NODE weave-net-1t1qg 2/2 Running 0 9d 192.168.2.10 worknode3 weave-net-231d7 2/2 Running 1 7d 10.2.0.17 worknodegpu weave-net-7nmwt 2/2 Running 3 9d 192.168.2.131 masternode weave-net-pmw8w 2/2 Running 0 9d 192.168.2.216 worknode2 每个 Node 都有一个 weave Pod，所有 Pod 都是Running和2/2 READY。（2/2表示每个Pod都有weave和weave-npc。）\n. heading \u0026ldquo;whatsnext\u0026rdquo; %}} 安装Weave Net插件后，您可以按照声明网络策略来试用 Kubernetes NetworkPolicy。 如果您有任何疑问，请联系我们#weave-community on Slack 或 Weave User Group。\n"
},
{
	"uri": "https://lijun.in/concepts/configuration/assign-pod-node/",
	"title": "将 Pod 分配给节点",
	"tags": [],
	"description": "",
	"content": "你可以约束一个 text=\u0026quot;Pod\u0026rdquo; term_id=\u0026quot;pod\u0026rdquo; \u0026gt;}} 只能在特定的 text=\u0026quot;Node(s)\u0026rdquo; term_id=\u0026quot;node\u0026rdquo; \u0026gt;}} 上运行，或者优先运行在特定的节点上。有几种方法可以实现这点，推荐的方法都是用标签选择器来进行选择。通常这样的约束不是必须的，因为调度器将自动进行合理的放置（比如，将 pod 分散到节点上，而不是将 pod 放置在可用资源不足的节点上等等），但在某些情况下，你可以需要更多控制 pod 停靠的节点，例如，确保 pod 最终落在连接了 SSD 的机器上，或者将来自两个不同的服务且有大量通信的 pod 放置在同一个可用区。\nnodeSelector nodeSelector 是节点选择约束的最简单推荐形式。nodeSelector 是 PodSpec 的一个字段。它指定键值对的映射。为了使 pod 可以在节点上运行，节点必须具有每个指定的键值对作为标签（它也可以具有其他标签）。最常用的是一对键值对。\n让我们来看一个使用 nodeSelector 的例子。\n步骤零：先决条件 本示例假设你已基本了解 Kubernetes 的 pod 并且已经建立一个 Kubernetes 集群。\n步骤一：添加标签到节点 执行 kubectl get nodes 命令获取集群的节点名称。选择一个你要增加标签的节点，然后执行 kubectl label nodes \u0026lt;node-name\u0026gt; \u0026lt;label-key\u0026gt;=\u0026lt;label-value\u0026gt; 命令将标签添加到你所选择的节点上。例如，如果你的节点名称为 \u0026lsquo;kubernetes-foo-node-1.c.a-robinson.internal\u0026rsquo; 并且想要的标签是 \u0026lsquo;disktype=ssd\u0026rsquo;，则可以执行 kubectl label nodes kubernetes-foo-node-1.c.a-robinson.internal disktype=ssd 命令。\n你可以通过重新运行 kubectl get nodes --show-labels 并且查看节点当前具有了一个标签来验证它是否有效。你也可以使用 kubectl describe node \u0026quot;nodename\u0026quot; 命令查看指定节点的标签完整列表。\n步骤二：添加 nodeSelector 字段到 pod 配置中 拿任意一个你想运行的 pod 的配置文件，并且在其中添加一个 nodeSelector 部分。例如，如果下面是我的 pod 配置：\napiVersion: v1 kind: Pod metadata: name: nginx labels: env: test spec: containers: - name: nginx image: nginx 然后像下面这样添加 nodeSelector：\nfile=\u0026quot;pods/pod-nginx.yaml\u0026rdquo; \u0026gt;}}\n当你之后运行 kubectl apply -f https://k8s.io/examples/pods/pod-nginx.yaml 命令，pod 将会调度到将标签添加到的节点上。你可以通过运行 kubectl get pods -o wide 并查看分配给 pod 的 “NODE” 来验证其是否有效。\n插曲：内置的节点标签 除了你附加的标签外，节点还预先填充了一组标准标签。这些标签是\n kubernetes.io/hostname failure-domain.beta.kubernetes.io/zone failure-domain.beta.kubernetes.io/region topology.kubernetes.io/zone topology.kubernetes.io/region beta.kubernetes.io/instance-type node.kubernetes.io/instance-type kubernetes.io/os kubernetes.io/arch  这些标签的值是特定于云供应商的，因此不能保证可靠。例如，kubernetes.io/hostname 的值在某些环境中可能与节点名称相同，但在其他环境中可能是一个不同的值。\n节点隔离/限制 向 Node 对象添加标签可以将 pod 定位到特定的节点或节点组。这可以用来确保指定的 pod 只能运行在具有一定隔离性，安全性或监管属性的节点上。当为此目的使用标签时，强烈建议选择节点上的 kubelet 进程无法修改的标签键。这可以防止受感染的节点使用其 kubelet 凭据在自己的 Node 对象上设置这些标签，并影响调度器将工作负载调度到受感染的节点。\nNodeRestriction 准入插件防止 kubelet 使用 node-restriction.kubernetes.io/ 前缀设置或修改标签。要使用该标签前缀进行节点隔离：\n 检查是否在使用 Kubernetes v1.11+，以便 NodeRestriction 功能可用。 确保你在使用节点授权并且已经_启用_ NodeRestriction 准入插件。 将 node-restriction.kubernetes.io/ 前缀下的标签添加到 Node 对象，然后在节点选择器中使用这些标签。例如，example.com.node-restriction.kubernetes.io/fips=true 或 example.com.node-restriction.kubernetes.io/pci-dss=true。  亲和与反亲和 nodeSelector 提供了一种非常简单的方法来将 pod 约束到具有特定标签的节点上。亲和/反亲和功能极大地扩展了你可以表达约束的类型。关键的增强点是\n 语言更具表现力（不仅仅是“完全匹配的 AND”） 你可以发现规则是“软”/“偏好”，而不是硬性要求，因此，如果调度器无法满足该要求，仍然调度该 pod 你可以使用节点上（或其他拓扑域中）的 pod 的标签来约束，而不是使用节点本身的标签，来允许哪些 pod 可以或者不可以被放置在一起。  亲和功能包含两种类型的亲和，即“节点亲和”和“pod 间亲和/反亲和”。节点亲和就像现有的 nodeSelector（但具有上面列出的前两个好处），然而 pod 间亲和/反亲和约束 pod 标签而不是节点标签（在上面列出的第三项中描述，除了具有上面列出的第一和第二属性）。\n节点亲和 节点亲和概念上类似于 nodeSelector，它使你可以根据节点上的标签来约束 pod 可以调度到哪些节点。\n目前有两种类型的节点亲和，分别为 requiredDuringSchedulingIgnoredDuringExecution 和 preferredDuringSchedulingIgnoredDuringExecution。你可以视它们为“硬”和“软”，意思是，前者指定了将 pod 调度到一个节点上必须满足的规则（就像 nodeSelector 但使用更具表现力的语法），后者指定调度器将尝试执行但不能保证的偏好。名称的“IgnoredDuringExecution”部分意味着，类似于 nodeSelector 的工作原理，如果节点的标签在运行时发生变更，从而不再满足 pod 上的亲和规则，那么 pod 将仍然继续在该节点上运行。将来我们计划提供 requiredDuringSchedulingRequiredDuringExecution，它将类似于 requiredDuringSchedulingIgnoredDuringExecution，除了它会将 pod 从不再满足 pod 的节点亲和要求的节点上驱逐。\n因此，requiredDuringSchedulingIgnoredDuringExecution 的示例将是“仅将 pod 运行在具有 Intel CPU 的节点上”，而 preferredDuringSchedulingIgnoredDuringExecution 的示例为“尝试将这组 pod 运行在 XYZ 故障区域，如果这不可能的话，则允许一些 pod 在其他地方运行”。\n节点亲和通过 PodSpec 的 affinity 字段下的 nodeAffinity 字段进行指定。\n下面是一个使用节点亲和的 pod 的实例：\nfile=\u0026quot;pods/pod-with-node-affinity.yaml\u0026rdquo; \u0026gt;}}\n此节点亲和规则表示，pod 只能放置在具有标签键为 kubernetes.io/e2e-az-name 且 标签值为 e2e-az1 或 e2e-az2 的节点上。另外，在满足这些标准的节点中，具有标签键为 another-node-label-key 且标签值为 another-node-label-value 的节点应该优先使用。\n你可以在上面的例子中看到 In 操作符的使用。新的节点亲和语法支持下面的操作符： In，NotIn，Exists，DoesNotExist，Gt，Lt。你可以使用 NotIn 和 DoesNotExist 来实现节点反亲和行为，或者使用节点污点将 pod 从特定节点中驱逐。\n如果你同时指定了 nodeSelector 和 nodeAffinity，两者必须都要满足，才能将 pod 调度到候选节点上。\n如果你指定了多个与 nodeAffinity 类型关联的 nodeSelectorTerms，则如果其中一个 nodeSelectorTerms 满足的话，pod将可以调度到节点上。\n如果你指定了多个与 nodeSelectorTerms 关联的 matchExpressions，则只有当所有 matchExpressions 满足的话，pod 才会可以调度到节点上。\n如果你修改或删除了 pod 所调度到的节点的标签，pod 不会被删除。换句话说，亲和选择只在 pod 调度期间有效。\npreferredDuringSchedulingIgnoredDuringExecution 中的 weight 字段值的范围是 1-100。对于每个符合所有调度要求（资源请求，RequiredDuringScheduling 亲和表达式等）的节点，调度器将遍历该字段的元素来计算总和，并且如果节点匹配对应的MatchExpressions，则添加“权重”到总和。然后将这个评分与该节点的其他优先级函数的评分进行组合。总分最高的节点是最优选的。\npod 间亲和与反亲和 pod 间亲和与反亲和使你可以基于已经在节点上运行的 pod 的标签来约束 pod 可以调度到的节点，而不是基于节点上的标签。规则的格式为“如果 X 节点上已经运行了一个或多个 满足规则 Y 的pod，则这个 pod 应该（或者在非亲和的情况下不应该）运行在 X 节点”。Y 表示一个具有可选的关联命令空间列表的 LabelSelector；与节点不同，因为 pod 是命名空间限定的（因此 pod 上的标签也是命名空间限定的），因此作用于 pod 标签的标签选择器必须指定选择器应用在哪个命名空间。从概念上讲，X 是一个拓扑域，如节点，机架，云供应商地区，云供应商区域等。你可以使用 topologyKey 来表示它，topologyKey 是节点标签的键以便系统用来表示这样的拓扑域。请参阅上面插曲：内置的节点标签部分中列出的标签键。\nPod 间亲和与反亲和需要大量的处理，这可能会显著减慢大规模集群中的调度。我们不建议在超过数百个节点的集群中使用它们。\nPod 反亲和需要对节点进行一致的标记，即集群中的每个节点必须具有适当的标签能够匹配 topologyKey。如果某些或所有节点缺少指定的 topologyKey 标签，可能会导致意外行为。\n与节点亲和一样，当前有两种类型的 pod 亲和与反亲和，即 requiredDuringSchedulingIgnoredDuringExecution 和 preferredDuringSchedulingIgnoredDuringExecution，分表表示“硬性”与“软性”要求。请参阅前面节点亲和部分中的描述。requiredDuringSchedulingIgnoredDuringExecution 亲和的一个示例是“将服务 A 和服务 B 的 pod 放置在同一区域，因为它们之间进行大量交流”，而 preferredDuringSchedulingIgnoredDuringExecution 反亲和的示例将是“将此服务的 pod 跨区域分布”（硬性要求是说不通的，因为你可能拥有的 pod 数多于区域数）。\nPod 间亲和通过 PodSpec 中 affinity 字段下的 podAffinity 字段进行指定。而 pod 间反亲和通过 PodSpec 中 affinity 字段下的 podAntiAffinity 字段进行指定。\nPod 使用 pod 亲和 的示例： file=\u0026quot;pods/pod-with-pod-affinity.yaml\u0026rdquo; \u0026gt;}}\n在这个 pod 的 affinity 配置定义了一条 pod 亲和规则和一条 pod 反亲和规则。在此示例中，podAffinity 配置为 requiredDuringSchedulingIgnoredDuringExecution，然而 podAntiAffinity 配置为 preferredDuringSchedulingIgnoredDuringExecution。pod 亲和规则表示，仅当节点和至少一个已运行且有键为“security”且值为“S1”的标签的 pod 处于同一区域时，才可以将该 pod 调度到节点上。（更确切的说，如果节点 N 具有带有键 failure-domain.beta.kubernetes.io/zone 和某个值 V 的标签，则 pod 有资格在节点 N 上运行，以便集群中至少有一个节点具有键 failure-domain.beta.kubernetes.io/zone 和值为 V 的节点正在运行具有键“security”和值“S1”的标签的 pod。）pod 反亲和规则表示，如果节点已经运行了一个具有键“security”和值“S2”的标签的 pod，则该 pod 不希望将其调度到该节点上。（如果 topologyKey 为 failure-domain.beta.kubernetes.io/zone，则意味着当节点和具有键“security”和值“S2”的标签的 pod 处于相同的区域，pod 不能被调度到该节点上。）查阅设计文档来获取更多 pod 亲和与反亲和的样例，包括 requiredDuringSchedulingIgnoredDuringExecution 和 preferredDuringSchedulingIgnoredDuringExecution 两种配置。\nPod 亲和与反亲和的合法操作符有 In，NotIn，Exists，DoesNotExist。\n原则上，topologyKey 可以是任何合法的标签键。然而，出于性能和安全原因，topologyKey 受到一些限制：\n 对于亲和与 requiredDuringSchedulingIgnoredDuringExecution 要求的 pod 反亲和，topologyKey 不允许为空。 对于 requiredDuringSchedulingIgnoredDuringExecution 要求的 pod 反亲和，准入控制器 LimitPodHardAntiAffinityTopology 被引入来限制 topologyKey 不为 kubernetes.io/hostname。如果你想使它可用于自定义拓扑结构，你必须修改准入控制器或者禁用它。 对于 preferredDuringSchedulingIgnoredDuringExecution 要求的 pod 反亲和，空的 topologyKey 被解释为“所有拓扑结构”（这里的“所有拓扑结构”限制为 kubernetes.io/hostname，failure-domain.beta.kubernetes.io/zone 和 failure-domain.beta.kubernetes.io/region 的组合）。 除上述情况外，topologyKey 可以是任何合法的标签键。  除了 labelSelector 和 topologyKey，你也可以指定表示命名空间的 namespaces 队列，labelSelector 也应该匹配它（这个与 labelSelector 和 topologyKey 的定义位于相同的级别）。如果忽略或者为空，则默认为 pod 亲和/反亲和的定义所在的命名空间。\n所有与 requiredDuringSchedulingIgnoredDuringExecution 亲和与反亲和关联的 matchExpressions 必须满足，才能将 pod 调度到节点上。\n更实际的用例 Pod 间亲和与反亲和在与更高级别的集合（例如 ReplicaSets，StatefulSets，Deployments 等）一起使用时，它们可能更加有用。可以轻松配置一组应位于相同定义拓扑（例如，节点）中的工作负载。\n始终放置在相同节点上 在三节点集群中，一个 web 应用程序具有内存缓存，例如 redis。我们希望 web 服务器尽可能与缓存放置在同一位置。\n下面是一个简单 redis deployment 的 yaml 代码段，它有三个副本和选择器标签 app=store。Deployment 配置了 PodAntiAffinity，用来确保调度器不会将副本调度到单个节点上。\napiVersion: apps/v1 kind: Deployment metadata: name: redis-cache spec: selector: matchLabels: app: store replicas: 3 template: metadata: labels: app: store spec: affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: app operator: In values: - store topologyKey: \u0026#34;kubernetes.io/hostname\u0026#34; containers: - name: redis-server image: redis:3.2-alpine 下面 webserver deployment 的 yaml 代码段中配置了 podAntiAffinity 和 podAffinity。这将通知调度器将它的所有副本与具有 app=store 选择器标签的 pod 放置在一起。这还确保每个 web 服务器副本不会调度到单个节点上。\napiVersion: apps/v1 kind: Deployment metadata: name: web-server spec: selector: matchLabels: app: web-store replicas: 3 template: metadata: labels: app: web-store spec: affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: app operator: In values: - web-store topologyKey: \u0026#34;kubernetes.io/hostname\u0026#34; podAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: app operator: In values: - store topologyKey: \u0026#34;kubernetes.io/hostname\u0026#34; containers: - name: web-app image: nginx:1.16-alpine 如果我们创建了上面的两个 deployment，我们的三节点集群将如下表所示。\n   node-1 node-2 node-3     webserver-1 webserver-2 webserver-3   cache-1 cache-2 cache-3    如你所见，web-server 的三个副本都按照预期那样自动放置在同一位置。\nkubectl get pods -o wide 输出类似于如下内容：\nNAME READY STATUS RESTARTS AGE IP NODE redis-cache-1450370735-6dzlj 1/1 Running 0 8m 10.192.4.2 kube-node-3 redis-cache-1450370735-j2j96 1/1 Running 0 8m 10.192.2.2 kube-node-1 redis-cache-1450370735-z73mh 1/1 Running 0 8m 10.192.3.1 kube-node-2 web-server-1287567482-5d4dz 1/1 Running 0 7m 10.192.2.3 kube-node-1 web-server-1287567482-6f7v5 1/1 Running 0 7m 10.192.4.3 kube-node-3 web-server-1287567482-s330j 1/1 Running 0 7m 10.192.3.2 kube-node-2 永远不放置在相同节点 上面的例子使用 PodAntiAffinity 规则和 topologyKey: \u0026quot;kubernetes.io/hostname\u0026quot; 来部署 redis 集群以便在同一主机上没有两个实例。参阅 ZooKeeper 教程，以获取配置反亲和来达到高可用性的 StatefulSet 的样例（使用了相同的技巧）。\nnodeName nodeName 是节点选择约束的最简单方法，但是由于其自身限制，通常不使用它。nodeName 是 PodSpec 的一个字段。如果它不为空，调度器将忽略 pod，并且运行在它指定节点上的 kubelet 进程尝试运行该 pod。因此，如果 nodeName 在 PodSpec 中指定了，则它优先于上面的节点选择方法。\n使用 nodeName 来选择节点的一些限制：\n 如果指定的节点不存在， 如果指定的节点没有资源来容纳 pod，pod 将会调度失败并且其原因将显示为，比如 OutOfmemory 或 OutOfcpu。 云环境中的节点名称并非总是可预测或稳定的。  下面的是使用 nodeName 字段的 pod 配置文件的例子：\napiVersion: v1 kind: Pod metadata: name: nginx spec: containers: - name: nginx image: nginx nodeName: kube-01 上面的 pod 将运行在 kube-01 节点上。\n 污点允许节点排斥一组 pod。\n节点亲和与 pod 间亲和/反亲和的设计文档包含这些功能的其他背景信息。\n一旦 pod 分配给 节点，kubelet 应用将运行该 pod 并且分配节点本地资源。拓扑管理\n"
},
{
	"uri": "https://lijun.in/concepts/overview/working-with-objects/annotations/",
	"title": "注解",
	"tags": [],
	"description": "",
	"content": "你可以使用 Kubernetes 注解为对象附加任意的非标识的元数据。客户端程序（例如工具和库）能够获取这些元数据信息。\n为对象附加元数据 您可以使用标签或注解将元数据附加到 Kubernetes 对象。 标签可以用来选择对象和查找满足某些条件的对象集合。 相反，注解不用于标识和选择对象。 注解中的元数据，可以很小，也可以很大，可以是结构化的，也可以是非结构化的，能够包含标签不允许的字符。\n注解和标签一样，是键/值对:\n\u0026#34;metadata\u0026#34;: { \u0026#34;annotations\u0026#34;: { \u0026#34;key1\u0026#34; : \u0026#34;value1\u0026#34;, \u0026#34;key2\u0026#34; : \u0026#34;value2\u0026#34; } } 以下是一些例子，用来说明哪些信息可以使用注解来记录:\n 由声明性配置所管理的字段。 将这些字段附加为注解，能够将它们与客户端或服务端设置的默认值、自动生成的字段以及通过自动调整大小或自动伸缩系统设置的字段区分开来。   构建、发布或镜像信息（如时间戳、发布 ID、Git 分支、PR 数量、镜像哈希、仓库地址）。   指向日志记录、监控、分析或审计仓库的指针。   可用于调试目的的客户端库或工具信息：例如，名称、版本和构建信息。   用户或者工具/系统的来源信息，例如来自其他生态系统组件的相关对象的 URL。   推出的轻量级工具的元数据信息：例如，配置或检查点。   负责人员的电话或呼机号码，或指定在何处可以找到该信息的目录条目，如团队网站。  从用户到最终运行的指令，以修改行为或使用非标准功能。\n您可以将这类信息存储在外部数据库或目录中而不使用注解，但这样做就使得开发人员很难生成用于部署、管理、自检的客户端共享库和工具。\n语法和字符集 注解 存储的形式是键/值对。有效的注解键分为两部分：可选的前缀和名称，以斜杠（/）分隔。 名称段是必需项，并且必须在63个字符以内，以字母数字字符（[a-z0-9A-Z]）开头和结尾，并允许使用破折号（-），下划线（_），点（.）和字母数字。 前缀是可选的。 如果指定，则前缀必须是DNS子域：一系列由点（.）分隔的DNS标签，总计不超过253个字符，后跟斜杠（/）。 如果省略前缀，则假定注释键对用户是私有的。 由系统组件添加的注释（例如，kube-scheduler，kube-controller-manager，kube-apiserver，kubectl 或其他第三方组件），必须为终端用户添加注释前缀。\nkubernetes.io / 和 k8s.io / 前缀是为Kubernetes核心组件保留的。\n例如，这是Pod的配置文件，其注释为 imageregistry：https：// hub.docker.com / ：\napiVersion: v1 kind: Pod metadata: name: annotations-demo annotations: imageregistry: \u0026#34;https://hub.docker.com/\u0026#34; spec: containers: - name: nginx image: nginx:1.7.9 ports: - containerPort: 80  进一步了解标签和选择器。\n"
},
{
	"uri": "https://lijun.in/concepts/services-networking/network-policies/",
	"title": "网络策略",
	"tags": [],
	"description": "",
	"content": "网络策略（NetworkPolicy）是一种关于 text=\u0026quot;Pod\u0026rdquo; term_id=\u0026quot;pod\u0026quot;\u0026gt;}} 间及与其他网络端点间所允许的通信规则的规范。\nNetworkPolicy 资源使用 text=\u0026quot;标签\u0026rdquo; term_id=\u0026quot;label\u0026quot;\u0026gt;}} 选择 Pod，并定义选定 Pod 所允许的通信规则。\n前提 网络策略通过网络插件来实现。要使用网络策略，用户必须使用支持 NetworkPolicy 的网络解决方案。创建一个资源对象，而没有控制器来使它生效的话，是没有任何作用的。\n隔离和非隔离的 Pod 默认情况下，Pod 是非隔离的，它们接受任何来源的流量。\nPod 可以通过相关的网络策略进行隔离。一旦命名空间中有网络策略选择了特定的 Pod，该 Pod 会拒绝网络策略所不允许的连接。 (命名空间下其他未被网络策略所选择的 Pod 会继续接收所有的流量)\n网络策略不会冲突，它们是附加的。如果任何一个或多个策略选择了一个 Pod, 则该 Pod 受限于这些策略的 ingress/egress 规则的并集。因此评估的顺序并不会影响策略的结果。\nNetworkPolicy 资源 查看 [网络策略](/docs/reference/generated/kubernetes-api/ param \u0026ldquo;version\u0026rdquo; \u0026gt;}}/#networkpolicy-v1-networking-k8s-io) 来了解完整的资源定义。\n下面是一个 NetworkPolicy 的示例:\napiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: test-network-policy namespace: default spec: podSelector: matchLabels: role: db policyTypes: - Ingress - Egress ingress: - from: - ipBlock: cidr: 172.17.0.0/16 except: - 172.17.1.0/24 - namespaceSelector: matchLabels: project: myproject - podSelector: matchLabels: role: frontend ports: - protocol: TCP port: 6379 egress: - to: - ipBlock: cidr: 10.0.0.0/24 ports: - protocol: TCP port: 5978 除非选择支持网络策略的网络解决方案，否则将上述示例发送到API服务器没有任何效果。\n必填字段: 与所有其他的 Kubernetes 配置一样，NetworkPolicy 需要 apiVersion、 kind 和 metadata 字段。 关于配置文件操作的一般信息，请参考 使用 ConfigMap 配置容器, 和 对象管理。\nspec: NetworkPolicy 规约 中包含了在一个命名空间中定义特定网络策略所需的所有信息。\npodSelector: 每个 NetworkPolicy 都包括一个 podSelector ，它对该策略所应用的一组 Pod 进行选择。示例中的策略选择带有 \u0026ldquo;role=db\u0026rdquo; 标签的 Pod。空的 podSelector 选择命名空间下的所有 Pod。\npolicyTypes: 每个 NetworkPolicy 都包含一个 policyTypes 列表，其中包含 Ingress 或 Egress 或两者兼具。policyTypes 字段表示给定的策略是否应用于进入所选 Pod 的入口流量或者来自所选 Pod 的出口流量，或两者兼有。如果 NetworkPolicy 未指定 policyTypes 则默认情况下始终设置 Ingress，如果 NetworkPolicy 有任何出口规则的话则设置 Egress。\ningress: 每个 NetworkPolicy 可包含一个 ingress 规则的白名单列表。每个规则都允许同时匹配 from 和 ports 部分的流量。示例策略中包含一条简单的规则： 它匹配一个单一的端口，来自三个来源中的一个， 第一个通过 ipBlock 指定，第二个通过 namespaceSelector 指定，第三个通过 podSelector 指定。\negress: 每个 NetworkPolicy 可包含一个 egress 规则的白名单列表。每个规则都允许匹配 to 和 port 部分的流量。该示例策略包含一条规则，该规则将单个端口上的流量匹配到 10.0.0.0/24 中的任何目的地。\n所以，该网络策略示例:\n 隔离 \u0026ldquo;default\u0026rdquo; 命名空间下 \u0026ldquo;role=db\u0026rdquo; 的 Pod (如果它们不是已经被隔离的话)。 （Ingress 规则）允许以下 Pod 连接到 \u0026ldquo;default\u0026rdquo; 命名空间下的带有 “role=db” 标签的所有 Pod 的 6379 TCP 端口：   \u0026ldquo;default\u0026rdquo; 命名空间下任意带有 \u0026ldquo;role=frontend\u0026rdquo; 标签的 Pod 带有 \u0026ldquo;project=myproject\u0026rdquo; 标签的任意命名空间中的 Pod IP 地址范围为 172.17.0.0–172.17.0.255 和 172.17.2.0–172.17.255.255（即，除了 172.17.1.0/24 之外的所有 172.17.0.0/16）  （Egress 规则）允许从带有 \u0026ldquo;role=db\u0026rdquo; 标签的命名空间下的任何 Pod 到 CIDR 10.0.0.0/24 下 5978 TCP 端口的连接。  查看 声明网络策略 来进行更多的示例演练。\n选择器 to 和 from 的行为 可以在 ingress from 部分或 egress to 部分中指定四种选择器：\npodSelector: 这将在与 NetworkPolicy 相同的命名空间中选择特定的 Pod，应将其允许作为入口源或出口目的地。\nnamespaceSelector: 这将选择特定的命名空间，应将所有 Pod 用作其输入源或输出目的地。\nnamespaceSelector 和 podSelector: 一个指定 namespaceSelector 和 podSelector 的 to/from 条目选择特定命名空间中的特定 Pod。注意使用正确的 YAML 语法；这项策略：\n... ingress: - from: - namespaceSelector: matchLabels: user: alice podSelector: matchLabels: role: client ... 在 from 数组中仅包含一个元素，只允许来自标有 role=client 的 Pod 且该 Pod 所在的命名空间中标有 user=alice 的连接。但是 这项 策略：\n... ingress: - from: - namespaceSelector: matchLabels: user: alice - podSelector: matchLabels: role: client ... 在 from 数组中包含两个元素，允许来自本地命名空间中标有 role=client 的 Pod 的连接，或 来自任何命名空间中标有 user = alice 的任何 Pod 的连接。\n如有疑问，请使用 kubectl describe 查看 Kubernetes 如何解释该策略。\nipBlock: 这将选择特定的 IP CIDR 范围以用作入口源或出口目的地。 这些应该是群集外部 IP，因为 Pod IP 存在时间短暂的且随机产生。\n群集的入口和出口机制通常需要重写数据包的源 IP 或目标 IP。在发生这种情况的情况下，不确定在 NetworkPolicy 处理之前还是之后发生，并且对于网络插件，云提供商，Service 实现等的不同组合，其行为可能会有所不同。\n在进入的情况下，这意味着在某些情况下，您可以根据实际的原始源 IP 过滤传入的数据包，而在其他情况下，NetworkPolicy 所作用的 源IP 则可能是 LoadBalancer 或 Pod 的节点等。\n对于出口，这意味着从 Pod 到被重写为集群外部 IP 的 Service IP 的连接可能会或可能不会受到基于 ipBlock 的策略的约束。\n默认策略 默认情况下，如果命名空间中不存在任何策略，则所有进出该命名空间中的 Pod 的流量都被允许。以下示例使您可以更改该命名空间中的默认行为。\n默认拒绝所有入口流量 您可以通过创建选择所有容器但不允许任何进入这些容器的入口流量的 NetworkPolicy 来为命名空间创建 \u0026ldquo;default\u0026rdquo; 隔离策略。\ncodenew file=\u0026quot;service/networking/network-policy-default-deny-ingress.yaml\u0026rdquo; \u0026gt;}}\n这样可以确保即使容器没有选择其他任何 NetworkPolicy，也仍然可以被隔离。此策略不会更改默认的出口隔离行为。\n默认允许所有入口流量 如果要允许所有流量进入某个命名空间中的所有 Pod（即使添加了导致某些 Pod 被视为“隔离”的策略），则可以创建一个策略来明确允许该命名空间中的所有流量。\ncodenew file=\u0026quot;service/networking/network-policy-allow-all-ingress.yaml\u0026rdquo; \u0026gt;}}\n默认拒绝所有出口流量 您可以通过创建选择所有容器但不允许来自这些容器的任何出口流量的 NetworkPolicy 来为命名空间创建 \u0026ldquo;default\u0026rdquo; egress 隔离策略。\ncodenew file=\u0026quot;service/networking/network-policy-default-deny-egress.yaml\u0026rdquo; \u0026gt;}}\n这样可以确保即使没有被其他任何 NetworkPolicy 选择的 Pod 也不会被允许流出流量。此策略不会更改默认的 ingress 隔离行为。\n默认允许所有出口流量 如果要允许来自命名空间中所有 Pod 的所有流量（即使添加了导致某些 Pod 被视为“隔离”的策略），则可以创建一个策略，该策略明确允许该命名空间中的所有出口流量。\ncodenew file=\u0026quot;service/networking/network-policy-allow-all-egress.yaml\u0026rdquo; \u0026gt;}}\n默认拒绝所有入口和所有出口流量 您可以为命名空间创建 \u0026ldquo;default\u0026rdquo; 策略，以通过在该命名空间中创建以下 NetworkPolicy 来阻止所有入站和出站流量。\ncodenew file=\u0026quot;service/networking/network-policy-default-deny-all.yaml\u0026rdquo; \u0026gt;}}\n这样可以确保即使没有被其他任何 NetworkPolicy 选择的 Pod 也不会被允许进入或流出流量。\nSCTP 支持 feature-state for_k8s_version=\u0026quot;v1.12\u0026rdquo; state=\u0026quot;alpha\u0026rdquo; \u0026gt;}}\n要启用此特性，你（或你的集群管理员）需要通过为 API server 指定 --feature-gates=SCTPSupport=true,… 来启用 SCTPSupport 特性开关。启用该特性开关后，用户可以将 NetworkPolicy 的 protocol 字段设置为 SCTP。\n必须使用支持 SCTP 协议网络策略的 CNI 插件。\nwhatsnext\n 查看 声明网络策略 来进行更多的示例演练 有关 NetworkPolicy 资源启用的常见场景的更多信息，请参见 指南。  "
},
{
	"uri": "https://lijun.in/tasks/configure-pod-container/configure-volume-storage/",
	"title": "配置 Pod 以使用卷进行存储",
	"tags": [],
	"description": "",
	"content": "此页面展示了如何配置 Pod 以使用卷进行存储。\n只要容器存在，容器的文件系统就会存在，因此当一个容器终止并重新启动，对该容器的文件系统改动将丢失。对于独立于容器的持久化存储，您可以使用卷。这对于有状态应用程序尤为重要，例如键值存储（如 Redis）和数据库。\n. heading \u0026ldquo;prerequisites\u0026rdquo; %}} . include \u0026ldquo;task-tutorial-prereqs.md\u0026rdquo; \u0026gt;}} . version-check \u0026gt;}}\n为 Pod 配置卷 在本练习中，您将创建一个运行 Pod，该 Pod 仅运行一个容器并拥有一个类型为 emptyDir 的卷，在整个 Pod 生命周期中一直存在，即使 Pod 中的容器被终止和重启。以下是 Pod 的配置：\n. codenew file=\u0026quot;pods/storage/redis.yaml\u0026rdquo; \u0026gt;}}\n 创建 Pod:  ```shell kubectl apply -f https://k8s.io/examples/pods/storage/redis.yaml ```   验证 Pod 中的容器是否正在运行，然后留意 Pod 的更改：  ```shell kubectl get pod redis --watch ``` 输出如下： ```shell NAME READY STATUS RESTARTS AGE redis 1/1 Running 0 13s ```   在另一个终端，用 shell 连接正在运行的容器：  ```shell kubectl exec -it redis -- /bin/bash ```   在您的 shell 终端中，切换到 /data/redis 目录下，然后创建一个文件：  ```shell root@redis:/data# cd /data/redis/ root@redis:/data/redis# echo Hello \u0026gt; test-file ```   在您的 shell 终端中，列出正在运行的进程：  ```shell root@redis:/data/redis# apt-get update root@redis:/data/redis# apt-get install procps root@redis:/data/redis# ps aux ``` 输出类似于： ```shell USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND redis 1 0.1 0.1 33308 3828 ? Ssl 00:46 0:00 redis-server *:6379 root 12 0.0 0.0 20228 3020 ? Ss 00:47 0:00 /bin/bash root 15 0.0 0.0 17500 2072 ? R+ 00:48 0:00 ps aux ```   在您的 shell 终端中，结束 Redis 进程：  ```shell root@redis:/data/redis# kill \u0026lt;pid\u0026gt; ``` 其中 `\u0026lt;pid\u0026gt;` 是 Redis 进程的 ID (PID)。   在您原先终端中，留意 Redis Pod 的更改。最终您将会看到和下面类似的输出：  ```shell NAME READY STATUS RESTARTS AGE redis 1/1 Running 0 13s redis 0/1 Completed 0 6m redis 1/1 Running 1 6m ```  此时，容器已经终止并重新启动。这是因为 Redis Pod 的 [restartPolicy](/docs/reference/generated/kubernetes-api/. param \u0026ldquo;version\u0026rdquo; \u0026gt;}}/#podspec-v1-core) 为 Always。\n 用 shell 终端进入重新启动的容器中：  ```shell kubectl exec -it redis -- /bin/bash ```   在您的 shell 终端中，进入到 /data/redis 目录下，并确认 test-file 文件是否仍然存在。  ```shell root@redis:/data/redis# cd /data/redis/ root@redis:/data/redis# ls test-file ```   删除为此练习所创建的 Pod：  ```shell kubectl delete pod redis ```  . heading \u0026ldquo;whatsnext\u0026rdquo; %}}   参阅[卷](/docs/reference/generated/kubernetes-api/. param \u0026ldquo;version\u0026rdquo; \u0026gt;}}/#volume-v1-core)。\n  参阅 [Pod](/docs/reference/generated/kubernetes-api/. param \u0026ldquo;version\u0026rdquo; \u0026gt;}}/#pod-v1-core)。\n  除了 emptyDir 提供的本地磁盘存储外，Kubernetes 还支持许多不同的网络附加存储解决方案，包括 GCE 上的 PD 和 EC2 上的 EBS，它们是关键数据的首选，并将处理节点上的一些细节，例如安装和卸载设备。了解更多详情请参阅卷。\n  "
},
{
	"uri": "https://lijun.in/concepts/cluster-administration/networking/",
	"title": "集群网络系统",
	"tags": [],
	"description": "",
	"content": "集群网络系统是 Kubernetes 的核心部分，但是想要准确了解它的工作原理可是个不小的挑战。下面列出的是网络系统的的四个主要问题：\n 高度耦合的容器间通信：这个已经被 pods 和 localhost 通信解决了。 Pod 间通信：这个是本文档的重点要讲述的。 Pod 和 Service 间通信：这个已经在 services 里讲述过了。 外部和 Service 间通信：这个也已经在 services 讲述过了。  Kubernetes 的宗旨就是在应用之间共享机器。通常来说，共享机器需要两个应用之间不能使用相同的端口，但是在多个应用开发者之间去大规模地协调端口是件很困难的事情，尤其是还要让用户暴露在他们控制范围之外的集群级别的问题上。\n动态分配端口也会给系统带来很多复杂度 - 每个应用都需要设置一个端口的参数，而 API 服务器还需要知道如何将动态端口数值插入到配置模块中，服务也需要知道如何找到对方等等。与其去解决这些问题，Kubernetes 选择了其他不同的方法。\nKubernetes 网络模型 每一个 Pod 都有它自己的IP地址，这就意味着你不需要显式地在每个 Pod 之间创建链接，你几乎不需要处理容器端口到主机端口之间的映射。这将创建一个干净的、向后兼容的模型，在这个模型里，从端口分配、命名、服务发现、负载均衡、应用配置和迁移的角度来看，Pod 可以被视作虚拟机或者物理主机。\nKubernetes 对所有网络设施的实施，都需要满足以下的基本要求（除非有设置一些特定的网络分段策略）：\n 节点上的 pods 可以不通过 NAT 和其他任何节点上的 pods 通信 节点上的代理（比如：系统守护进程、kubelet） 可以和节点上的所有pods通信  备注：仅针对那些支持 Pods 在主机网络中运行的平台(比如：Linux) ：\n 那些运行在节点的主机网络里的 pods 可以不通过 NAT 和所有节点上的 pods 通信  这个模型不仅不复杂，而且还和 Kubernetes 的实现廉价的从虚拟机向容器迁移的初衷相兼容，如果你的工作开始是在虚拟机中运行的，你的虚拟机有一个 IP ，这样就可以和其他的虚拟机进行通信，这是基本相同的模型。\nKubernetes 的 IP 地址存在于 Pod 范围内 - 容器分享他们的网络命名空间 - 包括他们的 IP 地址。这就意味着 Pod 内的容器都可以通过 localhost 到达各个端口。这也意味着 Pod 内的容器都需要相互协调端口的使用，但是这和虚拟机中的进程似乎没有什么不同，这也被称为“一个 pod 一个 IP” 模型。\n如何实现这一点是正在使用的容器运行时的特定信息。\n也可以在 node 本身通过端口去请求你的 Pod （称之为主机端口），但这是一个很特殊的操作。转发方式如何实现也是容器运行时的细节。Pod 自己并不知道这些主机端口是否存在。\n如何实现 Kubernetes 的网络模型 有很多种方式可以实现这种网络模型，本文档并不是对各种实现技术的详细研究，但是希望可以作为对各种技术的详细介绍，并且成为你研究的起点。\n接下来的网络技术是按照首字母排序，并无其他任何含义。\nACI Cisco Application Centric Infrastructure 提供了一个集成覆盖和底层 SDN 解决方案来支持容器、虚拟机和其他裸机服务器。ACI 为ACI提供了容器网络集成。点击这里查看概述\nAntrea Antrea 项目是一个开源的，旨在成为 Kubernetes 原生的网络解决方案。它利用 Open vSwitch 作为网络数据平面。Open vSwitch 是一个高性能可编程的虚拟交换机，支持 Linux 和 Windows 平台。Open vSwitch 使 Antrea 能够以高性能和高效的方式实现 Kubernetes 的网络策略。借助 Open vSwitch 可编程的特性， Antrea 能够在 Open vSwitch 之上实现广泛的网络，安全功能和服务。\nApstra 中的 AOS AOS 是一个基于意图的网络系统，可以通过一个简单的集成平台创建和管理复杂的数据中心环境。AOS 利用高度可扩展的分布式设计来消除网络中断，同时将成本降至最低。\nAOS 参考设计当前支持三层连接的主机，这些主机消除了旧的两层连接的交换问题。这些三层连接的主机可以是 Linux（Debian、Ubuntu、CentOS）系统，它们直接在机架式交换机（TOR）的顶部创建 BGP 邻居关系。AOS 自动执行路由邻接，然后提供对 Kubernetes 部署中常见的路由运行状况注入（RHI）的精细控制。\nAOS 具有一组丰富的 REST API 端点，这些端点使 Kubernetes 能够根据应用程序需求快速更改网络策略。进一步的增强功能将用于网络设计的 AOS Graph 模型与工作负载供应集成在一起，从而为私有云和公共云提供端到端管理系统。\nAOS 支持使用包括 Cisco、Arista、Dell、Mellanox、HPE 在内的制造商提供的通用供应商设备，以及大量白盒系统和开放网络操作系统，例如 Microsoft SONiC、Dell OPX 和 Cumulus Linux 。\n想要更详细地了解 AOS 系统是如何工作的可以点击这里： http://www.apstra.com/products/how-it-works/\nKubernetes 的 AWS VPC CNI AWS VPC CNI 为 Kubernetes 集群提供了集成的 AWS 虚拟私有云（VPC）网络。该 CNI 插件提供了高吞吐量和可用性，低延迟以及最小的网络抖动。此外，用户可以使用现有的 AWS VPC 网络和安全最佳实践来构建 Kubernetes 集群。这包括使用 VPC 流日志，VPC 路由策略和安全组进行网络流量隔离的功能。\n使用该 CNI 插件，可使 Kubernetes Pods 在 Pod 中拥有与在 VPC 网络上相同的 IP 地址。CNI 将 AWS 弹性网络接口（ENI）分配给每个 Kubernetes 节点，并将每个 ENI 的辅助 IP 范围用于该节点上的 Pod 。CNI 包含用于 ENI 和 IP 地址的预分配的控件，以便加快 Pod 的启动时间，并且能够支持多达2000个节点的大型集群。\n此外，CNI可以与用于执行网络策略的 Calico一起运行。 AWS VPC CNI项目是开源的，查看 GitHub 上的文档。\nKubernetes 的 Azure CNI Azure CNI 是一个开源插件，将 Kubernetes Pods 和 Azure 虚拟网络（也称为 VNet）集成在一起，可提供与 VN 相当的网络性能。Pod 可以通过 Express Route 或者 站点到站点的 VPN 来连接到对等的 VNet ，也可以从这些网络来直接访问 Pod。Pod 可以访问受服务端点或者受保护链接的 Azure 服务，比如存储和 SQL。你可以使用 VNet 安全策略和路由来筛选 Pod 流量。该插件通过利用在 Kubernetes 节点的网络接口上预分配的辅助 IP 池将 VNet 分配给 Pod 。\nAzure CNI 可以在 Azure Kubernetes Service (AKS) 中获得。\nBig Switch Networks 的 Big Cloud Fabric Big Cloud Fabric 是一个基于云原生的网络架构，旨在在私有云或者本地环境中运行 Kubernetes。它使用统一的物理和虚拟 SDN，Big Cloud Fabric 解决了固有的容器网络问题，比如负载均衡、可见性、故障排除、安全策略和容器流量监控。\n在 Big Cloud Fabric 的虚拟 Pod 多租户架构的帮助下，容器编排系统（比如 Kubernetes、RedHat OpenShift、Mesosphere DC/OS 和 Docker Swarm）将于VM本地编排系统（比如 VMware、OpenStack 和 Nutanix）进行本地集成。客户将能够安全地互联任意数量的这些集群，并且在需要时启用他们之间的租户间通信。\n在最新的 Magic Quadrant 上，BCF 被 Gartner 认为是非常有远见的。而 BCF 的一条关于 Kubernetes 的本地部署（其中包括 Kubernetes、DC/OS 和在不同地理区域的多个 DC 上运行的 VMware）也在这里被引用。\nCilium Cilium 是一个开源软件，用于提供并透明保护应用容器间的网络连接。Cilium 支持 L7/HTTP ，可以在 L3-L7 上通过使用与网络分离的基于身份的安全模型寻址来实施网络策略，并且可以与其他 CNI 插件结合使用。\n华为的 CNI-Genie CNI-Genie 是一个 CNI 插件，可以让 Kubernetes 在运行时允许不同的 Kubernetes 的网络模型的实现同时被访问。这包括以 CNI 插件运行的任何实现，比如 Flannel、Calico、Romana、Weave-net。\nCNI-Genie 还支持将多个 IP 地址分配给 Pod，每个都来自不同的 CNI 插件。\ncni-ipvlan-vpc-k8s cni-ipvlan-vpc-k8s 包含了一组 CNI 和 IPAM 插件来提供一个简单的、本地主机、低延迟、高吞吐量以及通过使用 Amazon 弹性网络接口（ENI）并使用 Linux 内核的 IPv2 驱动程序以 L2 模式将 AWS 管理的 IP 绑定到 Pod 中，在 Amazon Virtual Private Cloud（VPC）环境中为 Kubernetes 兼容的网络堆栈。\n这些插件旨在直接在 VPC 中进行配置和部署，Kubelets 先启动，然后根据需要进行自我配置和扩展它们的 IP 使用率，而无需经常建议复杂的管理覆盖网络， BGP ，禁用源/目标检查，或调整 VPC 路由表以向每个主机提供每个实例子网的复杂性（每个 VPC 限制为50-100个条目）。简而言之， cni-ipvlan-vpc-k8s 大大降低了在 AWS 中大规模部署 Kubernetes 所需的网络复杂性。\nContiv Contiv 为各种使用情况提供了一个可配置网络（使用了 BGP 的本地 l3 ，使用 vxlan 的覆盖，经典 l2 或 Cisco-SDN/ACI）。Contiv 是完全开源的。\nContrail / Tungsten Fabric Contrail 是基于 Tungsten Fabric 的，真正开放的，多云网络虚拟化和策略管理平台。Contrail 和 Tungsten Fabric 与各种编排系统集成在一起，例如 Kubernetes，OpenShift，OpenStack 和 Mesos，并为虚拟机、容器或 Pods 以及裸机工作负载提供了不同的隔离模式。\nDANM DANM 是一个针对在 Kubernetes 集群中运行的电信工作负载的网络解决方案。它由以下几个组件构成：\n* 能够配置具有高级功能的 IPVLAN 接口的 CNI 插件 * 一个内置的 IPAM 模块，能够管理多个、群集内的、不连续的 L3 网络，并按请求提供动态、静态或无 IP 分配方案 * CNI 元插件能够通过自己的 CNI 或通过将任务授权给其他任何流行的 CNI 解决方案（例如 SRI-OV 或 Flannel）来实现将多个网络接口连接到容器 * Kubernetes 控制器能够集中管理所有 Kubernetes 主机的 VxLAN 和 VLAN 接口 * 另一个 Kubernetes 控制器扩展了 Kubernetes 的基于服务的服务发现概念，以在 Pod 的所有网络接口上工作  通过这个工具集，DANM 可以提供多个分离的网络接口，可以为 pods 使用不同的网络后端和高级 IPAM 功能。\nFlannel Flannel 是一个非常简单的能够满足 Kubernetes 所需要的重叠网络。已经有许多人报告了使用 Flannel 和 Kubernetes 的成功案例。\nGoogle Compute Engine (GCE) 对于 Google Compute Engine 的集群配置脚本，advanced routing 用于为每个虚机分配一个子网（默认是 /24 - 254个 IP），绑定到该子网的任何流量都将通过 GCE 网络结构直接路由到虚机。这是除了分配给虚机的“主要” IP 地址之外的一个补充，该 IP 地址经过 NAT 转换以用于访问外网。linux网桥（称为“cbr0”）被配置为存在于该子网中，并被传递到 docker 的 \u0026ndash;bridge 参数上。\nDocker 会以这样的参数启动：\nDOCKER_OPTS=\u0026#34;--bridge=cbr0 --iptables=false --ip-masq=false\u0026#34; 这个网桥是由 Kubelet（由 \u0026ndash;network-plugin=kubenet 参数控制）根据节点的 .spec.podCIDR 参数创建的。\nDocker 将会从 cbr-cidr 块分配 IP 。容器之间可以通过 cbr0 网桥相互访问，也可以访问节点。这些 IP 都可以在 GCE 的网络中被路由。\n而 GCE 本身并不知道这些 IP，所以不会对访问外网的流量进行 NAT，为了实现此目的，使用了 iptables 规则来伪装（又称为 SNAT，使数据包看起来好像是来自“节点”本身），将通信绑定到 GCE 项目网络（10.0.0.0/8）之外的 IP。\niptables -t nat -A POSTROUTING ! -d 10.0.0.0/8 -o eth0 -j MASQUERADE 最后，在内核中启用了 IP 转发（因此内核将处理桥接容器的数据包）：\nsysctl net.ipv4.ip_forward=1 所有这些的结果是所有 Pods 都可以互相访问，并且可以将流量发送到互联网。\nJaguar Jaguar 是一个基于 OpenDaylight 的 Kubernetes 网络开源解决方案。Jaguar 使用 vxlan 提供覆盖网络，而 Jaguar CNIPlugin 为每个 Pod 提供一个 IP 地址。\nk-vswitch k-vswitch 是一个基于 Open vSwitch 的简易 Kubernetes 网络插件。它利用 Open vSwitch 中现有的功能来提供强大的网络插件，该插件易于操作，高效且安全。\nKnitter Knitter 是一个支持 Kubernetes 中实现多个网络系统的解决方案。它提供了租户管理和网络管理的功能。除了多个网络平面外，Knitter 还包括一组端到端的 NFV 容器网络解决方案，例如为应用程序保留 IP 地址，IP 地址迁移等。\nKube-OVN Kube-OVN 是一个基于 OVN 的用于企业的 Kubernetes 网络架构。借助于 OVN/OVS ，它提供了一些高级覆盖网络功能，例如子网、QoS、静态 IP 分配、流量镜像、网关、基于开放流的网络策略和服务代理。\nKube-router Kube-router 是 Kubernetes 的专用网络解决方案，旨在提供高性能和易操作性。 Kube-router 提供了一个基于 Linux LVS/IPVS 的服务代理，一个基于 Linux 内核转发的无覆盖 Pod-to-Pod 网络解决方案，和基于 iptables/ipset 的网络策略执行器。\nL2 networks and linux bridging 如果你具有一个“哑”的L2网络，例如“裸机”环境中的简单交换机，则应该能够执行与上述 GCE 设置类似的操作。请注意，这些说明仅是非常简单的尝试过-似乎可行，但尚未经过全面测试。如果您使用此技术并完善了流程，请告诉我们。\n根据 Lars Kellogg-Stedman 的这份非常不错的“Linux 网桥设备”使用说明来进行操作。\nMultus (a Multi Network plugin) Multus 是一个多 CNI 插件，使用 Kubernetes 中基于 CRD 的网络对象来支持实现 Kubernetes 多网络系统。\nMultus 支持所有[参考插件]（https://github.com/containernetworking/plugins）（比如： Flannel、DHCP、Macvlan ），来实现 CNI 规范和第三方插件（比如： Calico、Weave、Cilium、Contiv）。除此之外， Multus 还支持 SRIOV、DPDK、OVS-DPDK \u0026amp; VPP 的工作负载，以及 Kubernetes 中基于云的本机应用程序和基于 NFV 的应用程序。\nNSX-T VMware NSX-T 是一个网络虚拟化的安全平台。 NSX-T 可以为多云及多系统管理程序环境提供网络虚拟化，并专注于具有异构端点和技术堆栈的新兴应用程序框架和体系结构。除了 vSphere 管理程序之外，这些环境还包括其他虚拟机管理程序，例如 KVM，容器和裸机。\nNSX-T Container Plug-in (NCP) 提供了 NSX-T 与容器协调器（例如 Kubernetes）之间的结合， 以及 NSX-T 与基于容器的 CaaS/PaaS 平台（例如 Pivotal Container Service（PKS） 和 OpenShift ）之间的集成。\nNuage Networks VCS (Virtualized Cloud Services) Nuage 提供了一个高度可扩展的基于策略的软件定义网络（SDN）平台，Nuage 使用开源的 Open vSwitch 作为数据平面，以及基于开放标准构建具有丰富功能的 SDN 控制器。\nNuage 平台使用覆盖层在 Kubernetes Pod 和非 Kubernetes 环境（VM 和裸机服务器）之间提供基于策略的无缝联网。Nuage 的策略抽象模型在设计时就考虑到了应用程序，并且可以轻松声明应用程序的细粒度策略。该平台的实时分析引擎可为 Kubernetes 应用程序提供可见性和安全性监控。\nOpenVSwitch OpenVSwitch 是一个较为成熟的解决方案，但同时也增加了构建覆盖网络的复杂性，这也得到了几个网络系统的“大商店”的拥护。\nOVN (开放式虚拟网络) OVN 是一个由 Open vSwitch 社区开发的开源的网络虚拟化解决方案。它允许创建逻辑交换器，逻辑路由，状态 ACL，负载均衡等等来建立不同的虚拟网络拓扑。该项目有一个特定的Kubernetes插件和文档 ovn-kubernetes。\nProject Calico Project Calico 是一个开源的容器网络提供者和网络策略引擎。\nCalico 提供了高度可扩展的网络和网络解决方案，使用基于与 Internet 相同的 IP 网络原理来连接 Kubernetes Pod，适用于 Linux （开放源代码）和 Windows（专有-可从 Tigera 获得。可以无需封装或覆盖即可部署 Calico，以提供高性能，高可扩的数据中心网络。Calico 还通过其分布式防火墙为 Kubernetes Pod 提供了基于意图的细粒度网络安全策略。\nCalico 还可以和其他的网络解决方案（比如 Flannel、canal 或本机 GCE、AWS、Azure 等）一起以策略实施模式运行。\nRomana Romana 是一个开源网络和安全自动化解决方案。它可以让你在没有覆盖网络的情况下部署 Kubernetes。Romana 支持 Kubernetes 网络策略，来提供跨网络命名空间的隔离。\nWeaveworks 的 Weave Net Weave Net 是 Kubernetes 及其托管应用程序的弹性和易于使用的网络系统。Weave Net 可以作为 CNI plug-in 运行或者独立运行。在这两种运行方式里，都不需要任何配置或额外的代码即可运行，并且在两种情况下，网络都为每个 Pod 提供一个 IP 地址-这是 Kubernetes 的标准配置。\n 网络模型的早期设计、运行原理以及未来的一些计划，都在 networking design document 文档里进行了更详细的描述。\n"
},
{
	"uri": "https://lijun.in/setup/production-environment/tools/kubeadm/ha-topology/",
	"title": "高可用拓扑选项",
	"tags": [],
	"description": "",
	"content": "本页面介绍了配置高可用（HA） Kubernetes 集群拓扑的两个选项。\n您可以设置 HA 集群：\n 使用堆叠（stacked）控制平面节点，其中 etcd 节点与控制平面节点共存 使用外部 etcd 节点，其中 etcd 在与控制平面不同的节点上运行  在设置 HA 集群之前，您应该仔细考虑每种拓扑的优缺点。\n. note \u0026gt;}} kubeadm 静态引导 etcd 集群。 阅读 etcd 集群指南以获得更多详细信息。 . /note \u0026gt;}}\n堆叠（Stacked） etcd 拓扑 堆叠（Stacked） HA 集群是一种这样的拓扑，其中 etcd 分布式数据存储集群堆叠在 kubeadm 管理的控制平面节点上，作为控制平面的一个组件运行。\n每个控制平面节点运行 kube-apiserver，kube-scheduler 和 kube-controller-manager 实例。\nkube-apiserver 使用负载均衡器暴露给工作节点。\n每个控制平面节点创建一个本地 etcd 成员（member），这个 etcd 成员只与该节点的 kube-apiserver 通信。这同样适用于本地 kube-controller-manager 和 kube-scheduler 实例。\n这种拓扑将控制平面和 etcd 成员耦合在同一节点上。相对使用外部 etcd 集群，设置起来更简单，而且更易于副本管理。\n然而，堆叠集群存在耦合失败的风险。如果一个节点发生故障，则 etcd 成员和控制平面实例都将丢失，并且冗余会受到影响。您可以通过添加更多控制平面节点来降低此风险。\n因此，您应该为 HA 集群运行至少三个堆叠的控制平面节点。\n这是 kubeadm 中的默认拓扑。当使用 kubeadm init 和 kubeadm join --control-plane 时，在控制平面节点上会自动创建本地 etcd 成员。\n！堆叠的 etcd 拓扑\n外部 etcd 拓扑 具有外部 etcd 的 HA 集群是一种这样的拓扑，其中 etcd 分布式数据存储集群在独立于控制平面节点的其他节点上运行。\n就像堆叠的 etcd 拓扑一样，外部 etcd 拓扑中的每个控制平面节点都运行 kube-apiserver，kube-scheduler 和 kube-controller-manager 实例。同样， kube-apiserver 使用负载均衡器暴露给工作节点。但是，etcd 成员在不同的主机上运行，​​每个 etcd 主机与每个控制平面节点的 kube-apiserver 通信。\n这种拓扑结构解耦了控制平面和 etcd 成员。因此，它提供了一种 HA 设置，其中失去控制平面实例或者 etcd 成员的影响较小，并且不会像堆叠的 HA 拓扑那样影响集群冗余。\n但是，此拓扑需要两倍于堆叠 HA 拓扑的主机数量。\n具有此拓扑的 HA 集群至少需要三个用于控制平面节点的主机和三个用于 etcd 节点的主机。\n！外部 etcd 拓扑\n. heading \u0026ldquo;whatsnext\u0026rdquo; %}}  使用 kubeadm 设置高可用集群  "
},
{
	"uri": "https://lijun.in/concepts/workloads/",
	"title": "😊 - Workloads",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lijun.in/concepts/containers/",
	"title": "😊 - 容器",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lijun.in/reference/setup-tools/",
	"title": "😍 - 安装工具",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lijun.in/tutorials/stateful-application/",
	"title": "😎 - 有状态的应用",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lijun.in/tasks/",
	"title": "😝 - 任务",
	"tags": [],
	"description": "",
	"content": ". toc \u0026gt;}}\nKubernetes 文档这一部分包含的一些页面展示如何去做单个任务。一个任务页面展示了如何执行操作单一的项目，通常是通过给出若干步骤。\nWeb 用户界面 (Dashboard) 部署和访问 Dashboard Web 用户界面，以帮助您管理和监控 Kubernetes 集群中的容器化应用程序。\n使用 kubectl 命令行 下载并设置用于直接管理 Kubernetes 集群的 kubectl 命令行工具。\n配置 Pod 和容器 执行 Pod 和容器的常见配置任务。\n运行应用程序 执行常见的应用程序管理任务，例如滚动更新、将信息注入 Pod、以及 Pod 水平自动伸缩。\n运行 Job 使用并行处理方式运行 Job。\n访问集群中的应用程序 配置负载均衡和端口转发、或者设置防火墙、或者配置 DNS 以访问集群中的应用程序。\n监控、日志记录和调试 设置监控和日志记录以对集群进行故障排除或者调试容器化应用程序。\n访问 Kubernetes API 了解直接访问 Kubernetes API 的各种方法。\n使用 TLS 配置您的应用程序去信任和使用集群的根证书机构（ CA ）。\n管理集群 了解管理集群的常见任务。\n管理联邦 配置集群联邦中的组件。\n管理状态应用程序 执行管理有状态应用程序的常见任务，包括扩展、删除和调试 StatefulSets。\n集群 Daemons 执行管理 DaemonSet 的常见任务，例如执行滚动更新。\n管理 GPU 配置和调度 NVIDIA GPU 作为集群中节点使用的资源。\n管理 HugePage 配置和调度 HugePage 作为集群中的可调度资源。\nheading \u0026ldquo;whatsnext\u0026rdquo; 如果您想编写任务页面，请参阅创建文档提取请求。\n"
},
{
	"uri": "https://lijun.in/tasks/job/",
	"title": "😝 - 运行 Jobs",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lijun.in/reference/setup-tools/kubeadm/kubeadm-reset/",
	"title": "kubeadm reset",
	"tags": [],
	"description": "",
	"content": "该命令尽力还原由 kubeadm init 或 kubeadm join 所做的更改。\n. include \u0026ldquo;generated/kubeadm_reset.md\u0026rdquo; \u0026gt;}}\nReset 工作流程 kubeadm reset 负责从使用 kubeadm init 或 kubeadm join 命令创建的文件中清除节点本地文件系统。对于控制平面节点，reset 还从 etcd 集群中删除该节点的本地 etcd 堆成员，还从 kubeadm ClusterStatus 对象中删除该节点的信息。 ClusterStatus 是一个 kubeadm 管理的 Kubernetes API 对象，该对象包含 kube-apiserver 端点列表。\nkubeadm reset phase 可用于执行上述工作流程的各个阶段。 要跳过阶段列表，您可以使用 --skip-phases 参数，该参数的工作方式类似于 kubeadm join 和 kubeadm init 阶段运行器。\n外部 etcd 清理 如果使用了外部 etcd，kubeadm reset 将不会删除任何 etcd 中的数据。这意味着，如果再次使用相同的 etcd 端点运行 kubeadm init，您将看到先前集群的状态。\n要清理 etcd 中的数据，建议您使用 etcdctl 这样的客户端，例如：\netcdctl del \u0026#34;\u0026#34; --prefix 更多详情请参考 etcd 文档。\n. heading \u0026ldquo;whatsnext\u0026rdquo; %}}  参考 kubeadm init 来初始化 Kubernetes 主节点。 参考 kubeadm join 来初始化 Kubernetes 工作节点并加入集群。  "
},
{
	"uri": "https://lijun.in/concepts/cluster-administration/monitoring/",
	"title": "Kubernetes 控制面的指标",
	"tags": [],
	"description": "",
	"content": "系统组件的指标可以让我们更好的看清系统内部究竟发生了什么，尤其对于构建仪表盘和告警都非常有用。\nKubernetes 控制面板中的指标是以 prometheus 格式发出的，而且是易于阅读的。\nKubernetes 的指标 在大多数情况下，指标在 HTTP 服务器的 /metrics 端点使用，对于默认情况下不暴露端点的组件，可以使用 --bind-address 参数启用。\n举例下面这些组件：\n term_id=\u0026quot;kube-controller-manager\u0026rdquo; text=\u0026quot;kube-controller-manager\u0026rdquo; \u0026gt;}} term_id=\u0026quot;kube-proxy\u0026rdquo; text=\u0026quot;kube-proxy\u0026rdquo; \u0026gt;}} term_id=\u0026quot;kube-apiserver\u0026rdquo; text=\u0026quot;kube-apiserver\u0026rdquo; \u0026gt;}} term_id=\u0026quot;kube-scheduler\u0026rdquo; text=\u0026quot;kube-scheduler\u0026rdquo; \u0026gt;}} term_id=\u0026quot;kubelet\u0026rdquo; text=\u0026quot;kubelet\u0026rdquo; \u0026gt;}}  在生产环境中，你可能需要配置 Prometheus Server 或其他指标收集器来定期收集这些指标，并使它们在某种时间序列数据库中可用。\n请注意 term_id=\u0026quot;kubelet\u0026rdquo; text=\u0026quot;kubelet\u0026rdquo; \u0026gt;}} 同样在 /metrics/cadvisor、/metrics/resource 和 /metrics/probes 等端点提供性能指标。这些指标的生命周期并不相同。\n如果你的集群还使用了 term_id=\u0026quot;rbac\u0026rdquo; text=\u0026quot;RBAC\u0026rdquo; \u0026gt;}} ，那读取指标数据的时候，还需要通过具有 ClusterRole 的用户、组或者 ServiceAccount 来进行授权，才有权限访问 /metrics 。\n举例：\napiVersion: rbac.authorization.k8s.io/v1\tkind: ClusterRole\tmetadata:\tname: prometheus\trules:\t- nonResourceURLs:\t- \u0026quot;/metrics\u0026quot;\tverbs:\t- get 指标的生命周期 内测版指标 → 稳定版指标 → 弃用指标 → 隐藏指标 → 删除\n内测版指标没有任何稳定性保证，因此可能随时被修改或删除。\n稳定版指标可以保证不会改变，具体的说，稳定就意味着：\n 这个指标自身不会被删除或者重命名。 这个指标类型不会被更改  弃用指标表明这个指标最终将会被删除，要想查找是哪个版本，你需要检查其注释，注释中包括该指标从哪个 kubernetes 版本被弃用。\n指标弃用前：\n# HELP some_counter this counts things # TYPE some_counter counter some_counter 0 指标弃用后：\n# HELP some_counter (Deprecated since 1.15.0) this counts things # TYPE some_counter counter some_counter 0 一个指标一旦被隐藏，默认这个指标是不会发布来被抓取的。如果你想要使用这个隐藏指标，你需要覆盖相关集群组件的配置。\n一个指标一旦被删除，那这个指标就不会被发布，您也不可以通过覆盖配置来进行更改。\n显示隐藏指标 综上所述，管理员可以通过在运行可执行文件时添加一些特定的参数来开启一些隐藏的指标。当管理员错过了之前版本的的一些已弃用的指标时，这个可被视作是一个后门。\nshow-hidden-metrics-for-version 参数可以指定一个版本，用来显示这个版本中被隐藏的指标。这个版本号形式是x.y，x 是主要版本号，y 是次要版本号。补丁版本并不是必须的，尽管在一些补丁版本中也会有一些指标会被弃用，因为指标弃用策略主要是针对次要版本。\n这个参数只能使用上一版本作为其值，如果管理员将上一版本设置为 show-hidden-metrics-for-version 的值，那么就会显示上一版本所有被隐藏的指标，太老的版本是不允许的，因为这不符合指标弃用策略。\n以指标 A 为例，这里假设 A 指标在 1.n 版本中被弃用，根据指标弃用策略，我们可以得出以下结论：\n 在 1.n 版本中，这个指标被弃用，并且默认情况下，这个指标还是可以发出. 在 1.n+1 版本中，这个指标默认被隐藏，你可以通过设置参数 show-hidden-metrics-for-version=1.n 来使它可以被发出. 在 1.n+2 版本中，这个指标就被从代码库中删除，也不会再有后门了.  如果你想要从 1.12 版本升级到 1.13 ，但仍然需要依赖指标 A ，你可以通过命令行设置隐藏指标 --show-hidden-metrics=1.12 ，但是在升级到 1.14时就必须要删除这个指标的依赖，因为这个版本中这个指标已经被删除了。\n组件指标 kube-controller-manager 指标 控制器管理器指标提供了有关控制器管理器性能和运行状况的重要见解。这些指标包括常见的一些 Go 语言运行时的重要指标（比如 go_routine 的数量）和一些控制器的特定指标（比如 etcd 的请求时延），还有一些云供应商（比如 AWS、GCE、OpenStack）的 API 请求延迟，用来评估集群的整体运行状况。\n从 Kubernetes 1.7 开始，详细的云供应商指标便可用于 GCE、 AWS、Vsphere 和 OpenStack 的存储操作，这些指标可用于监控持久卷运行时的健康状况。\n举例，GCE 的这些指标是这些：\ncloudprovider_gce_api_request_duration_seconds { request = \u0026quot;instance_list\u0026quot;} cloudprovider_gce_api_request_duration_seconds { request = \u0026quot;disk_insert\u0026quot;} cloudprovider_gce_api_request_duration_seconds { request = \u0026quot;disk_delete\u0026quot;} cloudprovider_gce_api_request_duration_seconds { request = \u0026quot;attach_disk\u0026quot;} cloudprovider_gce_api_request_duration_seconds { request = \u0026quot;detach_disk\u0026quot;} cloudprovider_gce_api_request_duration_seconds { request = \u0026quot;list_disk\u0026quot;}   了解有关 Prometheus 指标相关的文本格式 查看 Kubernetes 稳定版指标列表 了解有关 Kubernetes 指标弃用策略  "
},
{
	"uri": "https://lijun.in/concepts/services-networking/add-entries-to-pod-etc-hosts-with-host-aliases/",
	"title": "使用 HostAliases 向 Pod /etc/hosts 文件添加条目",
	"tags": [],
	"description": "",
	"content": "当 DNS 配置以及其它选项不合理的时候，通过向 Pod 的 /etc/hosts 文件中添加条目，可以在 Pod 级别覆盖对主机名的解析。在 1.7 版本，用户可以通过 PodSpec 的 HostAliases 字段来添加这些自定义的条目。\n建议通过使用 HostAliases 来进行修改，因为该文件由 Kubelet 管理，并且可以在 Pod 创建/重启过程中被重写。\n默认 hosts 文件内容 让我们从一个 Nginx Pod 开始，给该 Pod 分配一个 IP：\nkubectl run nginx --image nginx --generator=run-pod/v1 pod/nginx created 检查Pod IP：\nkubectl get pods --output=wide NAME READY STATUS RESTARTS AGE IP NODE nginx 1/1 Running 0 13s 10.200.0.4 worker0 主机文件的内容如下所示：\nkubectl exec nginx -- cat /etc/hosts # Kubernetes-managed hosts file. 127.0.0.1\tlocalhost ::1\tlocalhost ip6-localhost ip6-loopback fe00::0\tip6-localnet fe00::0\tip6-mcastprefix fe00::1\tip6-allnodes fe00::2\tip6-allrouters 10.200.0.4\tnginx 默认，hosts 文件只包含 ipv4 和 ipv6 的样板内容，像 localhost 和主机名称。\n通过 HostAliases 增加额外的条目 除了默认的样板内容，我们可以向 hosts 文件添加额外的条目，将 foo.local、 bar.local 解析为127.0.0.1， 将 foo.remote、 bar.remote 解析为 10.1.2.3，我们可以在 .spec.hostAliases 下为 Pod 添加 HostAliases。\ncodenew file=\u0026quot;service/networking/hostaliases-pod.yaml\u0026rdquo; \u0026gt;}}\n可以使用以下命令启动此Pod：\nkubectl apply -f hostaliases-pod.yaml pod/hostaliases-pod created 检查Pod IP 和状态：\nkubectl get pod --output=wide NAME READY STATUS RESTARTS AGE IP NODE hostaliases-pod 0/1 Completed 0 6s 10.200.0.5 worker0 hosts 文件的内容看起来类似如下这样：\nkubectl logs hostaliases-pod # Kubernetes-managed hosts file. 127.0.0.1\tlocalhost ::1\tlocalhost ip6-localhost ip6-loopback fe00::0\tip6-localnet fe00::0\tip6-mcastprefix fe00::1\tip6-allnodes fe00::2\tip6-allrouters 10.200.0.5\thostaliases-pod # Entries added by HostAliases. 127.0.0.1\tfoo.local\tbar.local 10.1.2.3\tfoo.remote\tbar.remote 在最下面额外添加了一些条目。\n为什么 Kubelet 管理 hosts文件？ kubelet 管理 Pod 中每个容器的 hosts 文件，避免 Docker 在容器已经启动之后去 修改 该文件。\n因为该文件是托管性质的文件，无论容器重启或 Pod 重新调度，用户修改该 hosts 文件的任何内容，都会在 Kubelet 重新安装后被覆盖。因此，不建议修改该文件的内容。\n"
},
{
	"uri": "https://lijun.in/concepts/configuration/organize-cluster-access-kubeconfig/",
	"title": "使用 kubeconfig 文件组织集群访问",
	"tags": [],
	"description": "",
	"content": "使用 kubeconfig 文件来组织有关集群、用户、命名空间和身份认证机制的信息。kubectl 命令行工具使用 kubeconfig 文件来查找选择集群所需的信息，并与集群的 API 服务器进行通信。\n用于配置集群访问的文件称为 kubeconfig 文件。这是引用配置文件的通用方法。这并不意味着有一个名为 kubeconfig 的文件\n默认情况下，kubectl 在 $HOME/.kube 目录下查找名为 config 的文件。您可以通过设置 KUBECONFIG 环境变量或者设置--kubeconfig参数来指定其他 kubeconfig 文件。\n有关创建和指定 kubeconfig 文件的分步说明，请参阅配置对多集群的访问。\n支持多集群、用户和身份认证机制 假设您有多个集群，并且您的用户和组件以多种方式进行身份认证。比如：\n 正在运行的 kubelet 可能使用证书在进行认证。 用户可能通过令牌进行认证。 管理员可能拥有多个证书集合提供给各用户。  使用 kubeconfig 文件，您可以组织集群、用户和命名空间。您还可以定义上下文，以便在集群和命名空间之间快速轻松地切换。\n上下文（Context） 通过 kubeconfig 文件中的 context 元素，使用简便的名称来对访问参数进行分组。每个上下文都有三个参数：cluster、namespace 和 user。默认情况下，kubectl 命令行工具使用 当前上下文 中的参数与集群进行通信。\n选择当前上下文\nkubectl config use-context KUBECONFIG 环境变量 KUBECONFIG 环境变量包含一个 kubeconfig 文件列表。对于 Linux 和 Mac，列表以冒号分隔。对于 Windows，列表以分号分隔。KUBECONFIG 环境变量不是必要的。如果 KUBECONFIG 环境变量不存在，kubectl 使用默认的 kubeconfig 文件，$HOME/.kube/config。\n如果 KUBECONFIG 环境变量存在，kubectl 使用 KUBECONFIG 环境变量中列举的文件合并后的有效配置。\n合并 kubeconfig 文件 要查看配置，输入以下命令：\nkubectl config view 如前所述，输出可能来自 kubeconfig 文件，也可能是合并多个 kubeconfig 文件的结果。\n以下是 kubectl 在合并 kubeconfig 文件时使用的规则。\n  如果设置了 --kubeconfig 参数，则仅使用指定的文件。不进行合并。此参数只能使用一次。\n否则，如果设置了 KUBECONFIG 环境变量，将它用作应合并的文件列表。根据以下规则合并 KUBECONFIG 环境变量中列出的文件：\n 忽略空文件名。 对于内容无法反序列化的文件，产生错误信息。 第一个设置特定值或者映射键的文件将生效。 永远不会更改值或者映射键。示例：保留第一个文件的上下文以设置 current-context。示例：如果两个文件都指定了 red-user，则仅使用第一个文件的 red-user 中的值。即使第二个文件在 red-user 下有非冲突条目，也要丢弃它们。    有关设置 KUBECONFIG 环境变量的示例，请参阅设置 KUBECONFIG 环境变量。\n否则，使用默认的 kubeconfig 文件， $HOME/.kube/config，不进行合并。\n  根据此链中的第一个匹配确定要使用的上下文。\n 如果存在，使用 --context 命令行参数。 使用合并的 kubeconfig 文件中的 current-context。    这种场景下允许空上下文。\n  确定集群和用户。此时，可能有也可能没有上下文。根据此链中的第一个匹配确定集群和用户，这将运行两次：一次用于用户，一次用于集群。\n 如果存在，使用命令行参数：--user 或者 --cluster。 如果上下文非空，从上下文中获取用户或集群。    这种场景下用户和集群可以为空。\n  确定要使用的实际集群信息。此时，可能有也可能没有集群信息。基于此链构建每个集群信息；第一个匹配项会被采用：\n 如果存在：--server、--certificate-authority 和 --insecure-skip-tls-verify，使用命令行参数。 如果合并的 kubeconfig 文件中存在集群信息属性，则使用它们。 如果没有 server 配置，则配置无效。     确定要使用的实际用户信息。使用与集群信息相同的规则构建用户信息，但每个用户只允许一种身份认证技术：\n 如果存在：--client-certificate、--client-key、--username、--password 和 --token，使用命令行参数。 使用合并的 kubeconfig 文件中的 user 字段。 如果存在两种冲突技术，则配置无效。    对于仍然缺失的任何信息，使用其对应的默认值，并可能提示输入身份认证信息。  文件引用 kubeconfig 文件中的文件和路径引用是相对于 kubeconfig 文件的位置。命令行上的文件引用是相当对于当前工作目录的。在 $HOME/.kube/config 中，相对路径按相对路径存储，绝对路径按绝对路径存储。\n  配置对多集群的访问 kubectl config  "
},
{
	"uri": "https://lijun.in/tasks/access-application-cluster/service-access-application-cluster/",
	"title": "使用服务来访问集群中的应用",
	"tags": [],
	"description": "",
	"content": "本文展示如何创建一个 Kubernetes 服务对象，能让外部客户端访问在集群中运行的应用。该服务为一个应用的两个运行实例提供负载均衡。\n. heading \u0026ldquo;prerequisites\u0026rdquo; %}} . include \u0026ldquo;task-tutorial-prereqs.md\u0026rdquo; \u0026gt;}} . version-check \u0026gt;}}\n. heading \u0026ldquo;objectives\u0026rdquo; %}}  运行 Hello World 应用的两个实例。 创建一个服务对象来暴露 node port。 使用服务对象来访问正在运行的应用。  为运行在两个 pod 中的应用创建一个服务 这是应用程序部署的配置文件：\n. codenew file=\u0026quot;service/access/hello-application.yaml\u0026rdquo; \u0026gt;}}\n 在您的集群中运行一个 Hello World 应用： 使用上面的文件创建应用程序 Deployment： kubectl apply -f https://k8s.io/examples/service/access/hello-application.yaml 上面的命令创建一个 Deployment 对象和一个关联的 ReplicaSet 对象。这个 ReplicaSet 有两个 Pod，每个 Pod 都运行着 Hello World 应用。\n   展示 Deployment 的信息： kubectl get deployments hello-world kubectl describe deployments hello-world    展示您的 ReplicaSet 对象信息： kubectl get replicasets kubectl describe replicasets    创建一个服务对象来暴露 deployment： kubectl expose deployment hello-world --type=NodePort --name=example-service    展示服务信息： kubectl describe services example-service   输出类似于：\nName: example-service Namespace: default Labels: run=load-balancer-example Annotations: \u0026lt;none\u0026gt; Selector: run=load-balancer-example Type: NodePort IP: 10.32.0.16 Port: \u0026lt;unset\u0026gt; 8080/TCP TargetPort: 8080/TCP NodePort: \u0026lt;unset\u0026gt; 31496/TCP Endpoints: 10.200.1.4:8080,10.200.2.5:8080 Session Affinity: None Events: \u0026lt;none\u0026gt; 注意服务中的 NodePort 值。例如在上面的输出中，NodePort 是 31496。\n 列出运行 Hello World 应用的 pod： kubectl get pods --selector=\u0026#34;run=load-balancer-example\u0026#34; --output=wide   输出类似于：\nNAME READY STATUS ... IP NODE hello-world-2895499144-bsbk5 1/1 Running ... 10.200.1.4 worker1 hello-world-2895499144-m1pwt 1/1 Running ... 10.200.2.5 worker2   获取运行 Hello World 的 pod 的其中一个节点的公共 IP 地址。如何获得此地址取决于您设置集群的方式。 例如，如果您使用的是 Minikube，则可以通过运行 kubectl cluster-info 来查看节点地址。 如果您使用的是 Google Compute Engine 实例，则可以使用 gcloud compute instances list 命令查看节点的公共地址。\n  在您选择的节点上，创建一个防火墙规则以开放 node port 上的 TCP 流量。 例如，如果您的服务的 NodePort 值为 31568，请创建一个防火墙规则以允许 31568 端口上的 TCP 流量。 不同的云提供商提供了不同方法来配置防火墙规则。\n  使用节点地址和 node port 来访问 Hello World 应用：\ncurl http://\u0026lt;public-node-ip\u0026gt;:\u0026lt;node-port\u0026gt;   这里的 \u0026lt;public-node-ip\u0026gt; 是您节点的公共 IP 地址，\u0026lt;node-port\u0026gt; 是您服务的 NodePort 值。 对于请求成功的响应是一个 hello 消息：\nHello Kubernetes! 使用服务配置文件 作为 kubectl expose 的替代方法，您可以使用 服务配置文件 来创建服务。\n. heading \u0026ldquo;cleanup\u0026rdquo; %}} 想要删除服务，输入以下命令：\nkubectl delete services example-service  想要删除运行 Hello World 应用的 Deployment、ReplicaSet 和 Pod，输入以下命令：\nkubectl delete deployment hello-world  . heading \u0026ldquo;whatsnext\u0026rdquo; %}} 学习更多关于如何 通过服务连接应用。\n"
},
{
	"uri": "https://lijun.in/tasks/run-application/delete-stateful-set/",
	"title": "删除 StatefulSet",
	"tags": [],
	"description": "",
	"content": "本文介绍如何删除 StatefulSet。\n. heading \u0026ldquo;prerequisites\u0026rdquo; %}}  本文假设在您的集群上已经运行了由 StatefulSet 创建的应用。  删除 StatefulSet 您可以像删除 Kubernetes 中的其他资源一样删除 StatefulSet：使用 kubectl delete 命令，并按文件或者名字指定 StatefulSet。\nkubectl delete -f \u0026lt;file.yaml\u0026gt; kubectl delete statefulsets \u0026lt;statefulset-name\u0026gt; 删除 StatefulSet 之后，您可能需要单独删除关联的无头服务。\nkubectl delete service \u0026lt;service-name\u0026gt; 通过 kubectl 删除 StatefulSet 会将其缩容为0，因此删除属于它的所有pods。 如果您只想删除 StatefulSet 而不删除 pods，使用 --cascade=false。\nkubectl delete -f \u0026lt;file.yaml\u0026gt; --cascade=false 通过将 --cascade=false 传递给 kubectl delete，在删除 StatefulSet 对象之后，StatefulSet 管理的 pods 会被保留下来。如果 pods 有一个标签 app=myapp，则可以按照如下方式删除它们：\nkubectl delete pods -l app=myapp Persistent Volumes 删除 StatefulSet 管理的 pods 并不会删除关联的卷。这是为了确保您有机会在删除卷之前从卷中复制数据。在pods离开终止状态后删除 PVC 可能会触发删除支持的 Persistent Volumes，具体取决于存储类和回收策略。声明删除后，您永远不应该假设能够访问卷。\n注意：删除 PVC 时要谨慎，因为这可能会导致数据丢失。\n完全删除 StatefulSet 要简单地删除 StatefulSet 中的所有内容，包括关联的 pods，您可能需要运行一系列类似于以下内容的命令：\ngrace=$(kubectl get pods \u0026lt;stateful-set-pod\u0026gt; --template \u0026#39;{{.spec.terminationGracePeriodSeconds}}\u0026#39;) kubectl delete statefulset -l app=myapp sleep $grace kubectl delete pvc -l app=myapp 在上面的例子中，pods 的标签为 app=myapp；适当地替换您自己的标签。\n强制删除 StatefulSet 类型的 pods 如果您发现 StatefulSet 中的某些 pods 长时间处于 \u0026lsquo;Terminating\u0026rsquo; 或者 \u0026lsquo;Unknown\u0026rsquo; 状态，则可能需要手动干预以强制从 apiserver 中删除 pods。这是一项潜在的危险任务。详细信息请阅读删除 StatefulSet 类型的 Pods。\n. heading \u0026ldquo;whatsnext\u0026rdquo; %}} 了解更多有关强制删除 StatefulSet 类型的 Pods。\n"
},
{
	"uri": "https://lijun.in/setup/production-environment/tools/kubeadm/high-availability/",
	"title": "利用 kubeadm 创建高可用集群",
	"tags": [],
	"description": "",
	"content": "本文讲述了使用 kubeadm 设置一个高可用的 Kubernetes 集群的两种不同方式：\n 使用堆控制平面节点。这种方法所需基础设施较少。etcd 成员和控制平面节点位于同一位置。 使用外部集群。这种方法所需基础设施较多。控制平面的节点和 etcd 成员是分开的。  在下一步之前，您应该仔细考虑哪种方法更好的满足您的应用程序和环境的需求。 这是对比文档 讲述了每种方法的优缺点。\n如果您在安装 HA 集群时遇到问题，请在 kubeadm 问题跟踪里向我们提供反馈。\n您也可以阅读 升级文件。\n. caution \u0026gt;}} 这篇文档没有讲述在云提供商上运行集群的问题。在云环境中，此处记录的方法不适用于类型为 LoadBalancer 的服务对象，或者具有动态的 PersistentVolumes。 . /caution \u0026gt;}}\n. heading \u0026ldquo;prerequisites\u0026rdquo; %}} 对于这两种方法，您都需要以下基础设施：\n 配置三台机器kubeadm 的最低要求给主节点 配置三台机器 kubeadm 的最低要求 给工作节点 在集群中，所有计算机之间的完全网络连接（公网或私网） 所有机器上的 sudo 权限 每台设备对系统中所有节点的 SSH 访问 在所有机器上安装 kubeadm 和 kubelet，kubectl 是可选的。  仅对于外部 etcd 集群来说，您还需要：\n 给 etcd 成员使用的另外三台机器  这两种方法的第一步 为 kube-apiserver 创建负载均衡器 . note \u0026gt;}} 使用负载均衡器需要许多配置。您的集群搭建可能需要不同的配置。下面的例子只是其中的一方面配置。 . /note \u0026gt;}}\n  创建一个名为 kube-apiserver 的负载均衡器解析 DNS。\n  在云环境中，应该将控制平面节点放置在 TCP 后面转发负载平衡。 该负载均衡器将流量分配给目标列表中所有运行状况良好的控制平面节点。健康检查 apiserver 是在 kube-apiserver 监听端口(默认值 :6443)上的一个 TCP 检查。\n  不建议在云环境中直接使用 IP 地址。\n  负载均衡器必须能够在 apiserver 端口上与所有控制平面节点通信。它还必须允许其监听端口的传入流量。\n  确保负载均衡器的地址始终匹配 kubeadm 的 ControlPlaneEndpoint 地址。\n  阅读软件负载平衡选项指南以获取更多详细信息。\n      添加第一个控制平面节点到负载均衡器并测试连接：\nnc -v LOAD_BALANCER_IP PORT  由于 apiserver 尚未运行，预期会出现一个连接拒绝错误。然而超时意味着负载均衡器不能和控制平面节点通信。 如果发生超时，请重新配置负载均衡器与控制平面节点进行通信。    将其余控制平面节点添加到负载均衡器目标组。\n  使用堆控制平面和 etcd 节点 控制平面节点的第一步   初始化控制平面：\nsudo kubeadm init --control-plane-endpoint \u0026#34;LOAD_BALANCER_DNS:LOAD_BALANCER_PORT\u0026#34; --upload-certs  您可以使用 --kubernetes-version 标志来设置要使用的 Kubernetes 版本。建议将 kubeadm、kebelet、kubectl 和 Kubernetes 的版本匹配。 这个 --control-plane-endpoint 标志应该被设置成负载均衡器的地址或 DNS 和端口。 这个 --upload-certs 标志用来将在所有控制平面实例之间的共享证书上传到集群。如果正好相反，你更喜欢手动地通过控制平面节点或者使用自动化 工具复制证书，请删除此标志并参考如下部分证书分配手册。    . note \u0026gt;}} 标志 kubeadm init、--config 和 --certificate-key 不能混合使用，因此如果您要使用kubeadm 配置，您必须在相应的配置文件（位于 InitConfiguration 和 JoinConfiguration: controlPlane）添加 certificateKey 字段。 . /note \u0026gt;}}\n. note \u0026gt;}} 一些 CNI 网络插件如 Calico 需要 CIDR 例如 192.168.0.0/16 和一些像 Weave 没有。参考 CNI 网络文档。 通过传递 --pod-network-cidr 标志添加 pod CIDR，或者您可以使用 kubeadm 配置文件，在 ClusterConfiguration 的 networking 对象下设置 podSubnet 字段。 . /note \u0026gt;}}\n  命令完成后，您应该会看到类似以下内容：\n... 现在，您可以通过在根目录上运行以下命令来加入任意数量的控制平面节点： kubeadm join 192.168.0.200:6443 --token 9vr73a.a8uxyaju799qwdjv --discovery-token-ca-cert-hash sha256:7c2e69131a36ae2a042a339b33381c6d0d43887e2de83720eff5359e26aec866 --control-plane --certificate-key f8902e114ef118304e561c3ecd4d0b543adc226b7a07f675f56564185ffe0c07 请注意，证书密钥可以访问集群内敏感数据，请保密！ 为了安全起见，将在两个小时内删除上传的证书； 如有必要，您可以使用 kubeadm 初始化上传证书阶段，之后重新加载证书。 然后，您可以通过在根目录上运行以下命令来加入任意数量的工作节点： kubeadm join 192.168.0.200:6443 --token 9vr73a.a8uxyaju799qwdjv --discovery-token-ca-cert-hash sha256:7c2e69131a36ae2a042a339b33381c6d0d43887e2de83720eff5359e26aec866   将此输出复制到文本文件。 稍后您将需要它来将控制平面节点和辅助节点加入集群。\n  当 --upload-certs 与 kubeadm init 一起使用时，主控制平面的证书被加密并上传到 kubeadm-certs 密钥中。\n  要重新上传证书并生成新的解密密钥，请在已加入集群节点的控制平面上使用以下命令：\nsudo kubeadm init phase upload-certs --upload-certs   您还可以在 init 期间指定自定义的 --certificate-key，以后可以由 join 使用。 要生成这样的密钥，可以使用以下命令：\nkubeadm alpha certs certificate-key     . note \u0026gt;}} kubeadm-certs 密钥和解密密钥会在两个小时后失效。 . /note \u0026gt;}}\n. caution \u0026gt;}} 正如命令输出中所述，证书密钥可访问群集敏感数据，并将其保密！ . /caution \u0026gt;}}\n  应用您选择的 CNI 插件： 请遵循以下指示 安装 CNI 提供程序。如果适用，请确保配置与 kubeadm 配置文件中指定的 Pod CIDR 相对应。\n在此示例中，我们使用 Weave Net：\nkubectl apply -f \u0026#34;https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d \u0026#39;\\n\u0026#39;)\u0026#34;   输入以下内容，并查看 pods 的控制平面组件启动：\nkubectl get pod -n kube-system -w   其余控制平面节点的步骤 . note \u0026gt;}} 从 kubeadm 1.15 版本开始，您可以并行加入多个控制平面节点。 在此版本之前，您必须在第一个节点初始化后才能依序的增加新的控制平面节点。 . /note \u0026gt;}}\n对于每个其他控制平面节点，您应该：\n  执行先前由第一个节点上的 kubeadm init 输出提供给您的 join 命令。 它看起来应该像这样：\nsudo kubeadm join 192.168.0.200:6443 --token 9vr73a.a8uxyaju799qwdjv --discovery-token-ca-cert-hash sha256:7c2e69131a36ae2a042a339b33381c6d0d43887e2de83720eff5359e26aec866 --control-plane --certificate-key f8902e114ef118304e561c3ecd4d0b543adc226b7a07f675f56564185ffe0c07  这个 --control-plane 命令通知 kubeadm join 创建一个新的控制平面。 --certificate-key ... 将导致从集群中的 kubeadm-certs 秘钥下载控制平面证书并使用给定的密钥进行解密。    外部 etcd 节点 使用外部 etcd 节点设置集群类似于用于堆叠 etcd 的过程， 不同之处在于您应该首先设置 etcd，并在 kubeadm 配置文件中传递 etcd 信息。\n设置 ectd 集群   按照 这些指示 去设置 etcd 集群。\n  设置 SSH 在 这描述。\n  将以下文件从集群中的任何 etcd 节点复制到第一个控制平面节点：\nexport CONTROL_PLANE=\u0026#34;ubuntu@10.0.0.7\u0026#34; scp /etc/kubernetes/pki/etcd/ca.crt \u0026#34;${CONTROL_PLANE}\u0026#34;: scp /etc/kubernetes/pki/apiserver-etcd-client.crt \u0026#34;${CONTROL_PLANE}\u0026#34;: scp /etc/kubernetes/pki/apiserver-etcd-client.key \u0026#34;${CONTROL_PLANE}\u0026#34;:  用第一台控制平面机的 user@host 替换 CONTROL_PLANE 的值。    设置第一个控制平面节点   用以下内容创建一个名为 kubeadm-config.yaml 的文件：\napiVersion: kubeadm.k8s.io/v1beta2 kind: ClusterConfiguration kubernetesVersion: stable controlPlaneEndpoint: \u0026quot;LOAD_BALANCER_DNS:LOAD_BALANCER_PORT\u0026quot; etcd: external: endpoints: - https://ETCD_0_IP:2379 - https://ETCD_1_IP:2379 - https://ETCD_2_IP:2379 caFile: /etc/kubernetes/pki/etcd/ca.crt certFile: /etc/kubernetes/pki/apiserver-etcd-client.crt keyFile: /etc/kubernetes/pki/apiserver-etcd-client.key    . note \u0026gt;}} 这里堆 etcd 和外部 etcd 之前的区别在于设置外部 etcd 需要一个 etcd 的 external 对象下带有 etcd 端点的配置文件。 如果是堆 etcd 技术，是自动管理的。 . /note \u0026gt;}}\n  在您的集群中，将配置模板中的以下变量替换为适当值：\n LOAD_BALANCER_DNS LOAD_BALANCER_PORT ETCD_0_IP ETCD_1_IP ETCD_2_IP    以下的步骤与设置堆集群是相似的：\n  在节点上运行 sudo kubeadm init --config kubeadm-config.yaml --upload-certs 命令。\n  编写输出联接命令，这些命令将返回到文本文件以供以后使用。\n  应用您选择的 CNI 插件。 给定以下示例适用于 Weave Net：\nkubectl apply -f \u0026#34;https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d \u0026#39;\\n\u0026#39;)\u0026#34;   其他控制平面节点的步骤 步骤与设置堆 etcd 相同：\n 确保第一个控制平面节点已完全初始化。 使用保存到文本文件的连接命令将每个控制平面节点连接在一起。建议一次加入一个控制平面节点。 不要忘记默认情况下，--certificate-key 中的解密秘钥会在两个小时后过期。  列举控制平面之后的常见任务 安装工作节点 您可以使用之前存储的命令将工作节点加入集群中 作为 kubeadm init 命令的输出：\nsudo kubeadm join 192.168.0.200:6443 --token 9vr73a.a8uxyaju799qwdjv --discovery-token-ca-cert-hash sha256:7c2e69131a36ae2a042a339b33381c6d0d43887e2de83720eff5359e26aec866 手动证书分发 如果您选择不将 kubeadm init 与 --upload-certs 命令一起使用， 则意味着您将必须手动将证书从主控制平面节点复制到 将要加入的控制平面节点上。\n有许多方法可以实现这种操作。在下面的例子中我们使用 ssh 和 scp：\n如果要在单独的一台计算机控制所有节点，则需要 SSH。\n  在您的主设备上启动 ssh-agent，要求该设备能访问系统中的所有其他节点：\neval $(ssh-agent)   将 SSH 身份添加到会话中：\nssh-add ~/.ssh/path_to_private_key   检查节点间的 SSH 以确保连接是正常运行的\n  SSH 到任何节点时，请确保添加 -A 标志：\nssh -A 10.0.0.7   当在任何节点上使用 sudo 时，请确保环境完善，以便使用 SSH 转发任务：\nsudo -E -s       在所有节点上配置 SSH 之后，您应该在运行过 kubeadm init 命令的第一个控制平面节点上运行以下脚本。 该脚本会将证书从第一个控制平面节点复制到另一个控制平面节点：\n在以下示例中，用其他控制平面节点的 IP 地址替换 CONTROL_PLANE_IPS。\nUSER=ubuntu # 可自己设置 CONTROL_PLANE_IPS=\u0026#34;10.0.0.7 10.0.0.8\u0026#34; for host in ${CONTROL_PLANE_IPS}; do scp /etc/kubernetes/pki/ca.crt \u0026#34;${USER}\u0026#34;@$host: scp /etc/kubernetes/pki/ca.key \u0026#34;${USER}\u0026#34;@$host: scp /etc/kubernetes/pki/sa.key \u0026#34;${USER}\u0026#34;@$host: scp /etc/kubernetes/pki/sa.pub \u0026#34;${USER}\u0026#34;@$host: scp /etc/kubernetes/pki/front-proxy-ca.crt \u0026#34;${USER}\u0026#34;@$host: scp /etc/kubernetes/pki/front-proxy-ca.key \u0026#34;${USER}\u0026#34;@$host: scp /etc/kubernetes/pki/etcd/ca.crt \u0026#34;${USER}\u0026#34;@$host:etcd-ca.crt scp /etc/kubernetes/pki/etcd/ca.key \u0026#34;${USER}\u0026#34;@$host:etcd-ca.key done   . caution \u0026gt;}} 只需要复制上面列表中的证书。kubeadm 将负责生成其余证书以及加入控制平面实例所需的 SAN。 如果您错误地复制了所有证书，由于缺少所需的 SAN，创建其他节点可能会失败。 . /caution \u0026gt;}}\n  然后，在每个连接控制平面节点上，您必须先运行以下脚本，然后再运行 kubeadm join。 该脚本会将先前复制的证书从主目录移动到 /etc/kubernetes/pki：\nUSER=ubuntu # 可自己设置 mkdir -p /etc/kubernetes/pki/etcd mv /home/${USER}/ca.crt /etc/kubernetes/pki/ mv /home/${USER}/ca.key /etc/kubernetes/pki/ mv /home/${USER}/sa.pub /etc/kubernetes/pki/ mv /home/${USER}/sa.key /etc/kubernetes/pki/ mv /home/${USER}/front-proxy-ca.crt /etc/kubernetes/pki/ mv /home/${USER}/front-proxy-ca.key /etc/kubernetes/pki/ mv /home/${USER}/etcd-ca.crt /etc/kubernetes/pki/etcd/ca.crt mv /home/${USER}/etcd-ca.key /etc/kubernetes/pki/etcd/ca.key   "
},
{
	"uri": "https://lijun.in/concepts/workloads/controllers/garbage-collection/",
	"title": "垃圾收集",
	"tags": [],
	"description": "",
	"content": "Kubernetes 垃圾收集器的作用是删除某些曾经拥有所有者（owner）但现在不再拥有所有者的对象。\n所有者和附属 某些 Kubernetes 对象是其它一些对象的所有者。例如，一个 ReplicaSet 是一组 Pod 的所有者。 具有所有者的对象被称为是所有者的 附属 。 每个附属对象具有一个指向其所属对象的 metadata.ownerReferences 字段。\n有时，Kubernetes 会自动设置 ownerReference 的值。 例如，当创建一个 ReplicaSet 时，Kubernetes 自动设置 ReplicaSet 中每个 Pod 的 ownerReference 字段值。 在 Kubernetes 1.8 版本，Kubernetes 会自动为某些对象设置 ownerReference 的值，这些对象是由 ReplicationController、ReplicaSet、StatefulSet、DaemonSet、Deployment、Job 和 CronJob 所创建或管理。 也可以通过手动设置 ownerReference 的值，来指定所有者和附属之间的关系。\n这里有一个配置文件，表示一个具有 3 个 Pod 的 ReplicaSet：\ncodenew file=\u0026quot;controllers/replicaset.yaml\u0026rdquo; \u0026gt;}}\n如果创建该 ReplicaSet，然后查看 Pod 的 metadata 字段，能够看到 OwnerReferences 字段：\nkubectl apply -f https://k8s.io/examples/controllers/replicaset.yaml kubectl get pods --output=yaml 输出显示了 Pod 的所有者是名为 my-repset 的 ReplicaSet：\napiVersion: v1 kind: Pod metadata: ... ownerReferences: - apiVersion: apps/v1 controller: true blockOwnerDeletion: true kind: ReplicaSet name: my-repset uid: d9607e19-f88f-11e6-a518-42010a800195 ... 根据设计，kubernetes 不允许跨命名空间指定所有者。这意味着： 1）命名空间范围的附属只能在相同的命名空间中指定所有者，并且只能指定集群范围的所有者。 2）集群范围的附属只能指定集群范围的所有者，不能指定命名空间范围的。\n控制垃圾收集器删除附属者 当删除对象时，可以指定该对象的附属者是否也自动删除掉。 自动删除 Dependent 也称为 级联删除 。 Kubernetes 中有两种 级联删除 的模式：background 模式和 foreground 模式。\n如果删除对象时，不自动删除它的附属者，这些附属者被称作是原对象的 orphaned 。\n显式级联删除 在 显式级联删除 模式下，根对象首先进入 deletion in progress 状态。在 deletion in progress 状态会有如下的情况：\n 对象仍然可以通过 REST API 可见。 会设置对象的 deletionTimestamp 字段。 对象的 metadata.finalizers 字段包含了值 foregroundDeletion。  一旦对象被设置为 deletion in progress 状态，垃圾收集器会删除对象的所有附属。 垃圾收集器在删除了所有 Blocking 状态的附属（对象的 ownerReference.blockOwnerDeletion=true）之后，它会删除拥有者对象。\n注意，在 foregroundDeletion 模式下，只有设置了 ownerReference.blockOwnerDeletion 值的附属者才能阻止删除拥有者对象。 在 Kubernetes 1.7 版本中将增加准入控制器，基于拥有者对象上的删除权限来控制用户去设置 blockOwnerDeletion 的值为 true，所以未授权的附属者不能够延迟拥有者对象的删除。\n如果一个对象的 ownerReferences 字段被一个 Controller（例如 Deployment 或 ReplicaSet）设置，blockOwnerDeletion 会被自动设置，不需要手动修改这个字段。\n隐式级联删除 在 隐式级联删除 模式下，Kubernetes 会立即删除拥有者对象，然后垃圾收集器会在后台删除这些附属值。\n设置级联删除策略 通过为拥有者对象设置 deleteOptions.propagationPolicy 字段，可以控制级联删除策略。 可能的取值包括：orphan、Foreground 或者 Background。\n对很多 Controller 资源，包括 ReplicationController、ReplicaSet、StatefulSet、DaemonSet 和 Deployment，默认的垃圾收集策略是 orphan。 因此，对于使用 extensions/v1beta1、apps/v1beta1 和 apps/v1beta2 组版本中的 Kind,除非指定其它的垃圾收集策略，否则所有附属对象默认使用的都是 orphan 策略。\n下面是一个在 Background 中删除 Dependent 对象的示例：\nkubectl proxy --port=8080 curl -X DELETE localhost:8080/apis/apps/v1/namespaces/default/replicasets/my-repset \\  -d \u0026#39;{\u0026#34;kind\u0026#34;:\u0026#34;DeleteOptions\u0026#34;,\u0026#34;apiVersion\u0026#34;:\u0026#34;v1\u0026#34;,\u0026#34;propagationPolicy\u0026#34;:\u0026#34;Background\u0026#34;}\u0026#39; \\  -H \u0026#34;Content-Type: application/json\u0026#34; 下面是一个在 Foreground 中删除附属对象的示例：\nkubectl proxy --port=8080 curl -X DELETE localhost:8080/apis/apps/v1/namespaces/default/replicasets/my-repset \\  -d \u0026#39;{\u0026#34;kind\u0026#34;:\u0026#34;DeleteOptions\u0026#34;,\u0026#34;apiVersion\u0026#34;:\u0026#34;v1\u0026#34;,\u0026#34;propagationPolicy\u0026#34;:\u0026#34;Foreground\u0026#34;}\u0026#39; \\  -H \u0026#34;Content-Type: application/json\u0026#34; 这里是一个 Orphan 附属的示例：\nkubectl proxy --port=8080 curl -X DELETE localhost:8080/apis/apps/v1/namespaces/default/replicasets/my-repset \\  -d \u0026#39;{\u0026#34;kind\u0026#34;:\u0026#34;DeleteOptions\u0026#34;,\u0026#34;apiVersion\u0026#34;:\u0026#34;v1\u0026#34;,\u0026#34;propagationPolicy\u0026#34;:\u0026#34;Orphan\u0026#34;}\u0026#39; \\  -H \u0026#34;Content-Type: application/json\u0026#34; kubectl 也支持级联删除。 通过设置 --cascade 为 true，可以使用 kubectl 自动删除附属对象。设置 --cascade 为 false，会使附属对象成为孤儿附属对象。--cascade 的默认值是 true。\n下面是一个例子，使一个 ReplicaSet 的附属对象成为孤儿附属：\nkubectl delete replicaset my-repset --cascade=false Deployment 的其他说明 在 1.7 之前的版本中，当在 Deployment 中使用级联删除时，您必须使用 propagationPolicy:Foreground 模式。这样不仅删除所创建的 ReplicaSet，还删除其 Pod。如果不使用这种类型的 propagationPolicy，则将只删除 ReplicaSet，而 Pod 被孤立。\n更多信息，请参考 kubeadm/#149。\n已知的问题 跟踪 #26120\nwhatsnext\u0026rdquo; 设计文档 1\n设计文档 2\n"
},
{
	"uri": "https://lijun.in/concepts/overview/working-with-objects/field-selectors/",
	"title": "字段选择器",
	"tags": [],
	"description": "",
	"content": "字段选择器（Field selectors）允许您根据一个或多个资源字段的值筛选 Kubernetes 资源。 下面是一些使用字段选择器查询的例子：\n metadata.name=my-service metadata.namespace!=default status.phase=Pending  下面这个 kubectl 命令将筛选出 status.phase 字段值为 Running 的所有 Pod：\nkubectl get pods --field-selector status.phase=Running 字段选择器本质上是资源过滤器。默认情况下，字段选择器/过滤器是未被应用的，这意味着指定类型的所有资源都会被筛选出来。 这使得以下的两个 kubectl 查询是等价的：\nkubectl get pods kubectl get pods --field-selector \u0026#34;\u0026#34; 支持的字段 不同的 Kubernetes 资源类型支持不同的字段选择器。所有资源类型都支持 metadata.name 和 metadata.namespace 字段。使用不被支持的字段选择器会产生错误，例如：\nkubectl get ingress --field-selector foo.bar=baz Error from server (BadRequest): Unable to find \u0026quot;ingresses\u0026quot; that match label selector \u0026quot;\u0026quot;, field selector \u0026quot;foo.bar=baz\u0026quot;: \u0026quot;foo.bar\u0026quot; is not a known field selector: only \u0026quot;metadata.name\u0026quot;, \u0026quot;metadata.namespace\u0026quot; 支持的运算符 您可以使用 =、==和 != 对字段选择器进行运算（= 和 == 的意义是相同的）。例如，下面这个 kubectl 命令将筛选所有不属于 default 命名空间的 Kubernetes Service：\nkubectl get services --all-namespaces --field-selector metadata.namespace!=default 链式选择器 同标签和其他选择器一样，字段选择器可以通过使用逗号分隔的列表组成一个选择链。下面这个 kubectl 命令将筛选 status.phase 字段不等于 Running 同时 spec.restartPolicy 字段等于 Always 的所有 Pod：\nkubectl get pods --field-selector=status.phase!=Running,spec.restartPolicy=Always 多种资源类型 您能够跨多种资源类型来使用字段选择器。下面这个 kubectl 命令将筛选出所有不在 default 命名空间中的 StatefulSet 和 Service：\nkubectl get statefulsets,services --all-namespaces --field-selector metadata.namespace!=default "
},
{
	"uri": "https://lijun.in/concepts/workloads/pods/disruptions/",
	"title": "干扰",
	"tags": [],
	"description": "",
	"content": "本指南针对的是希望构建高可用性应用程序的应用所有者，他们有必要了解可能发生在 pod 上的干扰类型。\n文档同样适用于想要执行自动化集群操作（例如升级和自动扩展集群）的集群管理员。\n自愿干扰和非自愿干扰 Pod 不会消失，除非有人（用户或控制器）将其销毁，或者出现了不可避免的硬件或软件系统错误。\n我们把这些不可避免的情况称为应用的非自愿干扰。例如：\n 节点下层物理机的硬件故障 集群管理员错误地删除虚拟机（实例） 云提供商或虚拟机管理程序中的故障导致的虚拟机消失 内核错误 节点由于集群网络隔离从集群中消失 由于节点资源不足导致 pod 被驱逐。  除了资源不足的情况，大多数用户应该都熟悉这些情况；它们不是特定于 Kubernetes 的。\n我们称其他情况为自愿干扰。包括由应用程序所有者发起的操作和由集群管理员发起的操作。典型的应用程序所有者的 作包括：\n 删除 deployment 或其他管理 pod 的控制器 更新了 deployment 的 pod 模板导致 pod 重启 直接删除 pod（例如，因为误操作）  集群管理员操作包括：\n 排空（drain）节点进行修复或升级。 从集群中排空节点以缩小集群（了解集群自动扩缩）。 从节点中移除一个 pod，以允许其他 pod 使用该节点。  这些操作可能由集群管理员直接执行，也可能由集群管理员所使用的自动化工具执行，或者由集群托管提供商自动执行。\n咨询集群管理员或联系云提供商，或者查询发布文档，以确定是否为集群启用了任何资源干扰源。如果没有启用，可以不用创建 Pod Disruption Budgets（Pod 干扰预算）\n并非所有的自愿干扰都会受到 pod 干扰预算的限制。例如，删除 deployment 或 pod 的删除操作就会跳过 pod 干扰预算检查。\n处理干扰 以下是减轻非自愿干扰的一些方法：\n 确保 pod请求所需资源。 如果需要更高的可用性，请复制应用程序。（了解有关运行多副本的无状态和有状态应用程序的信息。） 为了在运行复制应用程序时获得更高的可用性，请跨机架（使用反亲和性）或跨区域（如果使用多区域集群）扩展应用程序。  自愿干扰的频率各不相同。在一个基本的 Kubernetes 集群中，根本没有自愿干扰。然而，集群管理 或托管提供商可能运行一些可能导致自愿干扰的额外服务。例如，节点软 更新可能导致自愿干扰。另外，集群（节点）自动缩放的某些 现可能导致碎片整理和紧缩节点的自愿干扰。集群 理员或托管提供商应该已经记录了各级别的自愿干扰（如果有的话）。\nKubernetes 提供特性来满足在出现频繁自愿干扰的同时运行高可用的应用程序。我们称这些特性为干扰预算\n干扰预算工作原理 应用程序所有者可以为每个应用程序创建 PodDisruptionBudget 对象（PDB）。PDB 将限制在同一时间因自愿干扰导致的复制应用程序中宕机的 pod 数量。例如，基于定额的应用程序希望确保运行的副本数 永远不会低于仲裁所需的数量。Web 前端可能希望确保提供负载的副本数量永远不会低于总数的某个百分比。\n集群管理员和托管提供商应该使用遵循 Pod Disruption Budgets 的接口（通过调用驱逐 API），而不是直接删除 pod 或 deployment。示例包括 kubectl drain 命令和 Kubernetes-on-GCE 集群升级脚本（cluster/gce/upgrade.sh）。\n当集群管理员想排空一个节点时，可以使用 kubectl drain 命令。该命令试图驱逐机器上的所有 pod。驱逐请求可能会暂时被拒绝，且该工具定时重试失败的请求直到所有的 pod 都被终止，或者达到配置的超时时间。\nPDB 指定应用程序可以容忍的副本数量（相当于应该有多少副本）。例如，具有 .spec.replicas: 5 的 deployment 在任何时间都应该有 5 个 pod。如果 PDB 允许其在某一时刻有 4 个副本，那么驱逐 API 将允许同一时刻仅有一个而不是两个 pod 自愿干扰。\n使用标签选择器来指定构成应用程序的一组 pod，这与应用程序的控制器（deployment，stateful-set 等）选择 pod 的逻辑一样。\nPod 控制器的 .spec.replicas 计算“预期的” pod 数量。根据 pod 对象的 .metadata.ownerReferences 字段来发现控制器。\nPDB 不能阻止非自愿干扰的发生，但是确实会计入 算。\n由于应用程序的滚动升级而被删除或不可用的 pod 确实会计入干扰预算，但是控制器（如 deployment 和 stateful-set）在进行滚动升级时不受 PDB 的限制。应用程序更新期间的故障处理是在控制器的 spec 中配置的。（了解更新 deployment。）\n当使用驱逐 API 驱逐 pod 时，pod 会被优雅地终止（参考 [PodSpec](/docs/reference/generated/kubernetes-api/\u0026lt; param \u0026ldquo;version\u0026rdquo; \u0026gt;}}/#podspec-v1-core) 中的 terminationGracePeriodSeconds）。\nPDB 例子 假设集群有 3 个节点，node-1 到 node-3。集群上运行了一些应用。其中一个应用有 3 个副本，分别是 pod-a，pod-b 和 pod-c。另外，还有一个不带 PDB 的无关 pod pod-x 也同样显示。最初，所有的 pod 分布如下：\n   node-1 node-2 node-3     pod-a available pod-b available pod-c available   pod-x available      3 个 pod 都是 deployment 的一部分，并且共同拥有同一个 PDB，要求 3 个 pod 中至少有 2 个 pod 始终处于可用状态。\n例如，假设集群管理员想要重启系统，升级内核版本来修复内核中的 bug。集群管理员首先使用 kubectl drain 命令尝试排空 node-1 节点。命令尝试驱逐 pod-a 和 pod-x。操作立即就成功了。两个 pod 同时进入 terminating 状态。这时的集群处于下面的状态：\n   node-1 draining node-2 node-3     pod-a terminating pod-b available pod-c available   pod-x terminating      Deployment 控制器观察到其中一个 pod 正在终止，因此它创建了一个替代 pod pod-d。由于 node-1 被封锁（cordon），pod-d 落在另一个节点上。同样其他控制器也创建了 pod-y 作为 pod-x 的替代品。\n（注意：对于 StatefulSet 来说，pod-a（也称为 pod-0）需要在替换 pod 创建之前完全终止，替代它的也称为 pod-0，但是具有不同的 UID。反之，样例也适用于 StatefulSet。）\n当前集群的状态如下：\n   node-1 draining node-2 node-3     pod-a terminating pod-b available pod-c available   pod-x terminating pod-d starting pod-y    在某一时刻，pod 被终止，集群如下所示：\n   node-1 drained node-2 node-3      pod-b available pod-c available    pod-d starting pod-y    此时，如果一个急躁的集群管理员试图排空（drain）node-2 或 node-3，drain 命令将被阻塞，因为对于 deployment 来说只有 2 个可用的 pod，并且它的 PDB 至少需要 2 个。经过一段时间，pod-d 变得可用。\n集群状态如下所示：\n   node-1 drained node-2 node-3      pod-b available pod-c available    pod-d available pod-y    现在，集群管理员试图排空（drain）node-2。drain 命令将尝试按照某种顺序驱逐两个 pod，假设先是 pod-b，然后是 pod-d。命令成功驱逐 pod-b，但是当它尝试驱逐 pod-d时将被拒绝，因为对于 deployment 来说只剩一个可用的 pod 了。\nDeployment 创建 pod-b 的替代 pod pod-e。因为集群中没有足够的资源来调度 pod-e，drain 命令再次阻塞。集群最终将是下面这种状态：\n   node-1 drained node-2 node-3 no node      pod-b available pod-c available pod-e pending    pod-d available pod-y     此时，集群管理员需要增加一个节点到集群中以继续升级操作。\n可以看到 Kubernetes 如何改变干扰发生的速率，根据：\n 应用程序需要多少个副本 优雅关闭应用实例需要多长时间 启动应用新实例需要多长时间 控制器的类型 集群的资源能力  分离集群所有者和应用所有者角色 通常，将集群管理者和应用所有者视为彼此了解有限的独立角色是很有用的。这种责任分离在下面这些场景下是有意义的：\n 当有许多应用程序团队共用一个 Kubernetes 集群，并且有自然的专业角色 当第三方工具或服务用于集群自动化管理  Pod 干扰预算通过在角色之间提供接口来支持这种分离。\n如果你的组织中没有这样的责任分离，则可能不需要使用 Pod 干扰预算。\n如何在集群上执行干扰操作 如果你是集群管理员，并且需要对集群中的所有节点执行干扰操作，例如节点或系统软件升级，则可以使用以下选项\n 接受升级期间的停机时间。 故障转移到另一个完整的副本集群。  没有停机时间，但是对于重复的节点和人工协调成本可能是昂贵的。   编写可容忍干扰的应用程序和使用 PDB。  不停机。 最小的资源重复。 允许更多的集群管理自动化。 编写可容忍干扰的应用程序是棘手的，但对于支持容忍自愿干扰所做的工作，和支持自动扩缩和容忍非 愿干扰所做工作相比，有大量的重叠    \u0026ldquo;whatsnext\u0026rdquo;  参考配置 Pod 干扰预算中的方法来保护你的 用。   了解更多关于排空节点的信息。  "
},
{
	"uri": "https://lijun.in/reference/access-authn-authz/authorization/",
	"title": "授权概述",
	"tags": [],
	"description": "",
	"content": "了解有关 Kubernetes 授权的更多信息，包括使用支持的授权模块创建策略的详细信息。\n在 Kubernetes 中，您必须在授权（授予访问权限）之前进行身份验证（登录），有关身份验证的信息， 请参阅 访问控制概述.\nKubernetes 期望 REST API 请求中常见的属性。 这意味着 Kubernetes 授权适用于现有的组织范围或云提供商范围的访问控制系统， 除了 Kubernetes API 之外，它还可以处理其他 API。\n确定是允许还是拒绝请求 Kubernetes 使用 API ​​服务器授权 API 请求。它根据所有策略评估所有请求属性来决定允许或拒绝请求。 一个 API 请求的所有部分必须被某些策略允许才能继续。这意味着默认情况下拒绝权限。\n（尽管 Kubernetes 使用 API ​​服务器，但是依赖于特定种类对象的特定字段的访问控制和策略由准入控制器处理。）\n配置多个授权模块时，将按顺序检查每个模块。 如果任何授权模块批准或拒绝请求，则立即返回该决定，并且不会与其他授权模块协商。 如果所有模块对请求没有意见，则拒绝该请求。一个拒绝响应返回 HTTP 状态代码 403 。\n审查您的请求属性 Kubernetes 仅审查以下 API 请求属性：\n user - 身份验证期间提供的 user 字符串。 group - 经过身份验证的用户所属的组名列表。 extra - 由身份验证层提供的任意字符串键到字符串值的映射。 API - 指示请求是否针对 API 资源。 Request path - 各种非资源端点的路径，如 /api 或 /healthz。 API request verb - API 动词 get，list，create，update，patch，watch，proxy，redirect，delete 和 deletecollection 用于资源请求。要确定资源 API 端点的请求动词，请参阅确定请求动词。 HTTP request verb - HTTP 动词 get，post，put 和 delete 用于非资源请求。 Resource - 正在访问的资源的 ID 或名称（仅限资源请求） - 对于使用 get，update，patch 和 delete 动词的资源请求，您必须提供资源名称。 Subresource - 正在访问的子资源（仅限资源请求）。 Namespace - 正在访问的对象的名称空间（仅适用于命名空间资源请求）。 API group - 正在访问的 API 组（仅限资源请求）。空字符串表示核心 API 组。  确定请求动词 要确定资源 API 端点的请求谓词，请检查所使用的 HTTP 动词以及请求是否对单个资源或资源集合起作用：\n   HTTP 动词 request 动词     POST create   GET, HEAD get (单个资源)，list (资源集合)   PUT update   PATCH patch   DELETE delete (单个资源)，deletecollection (资源集合)    Kubernetes 有时使用专门的动词检查授权以获得额外的权限。例如：\n Pod 安全策略 检查 policy API 组中 podsecuritypolicies 资源的 use 动词的授权。 RBAC 检查 rbac.authorization.k8s.io API 组中 roles 和 clusterroles 资源的 bind 动词的授权。 认证 layer 检查核心 API 组中 users，groups 和 serviceaccounts 的 impersonate 动词的授权，以及 authentication.k8s.io API 组中的 userextras。  授权模块  Node - 一个专用授权程序，根据计划运行的 pod 为 kubelet 授予权限。了解有关使用节点授权模式的更多信息，请参阅节点授权. ABAC - 基于属性的访问控制（ABAC）定义了一种访问控制范例，通过使用将属性组合在一起的策略，将访问权限授予用户。策略可以使用任何类型的属性（用户属性，资源属性，对象，环境属性等）。要了解有关使用 ABAC 模式的更多信息，请参阅 ABAC 模式。 RBAC - 基于角色的访问控制（RBAC）是一种基于企业内个人用户的角色来管理对计算机或网络资源的访问的方法。在此上下文中，权限是单个用户执行特定任务的能力，例如查看，创建或修改文件。要了解有关使用 RBAC 模式的更多信息，请参阅 RBAC 模式。  当指定的 RBAC（基于角色的访问控制）使用 rbac.authorization.k8s.io API 组来驱动授权决策时，允许管理员通过 Kubernetes API 动态配置权限策略。 要启用 RBAC，请使用 --authorization-mode = RBAC 启动 apiserver 。   Webhook - WebHook 是一个 HTTP 回调：发生某些事情时调用的 HTTP POST；通过 HTTP POST 进行简单的事件通知。实现 WebHook 的 Web 应用程序会在发生某些事情时将消息发布到 URL。要了解有关使用 Webhook 模式的更多信息，请参阅 Webhook 模式。  检查 API 访问 kubectl 提供 auth can-i 子命令，用于快速查询 API 授权层。 该命令使用 SelfSubjectAccessReview API 来确定当前用户是否可以执行给定操作，并且无论使用何种授权模式都可以工作。\n$ kubectl auth can-i create deployments --namespace dev yes $ kubectl auth can-i create deployments --namespace prod no 管理员可以将此与用户模拟结合使用，以确定其他用户可以执行的操作。\n$ kubectl auth can-i list secrets --namespace dev --as dave no SelfSubjectAccessReview 是 authorization.k8s.io API 组的一部分，它将 API 服务器授权公开给外部服务。 该组中的其他资源包括：\n SubjectAccessReview - 访问任何用户的 Review ，而不仅仅是当前用户。用于将授权决策委派给 API 服务器。例如，kubelet 和扩展 API 服务器使用它来确定用户对自己的 API 的访问权限。 LocalSubjectAccessReview - 与 SubjectAccessReview 类似，但仅限于特定的命名空间。 SelfSubjectRulesReview - 返回用户可在命名空间内执行的操作集的审阅。用户可以快速汇总自己的访问权限，或者用于 UI 中的隐藏/显示动作。  可以通过创建普通 Kubernetes 资源来查询这些 API ，其中返回对象的响应 “status” 字段是查询的结果。\n$ kubectl create -f - -o yaml \u0026lt;\u0026lt; EOF apiVersion: authorization.k8s.io/v1 kind: SelfSubjectAccessReview spec: resourceAttributes: group: apps name: deployments verb: create namespace: dev EOF apiVersion: authorization.k8s.io/v1 kind: SelfSubjectAccessReview metadata: creationTimestamp: null spec: resourceAttributes: group: apps name: deployments namespace: dev verb: create status: allowed: true denied: false 为您的授权模块应用参数 您必须在策略中包含一个参数标志，以指明您的策略包含哪个授权模块：\n可以使用以下参数：\n --authorization-mode=ABAC 基于属性的访问控制（ABAC）模式允许您使用本地文件配置策略。 --authorization-mode=RBAC 基于角色的访问控制（RBAC）模式允许您使用 Kubernetes API 创建和存储策略。 --authorization-mode=Webhook WebHook 是一种 HTTP 回调模式，允许您使用远程 REST 端点管理授权。 --authorization-mode=Node 节点授权是一种特殊用途的授权模式，专门授权由 kubelet 发出的 API 请求。 --authorization-mode=AlwaysDeny 该标志阻止所有请求。仅将此标志用于测试。 --authorization-mode=AlwaysAllow 此标志允许所有请求。仅在您不需要 API 请求的授权时才使用此标志。  您可以选择多个授权模块。模块按顺序检查，以便较早的模块具有更高的优先级来允许或拒绝请求。\n通过 Pod 创建升级权限 能够在命名空间中创建 Pod 的用户可能会升级其在该命名空间内的权限。 他们可以创建在该命名空间内访问其权限的 Pod 。 他们可以创建用户无法自己读取 secret 的 Pod ，或者在具有不同/更高权限的服务帐户下运行的 Pod 。\n. caution \u0026gt;}}\n注意： 系统管理员在授予对 Pod 创建的访问权限时要小心。 授予在命名空间中创建 Pod（或创建 Pod 的控制器）的权限的用户可以： 读取命名空间中的所有 secret；读取命名空间中的所有 ConfigMap； 并模拟命名空间中的任何服务帐户并执行帐户可以执行的任何操作。 无论采用何种授权方式，这都适用。 . /caution \u0026gt;}}\n. heading \u0026ldquo;whatsnext\u0026rdquo; %}}  要了解有关身份验证的更多信息，请参阅 身份验证 控制对 Kubernetes API 的访问。 要了解有关准入控制的更多信息，请参阅 使用准入控制器。  "
},
{
	"uri": "https://lijun.in/concepts/cluster-administration/logging/",
	"title": "日志架构",
	"tags": [],
	"description": "",
	"content": "应用和系统日志可以让您了解集群内部的运行状况。日志对调试问题和监控集群活动非常有用。大部分现代化应用都有某种日志记录机制；同样地，大多数容器引擎也被设计成支持某种日志记录机制。针对容器化应用，最简单且受欢迎的日志记录方式就是写入标准输出和标准错误流。\n但是，由容器引擎或 runtime 提供的原生功能通常不足以满足完整的日志记录方案。例如，如果发生容器崩溃、pod 被逐出或节点宕机等情况，您仍然想访问到应用日志。因此，日志应该具有独立的存储和生命周期，与节点、pod 或容器的生命周期相独立。这个概念叫 集群级的日志 。集群级日志方案需要一个独立的后台来存储、分析和查询日志。Kubernetes 没有为日志数据提供原生存储方案，但是您可以集成许多现有的日志解决方案到 Kubernetes 集群中。\n集群级日志架构假定在集群内部或者外部有一个日志后台。如果您对集群级日志不感兴趣，您仍会发现关于如何在节点上存储和处理日志的描述对您是有用的。\nKubernetes 中的基本日志记录 本节，您会看到一个kubernetes 中生成基本日志的例子，该例子中数据被写入到标准输出。 这里通过一个特定的 pod 规约 演示创建一个容器，并令该容器每秒钟向标准输出写入数据。\n用下面的命令运行 pod：\nkubectl apply -f https://k8s.io/examples/debug/counter-pod.yaml 输出结果为：\npod/counter created 使用 kubectl logs 命令获取日志:\nkubectl logs counter 输出结果为：\n0: Mon Jan 1 00:00:00 UTC 2001 1: Mon Jan 1 00:00:01 UTC 2001 2: Mon Jan 1 00:00:02 UTC 2001 ... 一旦发生容器崩溃，您可以使用命令 kubectl logs 和参数 --previous 检索之前的容器日志。 如果 pod 中有多个容器，您应该向该命令附加一个容器名以访问对应容器的日志。 详见 kubectl logs 文档。\n节点级日志记录 容器化应用写入 stdout 和 stderr 的任何数据，都会被容器引擎捕获并被重定向到某个位置。 例如，Docker 容器引擎将这两个输出流重定向到某个 日志驱动 ， 该日志驱动在 Kubernetes 中配置为以 json 格式写入文件。\nDocker json 日志驱动将日志的每一行当作一条独立的消息。该日志驱动不直接支持多行消息。您需要在日志代理级别或更高级别处理多行消息。\n默认情况下，如果容器重启，kubelet 会保留被终止的容器日志。 如果 pod 在工作节点被驱逐，该 pod 中所有的容器也会被驱逐，包括容器日志。\n节点级日志记录中，需要重点考虑实现日志的轮转，以此来保证日志不会消耗节点上所有的可用空间。 Kubernetes 当前并不负责轮转日志，而是通过部署工具建立一个解决问题的方案。 例如，在 Kubernetes 集群中，用 kube-up.sh 部署一个每小时运行的工具 logrotate。 您也可以设置容器 runtime 来自动地轮转应用日志，比如使用 Docker 的 log-opt 选项。 在 kube-up.sh 脚本中，使用后一种方式来处理 GCP 上的 COS 镜像，而使用前一种方式来处理其他环境。 这两种方式，默认日志超过 10MB 大小时都会触发日志轮转。\n例如，您可以找到关于 kube-up.sh 为 GCP 环境的 COS 镜像设置日志的详细信息， 相应的脚本在 [这里][cosConfigureHelper]。\n当运行 kubectl logs 时， 节点上的 kubelet 处理该请求并直接读取日志文件，同时在响应中返回日志文件内容。\n当前，如果有其他系统机制执行日志轮转，那么 kubectl logs 仅可查询到最新的日志内容。 比如，一个 10MB 大小的文件，通过logrotate 执行轮转后生成两个文件，一个 10MB 大小，一个为空，所以 kubectl logs 将返回空。\n[cosConfigureHelper]: https://github.com/kubernetes/kubernetes/blob/ param \u0026ldquo;githubbranch\u0026rdquo; \u0026gt;}}/cluster/gce/gci/configure-helper.sh\n系统组件日志 系统组件有两种类型：在容器中运行的和不在容器中运行的。例如：\n 在容器中运行的 kube-scheduler 和 kube-proxy。 不在容器中运行的 kubelet 和容器运行时（例如 Docker。  在使用 systemd 机制的服务器上，kubelet 和容器 runtime 写入日志到 journald。 如果没有 systemd，他们写入日志到 /var/log 目录的 .log 文件。 容器中的系统组件通常将日志写到 /var/log 目录，绕过了默认的日志机制。他们使用 klog 日志库。 您可以在日志开发文档找到这些组件的日志告警级别协议。\n和容器日志类似，/var/log 目录中的系统组件日志也应该被轮转。 通过脚本 kube-up.sh 启动的 Kubernetes 集群中，日志被工具 logrotate 执行每日轮转，或者日志大小超过 100MB 时触发轮转。\n集群级日志架构 虽然Kubernetes没有为集群级日志记录提供原生的解决方案，但您可以考虑几种常见的方法。以下是一些选项：\n 使用在每个节点上运行的节点级日志记录代理。 在应用程序的 pod 中，包含专门记录日志的 sidecar 容器。 将日志直接从应用程序中推送到日志记录后端。  使用节点级日志代理 您可以通过在每个节点上使用 节点级的日志记录代理 来实现群集级日志记录。日志记录代理是一种用于暴露日志或将日志推送到后端的专用工具。通常，日志记录代理程序是一个容器，它可以访问包含该节点上所有应用程序容器的日志文件的目录。\n由于日志记录代理必须在每个节点上运行，它可以用 DaemonSet 副本，Pod 或 本机进程来实现。然而，后两种方法被弃用并且非常不别推荐。\n对于 Kubernetes 集群来说，使用节点级的日志代理是最常用和被推荐的方式，因为在每个节点上仅创建一个代理，并且不需要对节点上的应用做修改。 但是，节点级的日志 仅适用于应用程序的标准输出和标准错误输出。\nKubernetes 并不指定日志代理，但是有两个可选的日志代理与 Kubernetes 发行版一起发布。 Stackdriver 日志 适用于 Google Cloud Platform，和 Elasticsearch。 您可以在专门的文档中找到更多的信息和说明。两者都使用 fluentd 与自定义配置作为节点上的代理。\n使用 sidecar 容器和日志代理 您可以通过以下方式之一使用 sidecar 容器：\n sidecar 容器将应用程序日志传送到自己的标准输出。 sidecar 容器运行一个日志代理，配置该日志代理以便从应用容器收集日志。  传输数据流的 sidecar 容器 利用 sidecar 容器向自己的 stdout 和 stderr 传输流的方式，您就可以利用每个节点上的 kubelet 和日志代理来处理日志。 sidecar 容器从文件，socket 或 journald 读取日志。每个 sidecar 容器打印其自己的 stdout 和 stderr 流。\n这种方法允许您将日志流从应用程序的不同部分分离开，其中一些可能缺乏对写入 stdout 或 stderr 的支持。重定向日志背后的逻辑是最小的，因此它的开销几乎可以忽略不计。 另外，因为 stdout、stderr 由 kubelet 处理，你可以使用内置的工具 kubectl logs。\n考虑接下来的例子。pod 的容器向两个文件写不同格式的日志，下面是这个 pod 的配置文件:\nfile=\u0026quot;admin/logging/two-files-counter-pod.yaml\u0026rdquo; \u0026gt;}}\n在同一个日志流中有两种不同格式的日志条目，这有点混乱，即使您试图重定向它们到容器的 stdout 流。 取而代之的是，您可以引入两个 sidecar 容器。 每一个 sidecar 容器可以从共享卷跟踪特定的日志文件，并重定向文件内容到各自的 stdout 流。\n这是运行两个 sidecar 容器的 pod 文件。\nfile=\u0026quot;admin/logging/two-files-counter-pod-streaming-sidecar.yaml\u0026rdquo; \u0026gt;}}\n现在当您运行这个 pod 时，您可以分别地访问每一个日志流，运行如下命令：\nkubectl logs counter count-log-1 0: Mon Jan 1 00:00:00 UTC 2001 1: Mon Jan 1 00:00:01 UTC 2001 2: Mon Jan 1 00:00:02 UTC 2001 ... kubectl logs counter count-log-2 Mon Jan 1 00:00:00 UTC 2001 INFO 0 Mon Jan 1 00:00:01 UTC 2001 INFO 1 Mon Jan 1 00:00:02 UTC 2001 INFO 2 ... 集群中安装的节点级代理会自动获取这些日志流，而无需进一步配置。如果您愿意，您可以配置代理程序来解析源容器的日志行。\n注意，尽管 CPU 和内存使用率都很低（以多个 cpu millicores 指标排序或者按内存的兆字节排序）， 向文件写日志然后输出到 stdout 流仍然会成倍地增加磁盘使用率。 如果您的应用向单一文件写日志，通常最好设置 /dev/stdout 作为目标路径，而不是使用流式的 sidecar 容器方式。\n应用本身如果不具备轮转日志文件的功能，可以通过 sidecar 容器实现。 该方式的一个例子是运行一个定期轮转日志的容器。 然而，还是推荐直接使用 stdout 和 stderr，将日志的轮转和保留策略交给 kubelet。\n具有日志代理功能的 sidecar 容器 如果节点级日志记录代理程序对于你的场景来说不够灵活，您可以创建一个带有单独日志记录代理程序的 sidecar 容器，将代理程序专门配置为与您的应用程序一起运行。\n在 sidecar 容器中使用日志代理会导致严重的资源损耗。此外，您不能使用 kubectl logs 命令访问日志，因为日志并没有被 kubelet 管理。\n例如，您可以使用 Stackdriver，它使用fluentd作为日志记录代理。 以下是两个可用于实现此方法的配置文件。 第一个文件包含配置 fluentd 的ConfigMap。\nfile=\u0026quot;admin/logging/fluentd-sidecar-config.yaml\u0026rdquo; \u0026gt;}}\n配置fluentd超出了本文的范围。要知道更多的关于如何配置fluentd，请参考fluentd 官方文档.\n第二个文件描述了运行 fluentd sidecar 容器的 pod 。flutend 通过 pod 的挂载卷获取它的配置数据。\nfile=\u0026quot;admin/logging/two-files-counter-pod-agent-sidecar.yaml\u0026rdquo; \u0026gt;}}\n一段时间后，您可以在 Stackdriver 界面看到日志消息。\n记住，这只是一个例子，事实上您可以用任何一个日志代理替换 fluentd ，并从应用容器中读取任何资源。\n从应用中直接暴露日志目录 通过暴露或推送每个应用的日志，您可以实现集群级日志记录；然而，这种日志记录机制的实现已超出 Kubernetes 的范围。\n"
},
{
	"uri": "https://lijun.in/tutorials/kubernetes-basics/update/",
	"title": "更新你的应用",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lijun.in/reference/kubernetes-api/labels-annotations-taints/",
	"title": "知名标签（Label）、注解（Annotation）和 Taints",
	"tags": [],
	"description": "",
	"content": "Kubernetes 保留了 kubernetes.io 命名空间下的所有标签和注解。\n本文既作为这些标签和注解的参考，也就这些标签和注解的赋值进行了说明。\nkubernetes.io/arch 示例：kubernetes.io/arch=amd64\n用于：Node\nKubelet 用 Go 中定义的 runtime.GOARCH 值来填充该标签。这在诸如混用 arm 和 x86 节点的情况下很有用。\nkubernetes.io/os 示例：kubernetes.io/os=linux\n用于：Node\nKubelet 用该 Go 中定义的 runtime.GOOS 值来填充该标签。这在集群中存在不同操作系统的节点时很有用（例如：混合 Linux 和 Windows 操作系统的节点）。\nbeta.kubernetes.io/arch (已弃用) 该标签已被弃用。请使用 kubernetes.io/arch。\nbeta.kubernetes.io/os (已弃用) 该标签已被弃用。请使用 kubernetes.io/arch。\nkubernetes.io/hostname 示例：kubernetes.io/hostname=ip-172-20-114-199.ec2.internal\n用于：Node\nKubelet 用 hostname 值来填充该标签。注意：可以通过向 kubelet 传入 --hostname-override 参数对 “真正的” hostname 进行修改。\nbeta.kubernetes.io/instance-type (已弃用) . note \u0026gt;}}\n从 kubernetes 1.17 版本开始，不推荐使用此标签，而推荐使用node.kubernetes.io/instance-type。 . /note \u0026gt;}}\nnode.kubernetes.io/instance-type 示例：node.kubernetes.io/instance-type=m3.medium\n用于：Node\nKubelet 用 cloudprovider 中定义的实例类型来填充该标签。未使用 cloudprovider 时不会设置该标签。该标签在想要将某些负载定向到特定实例类型的节点上时会很有用，但通常用户更希望依赖 Kubernetes 调度器来执行基于资源的调度，所以用户应该致力于基于属性而不是实例类型来进行调度(例如：需要一个 GPU，而不是 g2.2xlarge)。\nfailure-domain.beta.kubernetes.io/region (已弃用) 参考 failure-domain.beta.kubernetes.io/zone。\n. note \u0026gt;}}\n从 kubernetes 1.17 版本开始，不推荐使用此标签，而推荐使用topology.kubernetes.io/region。 . /note \u0026gt;}}\nfailure-domain.beta.kubernetes.io/zone (已弃用) 示例：\nfailure-domain.beta.kubernetes.io/region=us-east-1\nfailure-domain.beta.kubernetes.io/zone=us-east-1c\n用于：Node、PersistentVolume\n对于 Node： Kubelet 用 cloudprovider 中定义的区域（zone）信息来填充该标签。未使用 cloudprovider 时不会设置该标签，但如果该标签在你的拓扑中有意义的话，应该考虑设置。\n用于 PersistentVolume：在 GCE 和 AWS 中，PersistentVolumeLabel 准入控制器会自动添加区域标签。\n在单区的集群中，Kubernetes 会自动将同一副本控制器或服务下的 pod 分散到不同的节点上 (以降低故障的影响)。在多区的集群中，这种分散的行为扩展到跨区的层面 (以降低区域故障的影响)。跨区分散通过 SelectorSpreadPriority 来实现。\nSelectorSpreadPriority 是一种尽力而为（best-effort）的处理方式，如果集群中的区域是异构的 (例如：不同区域之间的节点数量、节点类型或 pod 资源需求不同），可能使得 pod 在各区域间无法均匀分布。如有需要，用户可以使用同质的区域(节点数量和类型相同) 来减小 pod 分布不均的可能性。\n由于卷不能跨区域挂载（attach），调度器 (通过 VolumeZonePredicate 预选) 也会保证需要特定卷的 pod 被调度到卷所在的区域中。\n区域和地域（region）的实际值无关紧要，两者的层次含义也没有严格的定义。最终期望是，除非整个地域故障， 否则某一区域节点的故障不应该影响到其他区域的节点。例如，通常区域间应该避免共用同一个网络交换机。 具体的规划取决于特定的基础设备 - three-rack 设备所选择的设置与多数据中心截然不同。\n如果 PersistentVolumeLabel 准入控制器不支持自动为 PersistentVolume 打标签，且用户希望防止 pod 跨区域进行卷的挂载，应考虑手动打标签 (或对 PersistentVolumeLabel 增加支持）。如果用户的基础设施没有这种约束，则不需要为卷添加区域标签。\n. note \u0026gt;}}\n从 kubernetes 1.17 版本开始，不推荐使用此标签，而推荐使用topology.kubernetes.io/zone。 . /note \u0026gt;}}\ntopology.kubernetes.io/region 参考 topology.kubernetes.io/zone。\ntopology.kubernetes.io/zone 示例：\nfailure-domain.beta.kubernetes.io/region=us-east-1\nfailure-domain.beta.kubernetes.io/zone=us-east-1c\n用于：Node、PersistentVolume\n对于 Node： Kubelet 用 cloudprovider 中定义的区域（zone）信息来填充该标签。未使用 cloudprovider 时不会设置该标签，但如果该标签在你的拓扑中有意义的话，应该考虑设置。\n用于 PersistentVolume：在 GCE 和 AWS 中，PersistentVolumeLabel 准入控制器会自动添加区域标签。\n在单区的集群中，Kubernetes 会自动将同一副本控制器或服务下的 pod 分散到不同的节点上 (以降低故障的影响)。在多区的集群中，这种分散的行为扩展到跨区的层面 (以降低区域故障的影响)。跨区分散通过 SelectorSpreadPriority 来实现。\nSelectorSpreadPriority 是一种尽力而为（best-effort）的处理方式，如果集群中的区域是异构的 (例如：不同区域之间的节点数量、节点类型或 pod 资源需求不同），可能使得 pod 在各区域间无法均匀分布。 如有需要，用户可以使用同质的区域(节点数量和类型相同) 来减小 pod 分布不均的可能性。\n由于卷不能跨区域挂载（attach），调度器 (通过 VolumeZonePredicate 预选) 也会保证需要特定卷的 pod 被调度到卷所在的区域中。\n区域和地域（region）的实际值无关紧要，两者的层次含义也没有严格的定义。最终期望是，除非整个地域故障， 否则某一区域节点的故障不应该影响到其他区域的节点。例如，通常区域间应该避免共用同一个网络交换机。 具体的规划取决于特定的基础设备 - 三机架安装所选择的设置与多数据中心截然不同。\n如果 PersistentVolumeLabel 准入控制器不支持自动为 PersistentVolume 打标签，且用户希望防止 pod 跨区域进行卷的挂载， 应考虑手动打标签 (或对 PersistentVolumeLabel 增加支持）。如果用户的基础设施没有这种约束，则不需要为卷添加区域标签。\n"
},
{
	"uri": "https://lijun.in/concepts/scheduling-eviction/scheduling-framework/",
	"title": "调度框架",
	"tags": [],
	"description": "",
	"content": "feature-state for_k8s_version=\u0026quot;1.15\u0026rdquo; state=\u0026quot;alpha\u0026rdquo; \u0026gt;}}\n调度框架是 Kubernetes Scheduler 的一种可插入架构，可以简化调度器的自定义。它向现有的调度器增加了一组新的“插件” API。插件被编译到调度器程序中。这些 API 允许大多数调度功能以插件的形式实现，同时使调度“核心”保持简单且可维护。请参考调度框架的设计提案获取框架设计的更多技术信息。\n框架工作流程 调度框架定义了一些扩展点。调度器插件注册后在一个或多个扩展点处被调用。这些插件中的一些可以改变调度决策，而另一些仅用于提供信息。\n每次调度一个 Pod 的尝试都分为两个阶段，即 调度周期 和 绑定周期。\n调度周期和绑定周期 调度周期为 Pod 选择一个节点，绑定周期将该决策应用于集群。调度周期和绑定周期一起被称为“调度上下文”。\n调度周期是串行运行的，而绑定周期可能是同时运行的。\n如果确定 Pod 不可调度或者存在内部错误，则可以终止调度周期或绑定周期。Pod 将返回队列并重试。\n扩展点 下图显示了一个 Pod 的调度上下文以及调度框架公开的扩展点。在此图片中，“过滤器”等同于“断言”，“评分”相当于“优先级函数”。\n一个插件可以在多个扩展点处注册，以执行更复杂或有状态的任务。\nfigure src=\u0026rdquo;/images/docs/scheduling-framework-extensions.png\u0026rdquo; title=\u0026quot;调度框架扩展点\u0026rdquo; \u0026gt;}}\n队列排序 队列排序插件用于对调度队列中的 Pod 进行排序。队列排序插件本质上提供 \u0026ldquo;less(Pod1, Pod2)\u0026rdquo; 函数。一次只能启动一个队列插件。\n前置过滤 前置过滤插件用于预处理 Pod 的相关信息，或者检查集群或 Pod 必须满足的某些条件。如果 PreFilter 插件返回错误，则调度周期将终止。\n过滤 过滤插件用于过滤出不能运行该 Pod 的节点。对于每个节点，调度器将按照其配置顺序调用这些过滤插件。如果任何过滤插件将节点标记为不可行，则不会为该节点调用剩下的过滤插件。节点可以被同时进行评估。\n前置评分 前置评分插件用于执行 “前置评分” 工作，即生成一个可共享状态供评分插件使用。如果 PreScore 插件返回错误，则调度周期将终止。\n评分 评分插件用于对通过过滤阶段的节点进行排名。调度器将为每个节点调用每个评分插件。将有一个定义明确的整数范围，代表最小和最大分数。在标准化评分阶段之后，调度器将根据配置的插件权重合并所有插件的节点分数。\n标准化评分 标准化评分插件用于在调度器计算节点的排名之前修改分数。在此扩展点注册的插件将使用同一插件的评分 结果被调用。每个插件在每个调度周期调用一次。\n例如，假设一个 BlinkingLightScorer 插件基于具有的闪烁指示灯数量来对节点进行排名。\nfunc ScoreNode(_ *v1.pod, n *v1.Node) (int, error) { return getBlinkingLightCount(n) } 然而，最大的闪烁灯个数值可能比 NodeScoreMax 小。要解决这个问题，BlinkingLightScorer 插件还应该注册该扩展点。\nfunc NormalizeScores(scores map[string]int) { highest := 0 for _, score := range scores { highest = max(highest, score) } for node, score := range scores { scores[node] = score*NodeScoreMax/highest } } 如果任何 NormalizeScore 插件返回错误，则调度阶段将终止。\n希望执行“预保留”工作的插件应该使用 NormalizeScore 扩展点。\n保留 保留是一个信息性的扩展点。管理运行时状态的插件（也成为“有状态插件”）应该使用此扩展点，以便调度器在节点给指定 Pod 预留了资源时能够通知该插件。这是在调度器真正将 Pod 绑定到节点之前发生的，并且它存在是为了防止在调度器等待绑定成功时发生竞争情况。\n这个是调度周期的最后一步。一旦 Pod 处于保留状态，它将在绑定周期结束时触发不保留 插件（失败时）或 绑定后 插件（成功时）。\n允许 Permit 插件在每个 Pod 调度周期的最后调用，用于防止或延迟 Pod 的绑定。一个允许插件可以做以下三件事之一：\n 批准\n一旦所有 Permit 插件批准 Pod 后，该 Pod 将被发送以进行绑定。   拒绝\n如果任何 Permit 插件拒绝 Pod，则该 Pod 将被返回到调度队列。这将触发不保留 插件。   等待（带有超时）\n如果一个 Permit 插件返回 “等待” 结果，则 Pod 将保持在一个内部的 “等待中” 的 Pod 列表，同时该 Pod 的绑定周期启动时即直接阻塞直到得到批准。如果超时发生，等待 变成 拒绝，并且 Pod 将返回调度队列，从而触发不保留 插件。  尽管任何插件可以访问 “等待中” 状态的 Pod 列表并批准它们 (查看 FrameworkHandle)。我们希望只有允许插件可以批准处于 “等待中” 状态的 预留 Pod 的绑定。一旦 Pod 被批准了，它将发送到预绑定 阶段。\n预绑定 预绑定插件用于执行 Pod 绑定前所需的任何工作。例如，一个预绑定插件可能需要提供网络卷并且在允许 Pod 运行在该节点之前将其挂载到目标节点上。\n如果任何 PreBind 插件返回错误，则 Pod 将被拒绝 并且返回到调度队列中。\n绑定 绑定插件用于将 Pod 绑定到节点上。直到所有的 PreBind 插件都完成，绑定插件才会被调用。每个绑定插件按照配置顺序被调用。绑定插件可以选择是否处理指定的 Pod。如果绑定插件选择处理 Pod，剩余的绑定插件将被跳过。\n绑定后 这是个信息性的扩展点。绑定后插件在 Pod 成功绑定后被调用。这是绑定周期的结尾，可用于清理相关的资源。\n不保留 这是个信息性的扩展点。如果 Pod 被保留，然后在后面的阶段中被拒绝，则不保留插件将被通知。不保留插件应该清楚保留 Pod 的相关状态。\n使用此扩展点的插件通常也使用保留。\n插件 API 插件 API 分为两个步骤。首先，插件必须注册并配置，然后才能使用扩展点接口。扩展点接口具有以下形式。\ntype Plugin interface { Name() string } type QueueSortPlugin interface { Plugin Less(*v1.pod, *v1.pod) bool } type PreFilterPlugin interface { Plugin PreFilter(context.Context, *framework.CycleState, *v1.pod) error } // ... 插件配置 你可以在调度器配置中启用或禁用插件。如果你在使用 Kubernetes v1.18 或更高版本，大部分调度插件 都在使用中且默认启用。\n除了默认的插件，你同样可以实现自己的调度插件并且将他们与默认插件一起配置。你可以访问 调度插件 了解更多详情。\n如果你正在使用 Kubernetes v1.18 或更高版本，你可以将一组插件设置为一个调度器配置文件，然后定义不同的配置文件来满足各类工作负载。 了解更多关于 多配置文件。\n"
},
{
	"uri": "https://lijun.in/tasks/configure-pod-container/configure-persistent-volume-storage/",
	"title": "配置 Pod 以使用 PersistentVolume 作为存储",
	"tags": [],
	"description": "",
	"content": "本文介绍如何配置 Pod 使用 PersistentVolumeClaim 作为存储。 以下是该过程的总结：\n  集群管理员创建由物理存储支持的 PersistentVolume。管理员不将卷与任何 Pod 关联。\n  群集用户创建一个 PersistentVolumeClaim，它将自动绑定到合适的 PersistentVolume。\n  用户创建一个使用 PersistentVolumeClaim 作为存储的 Pod。\n  . heading \u0026ldquo;prerequisites\u0026rdquo; %}}   您需要一个包含单个节点的 Kubernetes 集群，并且必须配置 kubectl 命令行工具以便与集群交互。 如果还没有单节点集群，可以使用 Minikube 创建一个。\n  熟悉持久卷中的材料。\n  在你的节点上创建一个 index.html 文件 打开集群中节点的一个 shell。 如何打开 shell 取决于集群的设置。 例如，如果您正在使用 Minikube，那么可以通过输入 minikube ssh 来打开节点的 shell。\n在 shell 中，创建一个 /mnt/data 目录：\nmkdir /mnt/data  在 /mnt/data 目录中创建一个 index.html 文件：\necho 'Hello from Kubernetes storage' \u0026gt; /mnt/data/index.html  创建 PersistentVolume 在本练习中，您将创建一个 hostPath 类型的 PersistentVolume。 Kubernetes 支持用于在单节点集群上开发和测试的 hostPath 类型的 PersistentVolume。 hostPath 类型的 PersistentVolume 使用节点上的文件或目录来模拟附带网络的存储。\n在生产集群中，您不会使用 hostPath。集群管理员会提供网络存储资源，比如 Google Compute Engine 持久盘卷、NFS 共享卷或 Amazon Elastic Block Store 卷。 集群管理员还可以使用 [StorageClasses](/docs/reference/generated/kubernetes-api/. param \u0026ldquo;version\u0026rdquo; \u0026gt;}}/#storageclass-v1-storage) 来设置动态提供存储。\n下面是 hostPath PersistentVolume 的配置文件：\n. codenew file=\u0026quot;pods/storage/pv-volume.yaml\u0026rdquo; \u0026gt;}}\n配置文件指定了该卷位于集群节点上的 /mnt/data 目录。 该配置还指定了 10 吉比特的卷大小和 ReadWriteOnce 的访问模式，这意味着该卷可以在单个节点上以读写方式挂载。 它为 PersistentVolume 定义了 StorageClass 名称 为 manual，StorageClass 名称用来将 PersistentVolumeClaim 请求绑定到该 PersistentVolume。\n创建 PersistentVolume：\nkubectl create -f https://k8s.io/examples/pods/storage/pv-volume.yaml  查看 PersistentVolume 的信息：\nkubectl get pv task-pv-volume  输出结果显示该 PersistentVolume 的状态（STATUS） 为 Available。 这意味着它还没有被绑定给 PersistentVolumeClaim。\nNAME CAPACITY ACCESSMODES RECLAIMPOLICY STATUS CLAIM STORAGECLASS REASON AGE task-pv-volume 10Gi RWO Retain Available manual 4s  创建 PersistentVolumeClaim 下一步是创建一个 PersistentVolumeClaim。 Pod 使用 PersistentVolumeClaim 来请求物理存储。 在本练习中，您将创建一个 PersistentVolumeClaim，它请求至少 3 吉比特容量的卷，该卷至少可以为一个节点提供读写访问。\n下面是 PersistentVolumeClaim 的配置文件：\n. codenew file=\u0026quot;pods/storage/pv-claim.yaml\u0026rdquo; \u0026gt;}}\n创建 PersistentVolumeClaim：\nkubectl create -f https://k8s.io/examples/pods/storage/pv-claim.yaml  创建 PersistentVolumeClaim 之后，Kubernetes 控制平面将查找满足申领要求的 PersistentVolume。 如果控制平面找到具有相同 StorageClass 的适当的 PersistentVolume，则将 PersistentVolumeClaim 绑定到该 PersistentVolume 上。\n再次查看 PersistentVolume 信息：\nkubectl get pv task-pv-volume  现在输出的 STATUS 为 Bound。\nNAME CAPACITY ACCESSMODES RECLAIMPOLICY STATUS CLAIM STORAGECLASS REASON AGE task-pv-volume 10Gi RWO Retain Bound default/task-pv-claim manual 2m  查看 PersistentVolumeClaim：\nkubectl get pvc task-pv-claim  输出结果表明该 PersistentVolumeClaim 绑定了你的 PersistentVolume task-pv-volume。\nNAME STATUS VOLUME CAPACITY ACCESSMODES STORAGECLASS AGE task-pv-claim Bound task-pv-volume 10Gi RWO manual 30s  创建 Pod 下一步是创建一个 Pod， 该 Pod 使用你的 PersistentVolumeClaim 作为存储卷。\n下面是 Pod 的 配置文件：\n. codenew file=\u0026quot;pods/storage/pv-pod.yaml\u0026rdquo; \u0026gt;}}\n注意 Pod 的配置文件指定了 PersistentVolumeClaim，但没有指定 PersistentVolume。对 Pod 而言，PersistentVolumeClaim 就是一个存储卷。\n创建 Pod：\nkubectl create -f https://k8s.io/examples/pods/storage/pv-pod.yaml  检查 Pod 中的容器是否运行正常：\nkubectl get pod task-pv-pod  打开一个 shell 访问 Pod 中的容器：\nkubectl exec -it task-pv-pod -- /bin/bash  在 shell 中，验证 nginx 是否正在从 hostPath 卷提供 index.html 文件：\nroot@task-pv-pod:/# apt-get update root@task-pv-pod:/# apt-get install curl root@task-pv-pod:/# curl localhost  输出结果是你之前写到 hostPath 卷中的 index.html 文件中的内容：\nHello from Kubernetes storage  访问控制 使用 group ID（GID）配置的存储仅允许 Pod 使用相同的 GID 进行写入。 GID 不匹配或缺少将会导致许可被拒绝的错误。 为了减少与用户的协调，管理员可以使用 GID 对 PersistentVolume 进行注解。 这样 GID 就能自动的添加到使用 PersistentVolume 的任何 Pod 中。\n使用 pv.beta.kubernetes.io/gid 注解的方法如下所示：\nkind: PersistentVolume apiVersion: v1 metadata: name: pv1 annotations: pv.beta.kubernetes.io/gid: \u0026#34;1234\u0026#34; 当 Pod 使用带有 GID 注解的 PersistentVolume 时，注解的 GID 会被应用于 Pod 中的所有容器，应用的方法与 Pod 的安全上下文中指定的 GID 相同。 每个 GID，无论是来自 PersistentVolume 注解还是来自 Pod 的规范，都应用于每个容器中运行的第一个进程。\n. note \u0026gt;}}\n当 Pod 使用 PersistentVolume 时，与 PersistentVolume 关联的 GID 不会在 Pod 本身的资源对象上出现。 . /note \u0026gt;}}\n. heading \u0026ldquo;whatsnext\u0026rdquo; %}}  进一步了解 PersistentVolumes。 阅读持久存储设计文档。  参考  [PersistentVolume](/docs/reference/generated/kubernetes-api/. param \u0026ldquo;version\u0026rdquo; \u0026gt;}}/#persistentvolume-v1-core) [PersistentVolumeSpec](/docs/reference/generated/kubernetes-api/. param \u0026ldquo;version\u0026rdquo; \u0026gt;}}/#persistentvolumespec-v1-core) [PersistentVolumeClaim](/docs/reference/generated/kubernetes-api/. param \u0026ldquo;version\u0026rdquo; \u0026gt;}}/#persistentvolumeclaim-v1-core) [PersistentVolumeClaimSpec](/docs/reference/generated/kubernetes-api/. param \u0026ldquo;version\u0026rdquo; \u0026gt;}}/#persistentvolumeclaimspec-v1-core)  "
},
{
	"uri": "https://lijun.in/tasks/administer-cluster/manage-resources/quota-pod-namespace/",
	"title": "配置命名空间下pod总数",
	"tags": [],
	"description": "",
	"content": "本文主要描述如何配置一个命名空间下可运行的pod总数。资源配额详细信息可查看：资源配额 。\n. heading \u0026ldquo;prerequisites\u0026rdquo; %}} . include \u0026ldquo;task-tutorial-prereqs.md\u0026rdquo; \u0026gt;}} . version-check \u0026gt;}}\n创建一个命名空间 首先创建一个命名空间，这样可以将本次操作中创建的资源与集群其他资源隔离开来。\nkubectl create namespace quota-pod-example 创建资源配额 下面是一个资源配额的配置文件：\n. codenew file=\u0026quot;admin/resource/quota-pod.yaml\u0026rdquo; \u0026gt;}}\n创建这个资源配额：\nkubectl apply -f https://k8s.io/examples/admin/resource/quota-pod.yaml --namespace=quota-pod-example 查看资源配额的详细信息：\nkubectl get resourcequota pod-demo --namespace=quota-pod-example --output=yaml 从输出的信息我们可以看到，该命名空间下pod的配额是2个，目前创建的pods数为0，配额使用率为0。\nspec: hard: pods: \u0026#34;2\u0026#34; status: hard: pods: \u0026#34;2\u0026#34; used: pods: \u0026#34;0\u0026#34; 下面是一个Deployment的配置文件：\n. codenew file=\u0026quot;admin/resource/quota-pod-deployment.yaml\u0026rdquo; \u0026gt;}}\n在配置文件中， replicas: 3 告诉kubernetes尝试创建三个pods，且运行相同的应用。\n创建这个Deployment：\nkubectl apply -f https://k8s.io/examples/admin/resource/quota-pod-deployment.yaml --namespace=quota-pod-example 查看Deployment的详细信息：\nkubectl get deployment pod-quota-demo --namespace=quota-pod-example --output=yaml 从输出的信息我们可以看到，尽管尝试创建三个pod，但是由于配额的限制，只有两个pod能被成功创建。\nspec: ... replicas: 3 ... status: availableReplicas: 2 ... lastUpdateTime: 2017-07-07T20:57:05Z message: \u0026#39;unable to create pods: pods \u0026#34;pod-quota-demo-1650323038-\u0026#34; is forbidden: exceeded quota: pod-demo, requested: pods=1, used: pods=2, limited: pods=2\u0026#39; 清理 删除命名空间：\nkubectl delete namespace quota-pod-example . heading \u0026ldquo;whatsnext\u0026rdquo; %}} 对于集群管理   配置命名空间下，内存默认的request值和limit值\n  配置命名空间下，CPU默认的request值和limit值\n  配置命名空间下，内存的最小值和最大值\n  配置命名空间下，CPU的最小值和最大值\n  配置命名空间下，内存和CPU的配额\n  配置命名空间下，API对象的配额\n  对于应用开发   给容器和pod分配内存资源\n  给容器和pod分配CPU资源\n  配置pod的QoS\n  "
},
{
	"uri": "https://lijun.in/reference/kubectl/",
	"title": "😍 - kubectl 命令行界面",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lijun.in/reference/command-line-tools-reference/",
	"title": "😍 - 命令行工具参考",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lijun.in/tutorials/",
	"title": "😎 - 教程",
	"tags": [],
	"description": "",
	"content": "Kubernetes 文档的这一部分包含教程。一个教程展示了如何完成一个比单个任务更大的目标。 通常一个教程有几个部分，每个部分都有一系列步骤。在浏览每个教程之前， 您可能希望将标准化术语表页面添加到书签，供以后参考。\n基础知识  Kubernetes 基础知识是一个深入的交互式教程，帮助您理解 Kubernetes 系统，并尝试一些基本的 Kubernetes 特性。   使用 Kubernetes (Udacity) 的可伸缩微服务   介绍 Kubernetes (edx)   你好 Minikube  配置  使用一个 ConfigMap 配置 Redis  无状态应用程序  公开外部 IP 地址访问集群中的应用程序   示例：使用 Redis 部署 PHP 留言板应用程序  有状态应用程序  StatefulSet 基础   示例：WordPress 和 MySQL 使用持久卷   示例：使用有状态集部署 Cassandra   运行 ZooKeeper，CP 分布式系统  CI/CD 管道  用 Kubernetes 第一部分：概述建立 CI/CD 管道   与 Kubernetes 中的 Jenkins Pod 一起建立 CI/CD 管道(第二部分)   在 Kubernetes 上运行并扩展一个带有 CI/CD 的分布式填字游戏应用程序(第三部分)   为 Kubernetes 上的分布式填字游戏应用程序设置 CI/CD (第四部分))  集群  AppArmor  服务  使用源 IP  . heading \u0026ldquo;whatsnext\u0026rdquo; %}} 如果您想编写教程，请参阅使用页面模板 以获取有关教程页面类型和教程模板的信息。\n"
},
{
	"uri": "https://lijun.in/tutorials/clusters/",
	"title": "😎 - 集群",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lijun.in/tasks/access-application-cluster/",
	"title": "😝 - 访问集群中的应用程序",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lijun.in/concepts/workloads/controllers/ttlafterfinished/",
	"title": "已完成资源的 TTL 控制器",
	"tags": [],
	"description": "",
	"content": "feature-state for_k8s_version=\u0026quot;v1.12\u0026rdquo; state=\u0026quot;alpha\u0026rdquo; \u0026gt;}}\nTTL 控制器提供了一种 TTL 机制来限制已完成执行的资源对象的生命周期。TTL 控制器目前只处理 Job，可能以后会扩展以处理将完成执行的其他资源，例如 Pod 和自定义资源。\nAlpha 免责声明：此功能目前是 alpha 版，并且可以通过 kube-apiserver 和 kube-controller-manager 特性开关 TTLAfterFinished 启用。\nTTL 控制器 TTL 控制器现在只支持 Job。集群操作员可以通过指定 Job 的 .spec.ttlSecondsAfterFinished 字段来自动清理已结束的作业（Complete 或 Failed），如下所示的示例。\nTTL 控制器假设资源能在执行完成后的 TTL 秒内被清理，也就是当 TTL 过期后。当 TTL 控制器清理资源时，它将做级联删除操作，如删除资源对象的同时也删除其依赖对象。注意，当资源被删除时，由该资源的生命周期保证其终结器（finalizers）等被执行。\n可以随时设置 TTL 秒。以下是设置 Job 的 .spec.ttlSecondsAfterFinished 字段的一些示例：\n 在资源清单（manifest）中指定此字段，以便 Job 在完成后的某个时间被自动清除。 将此字段设置为存在的、已完成的资源，以采用此新功能。 在创建资源时使用 mutating admission webhook 动态设置该字段。集群管理员可以使用它对完成的资源强制执行 TTL 策略。 使用 mutating admission webhook 在资源完成后动态设置该字段，并根据资源状态、标签等选择不同的 TTL 值。  警告 更新 TTL 秒 请注意，在创建资源或已经执行结束后，仍可以修改其 TTL 周期，例如 Job 的 .spec.ttlSecondsAfterFinished 字段。但是，一旦 Job 变为可被删除状态（当其 TTL 已过期时），即使您通过 API 扩展其 TTL 时长得到了成功的响应，系统也不保证 Job 将被保留。\n时间偏差 由于 TTL 控制器使用存储在 Kubernetes 资源中的时间戳来确定 TTL 是否已过期，因此该功能对集群中的时间偏差很敏感，这可能导致 TTL 控制器在错误的时间清理资源对象。\n在 Kubernetes 中，需要在所有节点上运行 NTP（参见 #6159）以避免时间偏差。时钟并不总是如此正确，但差异应该很小。设置非零 TTL 时请注意避免这种风险。\n% heading \u0026ldquo;whatsnext\u0026rdquo;\n自动清理 Job\n设计文档\n"
},
{
	"uri": "https://lijun.in/concepts/services-networking/dual-stack/",
	"title": "IPv4/IPv6 双协议栈",
	"tags": [],
	"description": "",
	"content": "feature-state for_k8s_version=\u0026quot;v1.16\u0026rdquo; state=\u0026quot;alpha\u0026rdquo; \u0026gt;}}\nIPv4/IPv6 双协议栈能够将 IPv4 和 IPv6 地址分配给 text=\u0026quot;Pods\u0026rdquo; term_id=\u0026quot;pod\u0026rdquo; \u0026gt;}} 和 text=\u0026quot;Services\u0026rdquo; term_id=\u0026quot;service\u0026rdquo; \u0026gt;}}。\n如果你为 Kubernetes 集群启用了 IPv4/IPv6 双协议栈网络，则该集群将支持同时分配 IPv4 和 IPv6 地址。\n支持的功能 在 Kubernetes 集群上启用 IPv4/IPv6 双协议栈可提供下面的功能：\n 双协议栈 pod 网络 (每个 pod 分配一个 IPv4 和 IPv6 地址) IPv4 和 IPv6 启用的服务 (每个服务必须是一个单独的地址族) Pod 的集群外出口通过 IPv4 和 IPv6 路由  先决条件 为了使用 IPv4/IPv6 双栈的 Kubernetes 集群，需要满足以下先决条件：\n Kubernetes 1.16 版本及更高版本 提供商支持双协议栈网络（云提供商或其他提供商必须能够为 Kubernetes 节点提供可路由的 IPv4/IPv6 网络接口） 支持双协议栈的网络插件（如 Kubenet 或 Calico）  启用 IPv4/IPv6 双协议栈 要启用 IPv4/IPv6 双协议栈，为集群的相关组件启用 IPv6DualStack 特性门控，并且设置双协议栈的集群网络分配：\n kube-apiserver:  --feature-gates=\u0026quot;IPv6DualStack=true\u0026quot;   kube-controller-manager:  --feature-gates=\u0026quot;IPv6DualStack=true\u0026quot; --cluster-cidr=\u0026lt;IPv4 CIDR\u0026gt;,\u0026lt;IPv6 CIDR\u0026gt; 例如 --cluster-cidr=10.244.0.0/16,fc00::/48 --service-cluster-ip-range=\u0026lt;IPv4 CIDR\u0026gt;,\u0026lt;IPv6 CIDR\u0026gt; 例如 --service-cluster-ip-range=10.0.0.0/16,fd00::/108 --node-cidr-mask-size-ipv4|--node-cidr-mask-size-ipv6 对于 IPv4 默认为 /24，对于 IPv6 默认为 /64   kubelet:  --feature-gates=\u0026quot;IPv6DualStack=true\u0026quot;   kube-proxy:  --cluster-cidr=\u0026lt;IPv4 CIDR\u0026gt;,\u0026lt;IPv6 CIDR\u0026gt; --feature-gates=\u0026quot;IPv6DualStack=true\u0026quot;    服务 如果你的集群启用了 IPv4/IPv6 双协议栈网络，则可以使用 IPv4 或 IPv6 地址来创建 text=\u0026quot;Services\u0026rdquo; term_id=\u0026quot;service\u0026rdquo; \u0026gt;}}。你可以通过设置服务的 .spec.ipFamily 字段来选择服务的集群 IP 的地址族。你只能在创建新服务时设置该字段。.spec.ipFamily 字段的设置是可选的，并且仅当你计划在集群上启用 IPv4 和 IPv6 的 text=\u0026quot;Services\u0026rdquo; term_id=\u0026quot;service\u0026rdquo; \u0026gt;}} 和 text=\u0026quot;Ingresses\u0026rdquo; term_id=\u0026quot;ingress\u0026rdquo; \u0026gt;}}。对于出口流量，该字段的配置不是必须的。\n集群的默认地址族是第一个服务集群 IP 范围的地址族，该地址范围通过 kube-controller-manager 上的 --service-cluster-ip-range 标志设置。\n你可以设置 .spec.ipFamily 为：\n IPv4：API 服务器将从 service-cluster-ip-range 中分配 ipv4 地址 IPv6：API 服务器将从 service-cluster-ip-range 中分配 ipv6 地址  以下服务规约不包含 ipFamily 字段。Kubernetes 将从最初配置的 service-cluster-ip-range 范围内分配一个 IP 地址（也称作“集群 IP”）给该服务。\ncodenew file=\u0026quot;service/networking/dual-stack-default-svc.yaml\u0026rdquo; \u0026gt;}}\n以下服务规约包含 ipFamily 字段。Kubernetes 将从已配置的 service-cluster-ip-range 范围内分配一个 IPv6 地址（也称作“集群 IP”）给该服务。\ncodenew file=\u0026quot;service/networking/dual-stack-ipv6-svc.yaml\u0026rdquo; \u0026gt;}}\n为了进行比较，将从已配置的 service-cluster-ip-range 向该服务分配以下 IPV4 地址（也称为“集群 IP”）。\ncodenew file=\u0026quot;service/networking/dual-stack-ipv4-svc.yaml\u0026rdquo; \u0026gt;}}\n负载均衡器类型 在支持启用了 IPv6 的外部服务均衡器的云驱动上，除了将 ipFamily 字段设置为 IPv6，将 type 字段设置为 LoadBalancer，为你的服务提供云负载均衡。\n出口流量 公共路由和非公共路由的 IPv6 地址块的使用是可以的。提供底层 CNI 的提供程序可以实现这种传输。如果你拥有使用非公共路由 IPv6 地址的 Pod，并且希望该 Pod 到达集群外目的（比如，公共网络），你必须为出口流量和任何响应消息设置 IP 伪装。ip-masq-agent 可以感知双栈，所以你可以在双栈集群中使用 ip-masq-agent 来进行 IP 伪装。\n已知问题  Kubenet 强制 IPv4，IPv6 的 IPs 位置报告 (\u0026ndash;cluster-cidr)    验证 IPv4/IPv6 双协议栈网络  "
},
{
	"uri": "https://lijun.in/reference/setup-tools/kubeadm/kubeadm-token/",
	"title": "kubeadm 令牌",
	"tags": [],
	"description": "",
	"content": "如使用引导令牌进行身份验证所描述的，引导令牌用于在即将加入集群的节点和主节点间建立双向认证。\nkubeadm init 创建了一个有效期为 24 小时的令牌，下面的命令允许您管理令牌，也可以创建和管理新的令牌。\nkubeadm token create . include \u0026ldquo;generated/kubeadm_token_create.md\u0026rdquo; \u0026gt;}}\nkubeadm token delete . include \u0026ldquo;generated/kubeadm_token_delete.md\u0026rdquo; \u0026gt;}}\nkubeadm token generate . include \u0026ldquo;generated/kubeadm_token_generate.md\u0026rdquo; \u0026gt;}}\nkubeadm token list . include \u0026ldquo;generated/kubeadm_token_list.md\u0026rdquo; \u0026gt;}}\n. heading \u0026ldquo;whatsnext\u0026rdquo; %}}  kubeadm join 引导 Kubernetes 工作节点并将其加入群集  "
},
{
	"uri": "https://lijun.in/setup/production-environment/tools/kubeadm/setup-ha-etcd-with-kubeadm/",
	"title": "使用 kubeadm 创建一个高可用 etcd 集群",
	"tags": [],
	"description": "",
	"content": ". note \u0026gt;}}\n在本指南中，当 kubeadm 用作为外部 etcd 节点管理工具，请注意 kubeadm 不计划支持此类节点的证书更换或升级。对于长期规划是使用 etcdadm 增强工具来管理这方面。 . /note \u0026gt;}}\n默认情况下，kubeadm 运行单成员的 etcd 集群，该集群由控制面节点上的 kubelet 以静态 Pod 的方式进行管理。由于 etcd 集群只包含一个成员且不能在任一成员不可用时保持运行，所以这不是一种高可用设置。本任务，将告诉您如何在使用 kubeadm 创建一个 kubernetes 集群时创建一个外部 etcd：有三个成员的高可用 etcd 集群。\n. heading \u0026ldquo;prerequisites\u0026rdquo; %}}  三个可以通过 2379 和 2380 端口相互通信的主机。本文档使用这些作为默认端口。不过，它们可以通过 kubeadm 的配置文件进行自定义。   每个主机必须 安装有 docker、kubelet 和 kubeadm。   一些可以用来在主机间复制文件的基础设施。例如 ssh 和 scp 就可以满足需求。  建立集群 一般来说，是在一个节点上生成所有证书并且只分发这些必要的文件到其它节点上。\n. note \u0026gt;}}\nkubeadm 包含生成下述证书所需的所有必要的密码学工具；在这个例子中，不需要其他加密工具。 . /note \u0026gt;}}\n  将 kubelet 配置为 etcd 的服务管理器。\n由于 etcd 是首先创建的，因此您必须通过创建具有更高优先级的新文件来覆盖 kubeadm 提供的 kubelet 单元文件。\ncat \u0026lt;\u0026lt; EOF \u0026gt; /etc/systemd/system/kubelet.service.d/20-etcd-service-manager.conf [Service] ExecStart= # Replace \u0026#34;systemd\u0026#34; with the cgroup driver of your container runtime. The default value in the kubelet is \u0026#34;cgroupfs\u0026#34;. ExecStart=/usr/bin/kubelet --address=127.0.0.1 --pod-manifest-path=/etc/kubernetes/manifests --cgroup-driver=systemd Restart=always EOF systemctl daemon-reload systemctl restart kubelet   为 kubeadm 创建配置文件。\n使用以下脚本为每个将要运行 etcd 成员的主机生成一个 kubeadm 配置文件。\n# 使用 IP 或可解析的主机名替换 HOST0、HOST1 和 HOST2 export HOST0=10.0.0.6 export HOST1=10.0.0.7 export HOST2=10.0.0.8 # 创建临时目录来存储将被分发到其它主机上的文件 mkdir -p /tmp/${HOST0}/ /tmp/${HOST1}/ /tmp/${HOST2}/ ETCDHOSTS=(${HOST0} ${HOST1} ${HOST2}) NAMES=(\u0026#34;infra0\u0026#34; \u0026#34;infra1\u0026#34; \u0026#34;infra2\u0026#34;) for i in \u0026#34;${!ETCDHOSTS[@]}\u0026#34;; do HOST=${ETCDHOSTS[$i]} NAME=${NAMES[$i]} cat \u0026lt;\u0026lt; EOF \u0026gt; /tmp/${HOST}/kubeadmcfg.yaml apiVersion: \u0026#34;kubeadm.k8s.io/v1beta2\u0026#34; kind: ClusterConfiguration etcd: local: serverCertSANs: - \u0026#34;${HOST}\u0026#34; peerCertSANs: - \u0026#34;${HOST}\u0026#34; extraArgs: initial-cluster: infra0=https://${ETCDHOSTS[0]}:2380,infra1=https://${ETCDHOSTS[1]}:2380,infra2=https://${ETCDHOSTS[2]}:2380 initial-cluster-state: new name: ${NAME} listen-peer-urls: https://${HOST}:2380 listen-client-urls: https://${HOST}:2379 advertise-client-urls: https://${HOST}:2379 initial-advertise-peer-urls: https://${HOST}:2380 EOF done   生成证书颁发机构\n如果您已经拥有 CA，那么唯一的操作是复制 CA 的 crt 和 key 文件到 etc/kubernetes/pki/etcd/ca.crt 和 /etc/kubernetes/pki/etcd/ca.key。复制完这些文件后继续下一步，“为每个成员创建证书”。\n如果您还没有 CA，则在 $HOST0（您为 kubeadm 生成配置文件的位置）上运行此命令。\nkubeadm init phase certs etcd-ca 创建了如下两个文件\n /etc/kubernetes/pki/etcd/ca.crt /etc/kubernetes/pki/etcd/ca.key    为每个成员创建证书\nkubeadm init phase certs etcd-server --config=/tmp/${HOST2}/kubeadmcfg.yaml kubeadm init phase certs etcd-peer --config=/tmp/${HOST2}/kubeadmcfg.yaml kubeadm init phase certs etcd-healthcheck-client --config=/tmp/${HOST2}/kubeadmcfg.yaml kubeadm init phase certs apiserver-etcd-client --config=/tmp/${HOST2}/kubeadmcfg.yaml cp -R /etc/kubernetes/pki /tmp/${HOST2}/ # 清理不可重复使用的证书 find /etc/kubernetes/pki -not -name ca.crt -not -name ca.key -type f -delete kubeadm init phase certs etcd-server --config=/tmp/${HOST1}/kubeadmcfg.yaml kubeadm init phase certs etcd-peer --config=/tmp/${HOST1}/kubeadmcfg.yaml kubeadm init phase certs etcd-healthcheck-client --config=/tmp/${HOST1}/kubeadmcfg.yaml kubeadm init phase certs apiserver-etcd-client --config=/tmp/${HOST1}/kubeadmcfg.yaml cp -R /etc/kubernetes/pki /tmp/${HOST1}/ find /etc/kubernetes/pki -not -name ca.crt -not -name ca.key -type f -delete kubeadm init phase certs etcd-server --config=/tmp/${HOST0}/kubeadmcfg.yaml kubeadm init phase certs etcd-peer --config=/tmp/${HOST0}/kubeadmcfg.yaml kubeadm init phase certs etcd-healthcheck-client --config=/tmp/${HOST0}/kubeadmcfg.yaml kubeadm init phase certs apiserver-etcd-client --config=/tmp/${HOST0}/kubeadmcfg.yaml # 不需要移动 certs 因为它们是给 HOST0 使用的 # 清理不应从此主机复制的证书 find /tmp/${HOST2} -name ca.key -type f -delete find /tmp/${HOST1} -name ca.key -type f -delete   复制证书和 kubeadm 配置\n证书已生成，现在必须将它们移动到对应的主机。\nUSER=ubuntu HOST=${HOST1} scp -r /tmp/${HOST}/* ${USER}@${HOST}: ssh ${USER}@${HOST} USER@HOST $ sudo -Es root@HOST $ chown -R root:root pki root@HOST $ mv pki /etc/kubernetes/   确保已经所有预期的文件都存在\n$HOST0 所需文件的完整列表如下：\n/tmp/${HOST0} └── kubeadmcfg.yaml --- /etc/kubernetes/pki ├── apiserver-etcd-client.crt ├── apiserver-etcd-client.key └── etcd ├── ca.crt ├── ca.key ├── healthcheck-client.crt ├── healthcheck-client.key ├── peer.crt ├── peer.key ├── server.crt └── server.key 在 $HOST1:\n$HOME └── kubeadmcfg.yaml --- /etc/kubernetes/pki ├── apiserver-etcd-client.crt ├── apiserver-etcd-client.key └── etcd ├── ca.crt ├── healthcheck-client.crt ├── healthcheck-client.key ├── peer.crt ├── peer.key ├── server.crt └── server.key 在 $HOST2\n$HOME └── kubeadmcfg.yaml --- /etc/kubernetes/pki ├── apiserver-etcd-client.crt ├── apiserver-etcd-client.key └── etcd ├── ca.crt ├── healthcheck-client.crt ├── healthcheck-client.key ├── peer.crt ├── peer.key ├── server.crt └── server.key   创建静态 Pod 清单\n既然证书和配置已经就绪，是时候去创建清单了。在每台主机上运行 kubeadm 命令来生成 etcd 使用的静态清单。\nroot@HOST0 $ kubeadm init phase etcd local --config=/tmp/${HOST0}/kubeadmcfg.yaml root@HOST1 $ kubeadm init phase etcd local --config=/home/ubuntu/kubeadmcfg.yaml root@HOST2 $ kubeadm init phase etcd local --config=/home/ubuntu/kubeadmcfg.yaml   可选：检查群集运行状况\ndocker run --rm -it \\ --net host \\ -v /etc/kubernetes:/etc/kubernetes k8s.gcr.io/etcd:${ETCD_TAG} etcdctl \\ --cert /etc/kubernetes/pki/etcd/peer.crt \\ --key /etc/kubernetes/pki/etcd/peer.key \\ --cacert /etc/kubernetes/pki/etcd/ca.crt \\ --endpoints https://${HOST0}:2379 endpoint health --cluster ... https://[HOST0 IP]:2379 is healthy: successfully committed proposal: took = 16.283339ms https://[HOST1 IP]:2379 is healthy: successfully committed proposal: took = 19.44402ms https://[HOST2 IP]:2379 is healthy: successfully committed proposal: took = 35.926451ms  将 ${ETCD_TAG} 设置为你的 etcd 镜像的版本标签，例如 3.4.3-0。要查看 kubeadm 使用的 etcd 镜像和标签，请执行 kubeadm config images list --kubernetes-version ${K8S_VERSION}，其中 ${K8S_VERSION} 是 v1.17.0 作为例子。   将 ${HOST0} 设置为要测试的主机的 IP 地址    . heading \u0026ldquo;whatsnext\u0026rdquo; %}} 一旦拥有了一个正常工作的 3 成员的 etcd 集群，你就可以基于使用 kubeadm 的外部 etcd 方法，继续部署一个高可用的控制平面。\n"
},
{
	"uri": "https://lijun.in/reference/access-authn-authz/rbac/",
	"title": "使用 RBAC 鉴权",
	"tags": [],
	"description": "",
	"content": "基于角色（Role）的访问控制（RBAC）是一种基于企业中用户的角色来调节控制对计算机或网络资源的访问方法。\nRBAC 使用 rbac.authorization.k8s.io . glossary_tooltip text=\u0026quot;API 组\u0026rdquo; term_id=\u0026quot;api-group\u0026rdquo; \u0026gt;}} 来驱动鉴权操作，允许管理员通过 Kubernetes API 动态配置策略。\n在 1.8 版本中，RBAC 模式是稳定的并通过 rbac.authorization.k8s.io/v1 API 提供支持。\n要启用 RBAC，在启动 API 服务器时添加 --authorization-mode=RBAC 参数。\nAPI 概述 本节介绍 RBAC API 所声明的四种顶级类型。用户可以像与其他 API 资源交互一样， （通过 kubectl、API 调用等方式）与这些资源交互。例如， 命令 kubectl apply -f (resource).yml 可以用在这里的任何一个例子之上。 尽管如此，建议读者循序渐进阅读下面的章节，由浅入深。\nRole 和 ClusterRole 在 RBAC API 中，一个角色包含一组相关权限的规则。权限是纯粹累加的（不存在拒绝某操作的规则）。 角色可以用 Role 来定义到某个命名空间上， 或者用 ClusterRole 来定义到整个集群作用域。\n一个 Role 只可以用来对某一命名空间中的资源赋予访问权限。 下面的 Role 示例定义到名称为 \u0026ldquo;default\u0026rdquo; 的命名空间，可以用来授予对该命名空间中的 Pods 的读取权限：\napiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: namespace: default name: pod-reader rules: - apiGroups: [\u0026#34;\u0026#34;] # \u0026#34;\u0026#34; 指定核心 API 组 resources: [\u0026#34;pods\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;watch\u0026#34;, \u0026#34;list\u0026#34;] ClusterRole 可以授予的权限和 Role 相同， 但是因为 ClusterRole 属于集群范围，所以它也可以授予以下访问权限：\n 集群范围资源 （比如 nodes） 非资源端点（比如 \u0026ldquo;/healthz\u0026rdquo;） 跨命名空间访问的有名字空间作用域的资源（如 Pods），比如运行命令kubectl get pods --all-namespaces 时需要此能力  下面的 ClusterRole 示例可用来对某特定命名空间下的 Secrets 的读取操作授权， 或者跨所有命名空间执行授权（取决于它是如何绑定的）:\napiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: # 此处的 \u0026#34;namespace\u0026#34; 被省略掉是因为 ClusterRoles 是没有命名空间的。 name: secret-reader rules: - apiGroups: [\u0026#34;\u0026#34;] resources: [\u0026#34;secrets\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;watch\u0026#34;, \u0026#34;list\u0026#34;] RoleBinding 和 ClusterRoleBinding 角色绑定（RoleBinding）是将角色中定义的权限赋予一个或者一组用户。 它包含若干主体（用户，组和服务账户）的列表和对这些主体所获得的角色的引用。 可以使用 RoleBinding 在指定的命名空间中执行授权， 或者在集群范围的命名空间使用 ClusterRoleBinding 来执行授权。\n一个 RoleBinding 可以引用同一的命名空间中的 Role 。 下面的例子 RoleBinding 将 \u0026ldquo;pod-reader\u0026rdquo; 角色授予在 \u0026ldquo;default\u0026rdquo; 命名空间中的用户 \u0026ldquo;jane\u0026rdquo;； 这样，用户 \u0026ldquo;jane\u0026rdquo; 就具有了读取 \u0026ldquo;default\u0026rdquo; 命名空间中 pods 的权限。\nroleRef 里的内容决定了实际创建绑定的方法。kind 可以是 Role 或 ClusterRole， name 将引用你要指定的 Role 或 ClusterRole 的名称。在下面的例子中，角色绑定使用 roleRef 将用户 \u0026ldquo;jane\u0026rdquo; 绑定到前文创建的角色 Role，其名称是 pod-reader。\napiVersion: rbac.authorization.k8s.io/v1 # 此角色绑定使得用户 \u0026#34;jane\u0026#34; 能够读取 \u0026#34;default\u0026#34; 命名空间中的 Pods kind: RoleBinding metadata: name: read-pods namespace: default subjects: - kind: User name: jane # Name is case sensitive apiGroup: rbac.authorization.k8s.io roleRef: kind: Role #this must be Role or ClusterRole name: pod-reader # 这里的名称必须与你想要绑定的 Role 或 ClusterRole 名称一致 apiGroup: rbac.authorization.k8s.io RoleBinding 也可以引用 ClusterRole，对 ClusterRole 所定义的、位于 RoleBinding 命名空间内的资源授权。 这可以允许管理者在 整个集群中定义一组通用的角色，然后在多个命名空间中重用它们。\n例如下面的例子，RoleBinding 指定的是 ClusterRole， \u0026ldquo;dave\u0026rdquo; （主体，区分大小写）将只可以读取在\u0026quot;development\u0026rdquo; 命名空间（ RoleBinding 的命名空间）中的\u0026quot;secrets\u0026rdquo;。\napiVersion: rbac.authorization.k8s.io/v1 # 这个角色绑定允许 \u0026#34;dave\u0026#34; 用户在 \u0026#34;development\u0026#34; 命名空间中有读取 secrets 的权限。  kind: RoleBinding metadata: name: read-secrets namespace: development # 这里只授予 \u0026#34;development\u0026#34; 命名空间的权限。 subjects: - kind: User name: dave # 名称区分大小写 apiGroup: rbac.authorization.k8s.io roleRef: kind: ClusterRole name: secret-reader apiGroup: rbac.authorization.k8s.io 最后，ClusterRoleBinding 可用来在集群级别或对所有命名空间执行授权。 下面的例子允许 \u0026ldquo;manager\u0026rdquo; 组中的任何用户读取任意命名空间中 \u0026ldquo;secrets\u0026rdquo;。\napiVersion: rbac.authorization.k8s.io/v1 # 这个集群角色绑定允许 \u0026#34;manager\u0026#34; 组中的任何用户读取任意命名空间中 \u0026#34;secrets\u0026#34;。 kind: ClusterRoleBinding metadata: name: read-secrets-global subjects: - kind: Group name: manager # 名称区分大小写 apiGroup: rbac.authorization.k8s.io roleRef: kind: ClusterRole name: secret-reader apiGroup: rbac.authorization.k8s.io 你不能修改绑定对象所引用的 Role 或 ClusterRole 。 试图改变绑定对象的 roleRef 将导致验证错误。想要 改变现有绑定对象中 roleRef 字段的内容，必须删除并 重新创建绑定对象。这种限制有两个主要原因：\n1.关于不同角色的绑定是完全不一样的。更改 roleRef 需要删除/重建绑定，确保要赋予绑定的完整主体列表是新 的角色（而不是只是启用修改 roleRef 在不验证所有现有 主体的情况下的，应该授予新角色对应的权限）。\n2.使得 roleRef 不可以改变现有绑定主体用户的 update 权限， 这样可以让它们能够管理主体列表，而不能更改授予这些主体相关 的角色。\n命令 kubectl auth reconcile 可以创建或者更新包含 RBAC 对象的清单文件， 并且在必要的情况下删除和重新创建绑定对象，以改变所引用的角色。 更多相关信息请参照命令用法和示例\n对资源的引用 大多数资源都是使用名称的字符串表示，例如在相关的 API 端点的 URL 之中出现的 \u0026ldquo;pods\u0026rdquo; 。 然而有一些 Kubernetes API 涉及 \u0026ldquo;子资源（subresources）\u0026quot;，例如 pod 的日志。Pod 日志相关的端点 URL 如下：\nGET /api/v1/namespaces/{namespace}/pods/{name}/log 在这种情况下，\u0026ldquo;pods\u0026rdquo; 是有命名空间的资源，而 \u0026ldquo;log\u0026rdquo; 是 pods 的子资源。在 RBAC 角色中， 使用\u0026rdquo;/\u0026ldquo;分隔资源和子资源。允许一个主体要同时读取 pods 和 pod logs，你可以这么写：\napiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: namespace: default name: pod-and-pod-logs-reader rules: - apiGroups: [\u0026#34;\u0026#34;] resources: [\u0026#34;pods\u0026#34;, \u0026#34;pods/log\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;] 对于某些请求，也可以通过 resourceNames 列表按名称引用资源。 在指定时，可以将请求类型限制资源的单个实例。限制只可以 \u0026ldquo;get\u0026rdquo; 和 \u0026ldquo;update\u0026rdquo; 的单一configmap，你可以这么写：\napiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: namespace: default name: configmap-updater rules: - apiGroups: [\u0026#34;\u0026#34;] resources: [\u0026#34;configmaps\u0026#34;] resourceNames: [\u0026#34;my-configmap\u0026#34;] verbs: [\u0026#34;update\u0026#34;, \u0026#34;get\u0026#34;] 需要注意的是，create 请求不能被 resourceName 限制，因为在鉴权时还不知道对象名称。 另一个例外是 deletecollection。\nAggregated ClusterRoles 从 1.9 开始，集群角色（ClusterRole）可以通过使用 aggregationRule 的方式并组合其他 ClusterRoles 来创建。 聚合集群角色的权限是由控制器管理的，方法是通过过滤与标签选择器匹配的 ClusterRules，并将其中的权限进行组合。 一个聚合集群角色的示例如下：\napiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: monitoring aggregationRule: clusterRoleSelectors: - matchLabels: rbac.example.com/aggregate-to-monitoring: \u0026#34;true\u0026#34; rules: [] # 具体规则由控制器管理器自动填写。 创建一个与标签选择器匹配的 ClusterRole 之后，其上定义的规则将成为聚合集群角色的一部分。在下面的例子中， 通过创建一个新的、标签同样为 rbac.example.com/aggregate-to-monitoring: true 的 ClusterRole，新的规则可被添加到 \u0026ldquo;monitoring\u0026rdquo; 集群角色中。\napiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: monitoring-endpoints labels: rbac.example.com/aggregate-to-monitoring: \u0026#34;true\u0026#34; # 这些规则将被添加到 \u0026#34;monitoring\u0026#34; 角色中。 rules: - apiGroups: [\u0026#34;\u0026#34;] resources: [\u0026#34;services\u0026#34;, \u0026#34;endpoints\u0026#34;, \u0026#34;pods\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;] 默认的面向用户的角色（如下所述）使用 ClusterRole 聚合。这使得管理者可以为自定义资源设置使用规则属性， 比如通过 CustomResourceDefinitions 或聚合 API 服务器为默认角色提供的服务。\n例如，在以下 ClusterRoles 中让 \u0026ldquo;admin\u0026rdquo; 和 \u0026ldquo;edit\u0026rdquo; 拥有管理自定义资源 \u0026ldquo;CronTabs\u0026rdquo; 的权限， \u0026ldquo;view\u0026rdquo; 角色对资源有只读操作权限。\napiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: aggregate-cron-tabs-edit labels: # 将这些权限添加到默认角色 \u0026#34;admin\u0026#34; 和 \u0026#34;edit\u0026#34; 中。 rbac.authorization.k8s.io/aggregate-to-admin: \u0026#34;true\u0026#34; rbac.authorization.k8s.io/aggregate-to-edit: \u0026#34;true\u0026#34; rules: - apiGroups: [\u0026#34;stable.example.com\u0026#34;] resources: [\u0026#34;crontabs\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;, \u0026#34;create\u0026#34;, \u0026#34;update\u0026#34;, \u0026#34;patch\u0026#34;, \u0026#34;delete\u0026#34;] --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: aggregate-cron-tabs-view labels: # 将这些权限添加到默认角色 \u0026#34;view\u0026#34; 中。 rbac.authorization.k8s.io/aggregate-to-view: \u0026#34;true\u0026#34; rules: - apiGroups: [\u0026#34;stable.example.com\u0026#34;] resources: [\u0026#34;crontabs\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;] 角色示例 在以下示例中，我们仅截取展示了 rules 对应部分， 允许读取在核心 . glossary_tooltip text=\u0026quot;API 组\u0026rdquo; term_id=\u0026quot;api-group\u0026rdquo; \u0026gt;}}下的 Pods:\nrules: - apiGroups: [\u0026#34;\u0026#34;] resources: [\u0026#34;pods\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;] 允许读/写在 \u0026ldquo;extensions\u0026rdquo; 和 \u0026ldquo;apps\u0026rdquo; API 组中的 \u0026ldquo;deployments\u0026rdquo; 资源：\nrules: - apiGroups: [\u0026#34;extensions\u0026#34;, \u0026#34;apps\u0026#34;] resources: [\u0026#34;deployments\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;, \u0026#34;create\u0026#34;, \u0026#34;update\u0026#34;, \u0026#34;patch\u0026#34;, \u0026#34;delete\u0026#34;] 允许读取 \u0026ldquo;pods\u0026rdquo; 和读/写 \u0026ldquo;jobs\u0026rdquo; :\nrules: - apiGroups: [\u0026#34;\u0026#34;] resources: [\u0026#34;pods\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;] - apiGroups: [\u0026#34;batch\u0026#34;, \u0026#34;extensions\u0026#34;] resources: [\u0026#34;jobs\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;, \u0026#34;create\u0026#34;, \u0026#34;update\u0026#34;, \u0026#34;patch\u0026#34;, \u0026#34;delete\u0026#34;] 允许读取名称为 \u0026ldquo;my-config\u0026quot;的 ConfigMap （需要通过 RoleBinding 绑定带某名字空间中特定的 ConfigMap）：\nrules: - apiGroups: [\u0026#34;\u0026#34;] resources: [\u0026#34;configmaps\u0026#34;] resourceNames: [\u0026#34;my-config\u0026#34;] verbs: [\u0026#34;get\u0026#34;] 允许读取在核心组中的 \u0026ldquo;nodes\u0026rdquo; 资源（因为 Node 是集群范围的，所以需要 ClusterRole 绑定到 ClusterRoleBinding 才生效）\nrules: - apiGroups: [\u0026#34;\u0026#34;] resources: [\u0026#34;nodes\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;] 允许在非资源端点 \u0026ldquo;/healthz\u0026rdquo; 和其子路径上发起 \u0026ldquo;GET\u0026rdquo; 和 \u0026ldquo;POST\u0026rdquo; 请求（必须在 ClusterRole 绑定 ClusterRoleBinding 才生效）\nrules: - nonResourceURLs: [\u0026#34;/healthz\u0026#34;, \u0026#34;/healthz/*\u0026#34;] # \u0026#39;*\u0026#39; 在 nonResourceURL 中的意思是后缀全局匹配。 verbs: [\u0026#34;get\u0026#34;, \u0026#34;post\u0026#34;] 对主体的引用 RoleBinding 或者 ClusterRoleBinding 需要绑定角色到 主体。 主体可以是组，用户或者服务账户。\n用户是由字符串表示，它们可以是普通的用户名，像 \u0026ldquo;alice\u0026rdquo;，或者是 邮件格式 \u0026ldquo;bob@example.com\u0026rdquo;，或者是数字ID。由 Kubernetes 管理员配置身份认证模块 需要的格式。RBAC 鉴权系统不对格式作任何要求，但是前缀 system: 是 Kubernetes 系统保留的， 所以管理员要确保配置的用户名不能出现上述前缀格式。\n用户组信息是 Kubernetes 现在提供的一种身份验证模块，与用户一样，对组的字符串没有格式要求， 只是不能使用保留的前缀 system: 。\n服务账号 的用户名前缀为system:serviceaccount:， 属于前缀为 system:serviceaccounts: 的用户组。\nRoleBinding的示例 下面的示例只是展示 RoleBinding 中 subjects 的部分。\n用户的名称为 \u0026ldquo;alice@example.com\u0026rdquo;:\nsubjects: - kind: User name: \u0026#34;alice@example.com\u0026#34; apiGroup: rbac.authorization.k8s.io 组的名称为 \u0026ldquo;frontend-admins\u0026rdquo;:\nsubjects: - kind: Group name: \u0026#34;frontend-admins\u0026#34; apiGroup: rbac.authorization.k8s.io 服务账号在 kube-system 命名空间中:\nsubjects: - kind: ServiceAccount name: default namespace: kube-system 在名称为 \u0026ldquo;qa\u0026rdquo; 命名空间中所有的服务账号:\nsubjects: - kind: Group name: system:serviceaccounts:qa apiGroup: rbac.authorization.k8s.io 所有的服务账号:\nsubjects: - kind: Group name: system:serviceaccounts apiGroup: rbac.authorization.k8s.io 所有认证过的用户 （版本 1.5+）:\nsubjects: - kind: Group name: system:authenticated apiGroup: rbac.authorization.k8s.io 所有未认证的用户 （版本 1.5+）:\nsubjects: - kind: Group name: system:unauthenticated apiGroup: rbac.authorization.k8s.io 所有用户 （版本 1.5+）:\nsubjects: - kind: Group name: system:authenticated apiGroup: rbac.authorization.k8s.io - kind: Group name: system:unauthenticated apiGroup: rbac.authorization.k8s.io 默认 Roles 和 Role Bindings API servers创建一组默认为 ClusterRole 和 ClusterRoleBinding 的对象。 其中许多是以 system: 为前缀的，它表示资源是基础设施 \u0026ldquo;owned\u0026rdquo; 的。对于这些资源的修改可能导致集群功能失效。 例如，system:node 是集群角色，它是定义 kubelets 相关的权限，如果这个角色被修改，它将导致 kubelets 无法正常工作。\n所有默认的 ClusterRole 和 ClusterRoleBinding 对象都会被标记为 kubernetes.io/bootstrapping=rbac-defaults。\n自动更新 在每次启动时，API Server 都会更新默认 ClusterRole 所缺少的各种权限，并更新默认 ClusterRoleBinding 所缺少的各个角色绑定主体。 这种自动更新机制允许集群去修复一些特殊的修改。 由于权限和角色绑定主体在新的 Kubernetes 版本中可能发生变化，所以这样的话也能够保证角色和角色绑定始终保持是最新的。\n如果要禁止此功能,请将默认ClusterRole以及ClusterRoleBinding的rbac.authorization.kubernetes.io/autoupdate设置成false。\n注意，缺乏默认权限和角色绑定主体可能会导致非功能性集群问题。\n自动更新功能在 Kubernetes 版本1.6+ 的 RBAC 认证是默认开启的。\nDiscovery Roles 无论是经过身份验证的还是未经过身份验证的用户，默认角色的用户读取API被认为是安全的，可以公开访问（包括CustomResourceDefinitions）， 如果要禁用匿名未经过身份验证的用户访问，请在 API server 中添加 --anonymous-auth=false 的配置选项。\n通过运行命令 kubectl 可以查看这些角色的配置信息:\nkubectl get clusterroles system:discovery -o yaml 注意：不建议编辑这个角色，因为更改将在 API server 重启时自动更新时覆盖（见上文）\n面向用户的角色 一些默认的角色不是前缀 system: 开头的。这些是面向用户的角色。它们包括 super-user 角色（cluster-admin）， 使用 ClusterRoleBindings （cluster-status）在集群范围内授予角色， 以及使用 RoleBindings （admin, edit, view）在特定命名空间中授予的角色。\n在 1.9 开始，面向用户的角色使用ClusterRole Aggregation允许管理员在包含这些角色上的 自定义资源上添加规则。如果想要添加 \u0026ldquo;admin\u0026rdquo; \u0026ldquo;edit\u0026rdquo; 或者 \u0026ldquo;view\u0026rdquo; ，需要先创建使用以下一个或多个的 ClusterRole 的标签：\nmetadata: labels: rbac.authorization.k8s.io/aggregate-to-admin: \u0026#34;true\u0026#34; rbac.authorization.k8s.io/aggregate-to-edit: \u0026#34;true\u0026#34; rbac.authorization.k8s.io/aggregate-to-view: \u0026#34;true\u0026#34; 核心组件角色 从版本 1.7 开始，推荐使用 Node authorizer和 NodeRestriction 准入插件来代替这个角色，它允许基于 kubelet 上调度执行的 Pods 来授权对 kubelet API 的访问。 在版本 1.7 之前，这个角色会自动绑定到 system:nodes 组。 在版本 1.7中，如果未启用Node 鉴权模式，这个角色将自动绑定到 system:nodes 组 在版本 1.8+ 之后，不再自动创建绑定。\n其他组件角色 控制器角色  Kubernetes 控制器管理器 运行核心控制环。 当使用 --use-service-account-credentials 参数时, 每个控制环使用一个单独的服务账号启动。 每个控制环都有相应的、前缀为 system:controller: 的角色。 如果控制管理器启动时未设置 --use-service-account-credentials， 它使用自己的身份信息来运行所有的控制环，该身份必须被授予所有相关的角色。 这些角色包括:\n system:controller:attachdetach-controller system:controller:certificate-controller system:controller:clusterrole-aggregation-controller system:controller:cronjob-controller system:controller:daemon-set-controller system:controller:deployment-controller system:controller:disruption-controller system:controller:endpoint-controller system:controller:expand-controller system:controller:generic-garbage-collector system:controller:horizontal-pod-autoscaler system:controller:job-controller system:controller:namespace-controller system:controller:node-controller system:controller:persistent-volume-binder system:controller:pod-garbage-collector system:controller:pv-protection-controller system:controller:pvc-protection-controller system:controller:replicaset-controller system:controller:replication-controller system:controller:resourcequota-controller system:controller:root-ca-cert-publisher system:controller:route-controller system:controller:service-account-controller system:controller:service-controller system:controller:statefulset-controller system:controller:ttl-controller  初始化与预防权限升级 RBAC API 会阻止用户通过编辑角色或者角色绑定来升级权限。 由于这一点是在 API 级别实现的，所以在 RBAC 鉴权器（RBAC authorizer）未启用的状态下依然可以正常工作。\n用户只有在符合下列条件之一的情况下，才能创建/更新角色:\n 他们已经拥有角色中包含的所有权限，且其作用域与正被修改的对象相同。 （对 ClusterRole 而言意味着集群范围，对 Role 而言意味着相同命名空间或者集群范围） 他们被明确允许在 rbac.authorization.k8s.io API 组中的 roles 或者 clusterroles 资源上使用 escalate 动词（Kubernetes 版本 1.12 及以上）  例如，如果 \u0026ldquo;user-1\u0026rdquo; 没有列举集群范围所有 Secrets 的权限，他将不能创建包含对应权限的 ClusterRole。 若要允许用户创建/更新角色：\n根据需要授予他们一个角色，允许他们根据需要创建/更新 Role 或者 ClusterRole 对象。 2. 授予他们在所创建/更新角色中包含特殊权限的权限: * 隐式的，通过给他们权限（如果它们试图创建或者更改 Role 或 ClusterRole 的权限，但自身没有被授权，API 请求将被禁止） * 或通过允许他们在 Role 或 ClusterRole 资源上执行 escalate 动作的权限，它包含在 rbac.authorization.k8s.io API 组中 （Kubernetes 1.12 及以上版本）\n如果用户已经拥有引用角色中包含的权限，那他则只能创建/更新角色绑定。 （在角色绑定相同的作用域内）或 如果他们被授予对所引用角色执行 bind 操作的显式权限。 例如，如果 \u0026ldquo;user-1\u0026rdquo; 没有集群范围内 Secret 的列表权限，他就不能创建可以授予角色权限的 ClusterRoleBinding。 通过以下方法可以允许用户创建/更新角色绑定：\n授予他们一个角色，允许他们根据需要创建/更新 RoleBinding 或者ClusterRoleBinding 对象。 2. 授予他们绑定特定角色所需的权限: * 隐式地，通过给他们授予角色中包含的权限。 * 显式地，通过允许他们对特定角色（或集群角色）执行bind 操作的权限。\n例如，这个集群角色和角色绑定将允许 \u0026ldquo;user-1\u0026rdquo; 有对\u0026quot;user-1-namespace\u0026rdquo; 命名空间中的角色执行 admin、edit 和 view 操作权限：\napiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: role-grantor rules: - apiGroups: [\u0026#34;rbac.authorization.k8s.io\u0026#34;] resources: [\u0026#34;rolebindings\u0026#34;] verbs: [\u0026#34;create\u0026#34;] - apiGroups: [\u0026#34;rbac.authorization.k8s.io\u0026#34;] resources: [\u0026#34;clusterroles\u0026#34;] verbs: [\u0026#34;bind\u0026#34;] resourceNames: [\u0026#34;admin\u0026#34;,\u0026#34;edit\u0026#34;,\u0026#34;view\u0026#34;] --- apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: role-grantor-binding namespace: user-1-namespace roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: role-grantor subjects: - apiGroup: rbac.authorization.k8s.io kind: User name: user-1 当初始化第一个角色和角色绑定时，需要为初始用户授予他们尚未拥有的权限。 对初始角色和角色绑定进行初始化时需要：\n 使用用户组为 system:masters 的凭据，该用户组由默认绑定关联到 cluster-admin 这个超级用户角色。 如果你的 API server 启动时启用了不安全端口（使用--insecure-port）, 你也可以通过该端口调用 API ，这样操作会绕过身份验证或鉴权。  一些命令行工具 kubectl create role 创建 Role 对象，定义在某命名空间中的权限。例如:\n  创建名称为 \u0026ldquo;pod-reader\u0026rdquo; 的 Role 对象，允许用户对 pods 执行 \u0026ldquo;get\u0026rdquo;、\u0026ldquo;watch\u0026rdquo; 和 \u0026ldquo;list\u0026rdquo; 操作：\nkubectl create role pod-reader --verb=get --verb=list --verb=watch --resource=pods   创建名称为 \u0026ldquo;pod-reader\u0026rdquo; 的 Role 对象并指定 resourceNames：\nkubectl create role pod-reader --verb=get --resource=pods --resource-name=readablepod --resource-name=anotherpod   创建名为 \u0026ldquo;foo\u0026rdquo; 的 Role 对象并指定 apiGroups:\nkubectl create role foo --verb=get,list,watch --resource=replicasets.apps   创建名为 \u0026ldquo;foo\u0026rdquo; 的 Role 对象并指定子资源权限:\nkubectl create role foo --verb=get,list,watch --resource=pods,pods/status   创建名为 \u0026ldquo;my-component-lease-holder\u0026rdquo; 的 Role 对象，使其具有对特定名称资源执行 get/update 的权限：\nkubectl create role my-component-lease-holder --verb=get,list,watch,update --resource=lease --resource-name=my-component   kubectl create clusterrole 创建 ClusterRole 对象。例如：\n  创建名称为 \u0026ldquo;pod-reader\u0026rdquo; 的 ClusterRole 对象，允许用户对 pods 对象执行 \u0026ldquo;get\u0026rdquo;、\u0026ldquo;watch\u0026rdquo; 和 \u0026ldquo;list\u0026rdquo; 操作：\nkubectl create clusterrole pod-reader --verb=get,list,watch --resource=pods   创建名为 \u0026ldquo;pod-reader\u0026rdquo; 的 ClusterRole 对象并指定资源名称：\nkubectl create clusterrole pod-reader --verb=get --resource=pods --resource-name=readablepod --resource-name=anotherpod   创建名为 \u0026ldquo;foo\u0026rdquo; 的 ClusterRole 对象并指定 apiGroups：\nkubectl create clusterrole foo --verb=get,list,watch --resource=replicasets.apps   创建名为 \u0026ldquo;foo\u0026rdquo; 的ClusterRole 对象并指定子资源:\nkubectl create clusterrole foo --verb=get,list,watch --resource=pods,pods/status   创建名为 \u0026ldquo;foo\u0026rdquo; 的 ClusterRole 对象并指定非资源路径：\nkubectl create clusterrole \u0026quot;foo\u0026quot; --verb=get --non-resource-url=/logs/*   创建名为 \u0026ldquo;monitoring\u0026rdquo; 的 ClusterRole 对象并指定聚合规则：\nkubectl create clusterrole monitoring --aggregation-rule=\u0026quot;rbac.example.com/aggregate-to-monitoring=true\u0026quot;   kubectl create rolebinding 在特定的命名空间中对 Role 或 ClusterRole 授权。例如：\n  在命名空间 \u0026ldquo;acme\u0026rdquo; 中，将名为 admin 的 ClusterRole 中的权限授予名称 \u0026ldquo;bob\u0026rdquo; 的用户:\nkubectl create rolebinding bob-admin-binding --clusterrole=admin --user=bob --namespace=acme   在命名空间 \u0026ldquo;acme\u0026quot;中，将名为 view 的 ClusterRole 中的权限授予该命名空间 \u0026ldquo;acme\u0026rdquo; 中名为 \u0026ldquo;myapp\u0026rdquo; 的服务账号：\nkubectl create rolebinding myapp-view-binding --clusterrole=view --serviceaccount=acme:myapp --namespace=acme   在命名空间 \u0026ldquo;acme\u0026rdquo; 中，将名为 view 的 ClusterRole 对象中的权限授予命名空间 \u0026ldquo;myappnamespace\u0026rdquo; 中名称为 \u0026ldquo;myapp\u0026rdquo; 的服务账号：\nkubectl create rolebinding myappnamespace-myapp-view-binding --clusterrole=view --serviceaccount=myappnamespace:myapp --namespace=acme   kubectl create clusterrolebinding 在整个集群、包括所有的命名空间中对 ClusterRole 授权。例如：\n  在整个集群范围，将名为 cluster-admin 的 ClusterRole 中定义的权限授予名为 \u0026ldquo;root\u0026rdquo; 用户：\nkubectl create clusterrolebinding root-cluster-admin-binding --clusterrole=cluster-admin --user=root   在整个集群范围，将名为 system:node-proxier 的 ClusterRole 的权限授予名为 \u0026ldquo;system:kube-proxy\u0026rdquo; 的用户：\nkubectl create clusterrolebinding kube-proxy-binding --clusterrole=system:node-proxier --user=system:kube-proxy   在整个集群范围，将名为 view 的 ClusterRole 对象中定义的权限授予 \u0026ldquo;acme\u0026rdquo; 命名空间中名为 \u0026ldquo;myapp\u0026rdquo; 的服务账号：\nkubectl create clusterrolebinding myapp-view-binding --clusterrole=view --serviceaccount=acme:myapp   kubectl auth reconcile 使用清单文件来创建或者更新 rbac.authorization.k8s.io/v1 API 对象。\n尚不存在的对象会被创建，如果对应的命名空间也不存在，必要的话也会被创建。 已经存在的角色会被更新，使之包含输入对象中所给的权限。如果指定了 --remove-extra-permissions，可以删除其余权限。\n已经存在的绑定也会被更新，使之包含输入对象中所给的主体。如果指定了 --remove-extra-permissions，则可以删除其余主体。\n例如:\n  测试应用 RBAC 对象的清单文件，显示将要进行的更改：\nkubectl auth reconcile -f my-rbac-rules.yaml --dry-run   应用 RBAC 对象的清单文件， 保留角色中的其余权限和绑定中的其他主体：\nkubectl auth reconcile -f my-rbac-rules.yaml   应用 RBAC 对象的清单文件, 删除角色中的其他权限和绑定中的其他主体：\nkubectl auth reconcile -f my-rbac-rules.yaml --remove-extra-subjects --remove-extra-permissions   查看 CLI 帮助获取详细的用法。\n服务账号权限 默认的 RBAC 策略为控制面组件、节点和控制器授予权限。 但是不会对 kube-system 命名空间之外的服务账号授予权限。 （除了授予所有已认证用户的 discovery 权限）\n这使得您可以根据需要向特定服务账号授予特定权限。 细粒度的角色绑定可带来更好的安全性，但需要更多精力管理。 更粗粒度的授权可能导致服务账号被授予不必要的 API 访问权限（甚至导致潜在的权限升级），但更易于管理。\n按从最安全到最不安全的顺序，存在以下方法：\n  为特定应用的服务账户授予角色（最佳实践）\n这要求应用在其 pod 规范中指定 serviceAccountName ， 并额外创建服务账号（包括通过 API、应用程序清单、kubectl create serviceaccount 等）。\n例如，在命名空间 \u0026ldquo;my-namespace\u0026rdquo; 中授予服务账号 \u0026ldquo;my-sa\u0026rdquo; 只读权限：\nkubectl create rolebinding my-sa-view \\  --clusterrole=view \\  --serviceaccount=my-namespace:my-sa \\  --namespace=my-namespace   将角色授予某命名空间中的 ”default” 服务账号\n如果一个应用没有指定 serviceAccountName，那么它将使用 \u0026ldquo;default\u0026rdquo; 服务账号。\n. note \u0026gt;}}不指定 serviceAccountName 的话， \u0026ldquo;default\u0026rdquo; 服务账号的权限会授予给命名空间中所有未指定 serviceAccountName 的 Pods。. /note \u0026gt;}}\n例如，在命名空间 \u0026ldquo;my-namespace\u0026rdquo; 中授予服务账号 \u0026ldquo;default\u0026rdquo; 只读权限：\nkubectl create rolebinding default-view \\  --clusterrole=view \\  --serviceaccount=my-namespace:default \\  --namespace=my-namespace 许多附加组件 add-ons 目前在 kube-system 命名空间以 \u0026ldquo;default\u0026rdquo; 服务账号运行。 要允许这些附加组件以超级用户权限运行，需要将集群的 cluster-admin 权限授予 kube-system 命名空间中的 \u0026ldquo;default\u0026rdquo; 服务账号。\n. note \u0026gt;}}启用这一配置意味着在 kube-system 命名空间中包含以超级用户账号来访问 API 的 Secrets。. /note \u0026gt;}}\nkubectl create clusterrolebinding add-on-cluster-admin \\  --clusterrole=cluster-admin \\  --serviceaccount=kube-system:default    将角色授予命名空间中所有的服务账号\n如果你想要在命名空间中所有的应用都具有某角色，无论它们使用的什么服务账号， 你可以将角色授予该命名空间的服务账号组。\n例如，在命名空间 \u0026ldquo;my-namespace\u0026rdquo; 中的只读权限授予该命名空间中的所有服务账号：\nkubectl create rolebinding serviceaccounts-view \\  --clusterrole=view \\  --group=system:serviceaccounts:my-namespace \\  --namespace=my-namespace   对集群范围内的所有服务账户授予一个受限角色（不鼓励）\n如果你不想管理每一个命名空间的权限，你可以向所有的服务账号授予集群范围的角色。\n例如，为集群范围的所有服务账号授予跨所有命名空间的只读权限：\nkubectl create clusterrolebinding serviceaccounts-view \\  --clusterrole=view \\  --group=system:serviceaccounts   授予超级用户访问权限给集群范围内的所有服务帐户（强烈不鼓励）\n如果你不关心如何区分权限，你可以将超级用户访问权限授予所有服务账号。\n. warning \u0026gt;}} 这将允许所有能够读取 Secrets 和创建 Pods 的用户访问超级用户的私密信息。 . /warning \u0026gt;}}\nkubectl create clusterrolebinding serviceaccounts-cluster-admin \\  --clusterrole=cluster-admin \\  --group=system:serviceaccounts   从版本1.5升级 在Kubernetes 1.6版本之前，许多部署可以使用非常宽松的 ABAC 策略， 包括授予所有服务帐户全权访问 API 的能力。\n默认的 RBAC 策略被授予控制面组件、节点和控制器。 kube-system 命名空间外的服务账号将没有权限 （除了授予所有认证用户的发现权限之外）\n这样做虽然安全得多，但可能会干扰期望自动获得 API 权限的现有工作负载。 这里有两种方法来完成这种转变:\n平行鉴权 同时运行 RBAC 和 ABAC 鉴权模式, 并指定包含 现有的 ABAC 策略 的策略文件：\n--authorization-mode=RBAC,ABAC --authorization-policy-file=mypolicy.json RBAC 鉴权器将首先尝试对请求进行鉴权。如果它拒绝 API 请求， 则 ABAC 鉴权器运行。这意味着被 RBAC 或 ABAC 策略所允许的任何请求 都是被允许的请求。\n如果 API 服务器启动时，RBAC 组件的日志级别为 5 或更高（--vmodule=rbac*=5 or --v=5）， 你可以在 API 服务器的日志中看到 RBAC 的细节 （前缀 RBAC DENY:） 您可以使用这些信息来确定需要将哪些角色授予哪些用户、组或服务帐户。 一旦你将 角色授予服务账号 ，工作负载运行时在服务器日志中 没有出现 RBAC 拒绝消息，就可以删除 ABAC 鉴权器。\n宽松的 RBAC 权限 可以使用 RBAC 角色绑定在多个场合使用宽松的策略。\n. warning \u0026gt;}} 下面的策略允许 所有 服务帐户充当集群管理员。 容器中运行的所有应用程序都会自动收到服务帐户的凭据， 可以对 API 执行任何操作，包括查看 Secrets 和修改权限。 这个策略是不被推荐的。\nkubectl create clusterrolebinding permissive-binding \\ --clusterrole=cluster-admin \\ --user=admin \\ --user=kubelet \\ --group=system:serviceaccounts . /warning \u0026gt;}}\n"
},
{
	"uri": "https://lijun.in/tasks/run-application/force-delete-stateful-set-pod/",
	"title": "强制删除 StatefulSet 类型的 Pods",
	"tags": [],
	"description": "",
	"content": "本文介绍了如何删除 StatefulSet 管理的部分 pods，并且解释了这样操作时需要记住的注意事项。\n. heading \u0026ldquo;prerequisites\u0026rdquo; %}}  这是一项相当高级的任务，并且可能会违反 StatefulSet 固有的某些属性。 继续任务之前，请熟悉下面列举的注意事项。  StatefulSet 注意事项 在 StatefulSet 的正常操作中，永远不需要强制删除 StatefulSet 管理的 pod。StatefulSet 控制器负责创建，扩容和删除 StatefulSet 管理的 pods。它尝试确保从序号 0 到 N-1 指定数量的 pods 处于活动状态并准备就绪。StatefulSet 确保在任何时候，集群中最多只有一个具有给定标识的 pod。这就是所谓的由 StatefulSet 提供的最多一个的语义。\n应谨慎进行手动强制删除操作，因为它可能会违反 StatefulSet 固有的至多一个的语义。StatefulSets 可用于运行分布式和集群级的应用，这些应用需要稳定的网络标识和可靠的存储。这些应用通常配置为具有固定标识固定数量的成员集合。具有相同身份的多个成员可能是灾难性的，并且可能导致数据丢失 (e.g. 基于 quorum 系统中的脑裂场景)。\n删除 Pods 您可以使用下面的命令执行优雅地删除 pod:\nkubectl delete pods \u0026lt;pod\u0026gt; 为了使上面的方法能够正常终止，Pod 一定不能设置 pod.Spec.TerminationGracePeriodSeconds 为 0。将 pod.Spec.TerminationGracePeriodSeconds 设置为 0s 的做法是不安全的，强烈建议 StatefulSet 类型的 pods 不要使用。优雅删除是安全的，并且会在 kubelet 从 apiserver 中删除名称之前确保 优雅地关闭 pod 。\nKubernetes (1.5 版本或者更新版本)不会因为一个 Node 无法访问而删除 pods。在无法访问节点上运行的 pods 在超时后会进入\u0026rsquo;Terminating\u0026rsquo; 或者 \u0026lsquo;Unknown\u0026rsquo; 状态。当用户尝试优雅删除无法访问节点上的 pod 时，pods 也可能会进入这些状态。从 apiserver 中删除处于这些状态 pod 的唯一方法如下：\n Node 对象被删除(要么您删除, 或者Node Controller)。 无响应节点上的 kubelet 开始响应，杀死 pod 并从 apiserver 中移除该条目。 用户强制删除 pod。  推荐使用第一种或者第二种方法。如果确认节点已经不可用了 (比如，永久断开网络，断电等)，则删除 Node 对象。如果节点遇到网裂，请尝试解决该问题或者等待其解决。当网裂愈合时，kubelet 将完成 pod 的删除并从 apiserver 中释放其名字。\n通常，pod 一旦不在节点上运行，或者管理员删除了节点，系统就会完成删除。你可以通过强制删除 pod 来覆盖它。\n强制删除 强制删除不要等待来自 kubelet 的确认 pod 已被终止。无论强制删除是否成功杀死了 pod，它都会立即从 apiserver 中释放该名字。这将让 StatefulSet 控制器创建一个具有相同标识的替换 pod；这可能导致正在运行 pod 的重复，并且如果所述 pod 仍然可以与 StatefulSet 的成员通信，则将违反 StatefulSet 旨在保证的最多一个的语义。\n当你强制删除 StatefulSet 类型的 pod 时，你要确保有问题的 pod 不会再和 StatefulSet 管理的其他 pods通信，并且可以安全地释放其名字以便创建替换 pod。\n如果要使用 kubectl version \u0026gt;= 1.5 强制删除 pod，请执行下面命令：\nkubectl delete pods \u0026lt;pod\u0026gt; --grace-period=0 --force 如果您使用 kubectl \u0026lt;= 1.4 的任何版本，则应省略 --force 选项：\nkubectl delete pods \u0026lt;pod\u0026gt; --grace-period=0 如果在这些命令后 pod 仍处于Unknown状态，请使用以下命令从集群中删除 pod:\nkubectl patch pod \u0026lt;pod\u0026gt; -p \u0026#39;{\u0026#34;metadata\u0026#34;:{\u0026#34;finalizers\u0026#34;:null}}\u0026#39; 请始终谨慎地执行强制删除 StatefulSet 类型的 pods，并完全了解所涉及地风险。\n. heading \u0026ldquo;whatsnext\u0026rdquo; %}} 进一步了解调试 StatefulSet。\n"
},
{
	"uri": "https://lijun.in/concepts/scheduling-eviction/scheduler-perf-tuning/",
	"title": "调度器性能调优",
	"tags": [],
	"description": "",
	"content": "feature-state for_k8s_version=\u0026quot;1.14\u0026rdquo; state=\u0026quot;beta\u0026rdquo; \u0026gt;}}\n作为 kubernetes 集群的默认调度器，kube-scheduler 主要负责将 Pod 调度到集群的 Node 上。\n在一个集群中，满足一个 Pod 调度请求的所有 Node 称之为 可调度 Node。调度器先在集群中找到一个 Pod 的可调度 Node，然后根据一系列函数对这些可调度 Node打分，之后选出其中得分最高的 Node 来运行 Pod。最后，调度器将这个调度决定告知 kube-apiserver，这个过程叫做 绑定。\n这篇文章将会介绍一些在大规模 Kubernetes 集群下调度器性能优化的方式。\n在大规模集群中，你可以调节调度器的表现来平衡调度的延迟（新 Pod 快速就位）和精度（调度器很少做出糟糕的放置决策）。\n你可以通过设置 kube-scheduler 的 percentageOfNodesToScore 来配置这个调优设置。这个 KubeSchedulerConfiguration 设置决定了调度集群中节点的阈值。\n设置阈值 percentageOfNodesToScore 选项接受从 0 到 100 之间的整数值。0 值比较特殊，表示 kube-scheduler 应该使用其编译后的默认值。 如果你设置 percentageOfNodesToScore 的值超过了 100，kube-scheduler 的表现等价于设置值为 100。\n要修改这个值，编辑 kube-scheduler 的配置文件（通常是 /etc/kubernetes/config/kube-scheduler.yaml），然后重启调度器。\n修改完成后，你可以执行\nkubectl get componentstatuses 来检查该 kube-scheduler 组件是否健康。输出类似如下：\nNAME STATUS MESSAGE ERROR controller-manager Healthy ok scheduler Healthy ok ... 节点打分阈值 要提升调度性能，kube-scheduler 可以在找到足够的可调度节点之后停止查找。在大规模集群中，比起考虑每个节点的简单方法相比可以节省时间。\n你可以使用整个集群节点总数的百分比作为阈值来指定需要多少节点就足够。 kube-scheduler 会将它转换为节点数的整数值。在调度期间，如果 kube-scheduler 已确认的可调度节点数足以超过了配置的百分比数量，kube-scheduler 将停止继续查找可调度节点并继续进行 打分阶段。\n调度器如何遍历节点 详细介绍了这个过程。\n默认阈值 如果你不指定阈值，Kubernetes 使用线性公式计算出一个比例，在 100-node 集群下取 50%，在 5000-node 的集群下取 10%。 这个自动设置的参数的最低值是 5%。\n这意味着，调度器至少会对集群中 5% 的节点进行打分，除非用户将该参数设置的低于 5。\n如果你想让调度器对集群内所有节点进行打分，则将 percentageOfNodesToScore 设置为 100。\n示例 下面就是一个将 percentageOfNodesToScore 参数设置为 50% 的例子。\napiVersion: kubescheduler.config.k8s.io/v1alpha1 kind: KubeSchedulerConfiguration algorithmSource: provider: DefaultProvider ... percentageOfNodesToScore: 50 调节 percentageOfNodesToScore 参数 percentageOfNodesToScore 的值必须在 1 到 100 之间，而且其默认值是通过集群的规模计算得来的。 另外，还有一个 50 个 Node 的最小值是硬编码在程序中。\n当集群中的可调度节点少于 50 个时，调度器仍然会去检查所有的 Node，因为可调度节点太少，不足以停止调度器最初的过滤选择。\n同理，在小规模集群中，如果你将 percentageOfNodesToScore 设置为一个较低的值，则没有或者只有很小的效果。\n如果集群只有几百个节点或者更少，请保持这个配置的默认值。改变基本不会对调度器的性能有明显的提升。\n值得注意的是，该参数设置后可能会导致只有集群中少数节点被选为可调度节点，很多 node 都没有进入到打分阶段。这样就会造成一种后果，一个本来可以在打分阶段得分很高的 Node 甚至都不能进入打分阶段。\n由于这个原因，这个参数不应该被设置成一个很低的值。通常的做法是不会将这个参数的值设置的低于 10。很低的参数值一般在调度器的吞吐量很高且对 node 的打分不重要的情况下才使用。换句话说，只有当你更倾向于在可调度节点中任意选择一个 Node 来运行这个 Pod 时，才使用很低的参数设置。\n调度器做调度选择的时候如何覆盖所有的 Node 如果你想要理解这一个特性的内部细节，那么请仔细阅读这一章节。\n在将 Pod 调度到 Node 上时，为了让集群中所有 Node 都有公平的机会去运行这些 Pod，调度器将会以轮询的方式覆盖全部的 Node。你可以将 Node 列表想象成一个数组。调度器从数组的头部开始筛选可调度节点，依次向后直到可调度节点的数量达到 percentageOfNodesToScore 参数的要求。在对下一个 Pod 进行调度的时候，前一个 Pod 调度筛选停止的 Node 列表的位置，将会来作为这次调度筛选 Node 开始的位置。\n如果集群中的 Node 在多个区域，那么调度器将从不同的区域中轮询 Node，来确保不同区域的 Node 接受可调度性检查。如下例，考虑两个区域中的六个节点：\nZone 1: Node 1, Node 2, Node 3, Node 4 Zone 2: Node 5, Node 6 调度器将会按照如下的顺序去评估 Node 的可调度性：\nNode 1, Node 5, Node 2, Node 6, Node 3, Node 4 在评估完所有 Node 后，将会返回到 Node 1，从头开始。\n"
},
{
	"uri": "https://lijun.in/concepts/cluster-administration/kubelet-garbage-collection/",
	"title": "配置 kubelet 垃圾回收策略",
	"tags": [],
	"description": "",
	"content": "垃圾回收是 kubelet 的一个有用功能，它将清理未使用的镜像和容器。\nKubelet 将每分钟对容器执行一次垃圾回收，每五分钟对镜像执行一次垃圾回收。\n不建议使用外部垃圾收集工具，因为这些工具可能会删除原本期望存在的容器进而破坏 kubelet 的行为。\n镜像回收 Kubernetes 借助于 cadvisor 通过 imageManager 来管理所有镜像的生命周期。\n镜像垃圾回收策略只考虑两个因素：HighThresholdPercent 和 LowThresholdPercent。\n磁盘使用率超过上限阈值（HighThresholdPercent）将触发垃圾回收。\n垃圾回收将删除最近最少使用的镜像，直到磁盘使用率满足下限阈值（LowThresholdPercent）。\n容器回收 容器垃圾回收策略考虑三个用户定义变量。MinAge 是容器可以被执行垃圾回收的最小生命周期。MaxPerPodContainer 是每个 pod 内允许存在的死亡容器的最大数量。 MaxContainers 是全部死亡容器的最大数量。可以分别独立地通过将 MinAge 设置为 0，以及将 MaxPerPodContainer 和 MaxContainers 设置为小于 0 来禁用这些变量。\nKubelet 将处理无法辨识的、已删除的以及超出前面提到的参数所设置范围的容器。最老的容器通常会先被移除。 MaxPerPodContainer 和 MaxContainer 在某些场景下可能会存在冲突，例如在保证每个 pod 内死亡容器的最大数量（MaxPerPodContainer）的条件下可能会超过允许存在的全部死亡容器的最大数量（MaxContainer）。 MaxPerPodContainer 在这种情况下会被进行调整：最坏的情况是将 MaxPerPodContainer 降级为 1，并驱逐最老的容器。 此外，pod 内已经被删除的容器一旦年龄超过 MinAge 就会被清理。\n不被 kubelet 管理的容器不受容器垃圾回收的约束。\n用户配置 用户可以使用以下 kubelet 参数调整相关阈值来优化镜像垃圾回收：\n  image-gc-high-threshold，触发镜像垃圾回收的磁盘使用率百分比。默认值为 85%。\n  image-gc-low-threshold，镜像垃圾回收试图释放资源后达到的磁盘使用率百分比。默认值为 80%。\n  我们还允许用户通过以下 kubelet 参数自定义垃圾收集策略：\n  minimum-container-ttl-duration，完成的容器在被垃圾回收之前的最小年龄，默认是 0 分钟，这意味着每个完成的容器都会被执行垃圾回收。\n  maximum-dead-containers-per-container，每个容器要保留的旧实例的最大数量。默认值为 1。\n  maximum-dead-containers，要全局保留的旧容器实例的最大数量。默认值是 -1，这意味着没有全局限制。\n  容器可能会在其效用过期之前被垃圾回收。这些容器可能包含日志和其他对故障诊断有用的数据。 强烈建议为 maximum-dead-containers-per-container 设置一个足够大的值，以便每个预期容器至少保留一个死亡容器。 由于同样的原因，maximum-dead-containers 也建议使用一个足够大的值。\n查阅 这个问题 获取更多细节。\n弃用 这篇文档中的一些 kubelet 垃圾收集（Garbage Collection）功能将在未来被 kubelet 驱逐回收（eviction）所替代。\n包括:\n   现存参数 新参数 解释     --image-gc-high-threshold --eviction-hard 或 --eviction-soft 现存的驱逐回收信号可以触发镜像垃圾回收   --image-gc-low-threshold --eviction-minimum-reclaim 驱逐回收实现相同行为   --maximum-dead-containers  一旦旧日志存储在容器上下文之外，就会被弃用   --maximum-dead-containers-per-container  一旦旧日志存储在容器上下文之外，就会被弃用   --minimum-container-ttl-duration  一旦旧日志存储在容器上下文之外，就会被弃用   --low-diskspace-threshold-mb --eviction-hard or eviction-soft 驱逐回收将磁盘阈值泛化到其他资源   --outofdisk-transition-frequency --eviction-pressure-transition-period 驱逐回收将磁盘压力转换到其他资源     查阅 配置驱逐回收资源的策略 获取更多细节。\n"
},
{
	"uri": "https://lijun.in/tasks/configure-pod-container/configure-projected-volume-storage/",
	"title": "配置 Pod 使用投射卷作存储",
	"tags": [],
	"description": "",
	"content": "本文介绍怎样通过投射 卷将现有的多个卷资源挂载到相同的目录。 当前，secret、configMap、downwardAPI 和 serviceAccountToken 卷可以被投射。\n. note \u0026gt;}}\nserviceAccountToken 不是一种卷类型 . /note \u0026gt;}}\n. heading \u0026ldquo;prerequisites\u0026rdquo; %}} . include \u0026ldquo;task-tutorial-prereqs.md\u0026rdquo; \u0026gt;}} . version-check \u0026gt;}}\n为 Pod 配置投射卷 本练习中，您将从本地文件来创建包含有用户名和密码的 Secret。然后创建运行一个容器的 Pod，该 Pod 使用投射 卷将 Secret 挂载到相同的路径下。\n下面是 Pod 的配置文件：\n. codenew file=\u0026quot;pods/storage/projected.yaml\u0026rdquo; \u0026gt;}}\n    \u0026lt;!--# Create files containing the username and password:--\u0026gt;# 创建包含用户名和密码的文件: echo -n \u0026#34;admin\u0026#34; \u0026gt; ./username.txt echo -n \u0026#34;1f2d1e2e67df\u0026#34; \u0026gt; ./password.txt--\u0026gt; \u0026lt;!--# Package these files into secrets:--\u0026gt;# 将上述文件引用到 Secret： kubectl create secret generic user --from-file=./username.txt kubectl create secret generic pass --from-file=./password.txt     kubectl create -f https://k8s.io/examples/pods/storage/projected.yaml kubectl get --watch pod test-projected-volume \u0026lt;!--The output looks like this:--\u0026gt;输出结果和下面类似： NAME READY STATUS RESTARTS AGE test-projected-volume 1/1 Running 0 14s      kubectl exec -it test-projected-volume -- /bin/sh     ls /projected-volume/ . heading \u0026ldquo;whatsnext\u0026rdquo; %}}  进一步了解投射 卷。 阅读[一体卷](https://github.com/kubernetes/community/blob/. param \u0026ldquo;githubbranch\u0026rdquo; \u0026gt;}}/contributors/design-proposals/node/all-in-one-volume.md)设计文档。  "
},
{
	"uri": "https://lijun.in/concepts/storage/",
	"title": "😊 - 存储",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lijun.in/concepts/configuration/",
	"title": "😊 - 配置",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lijun.in/reference/",
	"title": "😍 - 参考",
	"tags": [],
	"description": "",
	"content": "这是 Kubernetes 文档的参考部分。\nAPI 参考  Kubernetes API 概述 - Kubernetes API 概述。 Kubernetes API 版本  1.17 1.16 1.15 1.14 1.13    API 客户端库 如果您需要通过编程语言调用 Kubernetes API，您可以使用 客户端库。以下是官方支持的客户端库：\n Kubernetes Go 语言客户端库 Kubernetes Python 语言客户端库 Kubernetes Java 语言客户端库 Kubernetes JavaScript 语言客户端库  CLI 参考  kubectl - 主要的 CLI 工具，用于运行命令和管理 Kubernetes 集群。  JSONPath - 通过 kubectl 使用 JSONPath 表达式 的语法指南。   kubeadm - 此 CLI 工具可轻松配置安全的 Kubernetes 集群。 kubefed - 此 CLI 工具可帮助您管理集群联邦。  配置参考  kubelet - 在每个节点上运行的主 节点代理 。kubelet 采用一组 PodSpecs 并确保所描述的容器健康地运行。 kube-apiserver - REST API，用于验证和配置 API 对象（如 pod，服务，副本控制器）的数据。 kube-controller-manager - 一个守护进程，它嵌入到了 Kubernetes 的附带的核心控制循环。 kube-proxy - 可以跨一组后端进行简单的 TCP/UDP 流转发或循环 TCP/UDP 转发。 kube-scheduler - 一个调度程序，用于管理可用性、性能和容量。 federation-apiserver - 联邦集群的 API 服务器。 federation-controller-manager - 一个守护进程，它嵌入到了 Kubernetes 联邦的附带的核心控制循环。  设计文档 Kubernetes 功能的设计文档归档，不妨考虑从 Kubernetes 架构 和 Kubernetes 设计概述开始阅读。\n"
},
{
	"uri": "https://lijun.in/tutorials/services/",
	"title": "😎 - Services",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lijun.in/setup/production-environment/windows/user-guide-windows-containers/",
	"title": "Kubernetes 中调度 Windows 容器的指南",
	"tags": [],
	"description": "",
	"content": "Windows 应用程序构成了许多组织中运行的服务和应用程序的很大一部分。本指南将引导您完成在 Kubernetes 中配置和部署 Windows 容器的步骤。\n目标  配置一个示例 deployment 以在 Windows 节点上运行 Windows 容器 （可选）使用组托管服务帐户（GMSA）为您的 Pod 配置 Active Directory 身份  在你开始之前  创建一个 Kubernetes 集群，其中包括一个运行 Windows Server 的主节点和工作节点 重要的是要注意，对于 Linux 和 Windows 容器，在 Kubernetes 上创建和部署服务和工作负载的行为几乎相同。与集群接口的 Kubectl 命令相同。提供以下部分中的示例只是为了快速启动 Windows 容器的使用体验。  入门：部署 Windows 容器 要在 Kubernetes 上部署 Windows 容器，您必须首先创建一个示例应用程序。下面的示例 YAML 文件创建了一个简单的 Web 服务器应用程序。创建一个名为 win-webserver.yaml 的服务规约，其内容如下：\napiVersion: v1 kind: Service metadata: name: win-webserver labels: app: win-webserver spec: ports: # the port that this service should serve on - port: 80 targetPort: 80 selector: app: win-webserver type: NodePort --- apiVersion: apps/v1 kind: Deployment metadata: labels: app: win-webserver name: win-webserver spec: replicas: 2 selector: matchLabels: app: win-webserver template: metadata: labels: app: win-webserver name: win-webserver spec: containers: - name: windowswebserver image: mcr.microsoft.com/windows/servercore:ltsc2019 command: - powershell.exe - -command - \u0026#34;\u0026lt;#code used from https://gist.github.com/wagnerandrade/5424431#\u0026gt; ; $$listener = New-Object System.Net.HttpListener ; $$listener.Prefixes.Add(\u0026#39;http://*:80/\u0026#39;) ; $$listener.Start() ; $$callerCounts = @{} ; Write-Host(\u0026#39;Listening at http://*:80/\u0026#39;) ; while ($$listener.IsListening) { ;$$context = $$listener.GetContext() ;$$requestUrl = $$context.Request.Url ;$$clientIP = $$context.Request.RemoteEndPoint.Address ;$$response = $$context.Response ;Write-Host \u0026#39;\u0026#39; ;Write-Host(\u0026#39;\u0026gt; {0}\u0026#39; -f $$requestUrl) ; ;$$count = 1 ;$$k=$$callerCounts.Get_Item($$clientIP) ;if ($$k -ne $$null) { $$count += $$k } ;$$callerCounts.Set_Item($$clientIP, $$count) ;$$ip=(Get-NetAdapter | Get-NetIpAddress); $$header=\u0026#39;\u0026lt;html\u0026gt;\u0026lt;body\u0026gt;\u0026lt;H1\u0026gt;Windows Container Web Server\u0026lt;/H1\u0026gt;\u0026#39; ;$$callerCountsString=\u0026#39;\u0026#39; ;$$callerCounts.Keys | % { $$callerCountsString+=\u0026#39;\u0026lt;p\u0026gt;IP {0} callerCount {1} \u0026#39; -f $$ip[1].IPAddress,$$callerCounts.Item($$_) } ;$$footer=\u0026#39;\u0026lt;/body\u0026gt;\u0026lt;/html\u0026gt;\u0026#39; ;$$content=\u0026#39;{0}{1}{2}\u0026#39; -f $$header,$$callerCountsString,$$footer ;Write-Output $$content ;$$buffer = [System.Text.Encoding]::UTF8.GetBytes($$content) ;$$response.ContentLength64 = $$buffer.Length ;$$response.OutputStream.Write($$buffer, 0, $$buffer.Length) ;$$response.Close() ;$$responseStatus = $$response.StatusCode ;Write-Host(\u0026#39;\u0026lt; {0}\u0026#39; -f $$responseStatus) } ; \u0026#34; nodeSelector: kubernetes.io/os: windows . note \u0026gt;}}\n端口映射也是支持的，但为简单起见，在此示例中容器端口 80 直接暴露给服务。 . /note \u0026gt;}}\n  检查所有节点是否健康：\nkubectl get nodes   部署服务并观察 pod 更新：\nkubectl apply -f win-webserver.yaml kubectl get pods -o wide -w 正确部署服务后，两个 Pod 都标记为“Ready”。要退出 watch 命令，请按 Ctrl + C。\n  检查部署是否成功。验证：\n Windows 节点上每个 Pod 有两个容器，使用 docker ps Linux 主机列出两个 Pod，使用 kubectl get pods 跨网络的节点到 Pod 通信，从 Linux 主服务器 curl 您的 pod IPs 的端口80，以检查 Web 服务器响应 Pod 到 Pod 的通信，使用 docker exec 或 kubectl exec 在 pod 之间（以及跨主机，如果您有多个 Windows 节点）进行 ping 操作 服务到 Pod 的通信，从 Linux 主服务器和各个 Pod 中 curl 虚拟服务 IP（在 kubectl get services 下可见） 服务发现，使用 Kubernetes curl 服务名称默认 DNS 后缀 入站连接，从 Linux 主服务器或集群外部的计算机 curl NodePort 出站连接，使用 kubectl exec 从 Pod 内部 curl 外部 IP    . note \u0026gt;}}\n由于当前平台对 Windows 网络堆栈的限制，Windows 容器主机无法访问在其上调度的服务的 IP。只有 Windows pods 才能访问服务 IP。 . /note \u0026gt;}}\n使用可配置的容器用户名 从 Kubernetes v1.16 开始，可以为 Windows 容器配置与其镜像默认值不同的用户名来运行其入口点和进程。此能力的实现方式和 Linux 容器有些不同。在此处可了解更多信息。\n使用组托管服务帐户管理工作负载身份 从 Kubernetes v1.14 开始，可以将 Windows 容器工作负载配置为使用组托管服务帐户（GMSA）。组托管服务帐户是 Active Directory 帐户的一种特定类型，它提供自动密码管理，简化的服务主体名称（SPN）管理以及将管理委派给跨多台服务器的其他管理员的功能。配置了 GMSA 的容器可以访问外部 Active Directory 域资源，同时携带通过 GMSA 配置的身份。在此处了解有关为 Windows 容器配置和使用 GMSA 的更多信息。\n污点和容忍度 目前，用户需要将 Linux 和 Windows 工作负载运行在各自特定的操作系统的节点上，因而需要结合使用污点和节点选择算符。这可能仅给 Windows 用户造成不便。推荐的方法概述如下，其主要目标之一是该方法不应破坏与现有 Linux 工作负载的兼容性。\n确保特定操作系统的工作负载落在适当的容器主机上 用户可以使用污点和容忍度确保 Windows 容器可以调度在适当的主机上。目前所有 Kubernetes 节点都具有以下默认标签：\n kubernetes.io/os = [windows|linux] kubernetes.io/arch = [amd64|arm64|\u0026hellip;]  如果 Pod 规范未指定诸如 \u0026quot;kubernetes.io/os\u0026quot;: windows 之类的 nodeSelector，则该 Pod 可能会被调度到任何主机（Windows 或 Linux）上。这是有问题的，因为 Windows 容器只能在 Windows 上运行，而 Linux 容器只能在 Linux 上运行。最佳实践是使用 nodeSelector。\n但是，我们了解到，在许多情况下，用户都有既存的大量的 Linux 容器部署，以及一个现成的配置生态系统，例如社区 Helm charts，以及程序化 Pod 生成案例，例如 Operators。在这些情况下，您可能会不愿意更改配置添加 nodeSelector。替代方法是使用污点。由于 kubelet 可以在注册期间设置污点，因此可以轻松修改它，使其仅在 Windows 上运行时自动添加污点。\n例如：--register-with-taints='os=windows:NoSchedule'\n向所有 Windows 节点添加污点后，Kubernetes 将不会在它们上调度任何负载（包括现有的 Linux Pod）。为了使某 Windows Pod 调度到 Windows 节点上，该 Pod 既需要 nodeSelector 选择 Windows，也需要合适的匹配的容忍度设置。\nnodeSelector: kubernetes.io/os: windows node.kubernetes.io/windows-build: \u0026#39;10.0.17763\u0026#39; tolerations: - key: \u0026#34;os\u0026#34; operator: \u0026#34;Equal\u0026#34; value: \u0026#34;windows\u0026#34; effect: \u0026#34;NoSchedule\u0026#34; 处理同一集群中的多个 Windows 版本 每个 Pod 使用的 Windows Server 版本必须与该节点的 Windows Server 版本相匹配。 如果要在同一集群中使用多个 Windows Server 版本，则应该设置其他节点标签和 nodeSelector。\nKubernetes 1.17 自动添加了一个新标签 node.kubernetes.io/windows-build 来简化此操作。 如果您运行的是旧版本，则建议手动将此标签添加到 Windows 节点。\n此标签反映了需要兼容的 Windows 主要、次要和内部版本号。以下是当前每个 Windows Server 版本使用的值。\n   产品名称 内部编号     Windows Server 2019 10.0.17763   Windows Server version 1809 10.0.17763   Windows Server version 1903 10.0.18362    使用 RuntimeClass 简化 RuntimeClass 可用于简化使用污点和容忍度的过程。集群管理员可以创建 RuntimeClass 对象，用于封装这些污点和容忍度。\n 将此文件保存到 runtimeClasses.yml 文件。它包括适用于 Windows 操作系统、体系结构和版本的 nodeSelector。  apiVersion: node.k8s.io/v1beta1 kind: RuntimeClass metadata: name: windows-2019 handler: \u0026#39;docker\u0026#39; scheduling: nodeSelector: kubernetes.io/os: \u0026#39;windows\u0026#39; kubernetes.io/arch: \u0026#39;amd64\u0026#39; node.kubernetes.io/windows-build: \u0026#39;10.0.17763\u0026#39; tolerations: - effect: NoSchedule key: os operator: Equal value: \u0026#34;windows\u0026#34;  集群管理员运行 kubectl create -f runtimeClasses.yml 操作 根据需要向 Pod 规约中添加 runtimeClassName: windows-2019  例如：\napiVersion: apps/v1 kind: Deployment metadata: name: iis-2019 labels: app: iis-2019 spec: replicas: 1 template: metadata: name: iis-2019 labels: app: iis-2019 spec: runtimeClassName: windows-2019 containers: - name: iis image: mcr.microsoft.com/windows/servercore/iis:windowsservercore-ltsc2019 resources: limits: cpu: 1 memory: 800Mi requests: cpu: .1 memory: 300Mi ports: - containerPort: 80 selector: matchLabels: app: iis-2019 --- apiVersion: v1 kind: Service metadata: name: iis spec: type: LoadBalancer ports: - protocol: TCP port: 80 selector: app: iis-2019 "
},
{
	"uri": "https://lijun.in/concepts/workloads/controllers/cron-jobs/",
	"title": "CronJob",
	"tags": [],
	"description": "",
	"content": "feature-state for_k8s_version=\u0026quot;v1.8\u0026rdquo; state=\u0026quot;beta\u0026rdquo; \u0026gt;}}\nCron Job 创建基于时间调度的 Jobs。\n一个 CronJob 对象就像 crontab (cron table) 文件中的一行。它用 Cron 格式进行编写，并周期性地在给定的调度时间执行 Job。\n所有 CronJob 的 schedule: 时间都是基于初始 Job 的主控节点的时区。\n如果你的控制平面在 Pod 或是裸容器中运行了主控程序 (kube-controller-manager)， 那么为该容器设置的时区将会决定定时任务的控制器所使用的时区。\n为 CronJob 资源创建清单时，请确保创建的名称不超过 52 个字符。这是因为 CronJob 控制器将自动在提供的作业名称后附加 11 个字符，并且存在一个限制，即作业名称的最大长度不能超过 63 个字符。\n有关创建和使用 CronJob 的说明及规范文件的示例，请参见使用 CronJob 运行自动化任务。\nCronJob 限制 CronJob 创建 Job 对象，每个 Job 的执行次数大约为一次。 我们之所以说 \u0026ldquo;大约\u0026rdquo;，是因为在某些情况下，可能会创建两个 Job，或者不会创建任何 Job。 我们试图使这些情况尽量少发生，但不能完全杜绝。因此，Job 应该是 幂等的。\n如果 startingDeadlineSeconds 设置为很大的数值或未设置（默认），并且 concurrencyPolicy 设置为 Allow，则作业将始终至少运行一次。\n对于每个 CronJob，CronJob 控制器 检查从上一次调度的时间点到现在所错过了调度次数。如果错过的调度次数超过 100 次，那么它就不会启动这个任务，并记录这个错误:\nCannot determine if job needs to be started. Too many missed start time (\u0026gt; 100). Set or decrease .spec.startingDeadlineSeconds or check clock skew. 需要注意的是，如果 startingDeadlineSeconds 字段非空，则控制器会统计从 startingDeadlineSeconds 设置的值到现在而不是从上一个计划时间到现在错过了多少次 Job。例如，如果 startingDeadlineSeconds 是 200，则控制器会统计在过去 200 秒中错过了多少次 Job。\n如果未能在调度时间内创建 CronJob，则计为错过。例如，如果 concurrencyPolicy 被设置为 Forbid，并且当前有一个调度仍在运行的情况下，试图调度的 CronJob 将被计算为错过。\n例如，假设一个 CronJob 被设置为 08:30:00 准时开始，它的 startingDeadlineSeconds 字段被设置为 10，如果在 08:29:00 时将 CronJob 控制器的时间改为 08:42:00，Job 将不会启动。 如果觉得晚些开始比没有启动好，那请设置一个较长的 startingDeadlineSeconds。\n为了进一步阐述这个概念，假设将 CronJob 设置为从 08:30:00 开始每隔一分钟创建一个新的 Job，并将其 startingDeadlineSeconds 字段设置为 200 秒。 如果 CronJob 控制器恰好在与上一个示例相同的时间段（08:29:00 到 10:21:00）停机，则 Job 仍将从 10:22:00 开始。造成这种情况的原因是控制器现在检查在最近 200 秒（即 3 个错过的调度）中发生了多少次错过的 Job 调度，而不是从现在为止的最后一个调度时间开始。\nCronJob 仅负责创建与其调度时间相匹配的 Job，而 Job 又负责管理其代表的 Pod。\n"
},
{
	"uri": "https://lijun.in/reference/setup-tools/kubeadm/kubeadm-version/",
	"title": "kubeadm version",
	"tags": [],
	"description": "",
	"content": "此命令用来查询 kubeadm 的版本。\n. include \u0026ldquo;generated/kubeadm_version.md\u0026rdquo; \u0026gt;}}\n"
},
{
	"uri": "https://lijun.in/concepts/workloads/pods/ephemeral-containers/",
	"title": "临时容器",
	"tags": [],
	"description": "",
	"content": "feature-state state=\u0026quot;alpha\u0026rdquo; for_k8s_version=\u0026quot;v1.16\u0026rdquo; \u0026gt;}}\n此页面概述了临时容器：一种特殊的容器，该容器在现有 glossary_tooltip term_id=\u0026quot;pod\u0026rdquo; \u0026gt;}} 中临时运行，为了完成用户启动的操作，例如故障排查。使用临时容器来检查服务，而不是构建应用程序。\n临时容器处于早期的 alpha 阶段，不适用于生产环境集群。应该预料到临时容器在某些情况下不起作用，例如在定位容器的命名空间时。根据 Kubernetes 弃用政策，该 alpha 功能将来可能发生重大变化或完全删除。\n了解临时容器 glossary_tooltip text=\u0026quot;Pods\u0026rdquo; term_id=\u0026quot;pod\u0026rdquo; \u0026gt;}} 是 Kubernetes 应用程序的基本构建块。由于 pod 是一次性且可替换的，因此一旦 Pod 创建，就无法将容器加入到 Pod 中。取而代之的是，通常使用 glossary_tooltip text=\u0026quot;deployments\u0026rdquo; term_id=\u0026quot;deployment\u0026rdquo; \u0026gt;}} 以受控的方式来删除并替换 Pod。\n有时有必要检查现有 Pod 的状态，例如，对于难以复现的故障进行排查。在这些场景中，可以在现有 Pod 中运行临时容器来检查其状态并运行任意命令。\n什么是临时容器？ 临时容器与其他容器的不同之处在于，它们缺少对资源或执行的保证，并且永远不会自动重启，因此不适用于构建应用程序。临时容器使用与常规容器相同的 ContainerSpec 段进行描述，但许多字段是不相容且不允许的。\n 临时容器没有端口配置，因此像 ports，livenessProbe，readinessProbe 这样的字段是不允许的。 Pod 资源分配是不可变的，因此 resources 配置是不允许的。 有关允许字段的完整列表，请参见[临时容器参考文档](/docs/reference/generated/kubernetes-api/\u0026lt; param \u0026ldquo;version\u0026rdquo; \u0026gt;}}/#ephemeralcontainer-v1-core)。  临时容器是使用 API 中的一种特殊的 ephemeralcontainers 处理器进行创建的，而不是直接添加到 pod.spec 段，因此无法使用 kubectl edit 来添加一个临时容器。\n与常规容器一样，将临时容器添加到 Pod 后，将不能更改或删除临时容器。\n临时容器的用途 当由于容器崩溃或容器镜像不包含调试实用程序而导致 kubectl exec 无用时，临时容器对于交互式故障排查很有用。\n尤其是，distroless 镜像能够使得部署最小的容器镜像，从而减少攻击面并减少故障和漏洞的暴露。由于 distroless 镜像不包含 shell 或任何的调试工具，因此很难单独使用 kubectl exec 命令进行故障排查。\n使用临时容器时，启用进程命名空间共享很有帮助，可以查看其他容器中的进程。\n示例 本节中的示例要求启用 EphemeralContainers 特性，并且 kubernetes 客户端和服务端版本要求为 v1.16 或更高版本。\n本节中的示例演示了临时容器如何出现在 API 中。 通常，您可以使用 kubectl 插件进行故障排查，从而自动化执行这些步骤。\n临时容器是使用 Pod 的 ephemeralcontainers 子资源创建的，可以使用 kubectl --raw 命令进行显示。首先描述临时容器被添加为一个 EphemeralContainers 列表：\n{ \u0026#34;apiVersion\u0026#34;: \u0026#34;v1\u0026#34;, \u0026#34;kind\u0026#34;: \u0026#34;EphemeralContainers\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;example-pod\u0026#34; }, \u0026#34;ephemeralContainers\u0026#34;: [{ \u0026#34;command\u0026#34;: [ \u0026#34;sh\u0026#34; ], \u0026#34;image\u0026#34;: \u0026#34;busybox\u0026#34;, \u0026#34;imagePullPolicy\u0026#34;: \u0026#34;IfNotPresent\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;debugger\u0026#34;, \u0026#34;stdin\u0026#34;: true, \u0026#34;tty\u0026#34;: true, \u0026#34;terminationMessagePolicy\u0026#34;: \u0026#34;File\u0026#34; }] } 使用如下命令更新已运行的临时容器 example-pod：\nkubectl replace --raw /api/v1/namespaces/default/pods/example-pod/ephemeralcontainers -f ec.json 这将返回临时容器的新列表：\n{ \u0026#34;kind\u0026#34;:\u0026#34;EphemeralContainers\u0026#34;, \u0026#34;apiVersion\u0026#34;:\u0026#34;v1\u0026#34;, \u0026#34;metadata\u0026#34;:{ \u0026#34;name\u0026#34;:\u0026#34;example-pod\u0026#34;, \u0026#34;namespace\u0026#34;:\u0026#34;default\u0026#34;, \u0026#34;selfLink\u0026#34;:\u0026#34;/api/v1/namespaces/default/pods/example-pod/ephemeralcontainers\u0026#34;, \u0026#34;uid\u0026#34;:\u0026#34;a14a6d9b-62f2-4119-9d8e-e2ed6bc3a47c\u0026#34;, \u0026#34;resourceVersion\u0026#34;:\u0026#34;15886\u0026#34;, \u0026#34;creationTimestamp\u0026#34;:\u0026#34;2019-08-29T06:41:42Z\u0026#34; }, \u0026#34;ephemeralContainers\u0026#34;:[ { \u0026#34;name\u0026#34;:\u0026#34;debugger\u0026#34;, \u0026#34;image\u0026#34;:\u0026#34;busybox\u0026#34;, \u0026#34;command\u0026#34;:[ \u0026#34;sh\u0026#34; ], \u0026#34;resources\u0026#34;:{ }, \u0026#34;terminationMessagePolicy\u0026#34;:\u0026#34;File\u0026#34;, \u0026#34;imagePullPolicy\u0026#34;:\u0026#34;IfNotPresent\u0026#34;, \u0026#34;stdin\u0026#34;:true, \u0026#34;tty\u0026#34;:true } ] } 可以使用以下命令查看新创建的临时容器的状态：\nkubectl describe pod example-pod ... Ephemeral Containers: debugger: Container ID: docker://cf81908f149e7e9213d3c3644eda55c72efaff67652a2685c1146f0ce151e80f Image: busybox Image ID: docker-pullable://busybox@sha256:9f1003c480699be56815db0f8146ad2e22efea85129b5b5983d0e0fb52d9ab70 Port: \u0026lt;none\u0026gt; Host Port: \u0026lt;none\u0026gt; Command: sh State: Running Started: Thu, 29 Aug 2019 06:42:21 +0000 Ready: False Restart Count: 0 Environment: \u0026lt;none\u0026gt; Mounts: \u0026lt;none\u0026gt; ... 可以使用以下命令连接到新的临时容器：\nkubectl attach -it example-pod -c debugger 如果启用了进程命名空间共享，则可以查看该 Pod 所有容器中的进程。 例如，运行上述 attach 操作后，在调试器容器中运行 ps 操作：\n# 在 \u0026#34;debugger\u0026#34; 临时容器内中运行此 shell 命令 ps auxww 运行命令后，输出类似于：\nPID USER TIME COMMAND 1 root 0:00 /pause 6 root 0:00 nginx: master process nginx -g daemon off; 11 101 0:00 nginx: worker process 12 101 0:00 nginx: worker process 13 101 0:00 nginx: worker process 14 101 0:00 nginx: worker process 15 101 0:00 nginx: worker process 16 101 0:00 nginx: worker process 17 101 0:00 nginx: worker process 18 101 0:00 nginx: worker process 19 root 0:00 /pause 24 root 0:00 sh 29 root 0:00 ps auxww "
},
{
	"uri": "https://lijun.in/setup/production-environment/tools/kubeadm/kubelet-integration/",
	"title": "使用 kubeadm 配置集群中的每个 kubelet",
	"tags": [],
	"description": "",
	"content": ". feature-state for_k8s_version=\u0026quot;1.11\u0026rdquo; state=\u0026quot;stable\u0026rdquo; \u0026gt;}}\nkubeadm CLI 工具的生命周期与 kubelet解耦，它是一个守护程序，在 Kubernetes 集群中的每个节点上运行。 当 Kubernetes 初始化或升级时，kubeadm CLI 工具由用户执行，而 kubelet 始终在后台运行。\n由于kubelet是守护程序，因此需要通过某种初始化系统或服务管理器进行维护。 当使用 DEB 或 RPM 安装 kubelet 时，配置系统去管理 kubelet。 您可以改用其他服务管理器，但需要手动地配置。\n集群中涉及的所有 kubelet 的一些配置细节都必须相同，而其他配置方面则需要基于每个 kubelet 进行设置，以适应给定机器的不同特性，例如操作系统、存储和网络。 您可以手动地管理 kubelet 的配置，但是 kubeadm 现在提供一种 KubeletConfiguration API 类型，用于集中管理 kubelet 的配置。\nKubelet 配置模式 以下各节讲述了通过使用 kubeadm 简化 kubelet 配置模式，而不是在每个节点上手动地管理 kubelet 配置。\n将集群级配置传播到每个 kubelet 中 您可以通过使用 kubeadm init 和 kubeadm join 命令为 kubelet 提供默认值。 有趣的示例包括使用其他 CRI 运行时或通过服务器设置不同的默认子网。\n如果您想使用子网 10.96.0.0/12 作为默认的服务，您可以给 kubeadm 传递 --service-cidr 参数：\nkubeadm init --service-cidr 10.96.0.0/12 现在，可以从该子网分配服务的虚拟 IP。 您还需要通过 kubelet 使用 --cluster-dns 标志设置 DNS 地址。 在集群中的每个管理器和节点上的 kubelet 的设置需要相同。 kubelet 提供了一个版本化的结构化 API 对象，该对象可以配置 kubelet 中的大多数参数，并将此配置推送到集群中正在运行的每个 kubelet 上。 此对象被称为 kubelet 的配置组件。 该配置组件允许用户指定标志，例如用骆峰值代表集群的 DNS IP 地址，如下所示：\napiVersion: kubelet.config.k8s.io/v1beta1 kind: KubeletConfiguration clusterDNS: - 10.96.0.10 有关组件配置的更多详细信息，亲参阅 本节。\n提供指定实例的详细配置信息 由于硬件、操作系统、网络或者其他主机特定参数的差异。某些主机需要特定的 kubelet 配置。 以下列表提供了一些示例。\n  由 kubelet 配置标志 --resolv-confkubelet 指定的 DNS 解析文件的路径在操作系统之间可能有所不同， 它取决于您是否使用 systemd-resolved。 如果此路径错误，则在其 kubelet 配置错误的节点上 DNS 解析也将失败。\n  除非您使用云提供商，否则默认情况下，Node API 对象 .metadata.name 被设置为计算机的主机名。 如果您需要指定一个节点的名称与机器的主机名不同，您可以是使用 --hostname-override 标志覆盖默认操作。\n  当前，kubelet 无法自动检测 CRI 运行时使用的 cgroup 驱动程序， 但是值 --cgroup-driver 必须与 CRI 运行时使用的 cgroup 驱动程序匹配，以确保 kubelet 的健康运行状况。\n  根据您集群使用的 CRI 运行时，您可能需要为 kubelet 指定不同的标志。 例如，当使用 Docker 时，你要需要指定标志如 --network-plugin=cni，但是如果您使用的是外部运行时， 则需要指定 --container-runtime=remote 并使用 --container-runtime-path-endpoint=\u0026lt;path\u0026gt; 指定 CRI端点。\n  您可以在服务管理器（例如系统）中通过对单个的 kubelet 配置来指定这些标志。\n使用 kubeadm 配置 kubelet 如果自定义的 KubeletConfiguration API 对象使用像 kubeadm ... --config some-config-file.yaml 这样的配置文件进行传递，则可以配置 kubeadm 启动的 kubelet。\n通过调用 kubeadm config print init-defaults --component-configs KubeletConfiguration 您可以看到此结构中的所有默认值。\n也可以阅读 kubelet 配置组件的 API 参考来获取有关各个字段的更多信息。\n当使用 kubeadm init时的工作流程 当调用 kubeadm init 时，kubelet 配置被编组到磁盘上的 /var/lib/kubelet/config.yaml 中， 并且上传到集群中的 ConfigMap。 ConfigMap 名为 kubelet-config-1.X，其中 .X 是您正在初始化的 kubernetes 版本的次版本。 在集群中所有 kubelet 的基准集群范围内配置，将 kubelet 配置文件写入 /etc/kubernetes/kubelet.conf 中。 此配置文件指向允许 kubelet 与 API 服务器通信的客户端证书。 这解决了 将集群级配置传播到每个 kubelet的需求。\n该文档 提供特定实例的配置详细信息 是第二种解决模式， kubeadm 将环境文件写入 /var/lib/kubelet/kubeadm-flags.env，其中包含了一个标志列表， 当 kubelet 启动时，该标志列表会传递给 kubelet 标志在文件中的显示方式如下：\nKUBELET_KUBEADM_ARGS=\u0026#34;--flag1=value1 --flag2=value2 ...\u0026#34; 除了启动 kubelet 时使用该标志外，该文件还包含动态参数，例如 cgroup 驱动程序以及是否使用其他 CRI 运行时 socket（--cri-socket）。\n将这两个文件编组到磁盘后，如果使用 systemd，则 kubeadm 尝试运行以下两个命令：\nsystemctl daemon-reload \u0026amp;\u0026amp; systemctl restart kubelet 如果重新加载和重新启动成功，则正常的 kubeadm init 工作流程将继续。\n当使用 kubeadm join时的工作流程 当运行 kubeadm join 时，kubeadm 使用 Bootstrap Token 证书执行 TLS 引导，该引导会获取一份证书，该证书需要下载 kubelet-config-1.X ConfigMap 并把它写入 /var/lib/kubelet/config.yaml 中。 动态环境文件的生成方式恰好与 kubeadm init 相同。\n接下来，kubeadm 运行以下两个命令将新配置加载到 kubelet 中：\nsystemctl daemon-reload \u0026amp;\u0026amp; systemctl restart kubelet 在 kubelet 加载新配置后，kubeadm 将写入 /etc/kubernetes/bootstrap-kubelet.conf KubeConfig 文件中， 该文件包含 CA 证书和引导程序令牌。 kubelet 使用这些证书执行 TLS 引导程序并获取唯一的凭据，该凭据被存储在 /etc/kubernetes/kubelet.conf 中。 当此文件被写入后，kubelet 就完成了执行 TLS 引导程序。\n系统中的 kubelet 插件 kubeadm 中附带了有关系统如何运行 kubelet 的配置。 请注意 kubeadm CLI 命令不会触及此插件。\n通过 kubeadm DEB 或者 RPM 包 安装的配置文件已被写入 /etc/systemd/system/kubelet.service.d/10-kubeadm.conf 并由系统使用。 它加强了基础设施 kubelet.service for RPM (resp. kubelet.service for DEB))：\n[Service] Environment=\u0026quot;KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf\u0026quot; Environment=\u0026quot;KUBELET_CONFIG_ARGS=--config=/var/lib/kubelet/config.yaml\u0026quot; # 这是 \u0026quot;kubeadm init\u0026quot; 和 \u0026quot;kubeadm join\u0026quot; 运行时生成的文件，动态地填充 KUBELET_KUBEADM_ARGS 变量 EnvironmentFile=-/var/lib/kubelet/kubeadm-flags.env # 这是一个文件，用户在不得已下可以将其用作替代 kubelet args。 # 用户最好使用 .NodeRegistration.KubeletExtraArgs 对象在配置文件中替代。 # KUBELET_EXTRA_ARGS 应该从此文件中获取。 EnvironmentFile=-/etc/default/kubelet ExecStart= ExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_KUBEADM_ARGS $KUBELET_EXTRA_ARGS 该文件为 kubelet 指定由 kubeadm 管理的所有文件的默认位置。\n 用于 TLS 引导程序的 KubeConfig 文件为 /etc/kubernetes/bootstrap-kubelet.conf， 但仅当 /etc/kubernetes/kubelet.conf 不存在时才能使用。 具有唯一 kubelet 标识的 KubeConfig 文件为 /etc/kubernetes/kubelet.conf。 包含 kubelet 的组件配置的文件为 /var/lib/kubelet/config.yaml。 包含的动态环境的文件 KUBELET_KUBEADM_ARGS 是来源于 /var/lib/kubelet/kubeadm-flags.env。 包含用户指定标志替代的文件 KUBELET_EXTRA_ARGS 是来源于 /etc/default/kubelet（对于 DEB），或者 /etc/sysconfig/kubelet（对于 RPM）。 KUBELET_EXTRA_ARGS 在标志链中排在最后，并且在设置冲突时具有最高优先级。  Kubernetes 二进制文件和软件包内容 Kubernetes 版本对应的 DEB 和 RPM 软件包是：\n   Package name Description     kubeadm 给 kubelet 安装 /usr/bin/kubeadm CLI 工具和 kubelet 插件。   kubelet 安装 /usr/bin/kubelet 二进制文件。   kubectl 安装 /usr/bin/kubectl 二进制文件。   kubernetes-cni 将官方的 CNI 二进制文件安装到 /opt/cni/bin 目录中   cri-tools 从 cri-tools git 仓库中安装 /usr/bin/crictl 二进制文件。    "
},
{
	"uri": "https://lijun.in/tasks/access-application-cluster/create-external-load-balancer/",
	"title": "创建一个外部负载均衡器",
	"tags": [],
	"description": "",
	"content": "本文展示如何创建一个外部负载均衡器。\n. note \u0026gt;}}\n此功能仅适用于支持外部负载均衡器的云提供商或环境。 . /note \u0026gt;}}\n创建服务时，您可以选择自动创建云网络负载均衡器。这提供了一个外部可访问的 IP 地址，可将流量分配到集群节点上的正确端口上 假设集群在支持的环境中运行，并配置了正确的云负载平衡器提供商包。\n有关如何配置和使用 Ingress 资源为服务提供外部可访问的 URL、负载均衡流量、终止 SSL 等功能，请查看 Ingress 文档。\n. heading \u0026ldquo;prerequisites\u0026rdquo; %}}  . include \u0026ldquo;task-tutorial-prereqs.md\u0026rdquo; \u0026gt;}} . version-check \u0026gt;}}  配置文件 要创建外部负载均衡器，请将以下内容添加到 服务配置文件：\ntype: LoadBalancer 您的配置文件可能会如下所示：\napiVersion: v1 kind: Service metadata: name: example-service spec: selector: app: example ports: - port: 8765 targetPort: 9376 type: LoadBalancer 使用 kubectl 您也可以使用 kubectl expose 命令及其 --type=LoadBalancer 参数创建服务：\nkubectl expose rc example --port=8765 --target-port=9376 \\  --name=example-service --type=LoadBalancer 此命令通过使用与引用资源（在上面的示例的情况下，名为 example 的 replication controller）相同的选择器来创建一个新的服务。\n更多信息（包括更多的可选参数），请参阅 kubectl expose reference。\n找到您的 IP 地址 您可以通过 kubectl 获取服务信息，找到为您的服务创建的 IP 地址：\nkubectl describe services example-service 这将获得如下输出：\nName: example-service Namespace: default Labels: \u0026lt;none\u0026gt; Annotations: \u0026lt;none\u0026gt; Selector: app=example Type: LoadBalancer IP: 10.67.252.103 LoadBalancer Ingress: 192.0.2.89 Port: \u0026lt;unnamed\u0026gt; 80/TCP NodePort: \u0026lt;unnamed\u0026gt; 32445/TCP Endpoints: 10.64.0.4:80,10.64.1.5:80,10.64.2.4:80 Session Affinity: None Events: \u0026lt;none\u0026gt; IP 地址列在 LoadBalancer Ingress 旁边。\n. note \u0026gt;}}\n注意： 如果您在 Minikube 上运行服务，您可以通过以下命令找到分配的 IP 地址和端口： . /note \u0026gt;}}\nminikube service example-service --url 保留客户端源 IP 由于此功能的实现，目标容器中看到的源 IP 将 不是客户端的原始源 IP。要启用保留客户端 IP，可以在服务的 spec 中配置以下字段（支持 GCE/Google Kubernetes Engine 环境）：\n service.spec.externalTrafficPolicy - 表示此服务是否希望将外部流量路由到节点本地或集群范围的端点。有两个可用选项：Cluster（默认）和 Local。Cluster 隐藏了客户端源 IP，可能导致第二跳到另一个节点，但具有良好的整体负载分布。Local 保留客户端源 IP 并避免 LoadBalancer 和 NodePort 类型服务的第二跳，但存在潜在的不均衡流量传播风险。   service.spec.healthCheckNodePort - 指定服务的 healthcheck nodePort（数字端口号）。如果未指定，则 serviceCheckNodePort 由服务 API 后端使用已分配的 nodePort 创建。如果客户端指定，它将使用客户端指定的 nodePort 值。仅当 type 设置为 LoadBalancer 并且 externalTrafficPolicy 设置为 Local 时才生效。  可以通过在服务的配置文件中将 externalTrafficPolicy 设置为 Local 来激活此功能。\napiVersion: v1 kind: Service metadata: name: example-service spec: selector: app: example ports: - port: 8765 targetPort: 9376 externalTrafficPolicy: Local type: LoadBalancer 垃圾收集负载均衡器 在通常情况下，应在删除 LoadBalancer 类型服务后立即清除云提供商中的相关负载均衡器资源。但是，众所周知，在删除关联的服务后，云资源被孤立的情况很多。引入了针对服务负载均衡器的终结器保护，以防止这种情况发生。通过使用终结器，在删除相关的负载均衡器资源之前，也不会删除服务资源。\n具体来说，如果服务具有 type LoadBalancer，则服务控制器将附加一个名为 service.kubernetes.io/load-balancer-cleanup 的终结器。 仅在清除负载均衡器资源后才能删除终结器。 即使在诸如服务控制器崩溃之类的极端情况下，这也可以防止负载均衡器资源悬空。\n自 Kubernetes v1.16 起，此功能为 beta 版本并默认启用。您也可以通过功能开关ServiceLoadBalancerFinalizer 在 v1.15 （alpha）中启用它。\n外部负载均衡器提供商 请务必注意，此功能的数据路径由 Kubernetes 集群外部的负载均衡器提供。\n当服务 type 设置为 LoadBalancer 时，Kubernetes 向集群中的 pod 提供的功能等同于 type 等于 ClusterIP，并通过使用 Kubernetes pod 的条目对负载均衡器（从外部到 Kubernetes）进行编程来扩展它。 Kubernetes 服务控制器自动创建外部负载均衡器、健康检查（如果需要）、防火墙规则（如果需要），并获取云提供商分配的外部 IP 并将其填充到服务对象中。\n保留源 IP 时的注意事项和限制 GCE/AWS 负载均衡器不为其目标池提供权重。对于旧的 LB kube-proxy 规则来说，这不是一个问题，它可以在所有端点之间正确平衡。\n使用新功能，外部流量不会在 pod 之间平均负载，而是在节点级别平均负载（因为 GCE/AWS 和其他外部 LB 实现无法指定每个节点的权重，因此它们的平衡跨所有目标节点，并忽略每个节点上的 pod 数量）。\n但是，我们可以声明，对于 NumServicePods \u0026laquo; NumNodes 或 NumServicePods \u0026raquo; NumNodes 时，即使没有权重，也会看到接近相等的分布。\n一旦外部负载平衡器提供权重，就可以将此功能添加到 LB 编程路径中。 未来工作：1.4 版本不提供权重支持，但可能会在将来版本中添加\n内部 pod 到 pod 的流量应该与 ClusterIP 服务类似，所有 pod 的概率相同。\n"
},
{
	"uri": "https://lijun.in/tasks/run-application/rolling-update-replication-controller/",
	"title": "基于Replication Controller执行滚动升级",
	"tags": [],
	"description": "",
	"content": "概述 注: 创建副本应用的首选方法是使用[Deployment](/docs/api-reference/. param \u0026ldquo;version\u0026rdquo; \u0026gt;}}/#deployment-v1beta1-apps)，Deployment使用[ReplicaSet](/docs/api-reference/. param \u0026ldquo;version\u0026rdquo; \u0026gt;}}/#replicaset-v1beta1-extensions)来进行副本控制。 更多信息, 查看使用Deployment运行一个无状态应用。\n为了在更新服务的同时不中断业务， kubectl 支持\u0026lsquo;滚动更新\u0026rsquo;，它一次更新一个pod，而不是同时停止整个服务。 有关更多信息，请参阅 滚动更新设计文档 和 滚动更新示例。\n请注意， kubectl rolling-update 仅支持Replication Controllers。 但是，如果使用Replication Controllers部署应用，请考虑将其切换到Deployments. Deployment是一种被推荐使用的更高级别的控制器，它可以对应用进行声明性的自动滚动更新。 如果您仍然希望保留您的Replication Controllers并使用 kubectl rolling-update进行滚动更新， 请继续往下阅读：\n滚动更新可以对replication controller所管理的Pod的配置进行变更，变更可以通过一个新的配置文件来进行，或者，如果只更新镜像，则可以直接指定新的容器镜像。\n滚动更新的工作流程：\n 通过新的配置创建一个replication controller 在新的控制器上增加副本数，在旧的上面减少副本数，直到副本数达到期望值 删除之前的replication controller  使用kubectl rolling-update命令来进行滚动更新：\n$ kubectl rolling-update NAME \\ ([NEW_NAME] --image=IMAGE | -f FILE)  通过配置文件更新 通过配置文件来进行滚动更新，需要在kubectl rolling-update命令后面带上新的配置文件：\n$ kubectl rolling-update NAME -f FILE  这个配置文件必须满足以下条件：\n  指定不同的metadata.name值\n  至少要修改spec.selector中的一个标签值\n  metadata.namespace字段必须相同\n  Replication Controllers的配置文件详细介绍见创建Replication Controllers.\n示例 // 通过新的配置文件frontend-v2.json来更新frontend-v1的pods $ kubectl rolling-update frontend-v1 -f frontend-v2.json // 将frontend-v2.json数据传到标准输入来更新frontend-v1的pods $ cat frontend-v2.json | kubectl rolling-update frontend-v1 -f -  更新容器镜像 仅更新容器镜像的话，可通过如下命令，该命令可以指定一个新的控制器名称（可选），通过--image参数来指定新的镜像名称和标签。\n$ kubectl rolling-update NAME [NEW_NAME] --image=IMAGE:TAG  --image参数仅支持单容器pod，多容器pod使用--image参数会返回错误。\n如果没有指定 NEW_NAME ，新的replication controller创建后会使用一个临时名称，当更新完成，旧的controller被删除后，新的controller名称会被更新成旧的controller名称。\n如果IMAGE:TAG 和当前值相同，更新就会失败。 因此，我们建议使用版本号来作为标签，而不是使用 :latest。从一个 image:latest镜像升级到一个新的 image:latest 镜像将会失败，即使这两个镜像不是相同的。 所以，我们不建议使用 :latest 来作为标签，详细信息见最佳配置实践 。\n示例 // 更新frontend-v1的pod到frontend-v2 $ kubectl rolling-update frontend-v1 frontend-v2 --image=image:v2 // 更新frontend的pods，不更改replication controller的名称 $ kubectl rolling-update frontend --image=image:v2  必选和可选字段 必选字段：\n NAME: 需要进行滚动更新的replication controller名称  下面两个字段选其一：\n -f FILE: 新的replication controller的配置文件，JSON或者YAML格式均可。配置文件必须指定一个新的顶层id值，且至少包含一个现有spec.selector中的键值对。 详细信息见通过Replication Controller运行无状态应用。 或者：  --image IMAGE:TAG: 更新后的镜像的名称和标签。必须和当前的image:tag不同。  可选字段包括：\n NEW_NAME: 只和 --image 一起使用，不和 -f FILE 一起使用。标识新的replication controller的名称。 --poll-interval DURATION: 在更新后轮询控制器状态的间隔时间。有效单位有 ns （纳秒），us 或 µs（微秒），ms（毫秒），s（秒），m（分钟）或 h（小时）。 单位可以自由组合（例如 1m30s）。 默认值为 3s。 --timeout DURATION: 退出更新之前，等待控制器更新一个pod的最大时间。默认是5m0s。有效单位如--poll-interval所述。 --update-period DURATION: 更新两个pod之间等待的时间，默认值是1m0s。有效单位如--poll-interval所述。  有关kubectl rolling-update命令的更多信息见kubectl参考.\n实践 现在你运行了一个1.7.9版本的nginx应用：\napiVersion: v1 kind: ReplicationController metadata: name: my-nginx spec: replicas: 5 template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.7.9 ports: - containerPort: 80 要更新到1.9.1版本，你可以使用kubectl rolling-update --image来指定一个新的镜像：\n$ kubectl rolling-update my-nginx --image=nginx:1.9.1 Created my-nginx-ccba8fbd8cc8160970f63f9a2696fc46 在终端上打开另一个窗口 ，你可以看到kubectl 给每个pod都增加了一个值为配置文件哈希值的 deployment 标签，用来区分新旧pod：\n$ kubectl get pods -l app=nginx -L deployment NAME READY STATUS RESTARTS AGE DEPLOYMENT my-nginx-ccba8fbd8cc8160970f63f9a2696fc46-k156z 1/1 Running 0 1m ccba8fbd8cc8160970f63f9a2696fc46 my-nginx-ccba8fbd8cc8160970f63f9a2696fc46-v95yh 1/1 Running 0 35s ccba8fbd8cc8160970f63f9a2696fc46 my-nginx-divi2 1/1 Running 0 2h 2d1d7a8f682934a254002b56404b813e my-nginx-o0ef1 1/1 Running 0 2h 2d1d7a8f682934a254002b56404b813e my-nginx-q6all 1/1 Running 0 8m 2d1d7a8f682934a254002b56404b813e 使用kubectl rolling-update可以实时看到更新的进度：\nScaling up my-nginx-ccba8fbd8cc8160970f63f9a2696fc46 from 0 to 3, scaling down my-nginx from 3 to 0 (keep 3 pods available, don't exceed 4 pods) Scaling my-nginx-ccba8fbd8cc8160970f63f9a2696fc46 up to 1 Scaling my-nginx down to 2 Scaling my-nginx-ccba8fbd8cc8160970f63f9a2696fc46 up to 2 Scaling my-nginx down to 1 Scaling my-nginx-ccba8fbd8cc8160970f63f9a2696fc46 up to 3 Scaling my-nginx down to 0 Update succeeded. Deleting old controller: my-nginx Renaming my-nginx-ccba8fbd8cc8160970f63f9a2696fc46 to my-nginx replicationcontroller \u0026quot;my-nginx\u0026quot; rolling updated 如果遇到问题，你可以中途停止滚动更新，并且使用 --rollback 来回滚到以前的版本:\n$ kubectl rolling-update my-nginx --rollback Setting \u0026#34;my-nginx\u0026#34; replicas to 1 Continuing update with existing controller my-nginx. Scaling up nginx from 1 to 1, scaling down my-nginx-ccba8fbd8cc8160970f63f9a2696fc46 from 1 to 0 (keep 1 pods available, don\u0026#39;t exceed 2 pods) Scaling my-nginx-ccba8fbd8cc8160970f63f9a2696fc46 down to 0 Update succeeded. Deleting my-nginx-ccba8fbd8cc8160970f63f9a2696fc46 replicationcontroller \u0026#34;my-nginx\u0026#34; rolling updated 这个例子说明容器的不变性是个巨大的优点。\n如果你不仅仅是需要更新镜像，(例如，更新命令参数，环境变量等)，你可以创建一个新的replication controller配置文件，包含一个新的名称和不同的标签值，例如：\napiVersion: v1 kind: ReplicationController metadata: name: my-nginx-v4 spec: replicas: 5 selector: app: nginx deployment: v4 template: metadata: labels: app: nginx deployment: v4 spec: containers: - name: nginx image: nginx:1.9.2 args: [\u0026#34;nginx\u0026#34;, \u0026#34;-T\u0026#34;] ports: - containerPort: 80 然后使用它来进行更新：\n$ kubectl rolling-update my-nginx -f ./nginx-rc.yaml Created my-nginx-v4 Scaling up my-nginx-v4 from 0 to 5, scaling down my-nginx from 4 to 0 (keep 4 pods available, don\u0026#39;t exceed 5 pods) Scaling my-nginx-v4 up to 1 Scaling my-nginx down to 3 Scaling my-nginx-v4 up to 2 Scaling my-nginx down to 2 Scaling my-nginx-v4 up to 3 Scaling my-nginx down to 1 Scaling my-nginx-v4 up to 4 Scaling my-nginx down to 0 Scaling my-nginx-v4 up to 5 Update succeeded. Deleting old controller: my-nginx replicationcontroller \u0026#34;my-nginx-v4\u0026#34; rolling updated 故障分析 如果更新过程中，达到超时时长timeout后还没更新完成，则更新会失败。这时，一些pod会属于新的replication controller，一些会属于旧的。\n如果更新失败，可以尝试使用同样的命令来继续更新过程。\n在尝试更新之前如果需要回滚到之前的状态，可在之前的命令后面添加--rollback=true参数，这将回退所有的更改。\n"
},
{
	"uri": "https://lijun.in/concepts/services-networking/",
	"title": "😊 - 服务、负载均衡和联网",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lijun.in/tasks/debug-application-cluster/",
	"title": "😝 - 监控、日志和排错",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lijun.in/concepts/security/",
	"title": "😊 - 安全",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lijun.in/reference/setup-tools/kubeadm/kubeadm-alpha/",
	"title": "kubeadm alpha",
	"tags": [],
	"description": "",
	"content": ". caution \u0026gt;}}\nkubeadm alpha 提供了一组可用于收集社区反馈的功能的预览。请尝试一下这些功能并给我们反馈！ . /caution \u0026gt;}}\nkubeadm alpha certs renew 使用 all 子命令来更新所有 Kubernetes 证书或有选择性地更新它们。有关证书到期和续订的更多详细信息，请参见证书管理文档。\n. tabs name=\u0026quot;tab-certs-renew\u0026rdquo; \u0026gt;}} . tab name=\u0026quot;renew\u0026rdquo; include=\u0026quot;generated/kubeadm_alpha_certs_renew.md\u0026rdquo; /\u0026gt;}} . tab name=\u0026quot;all\u0026rdquo; include=\u0026quot;generated/kubeadm_alpha_certs_renew_all.md\u0026rdquo; /\u0026gt;}} . tab name=\u0026quot;admin.conf\u0026rdquo; include=\u0026quot;generated/kubeadm_alpha_certs_renew_admin.conf.md\u0026rdquo; /\u0026gt;}} . tab name=\u0026quot;apiserver-etcd-client\u0026rdquo; include=\u0026quot;generated/kubeadm_alpha_certs_renew_apiserver-etcd-client.md\u0026rdquo; /\u0026gt;}} . tab name=\u0026quot;apiserver-kubelet-client\u0026rdquo; include=\u0026quot;generated/kubeadm_alpha_certs_renew_apiserver-kubelet-client.md\u0026rdquo; /\u0026gt;}} . tab name=\u0026quot;apiserver\u0026rdquo; include=\u0026quot;generated/kubeadm_alpha_certs_renew_apiserver.md\u0026rdquo; /\u0026gt;}} . tab name=\u0026quot;controller-manager.conf\u0026rdquo; include=\u0026quot;generated/kubeadm_alpha_certs_renew_controller-manager.conf.md\u0026rdquo; /\u0026gt;}} . tab name=\u0026quot;etcd-healthcheck-client\u0026rdquo; include=\u0026quot;generated/kubeadm_alpha_certs_renew_etcd-healthcheck-client.md\u0026rdquo; /\u0026gt;}} . tab name=\u0026quot;etcd-peer\u0026rdquo; include=\u0026quot;generated/kubeadm_alpha_certs_renew_etcd-peer.md\u0026rdquo; /\u0026gt;}} . tab name=\u0026quot;etcd-server\u0026rdquo; include=\u0026quot;generated/kubeadm_alpha_certs_renew_etcd-server.md\u0026rdquo; /\u0026gt;}} . tab name=\u0026quot;front-proxy-client\u0026rdquo; include=\u0026quot;generated/kubeadm_alpha_certs_renew_front-proxy-client.md\u0026rdquo; /\u0026gt;}} . tab name=\u0026quot;scheduler.conf\u0026rdquo; include=\u0026quot;generated/kubeadm_alpha_certs_renew_scheduler.conf.md\u0026rdquo; /\u0026gt;}} . /tabs \u0026gt;}}\nkubeadm alpha certs certificate-key 该命令可用于生成新的控制平面证书密钥。密钥可以作为 --certificate-key 参数传递给 kubeadm init 和 kubeadm join 操作，以在加入其他控制平面节点时启用证书的自动复制。\n. tabs name=\u0026quot;tab-certs-certificate-key\u0026rdquo; \u0026gt;}} . tab name=\u0026quot;certificate-key\u0026rdquo; include=\u0026quot;generated/kubeadm_alpha_certs_certificate-key.md\u0026rdquo; /\u0026gt;}} . /tabs \u0026gt;}}\nkubeadm alpha certs check-expiration 此命令检查 kubeadm 管理的本地 PKI 中证书的到期时间。有关证书到期和续订的更多详细信息，请参见证书管理文档。\n. tabs name=\u0026quot;tab-certs-check-expiration\u0026rdquo; \u0026gt;}} . tab name=\u0026quot;check-expiration\u0026rdquo; include=\u0026quot;generated/kubeadm_alpha_certs_check-expiration.md\u0026rdquo; /\u0026gt;}} . /tabs \u0026gt;}}\nkubeadm alpha kubeconfig user 使用子命令 user 为其他用户创建 kubeconfig 文件。\n. tabs name=\u0026quot;tab-kubeconfig\u0026rdquo; \u0026gt;}} . tab name=\u0026quot;kubeconfig\u0026rdquo; include=\u0026quot;generated/kubeadm_alpha_kubeconfig.md\u0026rdquo; /\u0026gt;}} . tab name=\u0026quot;user\u0026rdquo; include=\u0026quot;generated/kubeadm_alpha_kubeconfig_user.md\u0026rdquo; /\u0026gt;}} . /tabs \u0026gt;}}\nkubeadm alpha kubelet config 使用以下命令从集群中下载 kubelet 配置或启用 DynamicKubeletConfiguration 功能。\n. tabs name=\u0026quot;tab-kubelet\u0026rdquo; \u0026gt;}} . tab name=\u0026quot;kubelet\u0026rdquo; include=\u0026quot;generated/kubeadm_alpha_kubelet.md\u0026rdquo; /\u0026gt;}} . tab name=\u0026quot;download\u0026rdquo; include=\u0026quot;generated/kubeadm_alpha_kubelet_config_download.md\u0026rdquo; /\u0026gt;}} . tab name=\u0026quot;enable-dynamic\u0026rdquo; include=\u0026quot;generated/kubeadm_alpha_kubelet_config_download.md\u0026rdquo; /\u0026gt;}} . /tabs \u0026gt;}}\nkubeadm alpha selfhosting pivot 子命令 pivot 可用于将 Pod 托管的静态控制平面转换为自托管的控制平面。有关 pivot 更多信息，请参见文档。\n. tabs name=\u0026quot;selfhosting\u0026rdquo; \u0026gt;}} . tab name=\u0026quot;selfhosting\u0026rdquo; include=\u0026quot;generated/kubeadm_alpha_selfhosting.md\u0026rdquo; /\u0026gt;}} . tab name=\u0026quot;pivot\u0026rdquo; include=\u0026quot;generated/kubeadm_alpha_selfhosting_pivot.md\u0026rdquo; /\u0026gt;}} . /tabs \u0026gt;}}\n接下来  kubeadm init 引导 Kubernetes 控制平面节点 kubeadm join 将节点连接到集群 kubeadm reset 会还原 kubeadm init 或 kubeadm join 操作对主机所做的任何更改。  "
},
{
	"uri": "https://lijun.in/reference/setup-tools/kubeadm/kubeadm-init-phase/",
	"title": "kubeadm init phase",
	"tags": [],
	"description": "",
	"content": "kubeadm init phase 能确保调用引导过程的原子步骤。因此，如果希望自定义应用，则可以让 kubeadm 做一些工作，然后填补空白。\nkubeadm init phase 与 kubeadm init 工作流程一致，后台都使用相同的代码。\nkubeadm init phase preflight 使用此命令可以在控制平面节点上执行启动前检查。\n. tabs name=\u0026quot;tab-preflight\u0026rdquo; \u0026gt;}} . tab name=\u0026quot;preflight\u0026rdquo; include=\u0026quot;generated/kubeadm_init_phase_preflight.md\u0026rdquo; /\u0026gt;}} . /tabs \u0026gt;}}\nkubeadm init phase kubelet-start 此阶段将检查 kubelet 配置文件和环境文件，然后启动 kubelet。\n. tabs name=\u0026quot;tab-kubelet-start\u0026rdquo; \u0026gt;}} . tab name=\u0026quot;kubelet-start\u0026rdquo; include=\u0026quot;generated/kubeadm_init_phase_kubelet-start.md\u0026rdquo; /\u0026gt;}} . /tabs \u0026gt;}}\nkubeadm init phase certs 该阶段可用于创建 kubeadm 所需的所有证书。\n. tabs name=\u0026quot;tab-certs\u0026rdquo; \u0026gt;}} . tab name=\u0026quot;certs\u0026rdquo; include=\u0026quot;generated/kubeadm_init_phase_certs.md\u0026rdquo; /\u0026gt;}} . tab name=\u0026quot;all\u0026rdquo; include=\u0026quot;generated/kubeadm_init_phase_certs_all.md\u0026rdquo; /\u0026gt;}} . tab name=\u0026quot;ca\u0026rdquo; include=\u0026quot;generated/kubeadm_init_phase_certs_ca.md\u0026rdquo; /\u0026gt;}} . tab name=\u0026quot;apiserver\u0026rdquo; include=\u0026quot;generated/kubeadm_init_phase_certs_apiserver.md\u0026rdquo; /\u0026gt;}} . tab name=\u0026quot;apiserver-kubelet-client\u0026rdquo; include=\u0026quot;generated/kubeadm_init_phase_certs_apiserver-kubelet-client.md\u0026rdquo; /\u0026gt;}} . tab name=\u0026quot;front-proxy-ca\u0026rdquo; include=\u0026quot;generated/kubeadm_init_phase_certs_front-proxy-ca.md\u0026rdquo; /\u0026gt;}} . tab name=\u0026quot;front-proxy-client\u0026rdquo; include=\u0026quot;generated/kubeadm_init_phase_certs_front-proxy-client.md\u0026rdquo; /\u0026gt;}} . tab name=\u0026quot;etcd-ca\u0026rdquo; include=\u0026quot;generated/kubeadm_init_phase_certs_etcd-ca.md\u0026rdquo; /\u0026gt;}} . tab name=\u0026quot;etcd-server\u0026rdquo; include=\u0026quot;generated/kubeadm_init_phase_certs_etcd-server.md\u0026rdquo; /\u0026gt;}} . tab name=\u0026quot;etcd-peer\u0026rdquo; include=\u0026quot;generated/kubeadm_init_phase_certs_etcd-peer.md\u0026rdquo; /\u0026gt;}} . tab name=\u0026quot;healthcheck-client\u0026rdquo; include=\u0026quot;generated/kubeadm_init_phase_certs_etcd-healthcheck-client.md\u0026rdquo; /\u0026gt;}} . tab name=\u0026quot;apiserver-etcd-client\u0026rdquo; include=\u0026quot;generated/kubeadm_init_phase_certs_apiserver-etcd-client.md\u0026rdquo; /\u0026gt;}} . tab name=\u0026quot;sa\u0026rdquo; include=\u0026quot;generated/kubeadm_init_phase_certs_sa.md\u0026rdquo; /\u0026gt;}} . /tabs \u0026gt;}}\nkubeadm init phase kubeconfig 可以通过调用 all 子命令来创建所有必需的 kubeconfig 文件，或者分别调用它们。\n. tabs name=\u0026quot;tab-kubeconfig\u0026rdquo; \u0026gt;}} . tab name=\u0026quot;kubeconfig\u0026rdquo; include=\u0026quot;generated/kubeadm_init_phase_kubeconfig.md\u0026rdquo; /\u0026gt;}} . tab name=\u0026quot;all\u0026rdquo; include=\u0026quot;generated/kubeadm_init_phase_kubeconfig_all.md\u0026rdquo; /\u0026gt;}} . tab name=\u0026quot;admin\u0026rdquo; include=\u0026quot;generated/kubeadm_init_phase_kubeconfig_admin.md\u0026rdquo; /\u0026gt;}} . tab name=\u0026quot;kubelet\u0026rdquo; include=\u0026quot;generated/kubeadm_init_phase_kubeconfig_kubelet.md\u0026rdquo; /\u0026gt;}} . tab name=\u0026quot;controller-manager\u0026rdquo; include=\u0026quot;generated/kubeadm_init_phase_kubeconfig_controller-manager.md\u0026rdquo; /\u0026gt;}} . tab name=\u0026quot;scheduler\u0026rdquo; include=\u0026quot;generated/kubeadm_init_phase_kubeconfig_scheduler.md\u0026rdquo; /\u0026gt;}} . /tabs \u0026gt;}}\nkubeadm init phase control-plane 使用此阶段，可以为控制平面组件创建所有必需的静态 Pod 文件。\n. tabs name=\u0026quot;tab-control-plane\u0026rdquo; \u0026gt;}} . tab name=\u0026quot;control-plane\u0026rdquo; include=\u0026quot;generated/kubeadm_init_phase_control-plane.md\u0026rdquo; /\u0026gt;}} . tab name=\u0026quot;all\u0026rdquo; include=\u0026quot;generated/kubeadm_init_phase_control-plane_all.md\u0026rdquo; /\u0026gt;}} . tab name=\u0026quot;apiserver\u0026rdquo; include=\u0026quot;generated/kubeadm_init_phase_control-plane_apiserver.md\u0026rdquo; /\u0026gt;}} . tab name=\u0026quot;controller-manager\u0026rdquo; include=\u0026quot;generated/kubeadm_init_phase_control-plane_controller-manager.md\u0026rdquo; /\u0026gt;}} . tab name=\u0026quot;scheduler\u0026rdquo; include=\u0026quot;generated/kubeadm_init_phase_control-plane_scheduler.md\u0026rdquo; /\u0026gt;}} . /tabs \u0026gt;}}\nkubeadm init phase etcd 根据静态 Pod 文件，使用以下阶段创建本地 etcd 实例。\n. tabs name=\u0026quot;tab-etcd\u0026rdquo; \u0026gt;}} . tab name=\u0026quot;etcd\u0026rdquo; include=\u0026quot;generated/kubeadm_init_phase_etcd.md\u0026rdquo; /\u0026gt;}} . tab name=\u0026quot;local\u0026rdquo; include=\u0026quot;generated/kubeadm_init_phase_etcd_local.md\u0026rdquo; /\u0026gt;}} . /tabs \u0026gt;}}\nkubeadm init phase upload-config 可以使用此命令将 kubeadm 配置文件上传到集群。或者，使用 kubeadm config 方式。\n. tabs name=\u0026quot;upload-config\u0026rdquo; \u0026gt;}} . tab name=\u0026quot;upload-config\u0026rdquo; include=\u0026quot;generated/kubeadm_init_phase_upload-config.md\u0026rdquo; /\u0026gt;}} . tab name=\u0026quot;all\u0026rdquo; include=\u0026quot;generated/kubeadm_init_phase_upload-config_all.md\u0026rdquo; /\u0026gt;}} . tab name=\u0026quot;kubeadm\u0026rdquo; include=\u0026quot;generated/kubeadm_init_phase_upload-config_kubeadm.md\u0026rdquo; /\u0026gt;}} . tab name=\u0026quot;kubelet\u0026rdquo; include=\u0026quot;generated/kubeadm_init_phase_upload-config_kubelet.md\u0026rdquo; /\u0026gt;}} . /tabs \u0026gt;}}\nkubeadm init phase upload-certs 使用以下阶段将控制平面证书上传到集群。默认情况下，证书和加密密钥会在两个小时后过期。\n. tabs name=\u0026quot;tab-upload-certs\u0026rdquo; \u0026gt;}} . tab name=\u0026quot;upload-certs\u0026rdquo; include=\u0026quot;generated/kubeadm_init_phase_upload-certs.md\u0026rdquo; /\u0026gt;}} . /tabs \u0026gt;}}\nkubeadm init phase mark-control-plane 使用以下阶段来给具有 node-role.kubernetes.io/master=\u0026quot;\u0026quot; 键值对的节点打标签（label）和记录污点（taint）。\n. tabs name=\u0026quot;tab-mark-control-plane\u0026rdquo; \u0026gt;}} . tab name=\u0026quot;mark-control-plane\u0026rdquo; include=\u0026quot;generated/kubeadm_init_phase_mark-control-plane.md\u0026rdquo; /\u0026gt;}} . /tabs \u0026gt;}}\nkubeadm init phase bootstrap-token 使用以下阶段来配置引导令牌。\n. tabs name=\u0026quot;tab-bootstrap-token\u0026rdquo; \u0026gt;}} . tab name=\u0026quot;bootstrap-token\u0026rdquo; include=\u0026quot;generated/kubeadm_init_phase_bootstrap-token.md\u0026rdquo; /\u0026gt;}} . /tabs \u0026gt;}}\nkubeadm init phase addon 可以使用 all 子命令安装所有可用的插件，或者有选择性地安装它们。\n. tabs name=\u0026quot;tab-addon\u0026rdquo; \u0026gt;}} . tab name=\u0026quot;addon\u0026rdquo; include=\u0026quot;generated/kubeadm_init_phase_addon.md\u0026rdquo; /\u0026gt;}} . tab name=\u0026quot;all\u0026rdquo; include=\u0026quot;generated/kubeadm_init_phase_addon_all.md\u0026rdquo; /\u0026gt;}} . tab name=\u0026quot;coredns\u0026rdquo; include=\u0026quot;generated/kubeadm_init_phase_addon_coredns.md\u0026rdquo; /\u0026gt;}} . tab name=\u0026quot;kube-proxy\u0026rdquo; include=\u0026quot;generated/kubeadm_init_phase_addon_kube-proxy.md\u0026rdquo; /\u0026gt;}} . /tabs \u0026gt;}}\n要使用 kube-dns 代替 CoreDNS，必须传递一个配置文件：\n# 仅用于安装 DNS 插件 kubeadm init phase addon coredns --config=someconfig.yaml # 用于创建完整的控制平面节点 kubeadm init --config=someconfig.yaml # 用于列出或者拉取镜像 kubeadm config images list/pull --config=someconfig.yaml # 升级 kubeadm upgrade apply --config=someconfig.yaml 该文件必须在 ClusterConfiguration 中包含一个 DNS 字段，以及包含一个插件的类型 - kube-dns（默认值为 CoreDNS）。\napiVersion: kubeadm.k8s.io/v1beta2 kind: ClusterConfiguration dns: type: \u0026#34;kube-dns\u0026#34; 有关 v1beta2 配置中每个字段的更多详细信息，可以访问 API。\n接下来  kubeadm init 引导 Kubernetes 控制平面节点 kubeadm join 将节点连接到集群 kubeadm reset 恢复通过 kubeadm init 或 kubeadm join 操作对主机所做的任何更改 kubeadm alpha 尝试实验性功能  "
},
{
	"uri": "https://lijun.in/reference/setup-tools/kubeadm/kubeadm-join-phase/",
	"title": "kubeadm join phase",
	"tags": [],
	"description": "",
	"content": "kubeadm join phase 使您能够调用 join 过程的基本原子步骤。因此，如果希望执行自定义操作，可以让 kubeadm 做一些工作，然后由用户来补足剩余操作。\nkubeadm join phase 与 kubeadm join 工作流程 一致，后台都使用相同的代码。\nkubeadm join phase . tabs name=\u0026quot;tab-phase\u0026rdquo; \u0026gt;}} . tab name=\u0026quot;phase\u0026rdquo; include=\u0026quot;generated/kubeadm_join_phase.md\u0026rdquo; /\u0026gt;}} . /tabs \u0026gt;}}\nkubeadm join phase preflight 使用此命令可以在即将加入集群的节点上执行启动前检查。\n. tabs name=\u0026quot;tab-preflight\u0026rdquo; \u0026gt;}} . tab name=\u0026quot;preflight\u0026rdquo; include=\u0026quot;generated/kubeadm_join_phase_preflight.md\u0026rdquo; /\u0026gt;}} . /tabs \u0026gt;}}\nkubeadm join phase control-plane-prepare 使用此阶段，您可以准备一个作为控制平面的节点。\n. tabs name=\u0026quot;tab-control-plane-prepare\u0026rdquo; \u0026gt;}} . tab name=\u0026quot;control-plane-prepare\u0026rdquo; include=\u0026quot;generated/kubeadm_join_phase_control-plane-prepare.md\u0026rdquo; /\u0026gt;}} . tab name=\u0026quot;all\u0026rdquo; include=\u0026quot;generated/kubeadm_join_phase_control-plane-prepare_all.md\u0026rdquo; /\u0026gt;}} . tab name=\u0026quot;download-certs\u0026rdquo; include=\u0026quot;generated/kubeadm_join_phase_control-plane-prepare_download-certs.md\u0026rdquo; /\u0026gt;}} . tab name=\u0026quot;certs\u0026rdquo; include=\u0026quot;generated/kubeadm_join_phase_control-plane-prepare_certs.md\u0026rdquo; /\u0026gt;}} . tab name=\u0026quot;kubeconfig\u0026rdquo; include=\u0026quot;generated/kubeadm_join_phase_control-plane-prepare_kubeconfig.md\u0026rdquo; /\u0026gt;}} . tab name=\u0026quot;control-plane\u0026rdquo; include=\u0026quot;generated/kubeadm_join_phase_control-plane-prepare_control-plane.md\u0026rdquo; /\u0026gt;}} . /tabs \u0026gt;}}\nkubeadm join phase kubelet-start 使用此阶段，您可以配置 kubelet 设置、证书和（重新）启动 kubelet。\n. tabs name=\u0026quot;tab-kubelet-start\u0026rdquo; \u0026gt;}} . tab name=\u0026quot;kubelet-start\u0026rdquo; include=\u0026quot;generated/kubeadm_join_phase_kubelet-start.md\u0026rdquo; /\u0026gt;}} . /tabs \u0026gt;}}\nkubeadm join phase control-plane-join 使用此阶段，您可以将节点作为控制平面实例加入。\n. tabs name=\u0026quot;tab-control-plane-join\u0026rdquo; \u0026gt;}} . tab name=\u0026quot;control-plane-join\u0026rdquo; include=\u0026quot;generated/kubeadm_join_phase_control-plane-join.md\u0026rdquo; /\u0026gt;}} . tab name=\u0026quot;all\u0026rdquo; include=\u0026quot;generated/kubeadm_join_phase_control-plane-join_all.md\u0026rdquo; /\u0026gt;}} . tab name=\u0026quot;etcd\u0026rdquo; include=\u0026quot;generated/kubeadm_join_phase_control-plane-join_etcd.md\u0026rdquo; /\u0026gt;}} . tab name=\u0026quot;update-status\u0026rdquo; include=\u0026quot;generated/kubeadm_join_phase_control-plane-join_update-status.md\u0026rdquo; /\u0026gt;}} . tab name=\u0026quot;mark-control-plane\u0026rdquo; include=\u0026quot;generated/kubeadm_join_phase_control-plane-join_mark-control-plane.md\u0026rdquo; /\u0026gt;}} . /tabs \u0026gt;}}\n下一步  kubeadm init 引导 Kubernetes 控制平面节点 kubeadm join 将节点连接到集群 kubeadm reset 恢复通过 kubeadm init 或 kubeadm join 操作对主机所做的任何更改 kubeadm alpha 尝试实验性功能  "
},
{
	"uri": "https://lijun.in/reference/setup-tools/kubeadm/kubeadm-reset-phase/",
	"title": "kubeadm reset phase",
	"tags": [],
	"description": "",
	"content": "kubeadm reset phase 使您能够调用 reset 过程的基本原子步骤。因此，如果希望执行自定义操作，可以让 kubeadm 做一些工作，然后由用户来补足剩余操作。\nkubeadm reset phase 与 kubeadm reset 工作流程 一致，后台都使用相同的代码。\nkubeadm reset phase . tabs name=\u0026quot;tab-phase\u0026rdquo; \u0026gt;}} . tab name=\u0026quot;phase\u0026rdquo; include=\u0026quot;generated/kubeadm_reset_phase.md\u0026rdquo; /\u0026gt;}} . /tabs \u0026gt;}}\nkubeadm reset phase preflight 使用此阶段，您可以在要重置的节点上执行启动前检查阶段。\n. tabs name=\u0026quot;tab-preflight\u0026rdquo; \u0026gt;}} . tab name=\u0026quot;preflight\u0026rdquo; include=\u0026quot;generated/kubeadm_reset_phase_preflight.md\u0026rdquo; /\u0026gt;}} . /tabs \u0026gt;}}\nkubeadm reset phase update-cluster-status 使用此阶段，您可以从 ClusterStatus 对象中删除此控制平面节点。\n. tabs name=\u0026quot;tab-update-cluster-status\u0026rdquo; \u0026gt;}} . tab name=\u0026quot;update-cluster-status\u0026rdquo; include=\u0026quot;generated/kubeadm_reset_phase_update-cluster-status.md\u0026rdquo; /\u0026gt;}} . /tabs \u0026gt;}}\nkubeadm reset phase remove-etcd-member 使用此阶段，您可以从 etcd 集群中删除此控制平面节点的 etcd 成员。\n. tabs name=\u0026quot;tab-remove-etcd-member\u0026rdquo; \u0026gt;}} . tab name=\u0026quot;remove-etcd-member\u0026rdquo; include=\u0026quot;generated/kubeadm_reset_phase_remove-etcd-member.md\u0026rdquo; /\u0026gt;}} . /tabs \u0026gt;}}\nkubeadm reset phase cleanup-node 使用此阶段，您可以在此节点上执行清理工作。\n. tabs name=\u0026quot;tab-cleanup-node\u0026rdquo; \u0026gt;}} . tab name=\u0026quot;cleanup-node\u0026rdquo; include=\u0026quot;generated/kubeadm_reset_phase_cleanup-node.md\u0026rdquo; /\u0026gt;}} . /tabs \u0026gt;}}\n下一步  kubeadm init 引导 Kubernetes 控制平面节点 kubeadm join 将节点连接到集群 kubeadm reset 恢复通过 kubeadm init 或 kubeadm join 操作对主机所做的任何更改 kubeadm alpha 尝试实验性功能  "
},
{
	"uri": "https://lijun.in/reference/setup-tools/kubeadm/kubeadm-upgrade-phase/",
	"title": "kubeadm upgrade phase",
	"tags": [],
	"description": "",
	"content": "在 Kubernetes v1.15.0 版本中，kubeadm 引入了对 kubeadm upgrade node 阶段的初步支持。其他 kubeadm upgrade 子命令如 apply 等阶段将在未来发行版中添加。\nkubeadm upgrade node phase 使用此阶段，可以选择执行辅助控制平面或工作节点升级的单独步骤。请注意，kubeadm upgrade apply 命令仍然必须在主控制平面节点上调用。\n. tabs name=\u0026quot;tab-phase\u0026rdquo; \u0026gt;}} . tab name=\u0026quot;phase\u0026rdquo; include=\u0026quot;generated/kubeadm_upgrade_node_phase.md\u0026rdquo; /\u0026gt;}} . tab name=\u0026quot;control-plane\u0026rdquo; include=\u0026quot;generated/kubeadm_upgrade_node_phase_control-plane.md\u0026rdquo; /\u0026gt;}} . tab name=\u0026quot;kubelet-config\u0026rdquo; include=\u0026quot;generated/kubeadm_upgrade_node_phase_kubelet-config.md\u0026rdquo; /\u0026gt;}} . /tabs \u0026gt;}}\n接下来  kubeadm init 引导一个 Kubernetes 控制平面节点 kubeadm join 将节点加入到群集 kubeadm reset 还原 kubeadm init 或 kubeadm join 命令对主机所做的任何更改 kubeadm upgrade 升级 kubeadm 节点 kubeadm alpha 尝试实验性功能  "
},
{
	"uri": "https://lijun.in/tasks/run-application/horizontal-pod-autoscale/",
	"title": "Pod 水平自动伸缩",
	"tags": [],
	"description": "",
	"content": "Pod 水平自动伸缩（Horizontal Pod Autoscaler）特性， 可以基于CPU利用率自动伸缩 replication controller、deployment和 replica set 中的 pod 数量，（除了 CPU 利用率）也可以 基于其他应程序提供的度量指标custom metrics。 pod 自动缩放不适用于无法缩放的对象，比如 DaemonSets。\nPod 水平自动伸缩特性由 Kubernetes API 资源和控制器实现。资源决定了控制器的行为。 控制器会周期性的获取平均 CPU 利用率，并与目标值相比较后来调整 replication controller 或 deployment 中的副本数量。\nPod 水平自动伸缩工作机制 Pod 水平自动伸缩的实现是一个控制循环，由 controller manager 的 --horizontal-pod-autoscaler-sync-period 参数 指定周期（默认值为15秒）。\n每个周期内，controller manager 根据每个 HorizontalPodAutoscaler 定义中指定的指标查询资源利用率。 controller manager 可以从 resource metrics API（每个pod 资源指标）和 custom metrics API（其他指标）获取指标。\n 对于每个 pod 的资源指标（如 CPU），控制器从资源指标 API 中获取每一个 HorizontalPodAutoscaler 指定 的 pod 的指标，然后，如果设置了目标使用率，控制器获取每个 pod 中的容器资源使用情况，并计算资源使用率。 如果使用原始值，将直接使用原始数据（不再计算百分比）。 然后，控制器根据平均的资源使用率或原始值计算出缩放的比例，进而计算出目标副本数。  需要注意的是，如果 pod 某些容器不支持资源采集，那么控制器将不会使用该 pod 的 CPU 使用率。 下面的算法细节章节将会介绍详细的算法。\n 如果 pod 使用自定义指示，控制器机制与资源指标类似，区别在于自定义指标只使用原始值，而不是使用率。   如果pod 使用对象指标和外部指标（每个指标描述一个对象信息）。 这个指标将直接跟据目标设定值相比较，并生成一个上面提到的缩放比例。在 autoscaling/v2beta2 版本API中， 这个指标也可以根据 pod 数量平分后再计算。  通常情况下，控制器将从一系列的聚合 API（metrics.k8s.io、custom.metrics.k8s.io和external.metrics.k8s.io） 中获取指标数据。 metrics.k8s.io API 通常由 metrics-server（需要额外启动）提供。 可以从metrics-server 获取更多信息。 另外，控制器也可以直接从 Heapster 获取指标。\n. note \u0026gt;}} . feature-state state=\u0026quot;deprecated\u0026rdquo; for_k8s_version=\u0026quot;1.11\u0026rdquo; \u0026gt;}}\n自 Kubernetes 1.11起，从 Heapster 获取指标特性已废弃。 . /note \u0026gt;}}\n关于指标 API 更多信息，请参考Support for metrics APIs。\n自动缩放控制器使用 scale sub-resource 访问相应可支持缩放的控制器（如replication controllers、deployments 和 replica sets）。 scale 是一个可以动态设定副本数量和检查当前状态的接口。 更多关于 scale sub-resource 的信息，请参考这里.\n算法细节 从最基本的角度来看，pod 水平自动缩放控制器跟据当前指标和期望指标来计算缩放比例。\n期望副本数 = ceil[当前副本数 * ( 当前指标 / 期望指标 )] 例如，当前指标为200m，目标设定值为100m,那么由于200.0 / 100.0 == 2.0， 副本数量将会翻倍。 如果当前指标为50m，副本数量将会减半，因为50.0 / 100.0 == 0.5。 如果计算出的缩放比例接近1.0（跟据--horizontal-pod-autoscaler-tolerance 参数全局配置的容忍值，默认为0.1）， 将会放弃本次缩放。\n如果 HorizontalPodAutoscaler 指定的是targetAverageValue 或 targetAverageUtilization， 那么将会把指定pod的平均指标做为currentMetricValue。 然而，在检查容忍度和决定最终缩放值前，我们仍然会把那些无法获取指标的pod统计进去。\n所有被标记了删除时间戳(Pod正在关闭过程中)的 pod 和 失败的 pod 都会被忽略。\n如果某个 pod 缺失指标信息，它将会被搁置，只在最终确定缩值时再考虑。\n当使用 CPU 指标来缩放时，任何还未就绪（例如还在初始化）状态的 pod 或 最近的指标为就绪状态前的 pod， 也会被搁置\n由于受技术限制，pod 水平缩放控制器无法准确的知道 pod 什么时候就绪， 也就无法决定是否暂时搁置该 pod。 --horizontal-pod-autoscaler-initial-readiness-delay 参数（默认为30s），用于设置 pod 准备时间， 在此时间内的 pod 统统被认为未就绪。 --horizontal-pod-autoscaler-cpu-initialization-period参数（默认为5分钟），用于设置 pod 的初始化时间， 在此时间内的 pod，CPU 资源指标将不会被采纳。\n在排除掉被搁置的 pod 后，缩放比例就会跟据currentMetricValue / desiredMetricValue计算出来。\n如果有任何 pod 的指标缺失，我们会更保守地重新计算平均值， 在需要缩小时假设这些 pod 消耗了目标值的 100%， 在需要放大时假设这些 pod 消耗了0%目标值。 这可以在一定程度上抑制伸缩的幅度。\n此外，如果存在任何尚未就绪的pod，我们可以在不考虑遗漏指标或尚未就绪的pods的情况下进行伸缩， 我们保守地假设尚未就绪的pods消耗了试题指标的0%，从而进一步降低了伸缩的幅度。\n在缩放方向（缩小或放大）确定后，我们会把未就绪的 pod 和缺少指标的 pod 考虑进来再次计算使用率。 如果新的比率与缩放方向相反，或者在容忍范围内，则跳过缩放。 否则，我们使用新的缩放比例。\n注意，平均利用率的原始值会通过 HorizontalPodAutoscaler 的状态体现（ 即使使用了新的使用率，也不考虑未就绪 pod 和 缺少指标的 pod)。\n如果创建 HorizontalPodAutoscaler 时指定了多个指标， 那么会按照每个指标分别计算缩放副本数，取最大的进行缩放。 如果任何一个指标无法顺利的计算出缩放副本数（比如，通过 API 获取指标时出错）， 那么本次缩放会被跳过。\n最后，在 HPA 控制器执行缩放操作之前，会记录缩放建议信息（scale recommendation）。 控制器会在操作时间窗口中考虑所有的建议信息，并从中选择得分最高的建议。 这个值可通过 kube-controller-manager 服务的启动参数 --horizontal-pod-autoscaler-downscale-stabilization 进行配置， 默认值为 5min。 这个配置可以让系统更为平滑地进行缩容操作，从而消除短时间内指标值快速波动产生的影响。\nAPI 对象 HorizontalPodAutoscaler 是 Kubernetes autoscaling API 组的资源。 在当前稳定版本（autoscaling/v1）中只支持基于CPU指标的缩放。\n在 beta 版本（autoscaling/v2beta2），引入了基于内存和自定义指标的缩放。 在autoscaling/v2beta2版本中新引入的字段在autoscaling/v1版本中基于 annotation 实现。\n更多有关 API 对象的信息，请查阅HorizontalPodAutoscaler Object。\n使用 kubectl 操作 Horizontal Pod Autoscaler 与其他 API 资源类似，kubectl 也标准支持 Pod 自动伸缩。 我们可以通过 kubectl create 命令创建一个自动伸缩对象， 通过 kubectl get hpa 命令来获取所有自动伸缩对象， 通过 kubectl describe hpa 命令来查看自动伸缩对象的详细信息。 最后，可以使用 kubectl delete hpa 命令删除对象。\n此外，还有个简便的命令 kubectl autoscale 来创建自动伸缩对象。 例如，命令 kubectl autoscale rs foo --min=2 --max=5 --cpu-percent=80 将会为名 为 foo 的 replication set 创建一个自动伸缩对象， 对象目标CPU使用率为 80%，副本数量配置为 2 到 5 之间。\n滚动升级时缩放 目前在 Kubernetes 中，可以针对 replication controllers 或 deployment 执行 滚动升级rolling update，他们会为你管理底层副本数。 Pod 水平缩放只支持后一种：Horizontal Pod Autoscaler 会被绑定到 deployment 对象中，Horizontal Pod Autoscaler 设置副本数量时， deployment 会设置底层副本数。\n当使用 replication controllers 执行滚动升级时， Horizontal Pod Autoscaler 不能工作， 也就是说你不能将 Horizontal Pod Autoscaler 绑定到某个 replication controller 再执行滚动升级（例如使用 kubectl rolling-update 命令）。 Horizontal Pod Autoscaler 不能工作的原因是，Horizontal Pod Autoscaler 无法绑定到滚动升级时创建的新副本。\n冷却/延迟 当使用 Horizontal Pod Autoscaler 管理一组副本缩放时， 有可能因为指标动态的变化造成副本数量频繁的变化，有时这被称为 抖动。\n从 v1.6 版本起，集群操作员可以开启某些 kube-controller-manager 全局的参数来缓和这个问题。\n从 v1.12 开始，算法调整后，就不用这么做了。\n --horizontal-pod-autoscaler-downscale-stabilization: 这个 kube-controller-manager 的参数表示缩容冷却时间。 即自从上次缩容执行结束后，多久可以再次执行缩容，默认时间是5分钟(5m0s)。  . note \u0026gt;}}\n当启用这个参数时，集群操作员需要明白其可能的影响。 如果延迟（冷却）时间设置的太长，那么 Horizontal Pod Autoscaler 可能会不能很好的改变负载。 如果延迟（冷却）时间设备的太短，那么副本数量有可能跟以前一样抖动。 . /note \u0026gt;}}\n多指标支持 在 Kubernetes 1.6 支持了基于多个指标进行缩放。 你可以使用 autoscaling/v2beta2 API 来为 Horizontal Pod Autoscaler 指定多个指标。 Horizontal Pod Autoscaler 会跟据每个指标计算，并生成一个缩放建议。 幅度最大的缩放建议会被采纳。\n自定义指标支持 . note \u0026gt;}}\n在 Kubernetes 1.2 增加的 alpha 的缩放支持基于特定的 annotation。 自从 Kubernetes 1.6 起，由于缩放 API 的引入，这些 annotation 就不再支持了。 虽然收集自定义指标的旧方法仍然可用，但是 Horizontal Pod Autoscaler 调度器将不会再使用这些指标， 同时，Horizontal Pod Autoscaler 也不再使用之前的用于指定用户自定义指标的 annotation 了。 . /note \u0026gt;}}\n自 Kubernetes 1.6 起，Horizontal Pod Autoscaler 支持使用自定义指标。 你可以使用 autoscaling/v2beta2 API 为 Horizontal Pod Autoscaler 指定用户自定义指标。 Kubernetes 会通过用户自定义指标 API 来获取相应的指标。\n关于指标 API 的要求，请查阅 Support for metrics APIs。\n指标 API 默认情况下，HorizontalPodAutoscaler 控制器会从一系列的 API 中请求指标数据。 集群管理员需要确保下述条件，以保证这些 API 可以访问：\n API aggregation layer 已开启    相应的 API 已注册：\n 资源指标会使用 metrics.k8s.io API，一般由 metrics-server 提供。 它可以做为集群组件启动。 用户指标会使用 custom.metrics.k8s.io API。 它由其他厂商的“适配器”API 服务器提供。 确认你的指标管道，或者查看 list of known solutions。 外部指标会使用 external.metrics.k8s.io API。可能由上面的用户指标适配器提供。     --horizontal-pod-autoscaler-use-rest-clients 参数设置为 true 或者不设置。 如果设置为 false，则会切换到基于 Heapster 的自动缩放，这个特性已经被弃用了。  更多关于指标来源以及其区别，请参阅相关的设计文档， the HPA V2、 custom.metrics.k8s.io和 external.metrics.k8s.io。\n如何使用它们的示例，请参考 the walkthrough for using custom metrics 和 the walkthrough for using external metrics。\n. heading \u0026ldquo;whatsnext\u0026rdquo; %}}  设计文档：Horizontal Pod Autoscaling. kubectl 自动缩放命令： kubectl autoscale. 使用示例：Horizontal Pod Autoscaler.  "
},
{
	"uri": "https://lijun.in/tasks/configure-pod-container/configure-service-account/",
	"title": "为 Pod 配置服务账户",
	"tags": [],
	"description": "",
	"content": "服务账户为 Pod 中运行的进程提供了一个标识。\n本文是服务账户的用户使用介绍。您也可以参考集群管理指南之服务账户。\n. note \u0026gt;}}\n本文档描述 Kubernetes 项目推荐的集群中服务帐户的行为。 集群管理员也可能已经定制了服务账户在集群中的属性，在这种情况下，本文档可能并不适用。\n. /note \u0026gt;}}\n当您（人类）访问集群时（例如，使用 kubectl），api 服务器将您的身份验证为特定的用户帐户（当前这通常是 admin，除非您的集群管理员已经定制了您的集群配置）。 Pod 内的容器中的进程也可以与 api 服务器接触。 当它们进行身份验证时，它们被验证为特定的服务帐户（例如，default）。\n. heading \u0026ldquo;prerequisites\u0026rdquo; %}} . include \u0026ldquo;task-tutorial-prereqs.md\u0026rdquo; \u0026gt;}} . version-check \u0026gt;}}\n使用默认的服务账户访问 API 服务器 当您创建 Pod 时，如果没有指定服务账户，Pod 会被指定命名空间中的default服务账户。 如果您查看 Pod 的原始 json 或 yaml（例如：kubectl get pods/podname -o yaml）， 您可以看到 spec.serviceAccountName 字段已经被自动设置了。\n您可以使用自动挂载给 Pod 的服务账户凭据访问 API，访问集群 中有相关描述。 服务账户的 API 许可取决于您所使用的授权插件和策略。\n在 1.6 以上版本中，您可以通过在服务账户上设置 automountServiceAccountToken: false 来实现不给服务账号自动挂载 API 凭据：\napiVersion: v1 kind: ServiceAccount metadata: name: build-robot automountServiceAccountToken: false ... 在 1.6 以上版本中，您也可以选择不给特定 Pod 自动挂载 API 凭据：\napiVersion: v1 kind: Pod metadata: name: my-pod spec: serviceAccountName: build-robot automountServiceAccountToken: false ... 如果 Pod 和服务账户都指定了 automountServiceAccountToken 值，则 Pod 的 spec 优先于服务帐户。\n使用多个服务账户 每个命名空间都有一个名为 default 的服务账户资源。 您可以用下面的命令查询这个服务账户以及命名空间中的其他 serviceAccount 资源：\nkubectl get serviceAccounts NAME SECRETS AGE default 1 1d 您可以像这样来创建额外的 ServiceAccount 对象：\nkubectl create -f - \u0026lt;\u0026lt;EOF apiVersion: v1 kind: ServiceAccount metadata: name: build-robot EOF serviceaccount/build-robot created 如果您查询服务帐户对象的完整信息，如下所示：\nkubectl get serviceaccounts/build-robot -o yaml apiVersion: v1 kind: ServiceAccount metadata: creationTimestamp: 2015-06-16T00:12:59Z name: build-robot namespace: default resourceVersion: \u0026#34;272500\u0026#34; uid: 721ab723-13bc-11e5-aec2-42010af0021e secrets: - name: build-robot-token-bvbk5 那么您就能看到系统已经自动创建了一个令牌并且被服务账户所引用。\n您可以使用授权插件来 设置服务账户的访问许可。\n要使用非默认的服务账户，只需简单的将 Pod 的 spec.serviceAccountName 字段设置为您想用的服务账户名称。\nPod 被创建时服务账户必须存在，否则会被拒绝。\n您不能更新已经创建好的 Pod 的服务账户。\n您可以清除服务账户，如下所示：\nkubectl delete serviceaccount/build-robot 手动创建服务账户 API 令牌 假设我们有一个上面提到的名为 \u0026ldquo;build-robot\u0026rdquo; 的服务账户，然后我们手动创建一个新的 Secret。\nkubectl create -f - \u0026lt;\u0026lt;EOF apiVersion: v1 kind: Secret metadata: name: build-robot-secret annotations: kubernetes.io/service-account.name: build-robot type: kubernetes.io/service-account-token EOF secret/build-robot-secret created 现在，您可以确认新构建的 Secret 中填充了 \u0026ldquo;build-robot\u0026rdquo; 服务帐户的 API 令牌。\n令牌控制器将清理不存在的服务帐户的所有令牌。\nkubectl describe secrets/build-robot-secret Name: build-robot-secret Namespace: default Labels: \u0026lt;none\u0026gt; Annotations: kubernetes.io/service-account.name=build-robot kubernetes.io/service-account.uid=da68f9c6-9d26-11e7-b84e-002dc52800da Type: kubernetes.io/service-account-token Data ==== ca.crt: 1338 bytes namespace: 7 bytes token: ... . note \u0026gt;}}\n这里省略了 token 的内容。 . /note \u0026gt;}}\n为服务账户添加 ImagePullSecrets 首先，创建一个 ImagePullSecrets，可以参考这里 的描述。 然后，确认创建是否成功。例如：\nkubectl get secrets myregistrykey NAME TYPE DATA AGE myregistrykey kubernetes.io/.dockerconfigjson 1 1d 接着修改命名空间的默认服务帐户，以将该 Secret 用作 imagePullSecret。\nkubectl patch serviceaccount default -p \u0026#39;{\u0026#34;imagePullSecrets\u0026#34;: [{\u0026#34;name\u0026#34;: \u0026#34;myregistrykey\u0026#34;}]}\u0026#39; 需要手动编辑的交互式版本：\nkubectl get serviceaccounts default -o yaml \u0026gt; ./sa.yaml cat sa.yaml apiVersion: v1 kind: ServiceAccount metadata: creationTimestamp: 2015-08-07T22:02:39Z name: default namespace: default resourceVersion: \u0026#34;243024\u0026#34; uid: 052fb0f4-3d50-11e5-b066-42010af0d7b6 secrets: - name: default-token-uudge vi sa.yaml [editor session not shown] [delete line with key \u0026#34;resourceVersion\u0026#34;] [add lines with \u0026#34;imagePullSecrets:\u0026#34;] cat sa.yaml apiVersion: v1 kind: ServiceAccount metadata: creationTimestamp: 2015-08-07T22:02:39Z name: default namespace: default uid: 052fb0f4-3d50-11e5-b066-42010af0d7b6 secrets: - name: default-token-uudge imagePullSecrets: - name: myregistrykey kubectl replace serviceaccount default -f ./sa.yaml serviceaccounts/default 现在，在当前命名空间中创建的每个新 Pod 的 spec 中都会添加下面的内容：\nspec: imagePullSecrets: - name: myregistrykey 服务帐户令牌卷投影 . feature-state for_k8s_version=\u0026quot;v1.12\u0026rdquo; state=\u0026quot;beta\u0026rdquo; \u0026gt;}}\n. note \u0026gt;}}\nServiceAccountTokenVolumeProjection 在 1.12 版本中是 beta 阶段，可以通过向 API 服务器传递以下所有参数来启用它：\n --service-account-issuer --service-account-signing-key-file --service-account-api-audiences  . /note \u0026gt;}}\nkubelet 还可以将服务帐户令牌投影到 Pod 中。 您可以指定令牌的所需属性，例如受众和有效持续时间。 这些属性在默认服务帐户令牌上无法配置。 当删除 Pod 或 ServiceAccount 时，服务帐户令牌也将对 API 无效。\n使用名为 ServiceAccountToken 的 ProjectedVolume 类型在 PodSpec 上配置此功能。 要向 Pod 提供具有 \u0026ldquo;vault\u0026rdquo; 观众以及两个小时有效期的令牌，可以在 PodSpec 中配置以下内容：\nkind: Pod apiVersion: v1 spec: containers: - image: nginx name: nginx volumeMounts: - mountPath: /var/run/secrets/tokens name: vault-token volumes: - name: vault-token projected: sources: - serviceAccountToken: path: vault-token expirationSeconds: 7200 audience: vault Kubelet 将代表 Pod 请求和存储令牌，使令牌在可配置的文件路径上对 Pod 可用，并在令牌接近到期时刷新令牌。 如果令牌存活时间大于其总 TTL 的 80% 或者大于 24 小时，Kubelet 则会主动旋转令牌。\n应用程序负责在令牌旋转时重新加载令牌。 对于大多数情况，定期重新加载（例如，每 5 分钟一次）就足够了。\n"
},
{
	"uri": "https://lijun.in/reference/access-authn-authz/node/",
	"title": "使用 Node 鉴权",
	"tags": [],
	"description": "",
	"content": "节点鉴权是一种特殊用途的鉴权模式，专门对 kubelet 发出的 API 请求进行鉴权。\n概述 节点鉴权器允许 kubelet 执行 API 操作。包括：\n读取操作：\n services endpoints nodes pods secrets、configmaps、以及绑定到 kubelet 的节点的 pod 的持久卷申领和持久卷  写入操作：\n 节点和节点状态（启用 NodeRestriction 准入插件以限制 kubelet 只能修改自己的节点） Pod 和 Pod 状态 (启用 NodeRestriction 准入插件以限制 kubelet 只能修改绑定到自身的 Pod) 事件  鉴权相关操作：\n 对于基于 TLS 的启动引导过程时使用的 certificationsigningrequests API 的读/写权限 为委派的身份验证/授权检查创建 tokenreviews 和 subjectaccessreviews 的能力  在将来的版本中，节点鉴权器可能会添加或删除权限，以确保 kubelet 具有正确操作所需的最小权限集。\n为了获得节点鉴权器的授权，kubelet 必须使用一个凭证以表示它在 system:nodes 组中，用户名为 system:node:\u0026lt;nodeName\u0026gt;。 上述的组名和用户名格式要与 kubelet TLS 启动引导过程中为每个 kubelet 创建的标识相匹配。\n要启用节点授权器，请使用 --authorization-mode = Node 启动 apiserver。\n要限制 kubelet 具有写入权限的 API 对象，请使用 --enable-admission-plugins=...,NodeRestriction,... 启动 apiserver，从而启用 NodeRestriction 准入插件。\n迁移考虑因素 在 system:nodes 组之外的 Kubelet system:nodes 组之外的 kubelet 不会被 Node 鉴权模式授权，并且需要继续通过当前授权它们的机制来授权。 节点准入插件不会限制来自这些 kubelet 的请求。\n具有无差别用户名的 Kubelet 在一些部署中，kubelet 具有 system:nodes 组的凭证，但是无法给出它们所关联的节点的标识，因为它们没有 system:node:... 格式的用户名。 这些 kubelet 不会被 Node 授权模式授权，并且需要继续通过当前授权它们的任何机制来授权。\n因为默认的节点标识符实现不会把它当作节点身份标识，NodeRestriction 准入插件会忽略来自这些 kubelet 的请求。\n相对于以前使用 RBAC 的版本的更新 升级的 1.7 之前的使用 RBAC 的集群将继续按原样运行，因为 system:nodes 组绑定已经存在。\n如果集群管理员希望开始使用 Node 鉴权器和 NodeRestriction 准入插件来限制节点对 API 的访问，这一需求可以通过下列操作来完成且不会影响已部署的应用：\n 启用 Node 鉴权模式 (--authorization-mode=Node,RBAC) 和 NodeRestriction 准入插件 确保所有 kubelet 的凭据符合组/用户名要求 审核 apiserver 日志以确保 Node 鉴权器不会拒绝来自 kubelet 的请求（日志中没有持续的 NODE DENY 消息） 删除 system:node 集群角色绑定  RBAC 节点权限 在 1.6 版本中，当使用 RBAC 鉴权模式 时，system:nodes 集群角色会被自动绑定到 system:node 组。\n在 1.7 版本中，不再推荐将 system:nodes 组自动绑定到 system:node 角色，因为节点鉴权器通过对 secret 和 configmap 访问的额外限制完成了相同的任务。 如果同时启用了 Node 和 RBAC 授权模式，1.7 版本则不会创建 system:nodes 组到 system:node 角色的自动绑定。\n在 1.8 版本中，绑定将根本不会被创建。\n使用 RBAC 时，将继续创建 system:node 集群角色，以便与将其他用户或组绑定到该角色的部署方法兼容。\n"
},
{
	"uri": "https://lijun.in/tasks/access-application-cluster/configure-cloud-provider-firewall/",
	"title": "配置你的云平台防火墙",
	"tags": [],
	"description": "",
	"content": "许多云服务提供商（比如 谷歌计算引擎）定义防火墙以防止服务无意间暴露到互联网上。 当暴露服务给外网时，你可能需要在防火墙上开启一个或者更多的端口来支持服务。 本文描述了这个过程，以及其他云服务商的具体信息。\n. heading \u0026ldquo;prerequisites\u0026rdquo; %}} . include \u0026ldquo;task-tutorial-prereqs.md\u0026rdquo; \u0026gt;}} . version-check \u0026gt;}}\n负载均衡（LoadBalancer）服务的访问限制 当以 spec.type: LoadBalancer 方式配置服务时，你可以使用 spec.loadBalancerSourceRanges 来指定允许访问负载均衡器的 ip 段。 这个字段采用 CIDR 的 IP 段， kubernetes 使用该段配置防火墙。目前只有 谷歌计算引擎，谷歌云原生引擎，亚马逊弹性原生云服务 和 微软云原生平台支持此功能。 如果云服务提供商不支持这个功能，这个字段将被忽略。\n假设内部子网为假设10.0.0.0/8，在下面这个例子中，将创建一个仅能由群集内部IP访问的负载均衡器。此负载均衡器不允许来自 kubernetes 集群外部客户端的访问。\napiVersion: v1 kind: Service metadata: name: myapp spec: ports: - port: 8765 targetPort: 9376 selector: app: example type: LoadBalancer loadBalancerSourceRanges: - 10.0.0.0/8 在下面这个例子中，将创建一个只能被 IP 为 130.211.204.1 和 130.211.204.2 的客户端访问的负载据衡器。\napiVersion: v1 kind: Service metadata: name: myapp spec: ports: - port: 8765 targetPort: 9376 selector: app: example type: LoadBalancer loadBalancerSourceRanges: - 130.211.204.1/32 - 130.211.204.2/32 谷歌计算引擎 （Google Compute Engine） 当以 spec.type: LoadBalancer 方式配置服务时，该服务的防火墙将自动打开。 当以 spec.type: NodePort 方式配置服务时，该服务的防火墙在默认情况下不会打开。\n谷歌计算引擎的防火墙会进行记录 [他处] (https://cloud.google.com/compute/docs/networking#firewalls_1)。\n你也可以使用 gcloud 命令行工具自行添加防火墙：\ngcloud compute firewall-rules create my-rule --allow=tcp:\u0026lt;port\u0026gt; . note \u0026gt;}}\nGCE 防火墙是按照虚拟机来定义的，而不是通过ip地址来定义的。 这就意味着当你在防火墙上打开一个服务端口时，任何在那台虚拟机 IP 上的同一端口的服务 都有被外部访问的潜在可能。需要注意的是，对于其他的 kubernetes 服务而言，这不是问题。 因为他们监听的ip 地址与主机节点外部的 ip 地址并不相同。\n试想一下：\n 你建立一个（ ip 地址为1.2.3.4）端口为80的外部负载均衡器   因为在防火墙上为集群的所有节点都打开了 80 端口，所以外部的服务可以向你的 服务发送数据包。   最后你又虚拟机上的80端口启动 nginx 服务器（ip地址2.3.4.5）。 这个 nginx 在虚拟机的外部 IP 地址上也被暴露到了互联网上。  因此请务必小心，在谷歌计算引擎或者谷歌云原生引擎中打开防火墙时，可能无意间把其他服务也暴露给了互联网。\n. /note \u0026gt;}}\n"
},
{
	"uri": "https://lijun.in/concepts/scheduling-eviction/",
	"title": "😊 - 调度和驱逐(Scheduling and Eviction)",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lijun.in/tasks/access-kubernetes-api/",
	"title": "😝 - 扩展 Kubernetes",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lijun.in/reference/access-authn-authz/webhook/",
	"title": "Webhook 模式",
	"tags": [],
	"description": "",
	"content": "WebHook 是一种 HTTP 回调：某些条件下触发的 HTTP POST 请求；通过 HTTP POST 发送的简单事件通知。一个基于 web 应用实现的 WebHook 会在特定事件发生时把消息发送给特定的 URL。\n具体来说，当在判断用户权限时，Webhook 模式会使 Kubernetes 查询外部的 REST 服务。\n配置文件格式 Webhook 模式需要一个 HTTP 配置文件，通过 --authorization-webhook-config-file=SOME_FILENAME 的参数声明。\n配置文件的格式使用 kubeconfig。在文件中，\u0026ldquo;users\u0026rdquo; 代表着 API 服务器的 webhook，而 \u0026ldquo;cluster\u0026rdquo; 代表着远程服务。\n使用 HTTPS 客户端认证的配置例子：\n# Kubernetes API 版本 apiVersion: v1 # API 对象种类 kind: Config # clusters 代表远程服务。 clusters: - name: name-of-remote-authz-service cluster: # 对远程服务进行身份认证的 CA。 certificate-authority: /path/to/ca.pem # 远程服务的查询 URL。必须使用 \u0026#39;https\u0026#39;。 server: https://authz.example.com/authorize # users 代表 API 服务器的 webhook 配置 users: - name: name-of-api-server user: client-certificate: /path/to/cert.pem # webhook plugin 使用 cert client-key: /path/to/key.pem # cert 所对应的 key # kubeconfig 文件必须有 context。需要提供一个给 API 服务器。 current-context: webhook contexts: - context: cluster: name-of-remote-authz-service user: name-of-api-server name: webhook 请求载荷 在做认证决策时，API 服务器会 POST 一个 JSON 序列化的 authorization.k8s.io/v1beta1 SubjectAccessReview 对象来描述这个动作。这个对象包含了描述用户请求的字段，同时也包含了需要被访问资源或请求特征的具体信息。\n需要注意的是 webhook API 对象与其他 Kubernetes API 对象一样都同样都服从版本兼容规则。实施人员应该了解 beta 对象的更宽松的兼容性承诺，同时确认请求的 \u0026ldquo;apiVersion\u0026rdquo; 字段能被正确地反序列化。此外，API 服务器还必须启用 authorization.k8s.io/v1beta1 API 扩展组 (--runtime-config=authorization.k8s.io/v1beta1=true)。\n一个请求内容的例子：\n{ \u0026#34;apiVersion\u0026#34;: \u0026#34;authorization.k8s.io/v1beta1\u0026#34;, \u0026#34;kind\u0026#34;: \u0026#34;SubjectAccessReview\u0026#34;, \u0026#34;spec\u0026#34;: { \u0026#34;resourceAttributes\u0026#34;: { \u0026#34;namespace\u0026#34;: \u0026#34;kittensandponies\u0026#34;, \u0026#34;verb\u0026#34;: \u0026#34;get\u0026#34;, \u0026#34;group\u0026#34;: \u0026#34;unicorn.example.org\u0026#34;, \u0026#34;resource\u0026#34;: \u0026#34;pods\u0026#34; }, \u0026#34;user\u0026#34;: \u0026#34;jane\u0026#34;, \u0026#34;group\u0026#34;: [ \u0026#34;group1\u0026#34;, \u0026#34;group2\u0026#34; ] } } 期待远程服务填充请求的 status 字段并响应允许或禁止访问。响应主体的 spec 字段被忽略，可以省略。允许的响应将返回:\n{ \u0026#34;apiVersion\u0026#34;: \u0026#34;authorization.k8s.io/v1beta1\u0026#34;, \u0026#34;kind\u0026#34;: \u0026#34;SubjectAccessReview\u0026#34;, \u0026#34;status\u0026#34;: { \u0026#34;allowed\u0026#34;: true } } 为了禁止访问，有两种方法。\n在大多数情况下，第一种方法是首选方法，它指示授权 webhook 不允许或对请求\u0026quot;无意见\u0026rdquo;，但是，如果配置了其他授权者，则可以给他们机会允许请求。如果没有其他授权者，或者没有一个授权者，则该请求被禁止。webhook 将返回:\n{ \u0026#34;apiVersion\u0026#34;: \u0026#34;authorization.k8s.io/v1beta1\u0026#34;, \u0026#34;kind\u0026#34;: \u0026#34;SubjectAccessReview\u0026#34;, \u0026#34;status\u0026#34;: { \u0026#34;allowed\u0026#34;: false, \u0026#34;reason\u0026#34;: \u0026#34;user does not have read access to the namespace\u0026#34; } } 第二种方法立即拒绝其他配置的授权者进行短路评估。仅应由对集群的完整授权者配置有详细了解的 webhook 使用。webhook 将返回:\n{ \u0026#34;apiVersion\u0026#34;: \u0026#34;authorization.k8s.io/v1beta1\u0026#34;, \u0026#34;kind\u0026#34;: \u0026#34;SubjectAccessReview\u0026#34;, \u0026#34;status\u0026#34;: { \u0026#34;allowed\u0026#34;: false, \u0026#34;denied\u0026#34;: true, \u0026#34;reason\u0026#34;: \u0026#34;user does not have read access to the namespace\u0026#34; } } 对于非资源的路径访问是这么发送的:\n{ \u0026#34;apiVersion\u0026#34;: \u0026#34;authorization.k8s.io/v1beta1\u0026#34;, \u0026#34;kind\u0026#34;: \u0026#34;SubjectAccessReview\u0026#34;, \u0026#34;spec\u0026#34;: { \u0026#34;nonResourceAttributes\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;/debug\u0026#34;, \u0026#34;verb\u0026#34;: \u0026#34;get\u0026#34; }, \u0026#34;user\u0026#34;: \u0026#34;jane\u0026#34;, \u0026#34;group\u0026#34;: [ \u0026#34;group1\u0026#34;, \u0026#34;group2\u0026#34; ] } } 非资源类的路径包括：/api, /apis, /metrics, /resetMetrics, /logs, /debug, /healthz, /swagger-ui/, /swaggerapi/, /ui, 和 /version。客户端需要访问 /api, /api/*, /apis, /apis/*, 和 /version 以便 能发现服务器上有什么资源和版本。对于其他非资源类的路径访问在没有 REST API 访问限制的情况下拒绝。\n更多信息可以参考 authorization.v1beta1 API 对象和[webhook.go](https://github.com/kubernetes/kubernetes/blob/. param \u0026ldquo;githubbranch\u0026rdquo; \u0026gt;}}/staging/src/k8s.io/apiserver/plugin/pkg/authorizer/webhook/webhook.go)。\n"
},
{
	"uri": "https://lijun.in/tasks/run-application/horizontal-pod-autoscale-walkthrough/",
	"title": "Horizontal Pod Autoscaler演练",
	"tags": [],
	"description": "",
	"content": "Horizontal Pod Autoscaler 可以根据CPU利用率自动伸缩 replication controller、deployment 或者 replica set 中的Pod数量 （也可以基于其他应用程序提供的度量指标，目前这一功能处于 beta 版本）。\n本文将引导您了解如何为 php-apache 服务器配置和使用 Horizontal Pod Autoscaler。 更多 Horizontal Pod Autoscaler 的信息请参阅 Horizontal Pod Autoscaler user guide。\n. heading \u0026ldquo;prerequisites\u0026rdquo; %}} 本文示例需要一个1.2或者更高版本的可运行的 Kubernetes 集群以及 kubectl。 metrics-server 也需要部署到集群中， 它可以通过 resource metrics API 对外提供度量数据，Horizontal Pod Autoscaler 正是根据此 API 来获取度量数据，部署方法请参考 metrics-server 。 如果你正在使用GCE，按照 getting started on GCE guide 操作，metrics-server 会默认启动。\n如果需要为 Horizontal Pod Autoscaler 指定多种资源度量指标，您的 Kubernetes 集群以及 kubectl 至少需要达到1.6版本。 此外，如果要使用自定义度量指标，您的Kubernetes 集群还必须能够与提供这些自定义指标的API服务器通信。 最后，如果要使用与 Kubernetes 对象无关的度量指标，则 Kubernetes 集群版本至少需要达到1.10版本，同样，需要保证集群能够与提供这些外部指标的API服务器通信。 更多详细信息，请参阅Horizontal Pod Autoscaler user guide。\n第一步：运行 php-apache 服务器并暴露服务 为了演示 Horizontal Pod Autoscaler，我们将使用一个基于 php-apache 镜像的定制 Docker 镜像。 Dockerfile 内容如下：\nFROM php:5-apache ADD index.php /var/www/html/index.php RUN chmod a+rx index.php 它定义一个 index.php 页面来执行一些 CPU 密集型计算：\n\u0026lt;?php $x = 0.0001; for ($i = 0; $i \u0026lt;= 1000000; $i++) { $x += sqrt($x); } echo \u0026quot;OK!\u0026quot;; ?\u0026gt; 首先，我们先启动一个 deployment 来运行这个镜像并暴露一个服务:\nkubectl run php-apache --image=k8s.gcr.io/hpa-example --requests=cpu=200m --expose --port=80 service/php-apache created deployment.apps/php-apache created 创建 Horizontal Pod Autoscaler 现在，php-apache服务器已经运行，我们将通过 kubectl autoscale 命令创建 Horizontal Pod Autoscaler。 以下命令将创建一个 Horizontal Pod Autoscaler 用于控制我们上一步骤中创建的 deployment，使 Pod 的副本数量在维持在1到10之间。 大致来说，HPA 将通过增加或者减少 Pod 副本的数量（通过 Deployment ）以保持所有 Pod 的平均CPU利用率在50%以内 （由于每个 Pod 通过 [kubectl run](https://github.com/kubernetes/kubernetes/blob/. param \u0026ldquo;githubbranch\u0026rdquo; \u0026gt;}}/docs/user-guide/kubectl/kubectl_run.md) 申请了200 milli-cores CPU，所以50%的 CPU 利用率意味着平均 CPU 利用率为100 milli-cores）。 相关算法的详情请参阅here。\nkubectl autoscale deployment php-apache --cpu-percent=50 --min=1 --max=10 horizontalpodautoscaler.autoscaling/php-apache autoscaled 我们可以通过以下命令查看 autoscaler 的状态：\nkubectl get hpa NAME REFERENCE TARGET MINPODS MAXPODS REPLICAS AGE php-apache Deployment/php-apache/scale 0% / 50% 1 10 1 18s 请注意在上面的命令输出中，当前的CPU利用率是0%，这是由于我们尚未发送任何请求到服务器 （CURRENT 列显示了相应 deployment 所控制的所有 Pod 的平均 CPU 利用率）。\n增加负载 现在，我们将看到 autoscaler 如何对增加负载作出反应。 我们将启动一个容器，并通过一个循环向 php-apache 服务器发送无限的查询请求（请在另一个终端中运行以下命令）：\nkubectl run -i --tty load-generator --image=busybox /bin/sh Hit enter for command prompt while true; do wget -q -O- http://php-apache; done 在几分钟时间内，通过以下命令，我们可以看到CPU负载升高了：\nkubectl get hpa NAME REFERENCE TARGET CURRENT MINPODS MAXPODS REPLICAS AGE php-apache Deployment/php-apache/scale 305% / 50% 305% 1 10 1 3m 这时，由于请求增多，CPU利用率已经升至305%。 可以看到，deployment 的副本数量已经增长到了7：\nkubectl get deployment php-apache NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE php-apache 7 7 7 7 19m . note \u0026gt;}} 有时最终副本的数量可能需要几分钟才能稳定下来。 由于环境的差异，不同环境中最终的副本数量可能与本示例中的数量不同。 . /note \u0026gt;}}\n停止负载 我们将通过停止负载来结束我们的示例。\n在我们创建 busybox 容器的终端中，输入\u0026lt;Ctrl\u0026gt; + C来终止负载的产生。\n然后我们可以再次查看负载状态（等待几分钟时间）：\nkubectl get hpa NAME REFERENCE TARGET MINPODS MAXPODS REPLICAS AGE php-apache Deployment/php-apache/scale 0% / 50% 1 10 1 11m kubectl get deployment php-apache NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE php-apache 1 1 1 1 27m 这时，CPU利用率已经降到0，所以 HPA 将自动缩减副本数量至1。\n. note \u0026gt;}} 自动伸缩完成副本数量的改变可能需要几分钟的时间。 . /note \u0026gt;}}\n基于多项度量指标和自定义度量指标自动伸缩 利用autoscaling/v2beta2API版本，您可以在自动伸缩 php-apache 这个 Deployment 时引入其他度量指标。\n首先，获取autoscaling/v2beta2格式的 HorizontalPodAutoscaler 的YAML文件：\nkubectl get hpa.v2beta2.autoscaling -o yaml \u0026gt; /tmp/hpa-v2.yaml 在编辑器中打开/tmp/hpa-v2.yaml：\napiVersion: autoscaling/v2beta2 kind: HorizontalPodAutoscaler metadata: name: php-apache spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: php-apache minReplicas: 1 maxReplicas: 10 metrics: - type: Resource resource: name: cpu target: type: Utilization averageUtilization: 50 status: observedGeneration: 1 lastScaleTime: \u0026lt;some-time\u0026gt; currentReplicas: 1 desiredReplicas: 1 currentMetrics: - type: Resource resource: name: cpu current: averageUtilization: 0 averageValue: 0 需要注意的是，targetCPUUtilizationPercentage 字段已经被名为 metrics 的数组所取代。 CPU利用率这个度量指标是一个resource metric(资源度量指标)，因为它表示容器上指定资源的百分比。 除CPU外，您还可以指定其他资源度量指标。默认情况下，目前唯一支持的其他资源度量指标为内存。 只要metrics.k8s.io API存在，这些资源度量指标就是可用的，并且他们不会在不同的Kubernetes集群中改变名称。\n您还可以指定资源度量指标使用绝对数值，而不是百分比，你需要将target类型AverageUtilization替换成AverageValue，同时 将target.averageUtilization替换成target.averageValue并设定相应的值。\n还有两种其他类型的度量指标，他们被认为是custom metrics（自定义度量指标）： 即 Pod 度量指标和对象度量指标（pod metrics and object metrics）。 这些度量指标可能具有特定于集群的名称，并且需要更高级的集群监控设置。\n第一种可选的度量指标类型是 Pod 度量指标。这些指标从某一方面描述了Pod，在不同Pod之间进行平均，并通过与一个目标值比对来确定副本的数量。 它们的工作方式与资源度量指标非常相像，差别是它们仅支持target 类型为AverageValue。\nPod 度量指标通过如下代码块定义：\ntype: Pods pods: metric: name: packets-per-second target: type: AverageValue averageValue: 1k 第二种可选的度量指标类型是对象度量指标。相对于描述 Pod，这些度量指标用于描述一个在相同名字空间(namespace)中的其他对象。 请注意这些度量指标用于描述这些对象，并非从对象中获取。 对象度量指标支持的target类型包括Value和AverageValue。如果是Value类型，target值将直接与API返回的度量指标比较， 而AverageValue类型，API返回的度量指标将按照 Pod 数量拆分，然后再与target值比较。 下面的 YAML 文件展示了一个表示requests-per-second的度量指标。\ntype: Object object: metric: name: requests-per-second describedObject: apiVersion: networking.k8s.io/v1beta1 kind: Ingress name: main-route target: type: Value value: 2k 如果您指定了多个上述类型的度量指标，HorizontalPodAutoscaler 将会依次考量各个指标。 HorizontalPodAutoscaler 将会计算每一个指标所提议的副本数量，然后最终选择一个最高值。\n比如，如果您的监控系统能够提供网络流量数据，您可以通过kubectl edit命令将上述 Horizontal Pod Autoscaler 的定义更改为：\napiVersion: autoscaling/v2beta1 kind: HorizontalPodAutoscaler metadata: name: php-apache spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: php-apache minReplicas: 1 maxReplicas: 10 metrics: - type: Resource resource: name: cpu target: type: AverageUtilization averageUtilization: 50 - type: Pods pods: metric: name: packets-per-second targetAverageValue: 1k - type: Object object: metric: name: requests-per-second describedObject: apiVersion: networking.k8s.io/v1beta1 kind: Ingress name: main-route target: kind: Value value: 10k status: observedGeneration: 1 lastScaleTime: \u0026lt;some-time\u0026gt; currentReplicas: 1 desiredReplicas: 1 currentMetrics: - type: Resource resource: name: cpu current: averageUtilization: 0 averageValue: 0 - type: Object object: metric: name: requests-per-second describedObject: apiVersion: networking.k8s.io/v1beta1 kind: Ingress name: main-route current: value: 10k 然后，您的 HorizontalPodAutoscaler 将会尝试确保每个Pod的CPU利用率在50%以内，每秒能够服务1000个数据包请求， 并确保所有在Ingress后的Pod每秒能够服务的请求总数达到10000个。\n多个度量指标下伸缩 许多度量管道允许您通过名称或附加的_labels_来描述度量指标。对于所有非资源类型度量指标(pod、object和后面将介绍的external)， ，可以额外指定一个标签选择器。例如，如果你希望收集包含verb标签的http_requests度量指标， 你可以在 GET 请求中指定需要的度量指标，如下所示：\ntype: Object object: metric: name: `http_requests` selector: `verb=GET` 这个选择器使用与 Kubernetes 标签选择器相同的语法。 如果名称和标签选择器匹配到多个系列，监测管道会决定如何将多个系列合并成单个值。 选择器是附加的，它不会选择目标以外的对象（类型为Pods的目标和类型为Object的目标）。\n基于Kubernetes以外的度量指标伸缩 运行在 Kubernetes 上的应用程序可能需要基于与 Kubernetes 集群中的任何对象没有明显关系的度量指标进行自动伸缩， 例如那些描述不在 Kubernetes 任何 namespaces 服务的度量指标。\n使用外部的度量指标，需要了解你使用的监控系统，相关的设置与使用自定义试题指标类似。 External metrics 可以使用你的监控系统的任何指标来自动伸缩你的集群。你只需要在metric块中提供name 和 selector，同时将类型由Object改为External。 如果metricSelector匹配到多个度量指标，HorizontalPodAutoscaler 将会把它们加和。 External metrics 同时支持Value和AverageValue类型，这与Object类型的度量指标相同。\n例如，如果你的应用程序处理主机上的消息队列， 为了让每30个任务有1个worker，你可以将下面的内容添加到 HorizontalPodAutoscaler 的配置中。\n- type: External external: metric: name: queue_messages_ready selector: \u0026#34;queue=worker_tasks\u0026#34; target: type: AverageValue averageValue: 30 如果可能，还是推荐 custom metric 而不是 external metrics，因为这便于让系统管理员加固 custom metrics API。 而 external metrics API 可以允许访问所有的度量指标，当暴露这些服务时，系统管理员需要仔细考虑这个问题。\n附录：Horizontal Pod Autoscaler状态条件 当使用autoscaling/v2beta2格式的 HorizontalPodAutoscaler 时，您将可以看到 Kubernetes 为 HorizongtalPodAutoscaler 设置的状态条件（status conditions）。 这些状态条件可以显示当前 HorizontalPodAutoscaler 是否能够执行伸缩以及是否受到一定的限制。\nstatus.conditions字段展示了这些状态条件。 可以通过kubectl describe hpa命令查看当前影响 HorizontalPodAutoscaler 的各种状态条件信息：\nkubectl describe hpa cm-test Name: cm-test Namespace: prom Labels: \u0026lt;none\u0026gt; Annotations: \u0026lt;none\u0026gt; CreationTimestamp: Fri, 16 Jun 2017 18:09:22 +0000 Reference: ReplicationController/cm-test Metrics: ( current / target ) \u0026#34;http_requests\u0026#34; on pods: 66m / 500m Min replicas: 1 Max replicas: 4 ReplicationController pods: 1 current / 1 desired Conditions: Type Status Reason Message ---- ------ ------ ------- AbleToScale True ReadyForNewScale the last scale time was sufficiently old as to warrant a new scale ScalingActive True ValidMetricFound the HPA was able to successfully calculate a replica count from pods metric http_requests ScalingLimited False DesiredWithinRange the desired replica count is within the acceptable range Events: 对于上面展示的这个 HorizontalPodAutoscaler，我们可以看出有若干状态条件处于健康状态。 首先，AbleToScale 表明 HPA 是否可以获取和更新伸缩信息，以及是否存在阻止伸缩的各种回退条件。 其次，ScalingActive 表明HPA是否被启用（即目标的副本数量不为零） 以及是否能够完成伸缩计算。 当这一状态为 False 时，通常表明获取度量指标存在问题。 最后一个条件 ScalingLimitted 表明所需伸缩的值被 HorizontalPodAutoscaler 所定义的最大或者最小值所限制（即已经达到最大或者最小伸缩值）。 这通常表明您可能需要调整 HorizontalPodAutoscaler 所定义的最大或者最小副本数量的限制了。\n附录：Quantities HorizontalPodAutoscaler 和 metrics api 中的所有的度量指标使用 Kubernetes 中称为 quantity （）殊整数表示。 例如，数量10500m用十进制表示为10.5。 如果可能的话，metrics api 将返回没有后缀的整数，否则返回以千分单位的数量。 这意味着您可能会看到您的度量指标在1和1500m之间波动，或者在十进制记数法中的1和1.5。 更多信息，请参阅度量术语\n附录：其他可能的情况 使用YAML文件创建 autoscaler 除了使用 kubectl autoscale 命令，也可以文件创建 HorizontalPodAutoscaler ：\n. codenew file=\u0026quot;application/hpa/php-apache.yaml\u0026rdquo; \u0026gt;}}\n使用如下命令创建 autoscaler：\nkubectl create -f https://k8s.io/examples/application/hpa/php-apache.yaml horizontalpodautoscaler.autoscaling/php-apache created "
},
{
	"uri": "https://lijun.in/tasks/configure-pod-container/pull-image-private-registry/",
	"title": "从私有仓库拉取镜像",
	"tags": [],
	"description": "",
	"content": "本文介绍如何使用 Secret 从私有的 Docker 镜像仓库或代码仓库拉取镜像来创建 Pod。\n. heading \u0026ldquo;prerequisites\u0026rdquo; %}}  . include \u0026ldquo;task-tutorial-prereqs.md\u0026rdquo; \u0026gt;}} . version-check \u0026gt;}}  您需要 Docker ID 和密码来进行本练习。\n登录 Docker 镜像仓库 在个人电脑上，要想拉取私有镜像必须在镜像仓库上进行身份验证。\ndocker login 当提示时，输入 Docker 用户名和密码。\n登录过程会创建或更新保存有授权令牌的 config.json 文件。\n查看 config.json 文件：\ncat ~/.docker/config.json 输出结果包含类似于以下内容的部分：\n{ \u0026#34;auths\u0026#34;: { \u0026#34;https://index.docker.io/v1/\u0026#34;: { \u0026#34;auth\u0026#34;: \u0026#34;c3R...zE2\u0026#34; } } } . note \u0026gt;}}\n如果使用 Docker 凭证仓库，则不会看到 auth 条目，看到的将是以仓库名称作为值的 credsStore 条目。 . /note \u0026gt;}}\n在集群中创建保存授权令牌的 Secret Kubernetes 集群使用 docker-registry 类型的 Secret 来通过容器仓库的身份验证，进而提取私有映像。\n创建 Secret，命名为 regcred：\nkubectl create secret docker-registry regcred --docker-server=\u0026lt;your-registry-server\u0026gt; --docker-username=\u0026lt;your-name\u0026gt; --docker-password=\u0026lt;your-pword\u0026gt; --docker-email=\u0026lt;your-email\u0026gt; 在这里：\n \u0026lt;your-registry-server\u0026gt; 是你的私有 Docker 仓库全限定域名（FQDN）。(参考 https://index.docker.io/v1/ 中关于 DockerHub 的部分) \u0026lt;your-name\u0026gt; 是你的 Docker 用户名。 \u0026lt;your-pword\u0026gt; 是你的 Docker 密码。 \u0026lt;your-email\u0026gt; 是你的 Docker 邮箱。  这样您就成功地将集群中的 Docker 凭据设置为名为 regcred 的 Secret。\n检查 Secret regcred 要了解你创建的 regcred Secret 的内容，可以用 YAML 格式进行查看：\nkubectl get secret regcred --output=yaml 输出和下面类似：\napiVersion: v1 data: .dockerconfigjson: eyJodHRwczovL2luZGV4L ... J0QUl6RTIifX0= kind: Secret metadata: ... name: regcred ... type: kubernetes.io/dockerconfigjson .dockerconfigjson 字段的值是 Docker 凭据的 base64 表示。\n要了解 dockerconfigjson 字段中的内容，请将 Secret 数据转换为可读格式：\nkubectl get secret regcred --output=\u0026#34;jsonpath={.data.\\.dockerconfigjson}\u0026#34; | base64 --decode 输出和下面类似：\n{\u0026#34;auths\u0026#34;:{\u0026#34;yourprivateregistry.com\u0026#34;:{\u0026#34;username\u0026#34;:\u0026#34;janedoe\u0026#34;,\u0026#34;password\u0026#34;:\u0026#34;xxxxxxxxxxx\u0026#34;,\u0026#34;email\u0026#34;:\u0026#34;jdoe@example.com\u0026#34;,\u0026#34;auth\u0026#34;:\u0026#34;c3R...zE2\u0026#34;}}} 要了解 auth 字段中的内容，请将 base64 编码过的数据转换为可读格式：\necho \u0026#34;c3R...zE2\u0026#34; | base64 --decode 输出结果中，用户名和密码用 : 链接，类似下面这样：\njanedoe:xxxxxxxxxxx 注意，Secret 数据包含与本地 ~/.docker/config.json 文件类似的授权令牌。\n这样您就已经成功地将 Docker 凭据设置为集群中的名为 regcred 的 Secret。\n创建一个使用您的 Secret 的 Pod 下面是一个 Pod 配置文件，它需要访问 regcred 中的 Docker 凭据：\n. codenew file=\u0026quot;pods/private-reg-pod.yaml\u0026rdquo; \u0026gt;}}\n下载上述文件：\nwget -O my-private-reg-pod.yaml https://k8s.io/examples/pods/private-reg-pod.yaml 在my-private-reg-pod.yaml 文件中，使用私有仓库的镜像路径替换 \u0026lt;your-private-image\u0026gt;，例如：\njanedoe/jdoe-private:v1 要从私有仓库拉取镜像，Kubernetes 需要凭证。 配置文件中的 imagePullSecrets 字段表明 Kubernetes 应该通过名为 regcred 的 Secret 获取凭证。\n创建使用了你的 Secret 的 Pod，并检查它是否正常运行：\nkubectl create -f my-private-reg-pod.yaml kubectl get pod private-reg . heading \u0026ldquo;whatsnext\u0026rdquo; %}}  进一步了解 Secrets。 进一步了解 使用私有仓库。 参考 kubectl create secret docker-registry。 参考 [Secret](/docs/reference/generated/kubernetes-api/. param \u0026ldquo;version\u0026rdquo; \u0026gt;}}/#secret-v1-core)。 参考 [PodSpec](/docs/reference/generated/kubernetes-api/. param \u0026ldquo;version\u0026rdquo; \u0026gt;}}/#podspec-v1-core) 中的 imagePullSecrets 字段 。  "
},
{
	"uri": "https://lijun.in/tasks/access-application-cluster/list-all-running-container-images/",
	"title": "列出集群中所有运行容器的镜像",
	"tags": [],
	"description": "",
	"content": "本文展示如何使用 kubectl 来列出集群中所有运行 pod 的容器的镜像\n. heading \u0026ldquo;prerequisites\u0026rdquo; %}} . include \u0026ldquo;task-tutorial-prereqs.md\u0026rdquo; \u0026gt;}} . version-check \u0026gt;}}\n在本练习中，您将使用 kubectl 来获取集群中运行的所有 Pod，并格式化输出来提取每个 pod 中的容器列表。\n列出所有命名空间下的所有容器  使用 kubectl get pods --all-namespaces 获取所有命名空间下的所有 Pod 使用 -o jsonpath={..image} 来格式化输出，以仅包含容器镜像名称。 这将以递归方式从返回的 json 中解析出 image 字段。  参阅 jsonpath reference 来获取更多关于如何使用 jsonpath 的信息。   使用标准化工具来格式化输出：tr, sort, uniq  使用 tr 以用换行符替换空格 使用 sort 来对结果进行排序 使用 uniq 来聚合镜像计数    kubectl get pods --all-namespaces -o jsonpath=\u0026#34;{..image}\u0026#34; |\\ tr -s \u0026#39;[[:space:]]\u0026#39; \u0026#39;\\n\u0026#39; |\\ sort |\\ uniq -c 上面的命令将递归获取所有返回项目的名为 image 的字段。\n作为替代方案，可以使用 Pod 的镜像字段的绝对路径。这确保即使字段名称重复的情况下也能检索到正确的字段，例如，特定项目中的许多字段都称为 name：\nkubectl get pods --all-namespaces -o jsonpath=\u0026#34;{.items[*].spec.containers[*].image}\u0026#34; jsonpath 解释如下：\n .items[*]: 对于每个返回的值 .spec: 获取 spec .containers[*]: 对于每个容器 .image: 获取镜像  . note \u0026gt;}}\n注意： 按名字获取单个 Pod 时，例如 kubectl get pod nginx，路径的 .items[*] 部分应该省略，因为返回的是一个 Pod 而不是一个项目列表。 . /note \u0026gt;}}\n列出 Pod 中的容器 可以使用 range 操作进一步控制格式化，以单独操作每个元素。\nkubectl get pods --all-namespaces -o=jsonpath=\u0026#39;{range .items[*]}{\u0026#34;\\n\u0026#34;}{.metadata.name}{\u0026#34;:\\t\u0026#34;}{range .spec.containers[*]}{.image}{\u0026#34;, \u0026#34;}{end}{end}\u0026#39; |\\ sort 列出以 label 过滤后的 Pod 的所有容器 要获取匹配特定标签的 Pod，请使用 -l 参数。以下匹配仅与标签 app=nginx 相符的 Pod。\nkubectl get pods --all-namespaces -o=jsonpath=\u0026#34;{..image}\u0026#34; -l app=nginx 列出以命名空间过滤后的 Pod 的所有容器 要获取匹配特定命名空间的 Pod，请使用 namespace 参数。以下仅匹配 kube-system 命名空间下的 Pod。\nkubectl get pods --namespace kube-system -o jsonpath=\u0026#34;{..image}\u0026#34; 使用 go-template 代替 jsonpath 来获取容器 作为 jsonpath 的替代，Kubectl 支持使用 go-templates 来格式化输出：\nkubectl get pods --all-namespaces -o go-template --template=\u0026#34;{{range .items}}{{range .spec.containers}}{{.image}} {{end}}{{end}}\u0026#34; . heading \u0026ldquo;whatsnext\u0026rdquo; %}} 参考  Jsonpath 参考指南 Go template 参考指南  "
},
{
	"uri": "https://lijun.in/concepts/cluster-administration/controller-metrics/",
	"title": "控制器管理器指标",
	"tags": [],
	"description": "",
	"content": "控制器管理器指标为控制器管理器的性能和健康提供了重要的观测手段。\n什么是控制器管理器度量 控制器管理器指标为控制器管理器的性能和健康提供了重要的观测手段。 这些度量包括常见的 Go 语言运行时度量，比如 go_routine 计数，以及控制器特定的度量，比如 etcd 请求延迟或 云提供商（AWS、GCE、OpenStack）的 API 延迟，这些参数可以用来测量集群的健康状况。\n从 Kubernetes 1.7 版本开始，详细的云提供商指标可用于 GCE、AWS、Vsphere 和 OpenStack 的存储操作。 这些度量可用于监视持久卷操作的健康状况。\n例如，在 GCE 中这些指标叫做：\ncloudprovider_gce_api_request_duration_seconds { request = \u0026quot;instance_list\u0026quot;} cloudprovider_gce_api_request_duration_seconds { request = \u0026quot;disk_insert\u0026quot;} cloudprovider_gce_api_request_duration_seconds { request = \u0026quot;disk_delete\u0026quot;} cloudprovider_gce_api_request_duration_seconds { request = \u0026quot;attach_disk\u0026quot;} cloudprovider_gce_api_request_duration_seconds { request = \u0026quot;detach_disk\u0026quot;} cloudprovider_gce_api_request_duration_seconds { request = \u0026quot;list_disk\u0026quot;} 配置 在集群中，控制器管理器指标可从它所在的主机上的 http://localhost:10252/metrics 中获得。\n这些指标是以 prometheus 格式 发出的，是人类可读的。\n在生产环境中，您可能想配置 prometheus 或其他一些指标收集工具，以定期收集这些指标数据，并将它们应用到某种时间序列数据库中。\n"
},
{
	"uri": "https://lijun.in/setup/production-environment/tools/kubeadm/self-hosting/",
	"title": "配置您的 kubernetes 集群以自托管控制平台",
	"tags": [],
	"description": "",
	"content": "自托管 Kubernetes 控制平台 kubeadm 允许您实验性地创建 self-hosted Kubernetes 控制平面。 这意味着 API 服务器，控制管理器和调度程序之类的关键组件将通过配置 Kubernetes API 以 DaemonSet pods 的身份运行，而不是通过静态文件将 static pods 在 kubelet 中配置。\n要创建自托管集群，请参见 kubeadm alpha 自托管枢纽 命令。\n警告 . caution \u0026gt;}}\n此功能将您的集群设置为不受支持的状态，从而使 kubeadm 无法再管理您的集群。 这包括 kubeadm 升级 。 . /caution \u0026gt;}}\n 1.8及更高版本中的自托管功能有一些重要限制。 特别是，自托管集群在没有人工干预的情况下_无法从控制平面节点的重新启动中恢复_ 。   默认情况下，自托管的控制平面 Pod 依赖于从 hostPath 卷加载的凭据。 除初始创建外，这些凭据不由 kubeadm 管理。   控制平面的自托管部分不包括 etcd，后者仍作为静态 Pod 运行。  处理 自托管引导过程记录在 kubeadm 设计文档 中。\n总而言之，kubeadm alpha 自托管 的工作原理如下：\n 等待此引导静态控制平面运行且良好。 这与没有自我托管的 kubeadm init 过程相同。   使用静态控制平面 Pod 清单来构造一组 DaemonSet 清单，这些清单将运行自托管的控制平面。 它还会在必要时修改这些清单，例如添加新的秘密卷。   在 kube-system 名称空间中创建 DaemonSets ，并等待生成的 Pod 运行。   自托管 Pod 运行后，将删除其关联的静态 Pod，然后 kubeadm 继续安装下一个组件。 这将触发 kubelet 停止那些静态 Pod 。   当原始静态控制平面停止时，新的自托管控制平面能够绑定到侦听端口并变为活动状态。  "
},
{
	"uri": "https://lijun.in/tasks/tls/",
	"title": "😝 - TLS",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lijun.in/tasks/run-application/configure-pdb/",
	"title": "指定应用程序的中断预算（Disruption Budget）",
	"tags": [],
	"description": "",
	"content": "本文展示了如何限制应用程序的并发中断数量，在允许集群管理员管理集群节点的同时保证高可用。\n. heading \u0026ldquo;prerequisites\u0026rdquo; %}}  用户是 Kubernetes 集群中有高可用需求的应用的所有者。   用户应了解如何部署 无状态应用 和/或 有状态应用。   用户应当已经阅读过关于 Pod 中断 的文档。   用户应当与集群所有者或服务提供者确认其遵从 Pod 中断预算（Pod Disruption Budgets）的规则。  用 PodDisruptionBudget 来保护应用  确定想要使用 PodDisruptionBudget (PDB) 来保护的应用。 考虑应用对中断的反应。 以 YAML 文件形式定义 PDB 。 通过 YAML 文件创建 PDB 对象。  确定要保护的应用 用户想要保护通过内置的 Kubernetes 控制器指定的应用，这是最常见的使用场景：\n Deployment ReplicationController ReplicaSet StatefulSet  在这种情况下，在控制器的 .spec.selector 字段中做记录，并在 PDB 的 .spec.selector 字段中加入同样的选择器。\n用户也可以用 PDB 来保护不受上述控制器控制的 pod，或任意组（arbitrary groups）的 pod， 但是正如 任意控制器和选择器 中描述的，这里存在一些限制。\n考虑应用对中断的反应 确定在自发中断时，多少实例可以在短时间内同时关闭。\n 无状态的前端：  关注：不能降低服务能力 10% 以上。  解决方案：例如，使用 PDB，指定其 minAvailable 值为 90%。     单实例有状态应用：  关注：不要在不通知的情况下终止该应用。  可能的解决方案 1：不使用 PDB，并忍受偶尔的停机。 可能的解决方案 2：设置 maxUnavailable=0 的 PDB。意为（Kubernetes 范畴之外的） 集群操作人员需要在终止应用前与用户协商，协商后准备停机，然后删除 PDB 表示准备中断，后续再重新创建。     多实例有状态应用， 如 Consul、ZooKeeper 或 etcd：  关注：不要将实例数量减少至低于仲裁规模（below quorum），否则将写入失败。  可能的解决方案 1：设置 maxUnavailable 值为 1 (适用于不同规模的应用)。 可能的解决方案 2：设置 minAvailable 值为仲裁规模（例如规模为 5 时设置为 3）。 (允许每次更多的中断)。     可重新启动的批处理任务：  关注： 自发中断的情况下，需要确保任务完成。  可能的解决方案：不创建 PDB。 任务控制器会创建一个替换的 pod。      指定百分比时的舍入逻辑 minAvailable 或 maxUnavailable 的值可以表示为整数或百分比。\n 指定整数时，它表示许多Pod。 例如，如果将minAvailable设置为10，那么即使在中断期间，也必须始终有10个Pod可用。   通过将值设置为百分比的字符串表示形式（例如“50％”）来指定百分比时，它表示总 Pod 数的百分比。例如，如果将 \u0026ldquo;minUnavailable\u0026rdquo; 设置为“50％”，则只有50％的 Pod 可以中断。  如果将值指定为百分比，则可能无法映射到确切数量的 Pod 。例如，如果您有 7 个 Pod ，并且你将 minAvailable 设置为 \u0026quot;50％\u0026quot;，这不清楚是 3 个 Pod 或 4 个 Pod 必须可用。 Kubernetes 向上取整到最接近的整数，因此在这种情况下，必须有 4 个 Pod 。 您可以检查控制此行为的代码。\n指定 PodDisruptionBudget 一个 PodDisruptionBudget 有 3 个字段：\n 标签选择器 .spec.selector ，用于指定其所作用的 pod 集合， 该字段为必须字段。 .spec.minAvailable 表示驱逐后仍然保证可用的 pod 数量。即使因此影响到 pod 驱逐（即该条件在和 pod 驱逐发生冲突时优先保证）。 minAvailable 值可以是绝对值，也可以是百分比。 .spec.maxUnavailable （Kubernetes 1.7 及更高的版本中可用）表示驱逐后允许不可用的 pod 的最大数量。 其值可以是绝对值或是百分比。  . note \u0026gt;}}\n对于1.8及更早的版本：当你用 kubectl 命令行工具创建 PodDisruptionBudget对象时，如果既未指定 minAvailable 也未指定 maxUnavailable， 则 minAvailable 字段有一个默认值1。 . /note \u0026gt;}}\n用户在同一个 PodDisruptionBudget 中只能够指定 maxUnavailable 和 minAvailable 中的一个。maxUnavailable 只能够用于控制存在相应控制器的 pod 的驱逐（即不受控制器控制的 pod 不在 maxUnavailable 控制范围内）。在下面的示例中， “所需副本” 指的是相应控制器的 scale， 控制器对 PodDisruptionBudget 所选择的 pod 进行管理。\n示例 1：设置 minAvailable 值为 5 的情况下，驱逐时需保证 PodDisruptionBudget 的 selector 选中的 pod 中 5 个 或 5 个以上处于健康状态。\n示例 2：设置 minAvailable 值为 30% 的情况下，驱逐时需保证 pod 所需副本的至少 30% 处于健康状态。\n示例 3：设置 maxUnavailable 值为 5 的情况下，驱逐时需保证所需副本中最多 5 个处于不可用状态。\nExample 4: With a maxUnavailable of 30%, evictions are allowed as long as no more than 30% of the desired replicas are unhealthy. 示例4： 设置 maxUnavailable 值为 30% 的情况下，驱逐时需保证所需副本中最多 30% 处于不可用状态。\n在典型用法中，中断预算会被用于一个控制器管理的一组 pod 中——例如：一个 ReplicaSet 或 StatefulSet 中的 pod。\n. note \u0026gt;}}\n注意：中断预算并不能真正保证指定数量/百分比的 pod 一直处于运行状态。例如： 当 pod 集合的 规模处于预算指定的最小值时，承载集合中某个 pod 的节点发生了故障，这样就导致集合中可用 pod 的 数量低于预算指定值。预算只能够针对自发的驱逐提供保护，而不能针对所有 pod 不可用的诱因。 . /note \u0026gt;}}\n设置 maxUnavailable 值为 0% （或 0 ）或设置 minAvailable 值为 100% （或等于副本数） 可能会 阻塞节点，导致资源耗尽。按照 PodDisruptionBudget 的语义，这是允许的。\n用户可以在下面看到 pod 中断预算定义的示例，它们与带有 app: zookeeper 标签的 pod 相匹配：\n使用 minAvailable 的PDB 示例：\n. codenew file=\u0026quot;policy/zookeeper-pod-disruption-budget-minavailable.yaml\u0026rdquo; \u0026gt;}}\n使用 maxUnavailable 的 PDB 示例（Kubernetes 1.7 或更高的版本）：\n. codenew file=\u0026quot;policy/zookeeper-pod-disruption-budget-maxunavailable.yaml\u0026rdquo; \u0026gt;}}\n例如，如果上述 zk-pdb 选择的是一个规格为 3 的 StatefulSet 对应的 pod，那么上面两种规范的含义完全相同。 推荐使用 maxUnavailable ，因为它自动响应控制器副本数量的变化。\nCreate the PDB object 创建 PDB 对象 用户可以通过类似 kubectl create -f mypdb.yaml 的命令来创建 PDB。\nPDB 对象无法更新，必须删除后重新创建。\nCheck the status of the PDB 检查 PDB 的状态 使用 kubectl 来确认 PDB 被创建。\n假设用户的名字空间下没有匹配 app: zookeeper 的 pod，用户会看到类似下面的信息：\nkubectl get poddisruptionbudgets NAME MIN-AVAILABLE ALLOWED-DISRUPTIONS AGE zk-pdb 2 0 7s 假设有匹配的 pod (比如说 3 个), 那么用户会看到类似下面的信息：\nkubectl get poddisruptionbudgets NAME MIN-AVAILABLE ALLOWED-DISRUPTIONS AGE zk-pdb 2 1 7s ALLOWED-DISRUPTIONS 值非 0 意味着中断控制器已经感知到相应的 pod， 对匹配的 pod 进行统计，并更新了 PDB 的状态。\n用户可以通过以下命令获取更多 PDB 状态相关信息：\nkubectl get poddisruptionbudgets zk-pdb -o yaml apiVersion: policy/v1beta1 kind: PodDisruptionBudget metadata: creationTimestamp: 2017-08-28T02:38:26Z generation: 1 name: zk-pdb … status: currentHealthy: 3 desiredHealthy: 3 disruptedPods: null disruptionsAllowed: 1 expectedPods: 3 observedGeneration: 1 任意控制器和选择器 如果用户只使用与内置的应用控制器（Deployment、ReplicationController、ReplicaSet 和 StatefulSet） 对应的 PDB，也就是 PDB 的选择器与 控制器的选择器相匹配，那么可以跳过这一节。\n用户可以使用这样的 PDB：它对应的 pod 可能由其他类型的控制器控制，可能由 \u0026ldquo;operator\u0026rdquo; 控制， 也可能为“裸的（不受控制器控制）” pod，但该类 PDB 存在以下限制：\n only .spec.minAvailable can be used, not .spec.maxUnavailable. only an integer value can be used with .spec.minAvailable, not a percentage. 只能够使用 .spec.minAvailable ，而不能够使用 .spec.maxUnavailable。 只能够使用整数作为 .spec.minAvailable 的值，而不能使用百分比。  用户可以令选择器选择一个内置控制器所控制 pod 的子集或父集。然而，当名字空间下存在多个 PDB 时， 用户必须小心，保证 PDB 的选择器之间不重叠。\n"
},
{
	"uri": "https://lijun.in/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/",
	"title": "配置存活、就绪和启动探测器",
	"tags": [],
	"description": "",
	"content": "这篇文章介绍如何给容器配置存活、就绪和启动探测器。\nkubelet 使用存活探测器来知道什么时候要重启容器。例如，存活探测器可以捕捉到死锁（应用程序在运行，但是无法继续执行后面的步骤）。这样的情况下重启容器有助于让应用程序在有问题的情况下更可用。\nkubelet 使用就绪探测器可以知道容器什么时候准备好了并可以开始接受请求流量， 当一个 Pod 内的所有容器都准备好了，才能把这个 Pod 看作就绪了。这种信号的一个用途就是控制哪个 Pod 作为 Service 的后端。在 Pod 还没有准备好的时候，会从 Service 的负载均衡器中被剔除的。\nkubelet 使用启动探测器可以知道应用程序容器什么时候启动了。如果配置了这类探测器，就可以控制容器在启动成功后再进行存活性和就绪检查，确保这些存活、就绪探测器不会影响应用程序的启动。这可以用于对慢启动容器进行存活性检测，避免它们在启动运行之前就被杀掉。\n. heading \u0026ldquo;prerequisites\u0026rdquo; %}} . include \u0026ldquo;task-tutorial-prereqs.md\u0026rdquo; \u0026gt;}} . version-check \u0026gt;}}\n定义存活命令 许多长时间运行的应用程序最终会过渡到断开的状态，除非重新启动，否则无法恢复。Kubernetes 提供了存活探测器来发现并补救这种情况。\n在这篇练习中，会创建一个 Pod，其中运行一个基于 k8s.gcr.io/busybox 镜像的容器。下面是这个 Pod 的配置文件。\n. codenew file=\u0026quot;pods/probe/exec-liveness.yaml\u0026rdquo; \u0026gt;}}\n在这个配置文件中，可以看到 Pod 中只有一个容器。periodSeconds 字段指定了 kubelet 应该每 5 秒执行一次存活探测。initialDelaySeconds 字段告诉 kubelet 在执行第一次探测前应该等待 5 秒。kubelet 在容器内执行命令 cat /tmp/healthy 来进行探测。如果命令执行成功并且返回值为 0，kubelet 就会认为这个容器是健康存活的。如果这个命令返回非 0 值，kubelet 会杀死这个容器并重新启动它。\n当容器启动时，执行如下的命令：\n/bin/sh -c \u0026#34;touch /tmp/healthy; sleep 30; rm -rf /tmp/healthy; sleep 600\u0026#34; 这个容器生命的前 30 秒， /tmp/healthy 文件是存在的。所以在这最开始的 30 秒内，执行命令 cat /tmp/healthy 会返回成功码。30 秒之后，执行命令 cat /tmp/healthy 就会返回失败码。\n创建 Pod：\nkubectl apply -f https://k8s.io/examples/pods/probe/exec-liveness.yaml 在 30 秒内，查看 Pod 的事件：\nkubectl describe pod liveness-exec 输出结果显示还没有存活探测器失败：\nFirstSeen LastSeen Count From SubobjectPath Type Reason Message --------- -------- ----- ---- ------------- -------- ------ ------- 24s 24s 1 {default-scheduler } Normal Scheduled Successfully assigned liveness-exec to worker0 23s 23s 1 {kubelet worker0} spec.containers{liveness} Normal Pulling pulling image \u0026#34;k8s.gcr.io/busybox\u0026#34; 23s 23s 1 {kubelet worker0} spec.containers{liveness} Normal Pulled Successfully pulled image \u0026#34;k8s.gcr.io/busybox\u0026#34; 23s 23s 1 {kubelet worker0} spec.containers{liveness} Normal Created Created container with docker id 86849c15382e; Security:[seccomp=unconfined] 23s 23s 1 {kubelet worker0} spec.containers{liveness} Normal Started Started container with docker id 86849c15382e 35 秒之后，再来看 Pod 的事件：\nkubectl describe pod liveness-exec 在输出结果的最下面，有信息显示存活探测器失败了，这个容器被杀死并且被重建了。\nFirstSeen LastSeen Count From SubobjectPath Type Reason Message --------- -------- ----- ---- ------------- -------- ------ ------- 37s 37s 1 {default-scheduler } Normal Scheduled Successfully assigned liveness-exec to worker0 36s 36s 1 {kubelet worker0} spec.containers{liveness} Normal Pulling pulling image \u0026#34;k8s.gcr.io/busybox\u0026#34; 36s 36s 1 {kubelet worker0} spec.containers{liveness} Normal Pulled Successfully pulled image \u0026#34;k8s.gcr.io/busybox\u0026#34; 36s 36s 1 {kubelet worker0} spec.containers{liveness} Normal Created Created container with docker id 86849c15382e; Security:[seccomp=unconfined] 36s 36s 1 {kubelet worker0} spec.containers{liveness} Normal Started Started container with docker id 86849c15382e 2s 2s 1 {kubelet worker0} spec.containers{liveness} Warning Unhealthy Liveness probe failed: cat: can\u0026#39;t open \u0026#39;/tmp/healthy\u0026#39;: No such file or directory 再等另外 30 秒，检查看这个容器被重启了：\nkubectl get pod liveness-exec 输出结果显示 RESTARTS 的值增加了 1。\nNAME READY STATUS RESTARTS AGE liveness-exec 1/1 Running 1 1m 定义一个存活态 HTTP 请求接口 另外一种类型的存活探测方式是使用 HTTP GET 请求。下面是一个 Pod 的配置文件，其中运行一个基于 k8s.gcr.io/liveness 镜像的容器。\n. codenew file=\u0026quot;pods/probe/http-liveness.yaml\u0026rdquo; \u0026gt;}}\n在这个配置文件中，可以看到 Pod 也只有一个容器。periodSeconds 字段指定了 kubelet 每隔 3 秒执行一次存活探测。initialDelaySeconds 字段告诉 kubelet 在执行第一次探测前应该等待 3 秒。kubelet 会向容器内运行的服务（服务会监听 8080 端口）发送一个 HTTP GET 请求来执行探测。如果服务上 /healthz 路径下的处理程序返回成功码，则 kubelet 认为容器是健康存活的。如果处理程序返回失败码，则 kubelet 会杀死这个容器并且重新启动它。\n任何大于或等于 200 并且小于 400 的返回码标示成功，其它返回码都标示失败。\n可以在这里看服务的源码 server.go。\n容器存活的最开始 10 秒中，/healthz 处理程序返回一个 200 的状态码。之后处理程序返回 500 的状态码。\nhttp.HandleFunc(\u0026#34;/healthz\u0026#34;, func(w http.ResponseWriter, r *http.Request) { duration := time.Now().Sub(started) if duration.Seconds() \u0026gt; 10 { w.WriteHeader(500) w.Write([]byte(fmt.Sprintf(\u0026#34;error: %v\u0026#34;, duration.Seconds()))) } else { w.WriteHeader(200) w.Write([]byte(\u0026#34;ok\u0026#34;)) } }) kubelet 在容器启动之后 3 秒开始执行健康检测。所以前几次健康检查都是成功的。但是 10 秒之后，健康检查会失败，并且 kubelet 会杀死容器再重新启动容器。\n创建一个 Pod 来测试 HTTP 的存活检测：\nkubectl apply -f https://k8s.io/examples/pods/probe/http-liveness.yaml 10 秒之后，通过看 Pod 事件来检测存活探测器已经失败了并且容器被重新启动了。\nkubectl describe pod liveness-http 在 1.13（包括 1.13版本）之前的版本中，如果在 Pod 运行的节点上设置了环境变量 http_proxy（或者 HTTP_PROXY），HTTP 的存活探测会使用这个代理。在 1.13 之后的版本中，设置本地的 HTTP 代理环境变量不会影响 HTTP 的存活探测。\n定义 TCP 的存活探测 第三种类型的存活探测是使用 TCP 套接字。通过配置，kubelet 会尝试在指定端口和容器建立套接字链接。如果能建立链接，这个容器就被看作是健康的，如果不能则这个容器就被看作是有问题的。\n. codenew file=\u0026quot;pods/probe/tcp-liveness-readiness.yaml\u0026rdquo; \u0026gt;}}\n如你所见，TCP 检测的配置和 HTTP 检测非常相似。下面这个例子同时使用就绪和存活探测器。kubelet 会在容器启动 5 秒后发送第一个就绪探测。这会尝试连接 goproxy 容器的 8080 端口。如果探测成功，这个 Pod 会被标记为就绪状态，kubelet 将继续每隔 10 秒运行一次检测。\n除了就绪探测，这个配置包括了一个存活探测。kubelet 会在容器启动 15 秒后进行第一次存活探测。就像就绪探测一样，会尝试连接 goproxy 容器的 8080 端口。如果存活探测失败，这个容器会被重新启动。\nkubectl apply -f https://k8s.io/examples/pods/probe/tcp-liveness-readiness.yaml 15 秒之后，通过看 Pod 事件来检测存活探测器：\nkubectl describe pod goproxy 使用命名端口 对于 HTTP 或者 TCP 存活检测可以使用命名的[容器端口](/docs/reference/generated/kubernetes-api/. param \u0026ldquo;version\u0026rdquo; \u0026gt;}}/#containerport-v1-core)。\nports: - name: liveness-port containerPort: 8080 hostPort: 8080 livenessProbe: httpGet: path: /healthz port: liveness-port 使用启动探测器保护慢启动容器 有时候，会有一些现有的应用程序在启动时需要较多的初始化时间。要不影响对引起探测死锁的快速响应，这种情况下，设置存活探测参数是要技巧的。技巧就是使用一个命令来设置启动探测，针对HTTP 或者 TCP 检测，可以通过设置 failureThreshold * periodSeconds 参数来保证有足够长的时间应对糟糕情况下的启动时间。\n所以，前面的例子就变成了：\nports: - name: liveness-port containerPort: 8080 hostPort: 8080 livenessProbe: httpGet: path: /healthz port: liveness-port failureThreshold: 1 periodSeconds: 10 startupProbe: httpGet: path: /healthz port: liveness-port failureThreshold: 30 periodSeconds: 10 幸亏有启动探测，应用程序将会有最多 5 分钟(30 * 10 = 300s) 的时间来完成它的启动。 一旦启动探测成功一次，存活探测任务就会接管对容器的探测，对容器死锁可以快速响应。 如果启动探测一直没有成功，容器会在 300 秒后被杀死，并且根据 restartPolicy 来设置 Pod 状态。\n定义就绪探测器 有时候，应用程序会暂时性的不能提供通信服务。例如，应用程序在启动时可能需要加载很大的数据或配置文件，或是启动后要依赖等待外部服务。在这种情况下，既不想杀死应用程序，也不想给它发送请求。Kubernetes 提供了就绪探测器来发现并缓解这些情况。容器所在 Pod 上报还未就绪的信息，并且不接受通过 Kubernetes Service 的流量。\n. note \u0026gt;}}\n就绪探测器在容器的整个生命周期中保持运行状态。 . /note \u0026gt;}}\n就绪探测器的配置和存活探测器的配置相似。唯一区别就是要使用 readinessProbe 字段，而不是 livenessProbe 字段。\nreadinessProbe: exec: command: - cat - /tmp/healthy initialDelaySeconds: 5 periodSeconds: 5 HTTP 和 TCP 的就绪探测器配置也和存活探测器的配置一样的。\n就绪和存活探测可以在同一个容器上并行使用。两者都用可以确保流量不会发给还没有准备好的容器，并且容器会在它们失败的时候被重新启动。\n配置探测器 . comment \u0026gt;}}\n最后，本节的一些内容可以放到某个概念主题里。 . /comment \u0026gt;}}\n[探测器](/zh/docs/reference/generated/kubernetes-api/. param \u0026ldquo;version\u0026rdquo; \u0026gt;}}/#probe-v1-core)有很多配置字段，可以使用这些字段精确的控制存活和就绪检测的行为：\n initialDelaySeconds：容器启动后要等待多少秒后存活和就绪探测器才被初始化，默认是 0 秒，最小值是 0。 periodSeconds：执行探测的时间间隔（单位是秒）。默认是 10 秒。最小值是 1。 timeoutSeconds：探测的超时后等待多少秒。默认值是 1 秒。最小值是 1。 successThreshold：探测器在失败后，被视为成功的最小连续成功数。默认值是 1。存活探测的这个值必须是 1。最小值是 1。 failureThreshold：当 Pod 启动了并且探测到失败，Kubernetes 的重试次数。存活探测情况下的放弃就意味着重新启动容器。就绪探测情况下的放弃 Pod 会被打上未就绪的标签。默认值是 3。最小值是 1。  [HTTP 探测器](/zh/docs/reference/generated/kubernetes-api/. param \u0026ldquo;version\u0026rdquo; \u0026gt;}}/#httpgetaction-v1-core)可以在 httpGet 上配置额外的字段：\n host：连接使用的主机名，默认是 Pod 的 IP。也可以在 HTTP 头中设置 “Host” 来代替。 scheme ：用于设置连接主机的方式（HTTP 还是 HTTPS）。默认是 HTTP。 path：访问 HTTP 服务的路径。 httpHeaders：请求中自定义的 HTTP 头。HTTP 头字段允许重复。 port：访问容器的端口号或者端口名。如果数字必须在 1 ～ 65535 之间。  对于 HTTP 探测，kubelet 发送一个 HTTP 请求到指定的路径和端口来执行检测。除非 httpGet 中的 host 字段设置了，否则 kubelet 默认是给 Pod 的 IP 地址发送探测。如果 scheme 字段设置为了 HTTPS，kubelet 会跳过证书验证发送 HTTPS 请求。大多数情况下，不需要设置host 字段。这里有个需要设置 host 字段的场景，假设容器监听 127.0.0.1，并且 Pod 的 hostNetwork 字段设置为了 true。那么 httpGet 中的 host 字段应该设置为 127.0.0.1。可能更常见的情况是如果 Pod 依赖虚拟主机，你不应该设置 host 字段，而是应该在 httpHeaders 中设置 Host。\n对于一次 TCP 探测，kubelet 在节点上（不是在 Pod 里面）建立探测连接，这意味着你不能在 host 参数上配置 service name，因为 kubelet 不能解析 service name。\n. heading \u0026ldquo;whatsnext\u0026rdquo; %}}  进一步了解容器探测器。  参考  [Pod](/docs/reference/generated/kubernetes-api/. param \u0026ldquo;version\u0026rdquo; \u0026gt;}}/#pod-v1-core) [容器](/docs/reference/generated/kubernetes-api/. param \u0026ldquo;version\u0026rdquo; \u0026gt;}}/#container-v1-core) [探测器](/docs/reference/generated/kubernetes-api/. param \u0026ldquo;version\u0026rdquo; \u0026gt;}}/#probe-v1-core)  "
},
{
	"uri": "https://lijun.in/tasks/access-application-cluster/configure-dns-cluster/",
	"title": "为集群配置 DNS",
	"tags": [],
	"description": "",
	"content": "Kubernetes 提供 DNS 集群插件，大多数支持的环境默认情况下都会启用。\n有关如何为 Kubernetes 集群配置 DNS 的详细信息，请参阅 Kubernetes DNS 插件示例.\n"
},
{
	"uri": "https://lijun.in/tasks/configure-pod-container/assign-pods-nodes/",
	"title": "将 Pod 分配给节点",
	"tags": [],
	"description": "",
	"content": "此页面显示如何将 Kubernetes Pod 分配给 Kubernetes 集群中的特定节点。\n. heading \u0026ldquo;prerequisites\u0026rdquo; %}} . include \u0026ldquo;task-tutorial-prereqs.md\u0026rdquo; \u0026gt;}} . version-check \u0026gt;}}\n给节点添加标签   列出集群中的节点\n kubectl get nodes  输出类似如下：\n NAME STATUS AGE VERSION worker0 Ready 1d v1.6.0+fff5156 worker1 Ready 1d v1.6.0+fff5156 worker2 Ready 1d v1.6.0+fff5156      选择其中一个节点，为它添加标签：\n kubectl label nodes \u0026lt;your-node-name\u0026gt; disktype=ssd  \u0026lt;your-node-name\u0026gt; 是你选择的节点的名称。\n    验证你选择的节点是否有 disktype=ssd 标签：\n kubectl get nodes --show-labels  输出类似如下：\n NAME STATUS AGE VERSION LABELS worker0 Ready 1d v1.6.0+fff5156 ...,disktype=ssd,kubernetes.io/hostname=worker0 worker1 Ready 1d v1.6.0+fff5156 ...,kubernetes.io/hostname=worker1 worker2 Ready 1d v1.6.0+fff5156 ...,kubernetes.io/hostname=worker2  在前面的输出中，你可以看到 worker0 节点有 disktype=ssd 标签。\n  创建一个调度到你选择的节点的 pod 该 pod 配置文件描述了一个拥有节点选择器 disktype: ssd 的 pod。这表明该 pod 将被调度到 有 disktype=ssd 标签的节点。\n. codenew file=\u0026quot;pods/pod-nginx.yaml\u0026rdquo; \u0026gt;}}\n  使用该配置文件去创建一个 pod，该 pod 将被调度到你选择的节点上：\n kubectl create -f https://k8s.io/examples/pods/pod-nginx.yaml      验证 pod 是不是运行在你选择的节点上：\n kubectl get pods --output=wide  输出类似如下：\n NAME READY STATUS RESTARTS AGE IP NODE nginx 1/1 Running 0 13s 10.200.0.4 worker0    . heading \u0026ldquo;whatsnext\u0026rdquo; %}} 了解更多关于 标签和选择器。\n"
},
{
	"uri": "https://lijun.in/tasks/federation/",
	"title": "😝 - 联邦 - 在多个集群上运行一个应用",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lijun.in/tasks/federation/set-up-coredns-provider-federation/",
	"title": "将 CoreDNS 设置为联邦集群的 DNS 提供者",
	"tags": [],
	"description": "",
	"content": ". deprecationfilewarning \u0026gt;}} . include \u0026ldquo;federation-deprecation-warning-note.md\u0026rdquo; \u0026gt;}} . /deprecationfilewarning \u0026gt;}}\n此页面显示如何配置和部署 CoreDNS，将其用作联邦集群的 DNS 提供者\n. heading \u0026ldquo;objectives\u0026rdquo; %}}  配置和部署 CoreDNS 服务器 使用 CoreDNS 作为 dns 提供者设置联邦 在 nameserver 查找链中设置 CoreDNS 服务器  . heading \u0026ldquo;prerequisites\u0026rdquo; %}}  你需要有一个正在运行的 Kubernetes 集群（作为主机集群引用）。请参阅入门指南，了解平台的安装说明。 必须在联邦的集群成员中支持 LoadBalancer 服务，用来支持跨联邦集群的 CoreDNS 服务发现。  部署 CoreDNS 和 etcd 图表 CoreDNS 可以部署在各种配置中。下面解释的是一个参考，可以根据平台和联邦集群的需要进行调整。\n为了部署 CoreDNS，我们将利用图表。 CoreDNS 将部署 etcd 作为后端，并且应该预先安装。etcd 也可以使用图表进行部署。下面显示了部署 etcd 的说明。\nhelm install --namespace my-namespace --name etcd-operator stable/etcd-operator helm upgrade --namespace my-namespace --set cluster.enabled=true etcd-operator stable/etcd-operator  注意：etcd 默认部署配置可以被覆盖，适合主机集群。\n部署成功后，可以使用主机集群中的 http://etcd-cluster.my-namespace:2379 端点访问 etcd。\n应该定制 CoreDNS 默认配置适应联邦。 下面显示的是 Values.yaml，它覆盖了 CoreDNS 图表上的默认配置参数。\nisClusterService: false serviceType: \u0026#34;LoadBalancer\u0026#34; plugins: kubernetes: enabled: false etcd: enabled: true zones: - \u0026#34;example.com.\u0026#34; endpoint: \u0026#34;http://etcd-cluster.my-namespace:2379\u0026#34; 以上配置文件需要说明：\n isClusterService 指定是否应该将 CoreDNS 部署为集群服务，这是默认值。 你需要将其设置为 false，以便将 CoreDNS 部署为 Kubernetes 应用程序服务。 serviceType 指定为核心用户创建的 Kubernetes 服务的类型。 你需要选择 LoadBalancer 或 NodePort，以便在 Kubernetes 集群之外访问 CoreDNS 服务。 禁用 plugins.kubernetes，默认情况下通过设置 plugins.kubernetes.enabled 为 false。 启用 plugins.etcd，通过设置 plugins.etcd.enabled 为 true。 通过设置 plugins.etcd.zones 来配置 CoreDNS 具有权威性的 DNS 域（联邦域）。如上所示。 通过设置 plugins.etcd.endpoint 来配置早期部署的 etcd 端点  现在部署 CoreDNS 来运行\nhelm install --namespace my-namespace --name coredns -f Values.yaml stable/coredns  验证 etcd 和 CoreDNS，pod 都按预期运行。\n使用 CoreDNS 作为 DNS 提供者部署联邦 可以使用 kubefed init 部署联邦控制平面。通过指定两个附加参数，可以选择 CoreDNS 作为 DNS 提供者。\n--dns-provider=coredns --dns-provider-config=coredns-provider.conf  coredns-provider.conf 的格式如下：\n[Global] etcd-endpoints = http://etcd-cluster.my-namespace:2379 zones = example.com. coredns-endpoints = \u0026lt;coredns-server-ip\u0026gt;:\u0026lt;port\u0026gt;   etcd-endpoints 是访问 etcd 的端点。 zones 是 CoreDNS 具有权威性的联邦域，它与 kubefed init 的 \u0026ndash;dns-zone-name 参数相同。 coredns-endpoints 是访问 CoreDNS 服务器的端点。这是从 v1.7 开始引入的一个可选参数。  . note \u0026gt;}}\nCoreDNS 配置中的 plugins.etcd.zones 和 kubefed init 的 --dns-zone-name 参数应该匹配。 . /note \u0026gt;}}\n在 nameserver resolv.conf 链中设置 CoreDNS 服务器 . note \u0026gt;}}\n下面的部分只适用于 v1.7 之前的版本，如果 coredns-endpoint 参数是 在 coredns-provider.conf 中配置的，就会自动处理。\n. /note \u0026gt;}}\n一旦部署了联邦控制平面并将联邦集群连接到联邦， 你需要将 CoreDNS 服务器添加到所有联邦集群中 pod 的 nameserver resolv.conf 链，因为这个自托管的 CoreDNS 服务器是不可公开发现的。 这可以通过在 kube-dns 部署中将下面的行添加到 dnsmasq 容器的参数中来实现。\n--server=/example.com./\u0026lt;CoreDNS endpoint\u0026gt;  将上面的 example.com 替换为联邦域。\n现在联邦集群已经为跨集群服务发现做好了准备！\n"
},
{
	"uri": "https://lijun.in/tasks/configure-pod-container/configure-pod-initialization/",
	"title": "配置 Pod 初始化",
	"tags": [],
	"description": "",
	"content": "本文介绍在应用容器运行前，怎样利用 Init 容器初始化 Pod。\n. heading \u0026ldquo;prerequisites\u0026rdquo; %}} . include \u0026ldquo;task-tutorial-prereqs.md\u0026rdquo; \u0026gt;}} . version-check \u0026gt;}}\n创建一个包含 Init 容器的 Pod 本例中您将创建一个包含一个应用容器和一个 Init 容器的 Pod。Init 容器在应用容器启动前运行完成。\n下面是 Pod 的配置文件：\n. codenew file=\u0026quot;pods/init-containers.yaml\u0026rdquo; \u0026gt;}}\n配置文件中，您可以看到应用容器和 Init 容器共享了一个卷。\nInit 容器将共享卷挂载到了 /work-dir 目录，应用容器将共享卷挂载到了 /usr/share/nginx/html 目录。 Init 容器执行完下面的命令就终止：\nwget -O /work-dir/index.html http://kubernetes.io  请注意 Init 容器在 nginx 服务器的根目录写入 index.html。\n创建 Pod：\nkubectl create -f https://k8s.io/examples/pods/init-containers.yaml  检查 nginx 容器运行正常：\nkubectl get pod init-demo  结果表明 nginx 容器运行正常：\nNAME READY STATUS RESTARTS AGE init-demo 1/1 Running 0 1m  通过 shell 进入 init-demo Pod 中的 nginx 容器：\nkubectl exec -it init-demo -- /bin/bash  在 shell 中，发送个 GET 请求到 nginx 服务器：\nroot@nginx:~# apt-get update root@nginx:~# apt-get install curl root@nginx:~# curl localhost  结果表明 nginx 正在为 Init 容器编写的 web 页面服务：\n\u0026lt;!Doctype html\u0026gt; \u0026lt;html id=\u0026quot;home\u0026quot;\u0026gt; \u0026lt;head\u0026gt; ... \u0026quot;url\u0026quot;: \u0026quot;http://kubernetes.io/\u0026quot;}\u0026lt;/script\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; ... \u0026lt;p\u0026gt;Kubernetes is open source giving you the freedom to take advantage ...\u0026lt;/p\u0026gt; ...  . heading \u0026ldquo;whatsnext\u0026rdquo; %}}  进一步了解 相同 Pod 中的容器间的通信。 进一步了解 Init 容器。 进一步了解 卷。 进一步了解 Init 容器排错。  "
},
{
	"uri": "https://lijun.in/tasks/manage-daemon/",
	"title": "😝 - 管理集群守护进程",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lijun.in/tasks/configure-pod-container/attach-handler-lifecycle-event/",
	"title": "为容器的生命周期事件设置处理函数",
	"tags": [],
	"description": "",
	"content": "这个页面将演示如何为容器的生命周期事件挂接处理函数。Kubernetes 支持 postStart 和 preStop 事件。 当一个容器启动后，Kubernetes 将立即发送 postStart 事件；在容器被终结之前， Kubernetes 将发送一个 preStop 事件。\n. heading \u0026ldquo;prerequisites\u0026rdquo; %}} . include \u0026ldquo;task-tutorial-prereqs.md\u0026rdquo; \u0026gt;}} . version-check \u0026gt;}}\n定义 postStart 和 preStop 处理函数 在本练习中，你将创建一个包含一个容器的 Pod，该容器为 postStart 和 preStop 事件提供对应的处理函数。\n下面是对应 Pod 的配置文件\n. codenew file=\u0026quot;pods/lifecycle-events.yaml\u0026rdquo; \u0026gt;}}\n在上述配置文件中，你可以看到 postStart 命令在容器的 /usr/share 目录下写入文件 message。 命令 preStop 负责优雅地终止 nginx 服务。当因为失效而导致容器终止时，这一处理方式很有用。```\n创建 Pod：\nkubectl apply -f https://k8s.io/examples/pods/lifecycle-events.yaml  验证 Pod 中的容器已经运行：\nkubectl get pod lifecycle-demo  使用 shell 连接到你的 Pod 里的容器：\nkubectl exec -it lifecycle-demo -- /bin/bash  在 shell 中，验证 postStart 处理函数创建了 message 文件：\nroot@lifecycle-demo:/# cat /usr/share/message  命令行输出的是 postStart 处理函数所写入的文本\nHello from the postStart handler  讨论 Kubernetes 在容器创建后立即发送 postStart 事件。然而，postStart 处理函数的调用不保证早于容器的入口点（entrypoint） 的执行。postStart 处理函数与容器的代码是异步执行的，但 Kubernetes 的容器管理逻辑会一直阻塞等待 postStart 处理函数执行完毕。只有 postStart 处理函数执行完毕，容器的状态才会变成 RUNNING。\nKubernetes 在容器结束前立即发送 preStop 事件。除非 Pod 宽限期限超时，Kubernetes 的容器管理逻辑 会一直阻塞等待 preStop 处理函数执行完毕。更多的相关细节，可以参阅 Pods 的结束。\n. note \u0026gt;}} Kubernetes 只有在 Pod 结束（Terminated） 的时候才会发送 preStop 事件，这意味着在 Pod 完成（Completed） 时 preStop 的事件处理逻辑不会被触发。这个限制在 issue #55087 中被追踪。 . /note \u0026gt;}}\n. heading \u0026ldquo;whatsnext\u0026rdquo; %}}  进一步了解容器生命周期回调。 进一步了解Pod 的生命周期。  参考  [Lifecycle](/docs/reference/generated/kubernetes-api/. param \u0026ldquo;version\u0026rdquo; \u0026gt;}}/#lifecycle-v1-core) [Container](/docs/reference/generated/kubernetes-api/. param \u0026ldquo;version\u0026rdquo; \u0026gt;}}/#container-v1-core) 参阅 [PodSpec](/docs/reference/generated/kubernetes-api/. param \u0026ldquo;version\u0026rdquo; \u0026gt;}}/#podspec-v1-core) 中关于terminationGracePeriodSeconds 的部分  "
},
{
	"uri": "https://lijun.in/tasks/federation/federation-service-discovery/",
	"title": "使用联合服务来实现跨集群的服务发现",
	"tags": [],
	"description": "",
	"content": ". deprecationfilewarning \u0026gt;}} . include \u0026ldquo;federation-deprecation-warning-note.md\u0026rdquo; \u0026gt;}} . /deprecationfilewarning \u0026gt;}}\n本指南说明了如何使用 Kubernetes 联合服务跨多个 Kubernetes 集群部署通用服务。这样可以轻松实现 Kubernetes 应用程序的跨集群服务发现和可用区容错。\n联合服务的创建与传统服务几乎相同 Kubernetes Services 即通过 API 调用来指定所需的服务属性。对于联合服务，此 API 调用定向到联合身份验证 API 接入点，而不是 Kubernetes 集群 API 接入点。联合服务的 API 与传统 Kubernetes 服务的 API 是 100% 兼容的。\n创建后，联合服务会自动:\n 在基础集群联合的每个集群中创建匹配的 Kubernetes 服务, 监视那些服务 \u0026ldquo;分片\u0026rdquo;(及其驻留的集群)的运行状况,以及 在公共 DNS 提供商(例如 Google Cloud DNS 或 AWS Route 53)中管理一组 DNS 记录,即使在集群可用区域中断的情况下，也能确保您联合服务的客户端始终可以无缝地定位合适的健康服务接入点。  如果存在健康的分片，联合 Kubernetes 集群(即 Pods )中的客户端将自动在其中找到联合服务的本地分片集群或者集群中最接近的健康分片;如果不存在，则使用最接近的其他集群的健康分片。\n. toc \u0026gt;}}\n. heading \u0026ldquo;prerequisites\u0026rdquo; %}} . include \u0026ldquo;task-tutorial-prereqs.md\u0026rdquo; \u0026gt;}} . version-check \u0026gt;}}\n前提 本指南假设您已经安装 Kubernetes 联合集群。如果没有，则访问 联合集群管理指南了解如何建立联合集群(或让您的集群管理员为您执行此操作)。其他教程，例如 Kelsey Hightower 编写的 案例或许有用。\n一般而言，您应该还有基本的 Kubernetes 工作常识，特别是 Services。\n混合云功能 Kubernetes 联合集群需要可以在不同的云提供商(例如 Google Cloud 或 AWS)和本地(例如 OpenStack)环境中运行。只需在合适的云提供商创建所需的所有集群，向您的联合身份验证 API 服务器注册每个集群的 API 接入点和凭据(有关详细信息，请参见 联合管理指南)。\n此后，您的应用程序和服务可以跨越不同的集群和云提供商，如下所述。\n创建联合服务 常见方式创建，例如:\nkubectl --context=federation-cluster create -f services/nginx.yaml \u0026lsquo;\u0026ndash;context=federation-cluster\u0026rsquo; 标志通知 kubectl 使用合适的凭据将请求提交到联合 API 接入点。如果您尚未配置此类上下文，请访问 联合管理指南或者 管理教程找出解决方案。\n如上所述，联合服务将自动创建并在所有集群中维护匹配的 Kubernetes 服务以支持联合。\n您可以通过核对每个基础集群的信息来验证这一点, 例如:\nkubectl --context=gce-asia-east1a get services nginx NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE nginx ClusterIP 10.63.250.98 104.199.136.89 80/TCP 9m 以上假设您有一个名为 \u0026lsquo;gce-asia-east1a\u0026rsquo; 上下文在客户端中为该区域中的集群配置。基础服务的名称和命名空间将自动与您在上面创建的联合服务匹配(如果服务的名称和命名空间与集群中任意一个服务器的名称和命名空间相同，它们将被联合并更新为符合您的规范联合服务 - 无论哪种方式，最终结果都是相同的)。\n联合服务的状态将自动反映基础 Kubernetes 服务的实时状态，例如:\nkubectl --context=federation-cluster describe services nginx Name: nginx Namespace: default Labels: run=nginx Annotations: \u0026lt;none\u0026gt; Selector: run=nginx Type: LoadBalancer IP: 10.63.250.98 LoadBalancer Ingress: 104.197.246.190, 130.211.57.243, 104.196.14.231, 104.199.136.89, ... Port: http 80/TCP Endpoints: \u0026lt;none\u0026gt; Session Affinity: None Events: \u0026lt;none\u0026gt; . note \u0026gt;}} 联合服务的 \u0026lsquo;LoadBalancer Ingress\u0026rsquo; 地址与所有基础 Kubernetes 服务的 \u0026lsquo;LoadBalancer Ingress\u0026rsquo; 地址相对应(一旦分配了这些地址，这可能需要几秒钟)。为了使服务分片之间的集群和云提供商之间的网络正常工作，您的服务需要具有一个外部可见的 IP 地址。Service Type:Loadbalancer。尽管存在其他选项(例如 外部 IP),但通常会使用 Service 类型:Loadbalancer。 . /note \u0026gt;}}\n还要注意，我们尚未设置任何后端 Pod 来接收定向到这些地址的网络流量(即 \u0026lsquo;Service Endpoints\u0026rsquo;)，因此联合服务尚未将它们视为健康的服务分片，并且尚未将其地址添加到联合服务的 DNS 记录中(稍后在此方面进行介绍)。\n添加后端 pods 为了使基础服务分片健康，我们需要在它们后面添加后端 Pod。当前，这是直接针对基础集群 API 接入点完成的(尽管将来，联合服务将能够通过单个命令为您完成所有这些操作，从而省去了麻烦)。例如，在13个基础集群中创建后端 Pod:\nfor CLUSTER in asia-east1-c asia-east1-a asia-east1-b \\  europe-west1-d europe-west1-c europe-west1-b \\  us-central1-f us-central1-a us-central1-b us-central1-c \\  us-east1-d us-east1-c us-east1-b do kubectl --context=$CLUSTER run nginx --image=nginx:1.11.1-alpine --port=80 done 注意，kubectl run 会自动添加 run=nginx 标签，这是将后端 pod 与其服务关联起来所必需的。\n验证公共 DNS 记录 一旦上述 Pod 成功启动并开始侦听连接，Kubernetes 就会将它们报告为该集群中服务的正常接入点(通过自动运行状况检查)。反过来，联合集群会将这些服务 \u0026lsquo;分片\u0026rsquo; 中的每一个视为健康，并通过自动配置相应的公共 DNS 记录将其置于服务中。您可以使用首选接口访问已配置的 DNS 提供程序来进行验证。例如，如果您的联邦配置为使用 Google Cloud DNS 和托管 DNS 域名 \u0026lsquo;example.com\u0026rsquo;。\ngcloud dns managed-zones describe example-dot-com creationTime: '2016-06-26T18:18:39.229Z' description: Example domain for Kubernetes Cluster Federation dnsName: example.com. id: '3229332181334243121' kind: dns#managedZone name: example-dot-com nameServers: - ns-cloud-a1.googledomains.com. - ns-cloud-a2.googledomains.com. - ns-cloud-a3.googledomains.com. - ns-cloud-a4.googledomains.com. gcloud dns record-sets list --zone example-dot-com NAME TYPE TTL DATA example.com. NS 21600 ns-cloud-e1.googledomains.com., ns-cloud-e2.googledomains.com. example.com. OA 21600 ns-cloud-e1.googledomains.com. cloud-dns-hostmaster.google.com. 1 21600 3600 1209600 300 nginx.mynamespace.myfederation.svc.example.com. A 180 104.197.246.190, 130.211.57.243, 104.196.14.231, 104.199.136.89,... nginx.mynamespace.myfederation.svc.us-central1-a.example.com. A 180 104.197.247.191 nginx.mynamespace.myfederation.svc.us-central1-b.example.com. A 180 104.197.244.180 nginx.mynamespace.myfederation.svc.us-central1-c.example.com. A 180 104.197.245.170 nginx.mynamespace.myfederation.svc.us-central1-f.example.com. CNAME 180 nginx.mynamespace.myfederation.svc.us-central1.example.com. nginx.mynamespace.myfederation.svc.us-central1.example.com. A 180 104.197.247.191, 104.197.244.180, 104.197.245.170 nginx.mynamespace.myfederation.svc.asia-east1-a.example.com. A 180 130.211.57.243 nginx.mynamespace.myfederation.svc.asia-east1-b.example.com. CNAME 180 nginx.mynamespace.myfederation.svc.asia-east1.example.com. nginx.mynamespace.myfederation.svc.asia-east1-c.example.com. A 180 130.211.56.221 nginx.mynamespace.myfederation.svc.asia-east1.example.com. A 180 130.211.57.243, 130.211.56.221 nginx.mynamespace.myfederation.svc.europe-west1.example.com. CNAME 180 nginx.mynamespace.myfederation.svc.example.com. nginx.mynamespace.myfederation.svc.europe-west1-d.example.com. CNAME 180 nginx.mynamespace.myfederation.svc.europe-west1.example.com. ... etc. . note \u0026gt;}} 如果您的联邦配置为使用 AWS Route53，则可以使用类似的 AWS 工具，例如:\naws route53 list-hosted-zones 和\naws route53 list-resource-record-sets --hosted-zone-id Z3ECL0L9QLOVBX . /note \u0026gt;}}\n无论使用哪种 DNS 提供商，任何 DNS 查询工具(例如 \u0026lsquo;dig\u0026rsquo; 或者 \u0026lsquo;nslookup\u0026rsquo;)都将允许您查看联邦会为您创建的记录。请注意，您应该将这些工具直接指向您的 DNS 提供商(例如 dig @ ns-cloud-e1.googledomains.com ...),或者由于中间 DNS 服务器进行了缓存，因此在看到更新之前，预计延迟会按照配置的 TTL 顺序(默认为 180 秒)进行。\n有关上述示例的一些注意事项  请注意，每个具有至少一个正常后端端点的服务分片都有一条正常(\u0026lsquo;A\u0026rsquo;)记录。例如，在 us-central1-a 中，104.197.247.191 是该区域中服务分片的外部 IP 地址，在 asia-east1-a 中，该地址是 130.211.56.221。 同样，也有区域 \u0026lsquo;A\u0026rsquo; 记录，其中包括该区域中所有健康的分片。例如，\u0026lsquo;us-central1\u0026rsquo;。这些区域记录对于没有特定区域首选项的客户很有用，并且作为下文所述的自动位置和故障转移机制的基础。 对于当前没有健康后端终结点的区域，将使用 CNAME (\u0026lsquo;Canonical Name\u0026rsquo;) 记录将这些查询别名(自动重定向)到下一个最接近的健康区域。在此示例中，us-central1-f 中的服务分片当前没有健康的后端端点(即Pods)，因此已创建 CNAME 记录来自动将查询重定向到该区域中的其他分片(在本例中为 us-central1)。 类似地，如果封闭区域中不存在健康分片，则搜索将进一步进行。在 europe-west1-d 可用性区域中,没有健康的后端,因此查询将重定向到更广阔的 Europe-west1 区域(也没有健康的后端),然后再重定向到全局的健康地址集(\u0026lsquo;nginx.mynamespace.myfederation.svc.example.com.')。  上面的 DNS 记录集由联邦服务系统自动与全球所有服务分片的当前健康状况保持同步。DNS 解析库(由所有客户端调用)自动遍历 \u0026lsquo;CNAME\u0026rsquo; 与 \u0026lsquo;A\u0026rsquo; 记录的层次结构，以返回正确健康的 IP 地址集。然后，客户端可以选择任何返回的地址来启动网络连接(并根据需要自动故障转移到其他等效地址之一)。\n发现联合服务 从联合集群内的 Pods 来发现 默认情况下，Kubernetes 集群预先配置了本地集群 DNS 服务器(\u0026lsquo;KubeDNS\u0026rsquo;)以及智能构建的 DNS 搜索路径，这些路径共同确保由 Pods 内部运行软件发出的 DNS 查询如 \u0026ldquo;myservice\u0026rdquo;, \u0026ldquo;myservice.mynamespace\u0026rdquo;,\u0026ldquo;bobsservice.othernamespace\u0026rdquo; 等，会自动扩展并正确解析为本地集群运行服务的相应服务 IP。\n随着联合服务和跨集群服务发现的引入，该概念已扩展到涵盖在全球集群联盟中任何其他集群中运行的 Kubernetes 服务。要利用此扩展范围，您可以使用形式稍有不同的 DNS 名称，形式为 \u0026quot;\u0026lt;servicename\u0026gt;.\u0026lt;namespace\u0026gt;.\u0026lt;federationname\u0026gt;\u0026quot; 来解析联合服务。例如，您可以使用 myservice.mynamespace.myfederation。使用不同的 DNS 名称还可以避免现有应用程序意外穿越跨区域或跨区域网络，并且可能招致不必要的网络费用或延迟，而无需您明确选择采取这种行为。\n因此，使用上面的 NGINX 示例服务和刚才描述的联合服务 DNS 名称表单，让我们考虑一个示例:us-central1-f 可用性区域集群中的 Pod 需要联系我们的 NGINX 服务。现在，可以使用服务的联合 DNS 名称，而不是使用服务的传统集群本地 DNS 名称(\u0026quot;nginx.mynamespace\u0026quot; 会自动扩展为 \u0026quot;nginx.mynamespace.svc.cluster.local\u0026quot;)。无论位于世界何处，它都会自动扩展并解析为我的 NGINX 服务中最接近的健康分片。如果本地集群中存在健康的分片，则将返回该服务的集群本地(通常为10.x.y.z)的 IP 地址(由集群本地的 KubeDNS)。这几乎完全等同于非联合服务解析(几乎是因为 KubeDNS 实际上为本地联合服务返回了 CNAME 和 A 记录，但是应用程序将忽略这种微小的技术差异)。\n但是，如果服务在本地集群中不存在(或者存在但没有正常的后端 Pod),则 DNS 查询会自动扩展为 \u0026quot;nginx.mynamespace.myfederation.svc.us-central1-f.example.com\u0026quot;(也就是说，从逻辑上 \u0026ldquo;找到最接近我可用区的一个分片的外部 IP\u0026rdquo;)。此扩展由 KubeDNS 自动执行，它返回关联的 CNAME 记录。这将导致在上面的示例中自动遍历 DNS 记录的层次结构，并最终到达本地 us-central1 区域中联合服务的外部 IP 之一(即 104.197.247.191, 104.197.244.180 或 104.197.245.170 )。\n当然，可以通过明确地指定合适的 DNS 名称而不依赖于自动 DNS 扩展，在 Pod 本地的可用区域和可用区域之外的区域中明确地定位服务分片。例如，即使发出查询的 Pod 位于美国，\u0026ldquo;nginx.mynamespace.myfederation.svc.europe-west1.example.com\u0026rdquo; 也将解析欧洲目前所有健康的服务分片,并且无论美国是否有健康的服务分片。这对于远程监视和其他类似应用程序很有用。\n来自联合集群之外的其他客户端 上面大部分讨论都同样适用于外部客户端，除了不再描述所描述的自动 DNS 扩展。因此，外部客户端需要指定联合服务的标准 DNS 名称，可以是地带名称，区域名称或者全局名称。为了方便起见，通常最好在服务中手动配置其他静态 CNAME 记录，例如:\neu.nginx.acme.com CNAME nginx.mynamespace.myfederation.svc.europe-west1.example.com. us.nginx.acme.com CNAME nginx.mynamespace.myfederation.svc.us-central1.example.com. nginx.acme.com CNAME nginx.mynamespace.myfederation.svc.example.com. 这样，您的客户就可以始终使用左侧的缩写形式，并始终被自动路由到其本国大陆上最接近的健康分片。Kubernetes 联邦集群自动为您处理所有必需的故障转移。将来的发行版将对此进行进一步改进。\n处理后端 Pod 和整个集群的故障 标准的 Kubernetes 服务集群 IP 已确保无响应的单个 Pod 端点以低延迟(几秒钟)自动退出服务。此外，如上所述，Kubernetes 联邦集群系统会自动监视集群的状态以及联合服务的所有分片后面的端点，并根据需要使分片进入和退出服务(例如，当服务后面的所有端点或者整个集群或可用性区域出现故障时，或者相反地从中断中恢复时)。由于 DNS 缓存固有的延迟(默认情况下,缓存超时或联合服务 DNS 记录的 TTL 配置为3分钟，可以调整),在灾难性故障的情况下，所有客户端可能要花费很长时间才能完全故障转移到备用集群。但是，鉴于每个区域服务端点可以返回的离散 IP 地址数量(例如上面的 us-central1，它有三个替代方案),与给定的合适配置相比，许多客户端将在更少的时间内自动故障转移到其他 IP。\n故障排除 我无法连接到联合集群 API 检查您的\n 客户端(通常是 kubectl)已正确配置(包括 API 端点和登录凭据)。 联合集群 API 服务器正在运行并且可以访问网络。  请参阅 联合集群管理员指南了解如何正确启动联邦集群(或让您的集群管理员为您执行此操作),以及如何正确配置客户端。\n我可以针对联合集群 API 成功创建联合服务,但是在我的基础集群中没有创建匹配的服务。 检查:\n 您的集群已在联合集群 API 中正确注册(kubectl describe clusters)。 您的集群都是 \u0026ldquo;活跃的\u0026rdquo;。这意味着集群联合身份验证系统能够针对集群的端点进行连接和身份验证。如果不是，请查阅federation-controller-manager pod 的日志，以确定可能是什么故障。 kubectl --namespace=federation logs $(kubectl get pods --namespace=federation -l module=federation-controller-manager -o name)  集群提供给联合集群 API 的登录凭据具有正确的授权和配额，可以在集群的相关命名空间中创建服务。如果不是这种情况，您将再次在上述日志文件中看到相关的错误消息，以提供更多详细信息。 是否有其他错误阻止服务创建操作成功(请在 kubectl logs federation-controller-manager --namespace federation 的输出中查找 service-controller 错误)。  我可以成功创建联合服务，但是在我的 DNS 提供程序中没有创建匹配的 DNS 记录。 检查:\n 您的联邦集群名称，DNS 提供程序，DNS 域名已正确配置。请参阅 联邦集群管理指南或者 教程了解如何配置联合集群系统的 DNS 提供程序(或让您的集群管理员为您执行此操作)。 确认联合集群的服务控制器已成功连接到所选的 DNS 提供程序并对其进行身份验证(在 kubectl logs federation-controller-manager --namespace federation 的输出中查找 service-controller 错误或者成功)。 确认联合集群的服务控制器已在您的 DNS 提供程序中成功创建了 DNS 记录(或在其日志中输出错误，以更详细地解释失败原因)。  在我的 DNS 提供程序中创建了匹配的 DNS 记录，但是客户端无法根据这些名称进行解析 检查:\n 已正确配置用于管理联合 DNS 域名的 DNS 注册器，使其指向已配置的 DNS 提供程序的名称服务器。例如，请参见 Google Domains 文档与 Google Cloud DNS 文档,或者域名注册商和 DNS 提供商的等效指南。  此疑难解答指南没有帮助我解决问题  请使用我们的 支持渠道寻求帮助。  更多信息  联合提议 详细介绍了促进这项工作的用例。  "
},
{
	"uri": "https://lijun.in/tasks/configure-pod-container/configure-pod-configmap/",
	"title": "使用 ConfigMap 配置 Pod",
	"tags": [],
	"description": "",
	"content": "ConfigMap 允许您将配置文件与镜像文件分离，以使容器化的应用程序具有可移植性。该页面提供了一系列使用示例，这些示例演示了如何使用存储在 ConfigMap 中的数据创建 ConfigMap 和配置 Pod。\n. heading \u0026ldquo;prerequisites\u0026rdquo; %}} . include \u0026ldquo;task-tutorial-prereqs.md\u0026rdquo; \u0026gt;}} . version-check \u0026gt;}}\n创建 ConfigMap 您可以在 kustomization.yaml 中使用 kubectl create configmap 或 ConfigMap 生成器来创建ConfigMap。注意，从 1.14 版本开始， kubectl 开始支持 kustomization.yaml。\n使用 kubectl 创建 ConfigMap 在目录, 文件, 或者文字值中使用 kubectl create configmap 命令创建configmap：\nkubectl create configmap \u0026lt;map-name\u0026gt; \u0026lt;data-source\u0026gt; 其中， \u0026lt;map-name\u0026gt; 是要分配给 ConfigMap 的名称，\u0026lt;data-source\u0026gt; 是要从中提取数据的目录，文件或者文字值。\n数据源对应于 ConfigMap 中的 key-value (键值对)\n key = 您在命令行上提供的文件名或者密钥 value = 您在命令行上提供的文件内容或者文字值  您可以使用kubectl describe或者 kubectl get检索有关 ConfigMap 的信息。\n根据目录创建 ConfigMap 你可以使用 kubectl create configmap 从同一目录中的多个文件创建 ConfigMap。\n例如：\n# 创建本地目录 mkdir -p configure-pod-container/configmap/ # 将样本文件下载到 `configure-pod-container/configmap/` 目录 wget https://kubernetes.io/examples/configmap/game.properties -O configure-pod-container/configmap/game.properties wget https://kubernetes.io/examples/configmap/ui.properties -O configure-pod-container/configmap/ui.properties # 创建 configmap kubectl create c game-config --from-file=configure-pod-container/configmap/ 合并 configure-pod-container/configmap/ 目录的内容\ngame.properties ui.properties 进入以下 ConfigMap 中：\nkubectl describe configmaps game-config 输出类似以下内容：\nName: game-config Namespace: default Labels: \u0026lt;none\u0026gt; Annotations: \u0026lt;none\u0026gt; Data ==== game.properties: 158 bytes ui.properties: 83 bytes configure-pod-container/configmap/ 目录中的 game.properties 和 ui.properties 文件在 ConfigMap 的 data 部分中表示。\nkubectl get configmaps game-config -o yaml 输出类似以下内容:\napiVersion: v1 kind: ConfigMap metadata: creationTimestamp: 2016-02-18T18:52:05Z name: game-config namespace: default resourceVersion: \u0026#34;516\u0026#34; selfLink: /api/v1/namespaces/default/configmaps/game-config uid: b4952dc3-d670-11e5-8cd0-68f728db1985 data: game.properties: | enemies=aliens lives=3 enemies.cheat=true enemies.cheat.level=noGoodRotten secret.code.passphrase=UUDDLRLRBABAS secret.code.allowed=true secret.code.lives=30 ui.properties: | color.good=purple color.bad=yellow allow.textmode=true how.nice.to.look=fairlyNice 根据文件创建 ConfigMap 您可以使用 kubectl create configmap 从单个文件或多个文件创建 ConfigMap。\n例如\nkubectl create configmap game-config-2 --from-file=configure-pod-container/configmap/game.properties 将产生以下 ConfigMap:\nkubectl describe configmaps game-config-2 输出类似以下内容:\nName: game-config-2 Namespace: default Labels: \u0026lt;none\u0026gt; Annotations: \u0026lt;none\u0026gt; Data ==== game.properties: 158 bytes 您可以传入多个 --from-file 参数，从多个数据源创建 ConfigMap。\nkubectl create configmap game-config-2 --from-file=configure-pod-container/configmap/game.properties --from-file=configure-pod-container/configmap/ui.properties 描述上面创建的 game-config-2 configmap\nkubectl describe configmaps game-config-2 输出类似以下内容:\nName: game-config-2 Namespace: default Labels: \u0026lt;none\u0026gt; Annotations: \u0026lt;none\u0026gt; Data ==== game.properties: 158 bytes ui.properties: 83 bytes 使用 --from-env-file 选项从环境文件创建 ConfigMap，例如：\n# 环境文件包含环境变量列表。 # 语法规则: # env 文件中的每一行必须为 VAR = VAL 格式。 # 以＃开头的行(即注释)将被忽略。 # 空行将被忽略。 # 引号没有特殊处理(即它们将成为 ConfigMap 值的一部分)。 # 将样本文件下载到 `configure-pod-container/configmap/` 目录 wget https://kubernetes.io/examples/configmap/game-env-file.properties -O configure-pod-container/configmap/game-env-file.properties # env文件 `game-env-file.properties` 如下所示 cat configure-pod-container/configmap/game-env-file.properties enemies=aliens lives=3 allowed=\u0026#34;true\u0026#34; # 注释及其上方的空行将被忽略 kubectl create configmap game-config-env-file \\  --from-env-file=configure-pod-container/configmap/game-env-file.properties 将产生以下 ConfigMap:\nkubectl get configmap game-config-env-file -o yaml 输出类似以下内容:\napiVersion: v1 kind: ConfigMap metadata: creationTimestamp: 2017-12-27T18:36:28Z name: game-config-env-file namespace: default resourceVersion: \u0026#34;809965\u0026#34; selfLink: /api/v1/namespaces/default/configmaps/game-config-env-file uid: d9d1ca5b-eb34-11e7-887b-42010a8002b8 data: allowed: \u0026#39;\u0026#34;true\u0026#34;\u0026#39; enemies: aliens lives: \u0026#34;3\u0026#34; 当使用多个 --from-env-file 来从多个数据源创建 ConfigMap 时，仅仅最后一个 env 文件有效:\n​```shell\n将样本文件下载到 configure-pod-container/configmap/ 目录 wget https://k8s.io/examples/configmap/ui-env-file.properties -O configure-pod-container/configmap/ui-env-file.properties\n创建 configmap kubectl create configmap config-multi-env-files\n\u0026ndash;from-env-file=configure-pod-container/configmap/game-env-file.properties\n\u0026ndash;from-env-file=configure-pod-container/configmap/ui-env-file.properties\n \u0026lt;!-- would produce the following ConfigMap: --\u0026gt; 将产生以下 ConfigMap: ```shell kubectl get configmap config-multi-env-files -o yaml 输出类似以下内容:\napiVersion: v1 kind: ConfigMap metadata: creationTimestamp: 2017-12-27T18:38:34Z name: config-multi-env-files namespace: default resourceVersion: \u0026#34;810136\u0026#34; selfLink: /api/v1/namespaces/default/configmaps/config-multi-env-files uid: 252c4572-eb35-11e7-887b-42010a8002b8 data: color: purple how: fairlyNice textmode: \u0026#34;true\u0026#34; 定义从文件创建 ConfigMap 时要使用的密钥 您可以在使用 --from-file 参数时,在 ConfigMap 的 data 部分中定义除文件名以外的其他键:\nkubectl create configmap game-config-3 --from-file=\u0026lt;my-key-name\u0026gt;=\u0026lt;path-to-file\u0026gt; \u0026lt;my-key-name\u0026gt; 是您要在 ConfigMap 中使用的密钥， \u0026lt;path-to-file\u0026gt; 是您想要键表示数据源文件的位置。\n例如:\nkubectl create configmap game-config-3 --from-file=game-special-key=configure-pod-container/configmap/game.properties 将产生以下 ConfigMap:\nkubectl get configmaps game-config-3 -o yaml 输出类似以下内容:\napiVersion: v1 kind: ConfigMap metadata: creationTimestamp: 2016-02-18T18:54:22Z name: game-config-3 namespace: default resourceVersion: \u0026#34;530\u0026#34; selfLink: /api/v1/namespaces/default/configmaps/game-config-3 uid: 05f8da22-d671-11e5-8cd0-68f728db1985 data: game-special-key: | enemies=aliens lives=3 enemies.cheat=true enemies.cheat.level=noGoodRotten secret.code.passphrase=UUDDLRLRBABAS secret.code.allowed=true secret.code.lives=30 根据文字值创建 ConfigMap 您可以将 kubectl create configmap 与 --from-literal 参数一起使用，从命令行定义文字值:\nkubectl create configmap special-config --from-literal=special.how=very --from-literal=special.type=charm 您可以传入多个键值对。命令行中提供的每对在 ConfigMap 的 data 部分中均表示为单独的条目。\nkubectl get configmaps special-config -o yaml 输出类似以下内容:\napiVersion: v1 kind: ConfigMap metadata: creationTimestamp: 2016-02-18T19:14:38Z name: special-config namespace: default resourceVersion: \u0026#34;651\u0026#34; selfLink: /api/v1/namespaces/default/configmaps/special-config uid: dadce046-d673-11e5-8cd0-68f728db1985 data: special.how: very special.type: charm 根据生成器创建 ConfigMap 自 1.14 开始， kubectl 开始支持 kustomization.yaml。 您还可以从生成器创建 ConfigMap，然后将其应用于 Apiserver 创建对象。生成器应在目录内的 kustomization.yaml 中指定。\n根据文件生成 ConfigMap 例如，要从 configure-pod-container/configmap/kubectl/game.properties 文件生成一个 ConfigMap ​```shell\n使用 ConfigMapGenerator 创建 kustomization.yaml 文件 cat \u0026laquo;EOF \u0026gt;./kustomization.yaml configMapGenerator:\n name: game-config-4 files:  configure-pod-container/configmap/kubectl/game.properties EOF     \u0026lt;!-- Apply the kustomization directory to create the ConfigMap object. --\u0026gt; 使用 kustomization 目录创建 ConfigMap 对象 ```shell kubectl apply -k . configmap/game-config-4-m9dm2f92bt created 您可以检查 ConfigMap 是这样创建的:\nkubectl get configmap NAME DATA AGE game-config-4-m9dm2f92bt 1 37s kubectl describe configmaps/game-config-4-m9dm2f92bt Name: game-config-4-m9dm2f92bt Namespace: default Labels: \u0026lt;none\u0026gt; Annotations: kubectl.kubernetes.io/last-applied-configuration: {\u0026#34;apiVersion\u0026#34;:\u0026#34;v1\u0026#34;,\u0026#34;data\u0026#34;:{\u0026#34;game.properties\u0026#34;:\u0026#34;enemies=aliens\\nlives=3\\nenemies.cheat=true\\nenemies.cheat.level=noGoodRotten\\nsecret.code.p... Data ==== game.properties: ---- enemies=aliens lives=3 enemies.cheat=true enemies.cheat.level=noGoodRotten secret.code.passphrase=UUDDLRLRBABAS secret.code.allowed=true secret.code.lives=30 Events: \u0026lt;none\u0026gt; 请注意，生成的 ConfigMap 名称具有通过对内容进行散列而附加的后缀，这样可以确保每次修改内容时都会生成新的 ConfigMap。\n定义从文件生成 ConfigMap 时要使用的密钥 您可以定义一个非文件名的键，在 ConfigMap 生成器中使用。例如，使用 game-special-key 从 configure-pod-container / configmap / kubectl / game.properties 文件生成 ConfigMap。\n​```shell\n使用 ConfigMapGenerator 创建 kustomization.yaml 文件 cat \u0026laquo;EOF \u0026gt;./kustomization.yaml configMapGenerator:\n name: game-config-5 files:  game-special-key=configure-pod-container/configmap/kubectl/game.properties EOF     \u0026lt;!-- Apply the kustomization directory to create the ConfigMap object. --\u0026gt; 使用 Kustomization 目录创建 ConfigMap 对象。 ```shell kubectl apply -k . configmap/game-config-5-m67dt67794 created 从文字值生成 ConfigMap 要从文字 special.type=charm 和 special.how=very 生成 ConfigMap，可以在 kusotmization.yaml 中将 ConfigMap 生成器指定。\n# 使用 ConfigMapGenerator 创建 kustomization.yaml 文件 cat \u0026lt;\u0026lt;EOF \u0026gt;./kustomization.yaml configMapGenerator: - name: special-config-2 literals: - special.how=very - special.type=charm EOF 使用 Kustomization 目录创建 ConfigMap 对象。\nkubectl apply -k . configmap/special-config-2-c92b5mmcf2 created 使用 ConfigMap 数据定义容器环境变量 使用单个 ConfigMap 中的数据定义容器环境变量   在 ConfigMap 中将环境变量定义为键值对:\nkubectl create configmap special-config --from-literal=special.how=very    将 ConfigMap 中定义的 special.how 值分配给 Pod 规范中的 SPECIAL_LEVEL_KEY 环境变量。\n. codenew file=\u0026quot;pods/pod-single-configmap-env-variable.yaml\u0026rdquo; \u0026gt;}}\n创建 Pod:\n  kubectl create -f https://kubernetes.io/examples/pods/pod-single-configmap-env-variable.yaml 现在，Pod 的输出包含环境变量 SPECIAL_LEVEL_KEY=very。\n使用来自多个 ConfigMap 的数据定义容器环境变量   与前面的示例一样，首先创建 ConfigMap。\n. codenew file=\u0026quot;configmap/configmaps.yaml\u0026rdquo; \u0026gt;}}\n创建 ConfigMap:\n  kubectl create -f https://kubernetes.io/examples/configmap/configmaps.yaml   在 Pod 规范中定义环境变量。\n. codenew file=\u0026quot;pods/pod-multiple-configmap-env-variable.yaml\u0026rdquo; \u0026gt;}}\n创建 Pod:\n  kubectl create -f https://kubernetes.io/examples/pods/pod-multiple-configmap-env-variable.yaml 现在，Pod 的输出包含环境变量 SPECIAL_LEVEL_KEY=very 和 LOG_LEVEL=INFO。\n将 ConfigMap 中的所有键值对配置为容器环境变量 . note \u0026gt;}}\nKubernetes v1.6 和更高版本提供了此功能。 . /note \u0026gt;}}\n  创建一个包含多个键值对的 ConfigMap。\n. codenew file=\u0026quot;configmap/configmap-multikeys.yaml\u0026rdquo; \u0026gt;}}\n创建 ConfigMap:\n  kubectl create -f https://kubernetes.io/examples/configmap/configmap-multikeys.yaml  使用 envFrom 将所有 ConfigMap 的数据定义为容器环境变量，ConfigMap 中的键成为 Pod 中的环境变量名称。  . codenew file=\u0026quot;pods/pod-configmap-envFrom.yaml\u0026rdquo; \u0026gt;}}\n创建 Pod:\nkubectl create -f https://kubernetes.io/examples/pods/pod-configmap-envFrom.yaml 现在，Pod 的输出包含环境变量 SPECIAL_LEVEL=very 和 SPECIAL_TYPE=charm。\n在 Pod 命令中使用 ConfigMap 定义的环境变量 您可以使用 $(VAR_NAME) Kubernetes 替换语法在 Pod 规范的 command 部分中使用 ConfigMap 定义的环境变量。\n例如，以下 Pod 规范\n. codenew file=\u0026quot;pods/pod-configmap-env-var-valueFrom.yaml\u0026rdquo; \u0026gt;}}\n通过运行创建\nkubectl create -f https://kubernetes.io/examples/pods/pod-configmap-env-var-valueFrom.yaml 在 test-container 容器中产生以下输出:\nvery charm 将 ConfigMap 数据添加到一个容器中 如根据文件创建ConfigMap中所述，当您使用 --from-file 创建 ConfigMap 时，文件名成为存储在 ConfigMap 的 data 部分中的密钥，文件内容成为密钥的值。\n本节中的示例引用了一个名为 special-config 的 ConfigMap，如下所示：\n. codenew file=\u0026quot;configmap/configmap-multikeys.yaml\u0026rdquo; \u0026gt;}}\n创建 ConfigMap:\nkubectl create -f https://kubernetes.io/examples/configmap/configmap-multikeys.yaml 使用存储在 ConfigMap 中的数据填充容器 在 Pod 规范的 volumes 部分下添加 ConfigMap 名称。 这会将 ConfigMap 数据添加到指定为 volumeMounts.mountPath 的目录(在本例中为/etc/config)。 command 引用存储在 ConfigMap 中的 special.level。\n. codenew file=\u0026quot;pods/pod-configmap-volume.yaml\u0026rdquo; \u0026gt;}}\n创建Pod:\nkubectl create -f https://kubernetes.io/examples/pods/pod-configmap-volume.yaml 运行命令 ls /etc/config/ 产生下面的输出:\nSPECIAL_LEVEL SPECIAL_TYPE . caution \u0026gt;}}\n如果在 /etc/config/ 目录中有一些文件，它们将被删除。 . /caution \u0026gt;}}\n将 ConfigMap 数据添加到容器中的特定路径 使用 path 字段为特定的 ConfigMap 项目指定所需的文件路径。 在这种情况下, SPECIAL_LEVEL 将安装在 /etc/config/keys 目录下的 config-volume 容器中。\n. codenew file=\u0026quot;pods/pod-configmap-volume-specific-key.yaml\u0026rdquo; \u0026gt;}}\n创建Pod:\nkubectl create -f https://kubernetes.io/examples/pods/pod-configmap-volume-specific-key.yaml 当 pod 运行时，命令 cat /etc/config/keys 产生以下输出:\nvery . caution \u0026gt;}}\n和以前一样，/etc/config/ 目录中的所有先前文件都将被删除。 . /caution \u0026gt;}}\n项目密钥以指定路径和文件权限 您可以将密钥映射到每个文件的特定路径和特定权限。Secrets 用户指南说明了语法。\n挂载的 ConfigMap 将自动更新 更新已经在容器中使用的 ConfigMap 时，最终也会更新映射键。Kubelet 实时检查是否在每个定期同步中都更新已安装的 ConfigMap。它使用其基于本地 ttl 的缓存来获取 ConfigMap 的当前值。结果，从更新 ConfigMap 到将新密钥映射到 Pod 的总延迟可以与 ConfigMap 在 kubelet 中缓存的 kubelet 同步周期 ttl 一样长。\n. note \u0026gt;}}\n使用 ConfigMap 作为子路径subPath的容器将不会收到 ConfigMap 更新。 . /note \u0026gt;}}\n了解 ConfigMap 和 Pod ConfigMap API 资源将配置数据存储为键值对。数据可以在 Pod 中使用，也可以提供系统组件(如控制器)的配置。ConfigMap 与 Secrets类似，但是提供了一种使用不包含敏感信息的字符串的方法。用户和系统组件都可以在 ConfigMap 中存储配置数据。\n. note \u0026gt;}}\nConfigMap 应该引用属性文件，而不是替换它们。可以将 ConfigMap 表示为类似于 Linux /etc 目录及其内容的东西。例如，如果您从 ConfigMap 创建Kubernetes Volume，则 ConfigMap 中的每个数据项都由该容器中的单个文件表示。 . /note \u0026gt;}}\nConfigMap 的 data 字段包含配置数据。如下例所示，它可以很简单 \u0026ndash; 就像使用 --from-literal \u0026ndash; 定义的单个属性一样，也可以很复杂 \u0026ndash; 例如使用 --from-file 定义的配置文件或 JSON blob。\napiVersion: v1 kind: ConfigMap metadata: creationTimestamp: 2016-02-18T19:14:38Z name: example-config namespace: default data: # example of a simple property defined using --from-literal example.property.1: hello example.property.2: world # example of a complex property defined using --from-file example.property.file: |- property.1=value-1 property.2=value-2 property.3=value-3 限制规定  在 Pod 规范中引用它之前，必须先创建一个 ConfigMap(除非将 ConfigMap 标记为\u0026quot;可选\u0026rdquo;)。如果引用的 ConfigMap 不存在，则 Pod 将不会启动。同样，对 ConfigMap 中不存在的键的引用将阻止容器启动。    如果您使用 envFrom 从 ConfigMap 中定义环境变量，那么将忽略被认为无效的键。可以启动 Pod，但无效名称将记录在事件日志中(InvalidVariableNames)。日志消息列出了每个跳过的键。例如:\nkubectl get events 输出与此类似:\nLASTSEEN FIRSTSEEN COUNT NAME KIND SUBOBJECT TYPE REASON SOURCE MESSAGE 0s 0s 1 dapi-test-pod Pod Warning InvalidEnvironmentVariableNames {kubelet, 127.0.0.1} Keys [1badkey, 2alsobad] from the EnvFrom configMap default/myconfig were skipped since they are considered invalid environment variable names.    ConfigMaps reside in a specific 命令空间. A ConfigMap can only be referenced by pods residing in the same namespace. ConfigMap 驻留在特定的命令空间中。ConfigMap 只能由位于相同命令空间中的 Pod 引用。    Kubelet 不支持将 ConfigMap 用于未在 API 服务器上找到的 Pod。这包括通过 Kubelet 的 --manifest-url 参数，--config 参数或者 Kubelet REST API 创建的容器。\n. note \u0026gt;}}\n这些不是创建 pods 的常用方法。 . /note \u0026gt;}}\n  . heading \u0026ldquo;whatsnext\u0026rdquo; %}}  遵循使用ConfigMap配置Redis的真实案例。  "
},
{
	"uri": "https://lijun.in/tasks/service-catalog/",
	"title": "😝 - 安装服务目录",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lijun.in/tasks/configure-pod-container/share-process-namespace/",
	"title": "在 Pod 中的容器之间共享进程命名空间",
	"tags": [],
	"description": "",
	"content": ". feature-state state=\u0026quot;stable\u0026rdquo; for_k8s_version=\u0026quot;v1.17\u0026rdquo; \u0026gt;}}\n此页面展示如何为 pod 配置进程命名空间共享。 当启用进程命名空间共享时，容器中的进程对该 pod 中的所有其他容器都是可见的。\n您可以使用此功能来配置协作容器，比如日志处理 sidecar 容器，或者对那些不包含诸如 shell 等调试实用工具的镜像进行故障排查。\n. heading \u0026ldquo;prerequisites\u0026rdquo; %}} . include \u0026ldquo;task-tutorial-prereqs.md\u0026rdquo; \u0026gt;}} . version-check \u0026gt;}}\n配置 Pod 进程命名空间共享使用 v1.PodSpec 中的 ShareProcessNamespace 字段启用。例如：\n. codenew file=\u0026quot;pods/share-process-namespace.yaml\u0026rdquo; \u0026gt;}}\n  在集群中创建 nginx pod：\nkubectl apply -f https://k8s.io/examples/pods/share-process-namespace.yaml   获取容器 shell，执行 ps：\nkubectl attach -it nginx -c shell 如果没有看到命令提示符，请按 enter 回车键。\n/ # ps ax PID USER TIME COMMAND 1 root 0:00 /pause 8 root 0:00 nginx: master process nginx -g daemon off; 14 101 0:00 nginx: worker process 15 root 0:00 sh 21 root 0:00 ps ax   您可以在其他容器中对进程发出信号。例如，发送 SIGHUP 到 nginx 以重启工作进程。这需要 SYS_PTRACE 功能。\n/ # kill -HUP 8 / # ps ax PID USER TIME COMMAND 1 root 0:00 /pause 8 root 0:00 nginx: master process nginx -g daemon off; 15 root 0:00 sh 22 101 0:00 nginx: worker process 23 root 0:00 ps ax 甚至可以使用 /proc/$pid/root 链接访问另一个容器镜像。\n/ # head /proc/8/root/etc/nginx/nginx.conf user nginx; worker_processes 1; error_log /var/log/nginx/error.log warn; pid /var/run/nginx.pid; events { worker_connections 1024; 理解进程命名空间共享 Pod 共享许多资源，因此它们共享进程命名空间是很有意义的。 不过，有些容器镜像可能希望与其他容器隔离，因此了解这些差异很重要:\n  容器进程不再具有 PID 1。 在没有 PID 1 的情况下，一些容器镜像拒绝启动（例如，使用 systemd 的容器)，或者拒绝执行 kill -HUP 1 之类的命令来通知容器进程。在具有共享进程命名空间的 pod 中，kill -HUP 1 将通知 pod 沙箱（在上面的例子中是 /pause）。\n  进程对 pod 中的其他容器可见。 这包括 /proc 中可见的所有信息，例如作为参数或环境变量传递的密码。这些仅受常规 Unix 权限的保护。\n  容器文件系统通过 /proc/$pid/root 链接对 pod 中的其他容器可见。 这使调试更加容易，但也意味着文件系统安全性只受文件系统权限的保护。\n  "
},
{
	"uri": "https://lijun.in/tasks/federation/administer-federation/",
	"title": "管理联邦控制平面",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lijun.in/concepts/policy/",
	"title": "😊 - 策略",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lijun.in/tasks/network/",
	"title": "😝 - 网络",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lijun.in/tasks/configure-pod-container/static-pod/",
	"title": "创建静态 Pod",
	"tags": [],
	"description": "",
	"content": "静态 Pod 在指定的节点上由 kubelet 守护进程直接管理，不需要 . glossary_tooltip text=\u0026quot;API 服务\u0026rdquo; term_id=\u0026quot;kube-apiserver\u0026rdquo; \u0026gt;}} 监管。 不像 Pod 是由控制面管理的（例如，. glossary_tooltip text=\u0026quot;Deployment\u0026rdquo; term_id=\u0026quot;deployment\u0026rdquo; \u0026gt;}}）；相反 kubelet 监视每个静态 Pod（在它崩溃之后重新启动）。\n静态 Pod 永远都会绑定到一个指定节点上的 . glossary_tooltip term_id=\u0026quot;kubelet\u0026rdquo; \u0026gt;}}。\nkubelet 会尝试通过 Kubernetes API 服务器为每个静态 Pod 自动创建一个 . glossary_tooltip text=\u0026quot;镜像 Pod\u0026rdquo; term_id=\u0026quot;mirror-pod\u0026rdquo; \u0026gt;}}。 这意味着节点上运行的静态 Pod 对 API 服务来说是不可见的，但是不能通过 API 服务器来控制。\n. note \u0026gt;}} 如果你在运行一个 Kubernetes 集群，并且在每个节点上都运行一个静态 Pod，就可能需要考虑使用 . glossary_tooltip text=\u0026quot;DaemonSet\u0026rdquo; term_id=\u0026quot;daemonset\u0026rdquo; \u0026gt;}} 替代这种方式。 . /note \u0026gt;}}\n. heading \u0026ldquo;prerequisites\u0026rdquo; %}} . include \u0026ldquo;task-tutorial-prereqs.md\u0026rdquo; \u0026gt;}} . version-check \u0026gt;}}\n本文假定你在使用 . glossary_tooltip term_id=\u0026quot;docker\u0026rdquo; \u0026gt;}} 来运行 Pod，并且你的节点是运行着 Fedora 操作系统。 其它发行版或者 Kubernetes 部署版本上操作方式可能不一样。\n创建静态 Pod 可以通过文件系统上的配置文件或者 web 网络上的配置文件来配置静态 Pod。\n文件系统上的静态 Pod 声明文件 声明文件是标准的 Pod 定义文件，以 JSON 或者 YAML 格式存储在指定目录。路径设置在 Kubelet 配置文件的 staticPodPath: \u0026lt;目录\u0026gt; 字段，kubelet 会定期的扫描这个文件夹下的 YAML/JSON 文件来创建/删除静态 Pod。 注意 kubelet 扫描目录的时候会忽略以点开头的文件。\n例如：下面是如何以静态 Pod 的方式启动一个简单 web 服务：\n  选择一个要运行静态 Pod 的节点。在这个例子中选择 my-node1。\nssh my-node1    1. 选择一个目录，比如在 `/etc/kubelet.d` 目录来保存 web 服务 Pod 的定义文件， `/etc/kubelet.d/static-web.yaml`： ```shell # 在 kubelet 运行的节点上执行以下命令 mkdir /etc/kubelet.d/ cat \u0026lt;\u0026lt;EOF \u0026gt;/etc/kubelet.d/static-web.yaml apiVersion: v1 kind: Pod metadata: name: static-web labels: role: myrole spec: containers: - name: web image: nginx ports: - name: web containerPort: 80 protocol: TCP EOF ``` \u0026lt;!-- 1. Configure your kubelet on the node to use this directory by running it with `--pod-manifest-path=/etc/kubelet.d/` argument. On Fedora edit `/etc/kubernetes/kubelet` to include this line: ``` KUBELET_ARGS=\u0026quot;--cluster-dns=10.254.0.10 --cluster-domain=kube.local --pod-manifest-path=/etc/kubelet.d/\u0026quot; ``` or add the `staticPodPath: \u0026lt;the directory\u0026gt;` field in the [KubeletConfiguration file](/docs/tasks/administer-cluster/kubelet-config-file). --\u0026gt; 3. 配置这个节点上的 kubelet，使用这个参数执行 `--pod-manifest-path=/etc/kubelet.d/`： ``` KUBELET_ARGS=\u0026quot;--cluster-dns=10.254.0.10 --cluster-domain=kube.local --pod-manifest-path=/etc/kubelet.d/\u0026quot; ``` 或者在 [Kubelet配置文件](/docs/tasks/administer-cluster/kubelet-config-file)中添加 `staticPodPath: \u0026lt;目录\u0026gt;`字段。 \u0026lt;!-- 1. Restart the kubelet. On Fedora, you would run: ```shell # Run this command on the node where the kubelet is running systemctl restart kubelet ``` --\u0026gt; 1. 重启 kubelet。Fedora 上使用下面的命令： ```shell # 在 kubelet 运行的节点上执行以下命令 systemctl restart kubelet ``` \u0026lt;!-- ### Web-hosted static pod manifest {#pods-created-via-http} Kubelet periodically downloads a file specified by `--manifest-url=\u0026lt;URL\u0026gt;` argument and interprets it as a JSON/YAML file that contains Pod definitions. Similar to how [filesystem-hosted manifests](#configuration-files) work, the kubelet refetches the manifest on a schedule. If there are changes to the list of static Pods, the kubelet applies them. To use this approach: --\u0026gt; ### Web 网上的静态 Pod 声明文件 {#pods-created-via-http} Kubelet 根据 `--manifest-url=\u0026lt;URL\u0026gt;` 参数的配置定期的下载指定文件，并且转换成 JSON/YAML 格式的 Pod 容器定义文件。 与[文件系统上的声明文件](#configuration-files)使用方式类似，kubelet 调度获取声明文件。如果静态 Pod 的声明文件有改变，kubelet会应用这些改变。 按照下面的方式来： \u0026lt;!-- 1. Create a YAML file and store it on a web server so that you can pass the URL of that file to the kubelet. --\u0026gt; 1. 创建一个 YAML 文件，并保存在保存在 web 服务上，为 kubelet 生成一个 URL。 ```yaml apiVersion: v1 kind: Pod metadata: name: static-web labels: role: myrole spec: containers: - name: web image: nginx ports: - name: web containerPort: 80 protocol: TCP ``` \u0026lt;!-- 2. Configure the kubelet on your selected node to use this web manifest by running it with `--manifest-url=\u0026lt;manifest-url\u0026gt;`. On Fedora, edit `/etc/kubernetes/kubelet` to include this line: --\u0026gt; 1. 通过在选择的节点上使用 `--manifest-url=\u0026lt;manifest-url\u0026gt;` 配置运行 kubelet。在 Fedora 添加下面这行到 `/etc/kubernetes/kubelet` ： ``` KUBELET_ARGS=\u0026quot;--cluster-dns=10.254.0.10 --cluster-domain=kube.local --manifest-url=\u0026lt;manifest-url\u0026gt;\u0026quot; ``` \u0026lt;!-- 3. Restart the kubelet. On Fedora, you would run: ```shell # Run this command on the node where the kubelet is running systemctl restart kubelet ``` --\u0026gt; 1. 重启 kubelet。在 Fedora 上运行如下命令： ```shell # 在 kubelet 运行的节点上执行以下命令 systemctl restart kubelet ``` \u0026lt;!-- ## Observe static pod behavior {#behavior-of-static-pods} When the kubelet starts, it automatically starts all defined static Pods. As you have defined a static Pod and restarted the kubelet, the new static Pod should already be running. You can view running containers (including static Pods) by running (on the node): ```shell # Run this command on the node where kubelet is running docker ps The output might be something like: \u0026ndash;\u0026gt;\n观察静态 pod 的行为 当 kubelet 启动时，会自动启动所有定义的静态 Pod。当定义了一个静态 Pod 并重新启动 kubelet 时，新的静态 Pod 就应该已经在运行了。\n可以在节点上运行下面的命令来查看正在运行的容器（包括静态 Pod）：\n# 在 kubelet 运行的节点上执行以下命令 docker ps 输出可能会像这样：\nCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES f6d05272b57e nginx:latest \u0026quot;nginx\u0026quot; 8 minutes ago Up 8 minutes k8s_web.6f802af4_static-web-fk-node1_default_67e24ed9466ba55986d120c867395f3c_378e5f3c 可以在 API 服务上看到镜像 Pod：\nkubectl get pods NAME READY STATUS RESTARTS AGE static-web-my-node1 1/1 Running 0 2m . note \u0026gt;}} 要确保 kubelet 在 API 服务上有创建镜像 Pod 的权限。如果没有，创建请求会被 API 服务拒绝。可以看Pod安全策略。 . /note \u0026gt;}}\n静态 Pod 上的. glossary_tooltip term_id=\u0026quot;label\u0026rdquo; text=\u0026quot;标签\u0026rdquo; \u0026gt;}} 被传到镜像 Pod。 你可以通过 . glossary_tooltip term_id=\u0026quot;selector\u0026rdquo; text=\u0026quot;选择算符\u0026rdquo; \u0026gt;}} 使用这些标签，比如。\n如果你用 kubectl 从 API 服务上删除镜像 Pod，kubelet 不会 移除静态 Pod：\nkubectl delete pod static-web-my-node1 pod \u0026quot;static-web-my-node1\u0026quot; deleted 可以看到 Pod 还在运行：\nkubectl get pods NAME READY STATUS RESTARTS AGE static-web-my-node1 1/1 Running 0 12s 回到 kubelet 运行的节点上，可以手工停止 Docker 容器。 可以看到过了一段时间后 kubelet 会发现容器停止了并且会自动重启 Pod：\n# 在 kubelet 运行的节点上执行以下命令 docker stop f6d05272b57e # 把 ID 换为你的容器的 ID sleep 20 docker ps CONTAINER ID IMAGE COMMAND CREATED ... 5b920cbaf8b1 nginx:latest \u0026quot;nginx -g 'daemon of 2 seconds ago ... 动态增加和删除静态 pod 运行中的 kubelet 会定期扫描配置的目录(比如例子中的 /etc/kubelet.d 目录)中的变化，并且根据文件中出现/消失的 Pod 来添加/删除 Pod。\n# 前提是你在用主机文件系统上的静态 Pod 配置文件 # 在 kubelet 运行的节点上执行以下命令 # mv /etc/kubelet.d/static-web.yaml /tmp sleep 20 docker ps # 可以看到没有 nginx 容器在运行 mv /tmp/static-web.yaml /etc/kubelet.d/ sleep 20 docker ps CONTAINER ID IMAGE COMMAND CREATED ... e7a62e3427f1 nginx:latest \u0026quot;nginx -g 'daemon of 27 seconds ago "
},
{
	"uri": "https://lijun.in/tasks/configure-pod-container/translate-compose-kubernetes/",
	"title": "将 Docker Compose 文件转换为 Kubernetes 资源",
	"tags": [],
	"description": "",
	"content": "Kompose 是什么？它是个转换工具，可将 compose（即 Docker Compose）所组装的所有内容转换成容器编排器（Kubernetes 或 OpenShift）可识别的形式。\n更多信息请参考 Kompose 官网 http://kompose.io。\n. heading \u0026ldquo;prerequisites\u0026rdquo; %}} . include \u0026ldquo;task-tutorial-prereqs.md\u0026rdquo; \u0026gt;}} . version-check \u0026gt;}}\n安装 Kompose 我们有很多种方式安装 Kompose。首选方式是从最新的 GitHub 发布页面下载二进制文件。\nGitHub 发布版本 Kompose 通过 GitHub 发布版本，发布周期为三星期。您可以在GitHub 发布页面上看到所有当前版本。\n# Linux curl -L https://github.com/kubernetes/kompose/releases/download/v1.16.0/kompose-linux-amd64 -o kompose # macOS curl -L https://github.com/kubernetes/kompose/releases/download/v1.16.0/kompose-darwin-amd64 -o kompose # Windows curl -L https://github.com/kubernetes/kompose/releases/download/v1.16.0/kompose-windows-amd64.exe -o kompose.exe chmod +x kompose sudo mv ./kompose /usr/local/bin/kompose 或者，您可以下载 tarball。\nGo 用 go get 命令从主分支拉取最新的开发变更的方法安装 Kompose。\ngo get -u github.com/kubernetes/kompose CentOS Kompose 位于 EPEL CentOS 代码仓库。 如果您还没有安装启用 EPEL 代码仓库，请运行命令 sudo yum install epel-release。\n如果您的系统中已经启用了 EPEL，您就可以像安装其他软件包一样安装 Kompose。\nsudo yum -y install kompose Fedora Kompose 位于 Fedora 24、25 和 26 的代码仓库。您可以像安装其他软件包一样安装 Kompose。\nsudo dnf -y install kompose macOS 在 macOS 上您可以通过 Homebrew 安装 Kompose 的最新版本：\nbrew install kompose 使用 Kompose 再需几步，我们就把你从 Docker Compose 带到 Kubernetes。 您只需要一个现有的 docker-compose.yml 文件。\n  进入 docker-compose.yml 文件所在的目录。如果没有，请使用下面这个进行测试。\nversion: \u0026#34;2\u0026#34; services: redis-master: image: k8s.gcr.io/redis:e2e ports: - \u0026#34;6379\u0026#34; redis-slave: image: gcr.io/google_samples/gb-redisslave:v3 ports: - \u0026#34;6379\u0026#34; environment: - GET_HOSTS_FROM=dns frontend: image: gcr.io/google-samples/gb-frontend:v4 ports: - \u0026#34;80:80\u0026#34; environment: - GET_HOSTS_FROM=dns labels: kompose.service.type: LoadBalancer   运行 kompose up 命令直接部署到 Kubernetes，或者跳到下一步，生成 kubectl 使用的文件。\n$ kompose up We are going to create Kubernetes Deployments, Services and PersistentVolumeClaims for your Dockerized application. If you need different kind of resources, use the \u0026#39;kompose convert\u0026#39; and \u0026#39;kubectl create -f\u0026#39; commands instead. INFO Successfully created Service: redis INFO Successfully created Service: web INFO Successfully created Deployment: redis INFO Successfully created Deployment: web Your application has been deployed to Kubernetes. You can run \u0026#39;kubectl get deployment,svc,pods,pvc\u0026#39; for details.   要将 docker-compose.yml 转换为 kubectl 可用的文件，请运行 kompose convert 命令进行转换，然后运行 kubectl create -f \u0026lt;output file\u0026gt; 进行创建。\n$ kompose convert INFO Kubernetes file \u0026#34;frontend-service.yaml\u0026#34; created INFO Kubernetes file \u0026#34;redis-master-service.yaml\u0026#34; created INFO Kubernetes file \u0026#34;redis-slave-service.yaml\u0026#34; created INFO Kubernetes file \u0026#34;frontend-deployment.yaml\u0026#34; created INFO Kubernetes file \u0026#34;redis-master-deployment.yaml\u0026#34; created INFO Kubernetes file \u0026#34;redis-slave-deployment.yaml\u0026#34; created $ kubectl create -f frontend-service.yaml,redis-master-service.yaml,redis-slave-service.yaml,frontend-deployment.yaml,redis-master-deployment.yaml,redis-slave-deployment.yaml service/frontend created service/redis-master created service/redis-slave created deployment.apps/frontend created deployment.apps/redis-master created deployment.apps/redis-slave created    您部署的应用在 Kubernetes 中运行起来了。   如果您在开发过程中使用 minikube，请执行：\n$ minikube service frontend 否则，我们要查看一下您的服务使用了什么 IP！\n$ kubectl describe svc frontend Name: frontend Namespace: default Labels: service=frontend Selector: service=frontend Type: LoadBalancer IP: 10.0.0.183 LoadBalancer Ingress: 192.0.2.89 Port: 80 80/TCP NodePort: 80 31144/TCP Endpoints: 172.17.0.4:80 Session Affinity: None No events. 如果您使用的是云提供商，您的 IP 将在 LoadBalancer Ingress 字段给出。\n$ curl http://192.0.2.89   用户指南  CLI  kompose convert kompose up kompose down   文档  构建和推送 Docker 镜像 其他转换方式 标签 重启 Docker Compose 版本    Kompose 支持两种驱动：OpenShift 和 Kubernetes。 您可以通过全局选项 --provider 选择驱动方式。如果没有指定，会将 Kubernetes 作为默认驱动。\nkompose convert Kompose 支持将 V1、V2 和 V3 版本的 Docker Compose 文件转换为 Kubernetes 和 OpenShift 资源对象。\nKubernetes $ kompose --file docker-voting.yml convert WARN Unsupported key networks - ignoring WARN Unsupported key build - ignoring INFO Kubernetes file \u0026#34;worker-svc.yaml\u0026#34; created INFO Kubernetes file \u0026#34;db-svc.yaml\u0026#34; created INFO Kubernetes file \u0026#34;redis-svc.yaml\u0026#34; created INFO Kubernetes file \u0026#34;result-svc.yaml\u0026#34; created INFO Kubernetes file \u0026#34;vote-svc.yaml\u0026#34; created INFO Kubernetes file \u0026#34;redis-deployment.yaml\u0026#34; created INFO Kubernetes file \u0026#34;result-deployment.yaml\u0026#34; created INFO Kubernetes file \u0026#34;vote-deployment.yaml\u0026#34; created INFO Kubernetes file \u0026#34;worker-deployment.yaml\u0026#34; created INFO Kubernetes file \u0026#34;db-deployment.yaml\u0026#34; created $ ls db-deployment.yaml docker-compose.yml docker-gitlab.yml redis-deployment.yaml result-deployment.yaml vote-deployment.yaml worker-deployment.yaml db-svc.yaml docker-voting.yml redis-svc.yaml result-svc.yaml vote-svc.yaml worker-svc.yaml 您也可以同时提供多个 docker-compose 文件进行转换：\n$ kompose -f docker-compose.yml -f docker-guestbook.yml convert INFO Kubernetes file \u0026#34;frontend-service.yaml\u0026#34; created INFO Kubernetes file \u0026#34;mlbparks-service.yaml\u0026#34; created INFO Kubernetes file \u0026#34;mongodb-service.yaml\u0026#34; created INFO Kubernetes file \u0026#34;redis-master-service.yaml\u0026#34; created INFO Kubernetes file \u0026#34;redis-slave-service.yaml\u0026#34; created INFO Kubernetes file \u0026#34;frontend-deployment.yaml\u0026#34; created INFO Kubernetes file \u0026#34;mlbparks-deployment.yaml\u0026#34; created INFO Kubernetes file \u0026#34;mongodb-deployment.yaml\u0026#34; created INFO Kubernetes file \u0026#34;mongodb-claim0-persistentvolumeclaim.yaml\u0026#34; created INFO Kubernetes file \u0026#34;redis-master-deployment.yaml\u0026#34; created INFO Kubernetes file \u0026#34;redis-slave-deployment.yaml\u0026#34; created $ ls mlbparks-deployment.yaml mongodb-service.yaml redis-slave-service.jsonmlbparks-service.yaml frontend-deployment.yaml mongodb-claim0-persistentvolumeclaim.yaml redis-master-service.yaml frontend-service.yaml mongodb-deployment.yaml redis-slave-deployment.yaml redis-master-deployment.yaml 当提供多个 docker-compose 文件时，配置将会合并。任何通用的配置都将被后续文件覆盖。\nOpenShift $ kompose --provider openshift --file docker-voting.yml convert WARN [worker] Service cannot be created because of missing port. INFO OpenShift file \u0026#34;vote-service.yaml\u0026#34; created INFO OpenShift file \u0026#34;db-service.yaml\u0026#34; created INFO OpenShift file \u0026#34;redis-service.yaml\u0026#34; created INFO OpenShift file \u0026#34;result-service.yaml\u0026#34; created INFO OpenShift file \u0026#34;vote-deploymentconfig.yaml\u0026#34; created INFO OpenShift file \u0026#34;vote-imagestream.yaml\u0026#34; created INFO OpenShift file \u0026#34;worker-deploymentconfig.yaml\u0026#34; created INFO OpenShift file \u0026#34;worker-imagestream.yaml\u0026#34; created INFO OpenShift file \u0026#34;db-deploymentconfig.yaml\u0026#34; created INFO OpenShift file \u0026#34;db-imagestream.yaml\u0026#34; created INFO OpenShift file \u0026#34;redis-deploymentconfig.yaml\u0026#34; created INFO OpenShift file \u0026#34;redis-imagestream.yaml\u0026#34; created INFO OpenShift file \u0026#34;result-deploymentconfig.yaml\u0026#34; created INFO OpenShift file \u0026#34;result-imagestream.yaml\u0026#34; created kompose 还支持为服务中的构建指令创建 buildconfig。 默认情况下，它使用当前 git 分支的 remote 仓库作为源仓库，使用当前分支作为构建的源分支。 您可以分别使用 --build-repo 和 --build-branch 选项指定不同的源仓库和分支。\n$ kompose --provider openshift --file buildconfig/docker-compose.yml convert WARN [foo] Service cannot be created because of missing port. INFO OpenShift Buildconfig using git@github.com:rtnpro/kompose.git::master as source. INFO OpenShift file \u0026#34;foo-deploymentconfig.yaml\u0026#34; created INFO OpenShift file \u0026#34;foo-imagestream.yaml\u0026#34; created INFO OpenShift file \u0026#34;foo-buildconfig.yaml\u0026#34; created . note \u0026gt;}}\n如果使用 oc create -f 手动推送 Openshift 工件，则需要确保在构建配置工件之前推送 imagestream 工件，以解决 Openshift 的这个问题：https://github.com/openshift/origin/issues/4518 。 . /note \u0026gt;}}\nkompose up Kompose 支持通过 kompose up 直接将您的\u0026quot;复合的（composed）\u0026rdquo; 应用程序部署到 Kubernetes 或 OpenShift。\nKubernetes $ kompose --file ./examples/docker-guestbook.yml up We are going to create Kubernetes deployments and services for your Dockerized application. If you need different kind of resources, use the \u0026#39;kompose convert\u0026#39; and \u0026#39;kubectl create -f\u0026#39; commands instead. INFO Successfully created service: redis-master INFO Successfully created service: redis-slave INFO Successfully created service: frontend INFO Successfully created deployment: redis-master INFO Successfully created deployment: redis-slave INFO Successfully created deployment: frontend Your application has been deployed to Kubernetes. You can run \u0026#39;kubectl get deployment,svc,pods\u0026#39; for details. $ kubectl get deployment,svc,pods NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE deployment.extensions/frontend 1 1 1 1 4m deployment.extensions/redis-master 1 1 1 1 4m deployment.extensions/redis-slave 1 1 1 1 4m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/frontend ClusterIP 10.0.174.12 \u0026lt;none\u0026gt; 80/TCP 4m service/kubernetes ClusterIP 10.0.0.1 \u0026lt;none\u0026gt; 443/TCP 13d service/redis-master ClusterIP 10.0.202.43 \u0026lt;none\u0026gt; 6379/TCP 4m service/redis-slave ClusterIP 10.0.1.85 \u0026lt;none\u0026gt; 6379/TCP 4m NAME READY STATUS RESTARTS AGE pod/frontend-2768218532-cs5t5 1/1 Running 0 4m pod/redis-master-1432129712-63jn8 1/1 Running 0 4m pod/redis-slave-2504961300-nve7b 1/1 Running 0 4m 注意：\n 您必须有一个运行正常的 Kubernetes 集群，该集群具有预先配置的 kubectl 上下文。 此操作仅生成 Deployment 和 Service 对象并将其部署到 Kubernetes。如果需要部署其他不同类型的资源，请使用 kompose convert 和 kubectl create -f 命令。  OpenShift $ kompose --file ./examples/docker-guestbook.yml --provider openshift up We are going to create OpenShift DeploymentConfigs and Services for your Dockerized application. If you need different kind of resources, use the \u0026#39;kompose convert\u0026#39; and \u0026#39;oc create -f\u0026#39; commands instead. INFO Successfully created service: redis-slave INFO Successfully created service: frontend INFO Successfully created service: redis-master INFO Successfully created deployment: redis-slave INFO Successfully created ImageStream: redis-slave INFO Successfully created deployment: frontend INFO Successfully created ImageStream: frontend INFO Successfully created deployment: redis-master INFO Successfully created ImageStream: redis-master Your application has been deployed to OpenShift. You can run \u0026#39;oc get dc,svc,is\u0026#39; for details. $ oc get dc,svc,is NAME REVISION DESIRED CURRENT TRIGGERED BY dc/frontend 0 1 0 config,image(frontend:v4) dc/redis-master 0 1 0 config,image(redis-master:e2e) dc/redis-slave 0 1 0 config,image(redis-slave:v1) NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE svc/frontend 172.30.46.64 \u0026lt;none\u0026gt; 80/TCP 8s svc/redis-master 172.30.144.56 \u0026lt;none\u0026gt; 6379/TCP 8s svc/redis-slave 172.30.75.245 \u0026lt;none\u0026gt; 6379/TCP 8s NAME DOCKER REPO TAGS UPDATED is/frontend 172.30.12.200:5000/fff/frontend is/redis-master 172.30.12.200:5000/fff/redis-master is/redis-slave 172.30.12.200:5000/fff/redis-slave v1 注意：\n 您必须有一个运行正常的 OpenShift 集群，该集群具有预先配置的 oc 上下文 (oc login)。  kompose down 您一旦将\u0026quot;复合(composed)\u0026rdquo; 应用部署到 Kubernetes，$ kompose down 命令将能帮您通过删除 Deployment 和 Service 对象来删除应用。如果需要删除其他资源，请使用 \u0026lsquo;kubectl\u0026rsquo; 命令。\n$ kompose --file docker-guestbook.yml down INFO Successfully deleted service: redis-master INFO Successfully deleted deployment: redis-master INFO Successfully deleted service: redis-slave INFO Successfully deleted deployment: redis-slave INFO Successfully deleted service: frontend INFO Successfully deleted deployment: frontend 注意：\n 您必须有一个运行正常的 Kubernetes 集群，该集群具有预先配置的 kubectl 上下文。  构建和推送 Docker 镜像 Kompose 支持构建和推送 Docker 镜像。如果 Docker Compose 文件中使用了 build 关键字，您的镜像将会：\n 使用文档中指定的 image 键自动构建 Docker 镜像 使用本地凭据推送到正确的 Docker 仓库  使用 Docker Compose 文件示例\nversion: \u0026#34;2\u0026#34; services: foo: build: \u0026#34;./build\u0026#34; image: docker.io/foo/bar 使用带有 build 键的 kompose up 命令：\n$ kompose up INFO Build key detected. Attempting to build and push image 'docker.io/foo/bar' INFO Building image 'docker.io/foo/bar' from directory 'build' INFO Image 'docker.io/foo/bar' from directory 'build' built successfully INFO Pushing image 'foo/bar:latest' to registry 'docker.io' INFO Attempting authentication credentials 'https://index.docker.io/v1/ INFO Successfully pushed image 'foo/bar:latest' to registry 'docker.io' INFO We are going to create Kubernetes Deployments, Services and PersistentVolumeClaims for your Dockerized application. If you need different kind of resources, use the 'kompose convert' and 'kubectl create -f' commands instead. INFO Deploying application in \u0026quot;default\u0026quot; namespace INFO Successfully created Service: foo INFO Successfully created Deployment: foo Your application has been deployed to Kubernetes. You can run 'kubectl get deployment,svc,pods,pvc' for details. 要想禁用该功能，或者使用 BuildConfig 中的版本（在 OpenShift 中），可以通过传递 --build (local|build-config|none) 参数来实现。\n# Disable building/pushing Docker images $ kompose up --build none # Generate Build Config artifacts for OpenShift $ kompose up --provider openshift --build build-config 其他转换方式 默认的 kompose 转换会生成 yaml 格式的 Kubernetes Deployment 和 Service 对象。 您可以选择通过 -j 参数生成 json 格式的对象。 您也可以替换生成 Replication Controllers 对象、Daemon Sets 或 Helm charts。\n$ kompose convert -j INFO Kubernetes file \u0026#34;redis-svc.json\u0026#34; created INFO Kubernetes file \u0026#34;web-svc.json\u0026#34; created INFO Kubernetes file \u0026#34;redis-deployment.json\u0026#34; created INFO Kubernetes file \u0026#34;web-deployment.json\u0026#34; created *-deployment.json 文件中包含 Deployment 对象。\n$ kompose convert --replication-controller INFO Kubernetes file \u0026#34;redis-svc.yaml\u0026#34; created INFO Kubernetes file \u0026#34;web-svc.yaml\u0026#34; created INFO Kubernetes file \u0026#34;redis-replicationcontroller.yaml\u0026#34; created INFO Kubernetes file \u0026#34;web-replicationcontroller.yaml\u0026#34; created *-replicationcontroller.yaml 文件包含 Replication Controller 对象。如果您想指定副本数（默认为 1），可以使用 --replicas 参数：$ kompose convert --replication-controller --replicas 3\n$ kompose convert --daemon-set INFO Kubernetes file \u0026#34;redis-svc.yaml\u0026#34; created INFO Kubernetes file \u0026#34;web-svc.yaml\u0026#34; created INFO Kubernetes file \u0026#34;redis-daemonset.yaml\u0026#34; created INFO Kubernetes file \u0026#34;web-daemonset.yaml\u0026#34; created *-daemonset.yaml 文件包含 Daemon Set 对象。\n如果您想生成 Helm 可用的 Chart，只需简单的执行下面的命令：\n$ kompose convert -c INFO Kubernetes file \u0026#34;web-svc.yaml\u0026#34; created INFO Kubernetes file \u0026#34;redis-svc.yaml\u0026#34; created INFO Kubernetes file \u0026#34;web-deployment.yaml\u0026#34; created INFO Kubernetes file \u0026#34;redis-deployment.yaml\u0026#34; created chart created in \u0026#34;./docker-compose/\u0026#34; $ tree docker-compose/ docker-compose ├── Chart.yaml ├── README.md └── templates ├── redis-deployment.yaml ├── redis-svc.yaml ├── web-deployment.yaml └── web-svc.yaml 这个图标结构旨在为构建 Helm Chart 提供框架。\n标签 kompose 支持 docker-compose.yml 文件中用于 Kompose 的标签，以便在转换时明确定义 Service 的行为。\n kompose.service.type 定义要创建的 Service 类型。  version: \u0026#34;2\u0026#34; services: nginx: image: nginx dockerfile: foobar build: ./foobar cap_add: - ALL container_name: foobar labels: kompose.service.type: nodeport  kompose.service.expose 定义 是否允许从集群外部访问 Service。如果该值被设置为 \u0026ldquo;true\u0026rdquo;，提供程序将自动设置端点，对于任何其他值，该值将被设置为主机名。如果在 Service 中定义了多个端口，则选择第一个端口作为公开端口。  对于 Kubernetes 驱动程序，创建了一个 Ingress 资源，并且假定已经配置了相应的 Ingress 控制器。 对于 OpenShift 驱动程序, 创建一个 route。    例如：\nversion: \u0026#34;2\u0026#34; services: web: image: tuna/docker-counter23 ports: - \u0026#34;5000:5000\u0026#34; links: - redis labels: kompose.service.expose: \u0026#34;counter.example.com\u0026#34; redis: image: redis:3.0 ports: - \u0026#34;6379\u0026#34; 当前支持的选项有:\n   键 值     kompose.service.type nodeport / clusterip / loadbalancer   kompose.service.expose true / hostname    . note \u0026gt;}}\nkompose.service.type 标签应该只用ports来定义，否则 kompose 会失败。 . /note \u0026gt;}}\n重启 如果你想创建没有控制器的普通 Pod，可以使用 docker-compose 的 restart 结构来定义它。请参考下表了解 restart 的不同参数。\n   docker-compose restart 创建的对象 Pod restartPolicy     \u0026quot;\u0026quot; 控制器对象 Always   always 控制器对象 Always   on-failure Pod OnFailure   no Pod Never    . note \u0026gt;}}\n控制器对象可以是 deployment 或 replicationcontroller 等。 . /note \u0026gt;}}\n例如，pival Service 将在这里变成 Pod。这个容器的计算值为 pi。\nversion: \u0026#39;2\u0026#39; services: pival: image: perl command: [\u0026#34;perl\u0026#34;, \u0026#34;-Mbignum=bpi\u0026#34;, \u0026#34;-wle\u0026#34;, \u0026#34;print bpi(2000)\u0026#34;] restart: \u0026#34;on-failure\u0026#34; 关于 Deployment Config 的提醒 如果 Docker Compose 文件中为服务声明了卷，Deployment (Kubernetes) 或 DeploymentConfig (OpenShift) 的策略会从 \u0026ldquo;RollingUpdate\u0026rdquo; (默认) 变为 \u0026ldquo;Recreate\u0026rdquo;。 这样做的目的是为了避免服务的多个实例同时访问卷。\n如果 Docker Compose 文件中的服务名包含 _ (例如 web_service)，那么将会被替换为 -，服务也相应的会重命名(例如 web-service)。 Kompose 这样做的原因是 \u0026ldquo;Kubernetes\u0026rdquo; 不允许对象名称中包含 _。\nDocker Compose 版本 Kompose 支持的 Docker Compose 版本包括：1、2 和 3。有限支持 2.1 和 3.2 版本，因为它们还在实验阶段。\n所有三个版本的兼容性列表请查看我们的 转换文档，文档中列出了所有不兼容的 Docker Compose 关键字。\n"
},
{
	"uri": "https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "摘要 ┌──────────────────────────────────────────────────────────┐ │ KUBEADM │ │ 轻松创建一个安全的 Kubernetes 集群 │ │ │ │ 给我们反馈意见的地址： │ │ https://github.com/kubernetes/kubeadm/issues │ └──────────────────────────────────────────────────────────┘ 用途示例：\n创建一个有两台机器的集群，包含一个主节点（用来控制集群），和一个工作节点（运行您的工作负载，像 Pod 和 Deployment）。\n┌──────────────────────────────────────────────────────────┐ │ 在第一台机器上： │ ├──────────────────────────────────────────────────────────┤ │ control-plane# kubeadm init │ └──────────────────────────────────────────────────────────┘ ┌──────────────────────────────────────────────────────────┐ │ 在第二台机器上： │ ├──────────────────────────────────────────────────────────┤ │ worker# kubeadm join \u0026amp;lt;arguments-returned-from-init\u0026amp;gt;│ └──────────────────────────────────────────────────────────┘ 您可以重复第二步，向集群添加更多机器。\n选项 "
},
{
	"uri": "https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_alpha/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "概要 kubeadm 实验子命令\n选项 从父命令继承的选项 "
},
{
	"uri": "https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_alpha_certs_certificate-key/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "概要 该命令将打印出可以与 \u0026ldquo;init\u0026rdquo; 命令一起使用的安全的随机生成的证书密钥。\n您也可以使用 kubeadm init --upload-certs 而无需指定证书密钥，它将为您生成并打印一个证书密钥。\nkubeadm alpha certs certificate-key [flags] 选项 从父命令继承的选项 "
},
{
	"uri": "https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_alpha_certs_check-expiration/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "概要 检查 kubeadm 管理的本地 PKI 中证书的到期时间。\nkubeadm alpha certs check-expiration [flags] 选项 继承于父命令的选项 "
},
{
	"uri": "https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_alpha_certs_renew/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "概要 此命令并非设计用来单独运行。请参阅可用子命令列表。\nkubeadm alpha certs renew [flags] 选项 从父命令继承的选项 "
},
{
	"uri": "https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_alpha_certs_renew_admin.conf/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "概要 续订 kubeconfig 文件中嵌入的证书，供管理员 和 kubeadm 自身使用。\n无论证书的到期日期如何，续订都是无条件进行的；SAN 等额外属性将基于现有文件/证书，因此无需重新提供它们。\n默认情况下，续订会尝试使用由 kubeadm 管理的本地 PKI 中的证书机构；作为替代方案， 也可以使用 K8s certificate API 进行证书续订，或者（作为最后一种选择）生成 CSR 请求。\n续订后，为了使更改生效，需要重新启动控制平面组件，并最终重新分发更新的证书，以防证书文件在其他地方使用。\nkubeadm alpha certs renew admin.conf [flags] 选项 从父命令继承的选项 "
},
{
	"uri": "https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_alpha_certs_renew_all/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "概要 续订运行控制平面所需的所有已知证书。续订是无条件进行的，与到期日期无关。续订也可以单独运行以进行更多控制。\nkubeadm alpha certs renew all [flags] 选项 从父命令继承的选项 "
},
{
	"uri": "https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_alpha_certs_renew_apiserver-etcd-client/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "概要 续订 apiserver 用于访问 etcd 的证书。\n无论证书的到期日期如何，续订都会无条件地进行；SAN 等额外属性将基于现有文件/证书，因此无需重新提供它们。\n默认情况下，续订尝试在 kubeadm 管理的本地 PKI 中使用证书颁发机构；作为替代方案， 可以使用 K8s 证书 API 进行证书更新，或者作为最后一个选择来生成 CSR 请求。\n续订后，为了使更改生效，需要重新启动控制平面组件，并最终重新分发更新的证书，以防文件在其他地方使用。\nkubeadm alpha certs renew apiserver-etcd-client [flags] 选项 从父命令继承的选项 "
},
{
	"uri": "https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_alpha_certs_renew_apiserver-kubelet-client/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "概要 续订 apiserver 用于连接 kubelet 的证书。\n无论证书的到期日期如何，续订都会无条件地进行；SAN 等额外属性将基于现有文件/证书，因此无需重新提供它们。\n默认情况下，续订尝试使用位于 kubeadm 所管理的本地 PKI 中的证书颁发机构；作为替代方案， 也可能调用 K8s 证书 API 进行证书更新；亦或者，作为最后一个选择，生成 CSR 请求。\n续订后，为了使更改生效，需要重新启动控制平面组件，并最终重新分发更新的证书，以防文件在其他地方使用。\nkubeadm alpha certs renew apiserver-kubelet-client [flags] 选项 从父命令继承的选项 "
},
{
	"uri": "https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_alpha_certs_renew_apiserver/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "概要 续订用于提供 Kubernetes API 的证书。\n无论证书的到期日期如何，续订都会无条件地进行；SAN 等额外属性将基于现有文件/证书，因此无需重新提供它们。\n默认情况下，续订尝试在 kubeadm 管理的本地 PKI 中使用证书颁发机构；作为替代方案， 可以使用 K8s 证书 API 进行证书更新，或者作为最后一个选择来生成 CSR 请求。\n续订后，为了使更改生效，需要重新启动控制平面组件，并最终重新分发更新的证书，以防文件在其他地方使用。\nkubeadm alpha certs renew apiserver [flags] 选项 从父命令继承的选项 "
},
{
	"uri": "https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_alpha_certs_renew_controller-manager.conf/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "概要 续订 kubeconfig 文件中嵌入的证书，以供控制器管理器（controller manager）使用。\n续订无条件地进行，与证书的到期日期无关；SAN 等额外属性将基于现有的文件/证书，因此无需重新提供它们。\n默认情况下，续订会尝试在 kubeadm 管理的本地 PKI 中使用证书颁发机构；作为替代方案， 可以使用 K8s 证书 API 进行证书续订；亦或者，作为最后一种选择，生成 CSR 请求。\n续订后，为了使更改生效，需要重新启动控制平面组件，并最终重新分发更新的证书，以防文件在其他地方使用。\nkubeadm alpha certs renew controller-manager.conf [flags] 选项 从父命令继承的选项 "
},
{
	"uri": "https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_alpha_certs_renew_etcd-healthcheck-client/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "概要 续订 etcd 健康检查的活跃性探针的证书。\n无论证书的到期日期如何，续订都是无条件进行的；SAN 等额外属性将基于现有文件/证书，因此无需重新提供它们。\n默认情况下，续订会尝试使用由 kubeadm 管理的本地 PKI 中的证书机构；作为替代方案，也可以使用 K8s certificate API 进行证书续订，或者（作为最后一种选择）生成 CSR 请求。\n续订后，为了使更改生效，需要重新启动控制平面组件，并最终重新分发更新的证书，以防证书文件在其他地方使用。\nkubeadm alpha certs renew etcd-healthcheck-client [flags] 选项 从父命令继承的选项 "
},
{
	"uri": "https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_alpha_certs_renew_etcd-peer/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "概要 续订 etcd 节点间用来相互通信的证书。\n无论证书的到期日期如何，续订都是无条件进行的；SAN 等额外属性将基于现有文件/证书，因此无需重新提供它们。\n默认情况下，续订会尝试使用由 kubeadm 管理的本地 PKI 中的证书机构；作为替代方案，也可以使用 K8s certificate API 进行证书续订，或者（作为最后一种选择）生成 CSR 请求。\n续订后，为了使更改生效，需要重新启动控制平面组件，并最终重新分发更新的证书，以防证书文件在其他地方使用。\nkubeadm alpha certs renew etcd-peer [flags] 选项 从父命令继承的选项 "
},
{
	"uri": "https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_alpha_certs_renew_etcd-server/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "概要 续订用于提供 etcd 的证书。\n续订无条件地进行，与证书的到期日期无关；SAN 等额外属性将基于现有的文件/证书，因此无需重新提供它们。\n默认情况下，续订会尝试在 kubeadm 管理的本地 PKI 中使用证书颁发机构；作为替代方案，可以使用 K8s 证书 API 进行证书续订，或者作为最后一种选择来生成 CSR 请求。\n续订后，为了使更改生效，需要重新启动控制平面组件，并最终重新分发更新的证书，以防文件在其他地方使用。\nkubeadm alpha certs renew etcd-server [flags] 选项 从父命令继承的选项 "
},
{
	"uri": "https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_alpha_certs_renew_front-proxy-client/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "概要 为前端代理客户端续订证书。\n无论证书的到期日期如何，续订都会无条件地进行；SAN 等额外属性将基于现有文件/证书，因此无需重新提供它们。\n默认情况下，续订尝试使用位于 kubeadm 所管理的本地 PKI 中的证书颁发机构；作为替代方案， 也可以使用 K8s 证书 API 进行证书续订；亦或者，作为最后一种方案，生成 CSR 请求。\n续订后，为了使更改生效，需要重新启动控制平面组件，并最终重新分发更新的证书，以防文件在其他地方使用。\nkubeadm alpha certs renew front-proxy-client [flags] 选项 从父命令继承的选项 "
},
{
	"uri": "https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_alpha_certs_renew_scheduler.conf/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "概要 续订 kubeconfig 文件中嵌入的证书，以供调度管理器使用。\n续订无条件地进行，与证书的到期日期无关；SAN 等额外属性将基于现有的文件/证书，因此无需重新提供它们。\n默认情况下，续订会尝试在 kubeadm 管理的本地 PKI 中使用证书颁发机构；作为替代方案，可以使用 K8s 证书 API 进行证书续订；亦或者，作为最后一种选择，生成 CSR 请求。\n续订后，为了使更改生效，需要重新启动控制平面组件，并最终重新分发更新的证书，以防文件在其他地方使用。\nkubeadm alpha certs renew scheduler.conf [flags] 选项 从父命令继承的选项 "
},
{
	"uri": "https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_alpha_kubeconfig/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "概要 kubeconfig 文件应用程序。\nAlpha 免责声明：此命令当前为 alpha 功能。\n选项 从父命令继承的选项 "
},
{
	"uri": "https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_alpha_kubeconfig_user/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "概要 为其他用户输出 kubeconfig 文件。\nAlpha 免责声明：此命令当前为 Alpha 功能。\nkubeadm alpha kubeconfig user [flags] 示例 # 为名为 foo 的其他用户输出 kubeconfig 文件 kubeadm alpha kubeconfig user --client-name=foo 选项 从父命令继承的选项 "
},
{
	"uri": "https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_alpha_kubelet/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "概要 此命令并非设计用来单独运行。请参阅可用子命令列表。\n选项 从父命令继承的选项 "
},
{
	"uri": "https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_alpha_kubelet_config/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "概要 此命令并非设计用来单独运行。请参阅可用子命令列表。\n选项 从父命令继承的选项 "
},
{
	"uri": "https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_alpha_kubelet_config_download/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "概要 从集群中形式为 \u0026ldquo;kubelet-config-1.X\u0026rdquo; 的 ConfigMap 中下载 kubelet 配置，其中 X 是 kubelet 的次要版本。 kubeadm 要么通过执行 \u0026ldquo;kubelet \u0026ndash;version\u0026rdquo; 自动检测 kubelet 版本，要么传递 \u0026ndash;kubelet-version 参数。\nAlpha 免责声明：此命令当前为 Alpha 功能。\nkubeadm alpha kubelet config download [flags] 示例 # 从集群中的 ConfigMap 下载 kubelet 配置。自动检测 kubelet 版本。 kubeadm alpha phase kubelet config download # 从集群中的 ConfigMap 下载 kubelet 配置。使用特定的所需 kubelet 版本。 kubeadm alpha phase kubelet config download --kubelet-version 1.16.0 选项 从父命令继承的选项 "
},
{
	"uri": "https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_alpha_kubelet_config_enable-dynamic/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "概要 针对集群中的 kubelet-config-1.X ConfigMap 启用或更新节点的动态 kubelet 配置，其中 X 是所需 kubelet 版本的次要版本。\n警告：此功能仍处于试验阶段，默认情况下处于禁用状态。仅当知道自己在做什么时才启用它，因为在此阶段它可能会产生令人惊讶的副作用。\nAlpha 免责声明：此命令当前为 Alpha 功能。\nkubeadm alpha kubelet config enable-dynamic [flags] 示例  # 为节点启用动态 kubelet 配置。 kubeadm alpha phase kubelet enable-dynamic-config --node-name node-1 --kubelet-version 1.16.0 WARNING: This feature is still experimental, and disabled by default. Enable only if you know what you are doing, as it may have surprising side-effects at this stage. 选项 从父命令继承的选项 "
},
{
	"uri": "https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_alpha_selfhosting/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "概要 此命令并非设计用来单独运行。请参阅可用子命令列表。\n选项 从父命令继承的选项 "
},
{
	"uri": "https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_alpha_selfhosting_pivot/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "概要 将用于控制平面组件的静态 Pod 文件转换为通过 Kubernetes API 配置的自托管 DaemonSet。\n有关自托管的限制，请参阅相关文档。\nAlpha 免责声明：此命令当前为 alpha 功能。\nkubeadm alpha selfhosting pivot [flags] 示例 # 将静态 Pod 托管的控制平面转换为自托管的控制平面。 kubeadm alpha phase self-hosting convert-from-staticpods 选项 从父命令继承的选项 "
},
{
	"uri": "https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_completion/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "概要 为指定的 shell（bash 或 zsh）输出 shell 自动补全代码。 必须激活 shell 代码以提供交互式 kubeadm 命令补全。这可以通过加载 .bash_profile 文件完成。\n注意: 此功能依赖于 bash-completion 框架。\n在 Mac 上使用 homebrew 安装:\nbrew install bash-completion 安装后，必须激活 bash_completion。这可以通过在 .bash_profile 文件中添加下面的命令行来完成\nsource $(brew --prefix)/etc/bash_completion 如果在 Linux 上没有安装 bash-completion，请通过您的发行版的包管理器安装 bash-completion 软件包。\nzsh 用户注意事项：[1] zsh 自动补全仅在 \u0026gt;=v5.2 及以上版本中支持。\nkubeadm completion SHELL [flags] 示例 # 在 Mac 上使用 homebrew 安装 bash completion brew install bash-completion printf \u0026quot;\\n# Bash completion support\\nsource $(brew --prefix)/etc/bash_completion\\n\u0026quot; \u0026gt;\u0026gt; $HOME/.bash_profile source $HOME/.bash_profile # 将 bash 版本的 kubeadm 自动补全代码加载到当前 shell 中 source \u0026lt;(kubeadm completion bash) # 将 bash 自动补全完成代码写入文件并且从 .bash_profile 文件加载它 printf \u0026quot;\\n# Kubeadm shell completion\\nsource '$HOME/.kube/kubeadm_completion.bash.inc'\\n\u0026quot; \u0026gt;\u0026gt; $HOME/.bash_profile source $HOME/.bash_profile # 将 zsh 版本的 kubeadm 自动补全代码加载到当前 shell 中 source \u0026lt;(kubeadm completion zsh) 选项 从父命令继承的选项 "
},
{
	"uri": "https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_config/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "概要 kube-system 命名空间里有一个名为 \u0026ldquo;kubeadm-config\u0026rdquo; 的 ConfigMap，kubeadm 用它来存储有关集群的内部配置。 kubeadm CLI v1.8.0+ 通过一个配置自动创建该 ConfigMap，这个配置是和 \u0026lsquo;kubeadm init\u0026rsquo; 共用的。 但是您如果使用 kubeadm v1.7.x 或更低的版本初始化集群，那么必须使用 \u0026lsquo;config upload\u0026rsquo; 命令创建该 ConfigMap。 这是必要的操作，目的是使 \u0026lsquo;kubeadm upgrade\u0026rsquo; 能够正确地配置升级后的集群。\nkubeadm config [flags] 选项 从父命令继承的选项 "
},
{
	"uri": "https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_config_images/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "概要 与 kubeadm 使用的容器镜像交互。\nkubeadm config images [flags] 选项 从父命令继承的选项 "
},
{
	"uri": "https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_config_images_list/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "概要 打印 kubeadm 要使用的镜像列表。配置文件用于自定义任何镜像或镜像存储库。\nkubeadm config images list [flags] 选项 从父命令继承的选项 "
},
{
	"uri": "https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_config_images_pull/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "概要 拉取 kubeadm 使用的镜像。\nkubeadm config images pull [flags] 选项 从父命令继承的选项 "
},
{
	"uri": "https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_config_migrate/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "概要 此命令允许您在 CLI 工具中将本地旧版本的配置对象转换为最新支持的版本，而无需变更集群中的任何内容。在此版本的 kubeadm 中，支持以下 API 版本：\n kubeadm.k8s.io/v1beta2  因此，无论您在此处传递 \u0026ndash;old-config 参数的版本是什么，当写入到 stdout 或 \u0026ndash;new-config （如果已指定）时， 都会读取、反序列化、默认、转换、验证和重新序列化 API 对象。\n换句话说，如果您将此文件传递给 \u0026ldquo;kubeadm init\u0026rdquo;，则该命令的输出就是 kubeadm 实际上在内部读取的内容。\nkubeadm config migrate [flags] 选项 从父命令继承的选项 "
},
{
	"uri": "https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_config_print/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "概要 此命令显示所提供子命令的配置。 有关详细信息，请参阅：https://godoc.org/k8s.io/kubernetes/cmd/kubeadm/app/apis/kubeadm/v1beta2\nkubeadm config print [flags] 选项 从父命令继承的选项 "
},
{
	"uri": "https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_config_print_init-defaults/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "概要 此命令打印对象，例如用于 \u0026lsquo;kubeadm init\u0026rsquo; 的默认 init 配置对象。\n请注意，Bootstrap Token 字段之类的敏感值已替换为 {\u0026ldquo;abcdef.0123456789abcdef\u0026rdquo; \u0026quot;\u0026rdquo; \u0026ldquo;nil\u0026rdquo; \u0026lt;nil\u0026gt; [] []} 之类的占位符值以通过验证，但不执行创建令牌的实际计算。\nkubeadm config print init-defaults [flags] 选项 从父命令继承的选项 "
},
{
	"uri": "https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_config_print_join-defaults/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "概要 此命令打印对象，例如用于 \u0026lsquo;kubeadm join\u0026rsquo; 的默认 join 配置对象。\n请注意，诸如启动引导令牌字段之类的敏感值已替换为 {\u0026ldquo;abcdef.0123456789abcdef\u0026rdquo; \u0026quot;\u0026rdquo; \u0026ldquo;nil\u0026rdquo; \u0026lt;nil\u0026gt; [] []} 之类的占位符值以通过验证，但不执行创建令牌的实际计算。\nkubeadm config print join-defaults [flags] 选项 从父命令继承的选项 "
},
{
	"uri": "https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_config_view/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "概要 使用此命令，可以查看 kubeadm 配置的集群中的 ConfigMap。 该配置位于 \u0026ldquo;kube-system\u0026rdquo; 命名空间中的名为 \u0026ldquo;kubeadm-config\u0026rdquo; 的 ConfigMap 中。\nkubeadm config view [flags] 选项 继承于父命令的选项 "
},
{
	"uri": "https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_init/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "概要 运行此命令来搭建 Kubernetes 控制平面节点。\n\u0026ldquo;init\u0026rdquo; 命令执行以下阶段：\npreflight Run pre-flight checks kubelet-start Write kubelet settings and (re)start the kubelet certs Certificate generation /ca Generate the self-signed Kubernetes CA to provision identities for other Kubernetes components /apiserver Generate the certificate for serving the Kubernetes API /apiserver-kubelet-client Generate the certificate for the API server to connect to kubelet /front-proxy-ca Generate the self-signed CA to provision identities for front proxy /front-proxy-client Generate the certificate for the front proxy client /etcd-ca Generate the self-signed CA to provision identities for etcd /etcd-server Generate the certificate for serving etcd /etcd-peer Generate the certificate for etcd nodes to communicate with each other /etcd-healthcheck-client Generate the certificate for liveness probes to healthcheck etcd /apiserver-etcd-client Generate the certificate the apiserver uses to access etcd /sa Generate a private key for signing service account tokens along with its public key kubeconfig Generate all kubeconfig files necessary to establish the control plane and the admin kubeconfig file /admin Generate a kubeconfig file for the admin to use and for kubeadm itself /kubelet Generate a kubeconfig file for the kubelet to use *only* for cluster bootstrapping purposes /controller-manager Generate a kubeconfig file for the controller manager to use /scheduler Generate a kubeconfig file for the scheduler to use control-plane Generate all static Pod manifest files necessary to establish the control plane /apiserver Generates the kube-apiserver static Pod manifest /controller-manager Generates the kube-controller-manager static Pod manifest /scheduler Generates the kube-scheduler static Pod manifest etcd Generate static Pod manifest file for local etcd /local Generate the static Pod manifest file for a local, single-node local etcd instance upload-config Upload the kubeadm and kubelet configuration to a ConfigMap /kubeadm Upload the kubeadm ClusterConfiguration to a ConfigMap /kubelet Upload the kubelet component config to a ConfigMap upload-certs Upload certificates to kubeadm-certs mark-control-plane Mark a node as a control-plane bootstrap-token Generates bootstrap tokens used to join a node to a cluster addon Install required addons for passing Conformance tests /coredns Install the CoreDNS addon to a Kubernetes cluster /kube-proxy Install the kube-proxy addon to a Kubernetes cluster kubeadm init [flags] 选项 从父命令继承的选项 "
},
{
	"uri": "https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_init_phase/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "概要 使用此命令可以调用 init 工作流程的单个阶段\n选项 继承于父命令的选择项 "
},
{
	"uri": "https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_init_phase_addon/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "概要 此命令并非设计用来单独运行。请参阅可用子命令列表。\nkubeadm init phase addon [flags] 选项 继承于父命令的选项 "
},
{
	"uri": "https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_init_phase_addon_all/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "概要 安装所有插件（addon）\nkubeadm init phase addon all [flags] 选项 继承于父命令的选项 "
},
{
	"uri": "https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_init_phase_addon_coredns/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "概要 通过 API 服务器安装 CoreDNS 附加组件。请注意，即使 DNS 服务器已部署，在安装 CNI 之前 DNS 服务器不会被调度执行。\nkubeadm init phase addon coredns [flags] 选项 继承于父命令的选项 "
},
{
	"uri": "https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_init_phase_addon_kube-proxy/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "概要 通过 API 服务器安装 kube-proxy 附加组件。\nkubeadm init phase addon kube-proxy [flags] 选项 继承于父命令的选项 "
},
{
	"uri": "https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_init_phase_bootstrap-token/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "概要 启动引导令牌（bootstrap token）用于在即将加入集群的节点和控制平面节点之间建立双向信任。\n该命令使启动引导令牌（bootstrap token）所需的所有配置生效，然后创建初始令牌。\nkubeadm init phase bootstrap-token [flags] 示例 # 进行所有引导令牌配置，并创建一个初始令牌，功能上与 kubeadm init 生成的令牌等效。 kubeadm init phase bootstrap-token 选项 继承于父命令的选项 "
},
{
	"uri": "https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_init_phase_certs/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "概要 此命令并非设计用来单独运行。请参阅可用子命令列表。\nkubeadm init phase certs [flags] 选项 从父指令中继承的选项 "
},
{
	"uri": "https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_init_phase_certs_all/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "概要 生成所有证书\nkubeadm init phase certs all [flags] 选项 继承于父命令的选项 "
},
{
	"uri": "https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_init_phase_certs_apiserver-etcd-client/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "概要 生成 apiserver 用于访问 etcd 的证书，并将其保存到 apiserver-etcd-client.cert 和 apiserver-etcd-client.key 文件中。\n如果两个文件都已存在，则 kubeadm 将跳过生成步骤，使用现有文件。\nAlpha 免责声明：此命令当前为 Alpha 功能。\nkubeadm init phase certs apiserver-etcd-client [flags] 选项 继承于父命令的选项 "
},
{
	"uri": "https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_init_phase_certs_apiserver-kubelet-client/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "概要 生成供 API 服务器连接 kubelet 的证书，并将其保存到 apiserver-kubelet-client.cert 和 apiserver-kubelet-client.key 文件中。\n如果两个文件都已存在，则 kubeadm 将跳过生成步骤，使用现有文件。\nAlpha 免责声明：此命令当前为 Alpha 功能。\nkubeadm init phase certs apiserver-kubelet-client [flags] 选项 继承于父命令的选项 "
},
{
	"uri": "https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_init_phase_certs_apiserver/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "概要 生成用于服务 Kubernetes API 的证书，并将其保存到 apiserver.cert 和 apiserver.key 文件中。\n默认 SAN 是 kubernetes、kubernetes.default、kubernetes.default.svc、kubernetes.default.svc.cluster.local、10.96.0.1、127.0.0.1。\n如果两个文件都已存在，则 kubeadm 将跳过生成步骤，使用现有文件。\nAlpha 免责声明：此命令当前为 Alpha 功能。\nkubeadm init phase certs apiserver [flags] 选项 继承于父命令的选项 "
},
{
	"uri": "https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_init_phase_certs_ca/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "概要 生成自签名的 Kubernetes CA 以提供其他 Kubernetes 组件的身份，并将其保存到 ca.cert 和 ca.key 文件中。\n如果两个文件都已存在，则 kubeadm 将跳过生成步骤，使用现有文件。\nAlpha 免责声明：此命令当前为 Alpha 功能。\nkubeadm init phase certs ca [flags] 选项 继承于父命令的选项 "
},
{
	"uri": "https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_init_phase_certs_etcd-ca/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "概要 生成用于为 etcd 设置身份的自签名 CA，并将其保存到 etcd/ca.cert 和 etcd/ca.key 文件中。\n如果两个文件都已存在，则 kubeadm 将跳过生成步骤，使用现有文件。\nAlpha 免责声明：此命令当前为 Alpha 功能。\nkubeadm init phase certs etcd-ca [flags] 选项 继承于父命令的选项 "
},
{
	"uri": "https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_init_phase_certs_etcd-healthcheck-client/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "概要 生成用于 etcd 健康检查的活跃性探针的证书，并将其保存到 healthcheck-client.cert 和 etcd/healthcheck-client.key 文件中。\n如果两个文件都已存在，则 kubeadm 将跳过生成步骤，使用现有文件。\nAlpha 免责声明：此命令当前为 alpha 功能。\nkubeadm init phase certs etcd-healthcheck-client [flags] 选项 继承于父命令的选项 "
},
{
	"uri": "https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_init_phase_certs_etcd-peer/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "概要 生成 etcd 节点相互通信的证书，并将其保存到 etcd/peer.cert 和 etcd/peer.key 文件中。\n默认 SAN 为 localhost、127.0.0.1、127.0.0.1、:: 1\n如果两个文件都已存在，则 kubeadm 将跳过生成步骤，使用现有文件。\nAlpha 免责声明：此命令当前为 alpha 功能。\nkubeadm init phase certs etcd-peer [flags] 选项 继承于父命令的选项 "
},
{
	"uri": "https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_init_phase_certs_etcd-server/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "概要 生成用于提供 etcd 服务的证书，并将其保存到 etcd/server.cert 和 etcd/server.key 文件中。\n默认 SAN 为 localhost、127.0.0.1、127.0.0.1、:: 1\n如果两个文件都已存在，则 kubeadm 将跳过生成步骤，使用现有文件。\nAlpha 免责声明：此命令当前为 alpha 功能。\nkubeadm init phase certs etcd-server [flags] 选项 继承于父命令的选项 "
},
{
	"uri": "https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_init_phase_certs_front-proxy-ca/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "概要 生成自签名 CA 来提供前端代理的身份，并将其保存到 front-proxy-ca.cert 和 front-proxy-ca.key 文件中。\n如果两个文件都已存在，kubeadm 将跳过生成步骤并将使用现有文件。\nAlpha 免责声明：此命令目前是 alpha 阶段。\nkubeadm init phase certs front-proxy-ca [flags] 选项 从父命令继承的选项 "
},
{
	"uri": "https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_init_phase_certs_front-proxy-client/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "概要 为前端代理客户端生成证书，并将其保存到 front-proxy-client.cert 和 front-proxy-client.key 文件中。 如果两个文件都已存在，kubeadm 将跳过生成步骤并将使用现有文件。 Alpha 免责声明：此命令目前是 alpha 阶段。\nkubeadm init phase certs front-proxy-client [flags] 选项 从父命令继承的选项 "
},
{
	"uri": "https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_init_phase_certs_sa/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "概要 生成用于签名 service account 令牌的私钥及其公钥，并将其保存到 sa.key 和 sa.pub 文件中。如果两个文件都已存在，则 kubeadm 会跳过生成步骤，而将使用现有文件。\nAlpha 免责声明：此命令当前为 alpha 阶段。\nkubeadm init phase certs sa [flags] 选项 继承于父命令的选项 "
},
{
	"uri": "https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_init_phase_control-plane/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "概要 此命令并非设计用来单独运行。请参阅可用子命令列表。\nkubeadm init phase control-plane [flags] 选项 继承于父命令的选项 "
},
{
	"uri": "https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_init_phase_control-plane_all/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "概要 生成所有的静态 Pod 清单文件\nkubeadm init phase control-plane all [flags] 示例 # 为 etcd 生成静态 Pod 清单文件，其功能等效于 kubeadm init 生成的文件。 kubeadm init phase control-plane all # 使用从配置文件读取的选项为 etcd 生成静态 Pod 清单文件。 kubeadm init phase control-plane all --config config.yaml 选项 从父指令继承的选项 "
},
{
	"uri": "https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_init_phase_control-plane_apiserver/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "概要 生成 kube-apiserver 静态 Pod 清单\nkubeadm init phase control-plane apiserver [flags] 选项 继承于父命令的选项 "
},
{
	"uri": "https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_init_phase_control-plane_controller-manager/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "概要 生成 kube-controller-manager 静态 Pod 清单\nkubeadm init phase control-plane controller-manager [flags] 选项 从父命令继承的选项 "
},
{
	"uri": "https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_init_phase_control-plane_scheduler/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "概要 生成 kube-scheduler 静态 Pod 清单\nkubeadm init phase control-plane scheduler [flags] 选项 继承于父命令的选项 "
},
{
	"uri": "https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_init_phase_etcd/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "概要 此命令并非设计用来单独运行。请参阅可用子命令列表。\nkubeadm init phase etcd [flags] 选项 继承于父命令的选项 "
},
{
	"uri": "https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_init_phase_etcd_local/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "概要 为本地单节点 etcd 实例生成静态 Pod 清单文件\nkubeadm init phase etcd local [flags] 示例 # 为 etcd 生成静态 Pod 清单文件，其功能等效于 kubeadm init 生成的文件。 kubeadm init phase etcd local # 使用从配置文件读取的选项为 etcd 生成静态 Pod 清单文件。 kubeadm init phase etcd local --config config.yaml 选项 继承于父命令的选项 "
},
{
	"uri": "https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_init_phase_kubeconfig/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "概要 此命令并非设计用来单独运行。请阅读可用子命令列表。\nkubeadm init phase kubeconfig [flags] 选项 从父命令继承的选项 "
},
{
	"uri": "https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_init_phase_kubeconfig_admin/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "概要 为管理员和 kubeadm 本身生成 kubeconfig 文件，并将其保存到 admin.conf 文件中。\nkubeadm init phase kubeconfig admin [flags] 选项 继承于父命令的选项 "
},
{
	"uri": "https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_init_phase_kubeconfig_all/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "概要 生成所有 kubeconfig 文件\nkubeadm init phase kubeconfig all [flags] 选项 继承于父命令的选项 "
},
{
	"uri": "https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_init_phase_kubeconfig_controller-manager/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "概要 生成控制器管理器要使用的 kubeconfig 文件，并保存到 controller-manager.conf 文件中。\nkubeadm init phase kubeconfig controller-manager [flags] 选项 继承于父命令的选项 "
},
{
	"uri": "https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_init_phase_kubeconfig_kubelet/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "概要 生成 kubelet 要使用的 kubeconfig 文件，并将其保存到 kubelet.conf 文件。\n请注意，该操作目的是仅应用于引导集群。在控制平面启动之后，应该从 CSR API 请求所有 kubelet 凭据。\nkubeadm init phase kubeconfig kubelet [flags] 选项 继承于父命令的选项 "
},
{
	"uri": "https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_init_phase_kubeconfig_scheduler/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "概要 生成调度器（scheduler）要使用的 kubeconfig 文件，并保存到 scheduler.conf 文件中。\nkubeadm init phase kubeconfig scheduler [flags] 选项 继承于父命令的选项 "
},
{
	"uri": "https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_init_phase_kubelet-start/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "概要 使用 kubelet 配置文件编写一个文件，并使用特定节点的 kubelet 设置编写一个环境文件，然后（重新）启动 kubelet。\nkubeadm init phase kubelet-start [flags] 示例 # 从 InitConfiguration 文件中写入带有 kubelet 参数的动态环境文件。 kubeadm init phase kubelet-start --config config.yaml 选项 从父命令继承的选项 "
},
{
	"uri": "https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_init_phase_mark-control-plane/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "概要 标记 Node 节点为控制平面节点\nkubeadm init phase mark-control-plane [flags] 示例 # 将控制平面标签和污点应用于当前节点，其功能等效于 kubeadm init执行的操作。 kubeadm init phase mark-control-plane --config config.yml # 将控制平面标签和污点应用于特定节点 kubeadm init phase mark-control-plane --node-name myNode 选项 从父命令继承的选项 "
},
{
	"uri": "https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_init_phase_preflight/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "概要 运行 kubeadm init 前的启动检查。\nkubeadm init phase preflight [flags] 案例 # 使用配置文件对 kubeadm init 进行启动检查。 kubeadm init phase preflight --config kubeadm-config.yml 选项 继承于父命令的选项 "
},
{
	"uri": "https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_init_phase_upload-certs/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "概要 此命令并非设计用来单独运行。请参阅可用子命令列表。\nkubeadm init phase upload-certs [flags] 选项 从父命令继承的选项 "
},
{
	"uri": "https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_init_phase_upload-config/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "概要 此命令并非设计用来单独运行。请参阅可用的子命令列表。\nkubeadm init phase upload-config [flags] 选项 从父命令中继承的选项 "
},
{
	"uri": "https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_init_phase_upload-config_all/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "概要 将所有配置上传到 ConfigMap\nkubeadm init phase upload-config all [flags] 选项 从父命令继承的选项 "
},
{
	"uri": "https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_init_phase_upload-config_kubeadm/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "概要 将 kubeadm ClusterConfiguration 上传到 kube-system 命名空间中名为 kubeadm-config 的 ConfigMap 中。 这样就可以正确配置系统组件，并在升级时提供无缝的用户体验。\n另外，可以使用 kubeadm 配置。\nkubeadm init phase upload-config kubeadm [flags] 示例 # 上传集群配置 kubeadm init phase upload-config --config=myConfig.yaml 选项 从父命令继承的选项 "
},
{
	"uri": "https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_init_phase_upload-config_kubelet/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "概要 将从 kubeadm InitConfiguration 对象提取的 kubelet 配置上传到集群中 kubelet-config-1.X 形式的 ConfigMap，其中 X 是当前（API 服务器）Kubernetes 版本的次要版本。\nkubeadm init phase upload-config kubelet [flags]\r示例 # 将 kubelet 配置从 kubeadm 配置文件上传到集群中的 ConfigMap。\rkubeadm init phase upload-config kubelet --config kubeadm.yaml\r选项 从父命令继承的选项 "
},
{
	"uri": "https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_join/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "摘要 当节点加入 kubeadm 初始化的集群时，我们需要建立双向信任。 这个过程可以分解为发现（让待加入节点信任 Kubernetes 控制平面节点）和 TLS 引导（让Kubernetes 控制平面节点信任待加入节点）两个部分。\n有两种主要的发现方案。 第一种方法是使用共享令牌和 API 服务器的 IP 地址。 第二种是提供一个文件 - 标准 kubeconfig 文件的一个子集。 该文件可以是本地文件，也可以通过 HTTPS URL 下载。 格式是 kubeadm join --discovery-token abcdef.1234567890abcdef 1.2.3.4:6443、kubeadm join--discovery-file path/to/file.conf 或者kubeadm join --discovery-file https://url/file.conf。 只能使用其中一种。 如果发现信息是从 URL 加载的，必须使用 HTTPS。 此外，在这种情况下，主机安装的 CA 包用于验证连接。\n如果使用共享令牌进行发现，还应该传递 \u0026ndash;discovery-token-ca-cert-hash 参数来验证 Kubernetes 控制平面节点提供的根证书颁发机构（CA）的公钥。 此参数的值指定为 \u0026ldquo;\u0026lt;hash-type\u0026gt;:\u0026lt;hex-encoded-value\u0026gt;\u0026quot;，其中支持的哈希类型为 \u0026ldquo;sha256\u0026rdquo;。哈希是通过 Subject Public Key Info（SPKI）对象的字节计算的（如 RFC7469）。 这个值可以从 \u0026ldquo;kubeadm init\u0026rdquo; 的输出中获得，或者可以使用标准工具进行计算。 可以多次重复 \u0026ndash;discovery-token-ca-cert-hash 参数以允许多个公钥。\n如果无法提前知道 CA 公钥哈希，则可以通过 \u0026ndash;discovery-token-unsafe-skip-ca-verification 参数禁用此验证。 这削弱了kubeadm 安全模型，因为其他节点可能会模仿 Kubernetes 控制平面节点。\nTLS 引导机制也通过共享令牌驱动。 这用于向 Kubernetes 控制平面节点进行临时的身份验证，以提交本地创建的密钥对的证书签名请求（CSR）。 默认情况下，kubeadm 将设置 Kubernetes 控制平面节点自动批准这些签名请求。 这个令牌通过 \u0026ndash;tls-bootstrap-token abcdef.1234567890abcdef 参数传入。\n通常两个部分会使用相同的令牌。 在这种情况下可以使用 \u0026ndash;token 参数，而不是单独指定每个令牌。\n\u0026ldquo;join [api-server-endpoint]\u0026rdquo; 命令执行下列阶段：\npreflight Run join pre-flight checks control-plane-prepare Prepare the machine for serving a control plane /download-certs [EXPERIMENTAL] Download certificates shared among control-plane nodes from the kubeadm-certs Secret /certs Generate the certificates for the new control plane components /kubeconfig Generate the kubeconfig for the new control plane components /control-plane Generate the manifests for the new control plane components kubelet-start Write kubelet settings, certificates and (re)start the kubelet control-plane-join Join a machine as a control plane instance /etcd Add a new local etcd member /update-status Register the new control-plane node into the ClusterStatus maintained in the kubeadm-config ConfigMap /mark-control-plane Mark a node as a control-plane kubeadm join [api-server-endpoint] [flags] 选项 从父命令继承的选项 "
},
{
	"uri": "https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_join_phase/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "概要 使用此命令来调用 join 工作流程的某个阶段\n选项 从父命令中继承的选项 "
},
{
	"uri": "https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_join_phase_control-plane-join/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "概要 添加作为控制平面实例的机器\nkubeadm join phase control-plane-join [flags] 示例 # 将机器作为控制平面实例加入 kubeadm join phase control-plane-join all 选项 从父命令中继承的选项 "
},
{
	"uri": "https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_join_phase_control-plane-join_all/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "概要 添加作为控制平面实例的机器\nkubeadm join phase control-plane-join all [flags] 选项 从父命令继承的选项 "
},
{
	"uri": "https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_join_phase_control-plane-join_etcd/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "概要 添加新的本地 etcd 成员\nkubeadm join phase control-plane-join etcd [flags] 选项 从父命令中继承的选项 "
},
{
	"uri": "https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_join_phase_control-plane-join_mark-control-plane/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "概要 将 Node 节点标记为控制平面节点\nkubeadm join phase control-plane-join mark-control-plane [flags] 选项 从父命令中继承的选项 "
},
{
	"uri": "https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_join_phase_control-plane-join_update-status/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "概要 将新的控制平面节点注册到 kubeadm-config ConfigMap 维护的 ClusterStatus 中\nkubeadm join phase control-plane-join update-status [flags] 选项 从父命令中继承的选项 "
},
{
	"uri": "https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_join_phase_control-plane-prepare/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "概要 准备为控制平面服务的机器\nkubeadm join phase control-plane-prepare [flags] 示例 # 准备为控制平面服务的机器 kubeadm join phase control-plane-prepare all 选项 从父命令中继承的选项 "
},
{
	"uri": "https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_join_phase_control-plane-prepare_all/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "概要 准备为控制平面服务的机器\nkubeadm join phase control-plane-prepare all [api-server-endpoint] [flags] 选项 从父命令继承的选项 "
},
{
	"uri": "https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_join_phase_control-plane-prepare_certs/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "概要 为新的控制平面组件生成证书\nkubeadm join phase control-plane-prepare certs [api-server-endpoint] [flags] 选项 从父命令继承的选项 "
},
{
	"uri": "https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_join_phase_control-plane-prepare_control-plane/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "概要 为新的控制平面组件生成清单（manifest）\nkubeadm join phase control-plane-prepare control-plane [flags] 选项 从父命令中继承的选项 "
},
{
	"uri": "https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_join_phase_control-plane-prepare_download-certs/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "概要 [实验]从 kubeadm-certs Secret 下载控制平面节点之间共享的证书\nkubeadm join phase control-plane-prepare download-certs [api-server-endpoint] [flags] 选项 从父命令中继承的选项 "
},
{
	"uri": "https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_join_phase_control-plane-prepare_kubeconfig/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "概要 为新的控制平面组件生成 kubeconfig\nkubeadm join phase control-plane-prepare kubeconfig [api-server-endpoint] [flags] 选项 从父命令中继承的选项 "
},
{
	"uri": "https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_join_phase_kubelet-start/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "概要 生成一个包含 KubeletConfiguration 的文件和一个包含特定于节点的 kubelet 配置的环境文件，然后（重新）启动 kubelet。\nkubeadm join phase kubelet-start [api-server-endpoint] [flags] 选项 从父命令继承的选项 "
},
{
	"uri": "https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_join_phase_preflight/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "概要 运行 kubeadm join 命令添加节点前检查。\nkubeadm join phase preflight [api-server-endpoint] [flags] 示例 # 使用配置文件运行 kubeadm join 命令添加节点前检查。 kubeadm join phase preflight --config kubeadm-config.yml 选项 从父命令继承的选项 "
},
{
	"uri": "https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_reset/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "概要 尽最大努力还原通过 \u0026lsquo;kubeadm init\u0026rsquo; 或者 \u0026lsquo;kubeadm join\u0026rsquo; 操作对主机所做的更改\n\u0026ldquo;reset\u0026rdquo; 命令执行以下阶段：\npreflight Run reset pre-flight checks update-cluster-status Remove this node from the ClusterStatus object. remove-etcd-member Remove a local etcd member. cleanup-node Run cleanup node. kubeadm reset [flags] 选项 从父命令继承的选项 "
},
{
	"uri": "https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_reset_phase/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "概要 使用此命令来调用 reset 工作流程的某个阶段\n选项 从父命令继承的选项 "
},
{
	"uri": "https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_reset_phase_cleanup-node/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "概要 执行 cleanup node（清理节点）操作。\nkubeadm reset phase cleanup-node [flags] 选项 从父命令继承的选项 "
},
{
	"uri": "https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_reset_phase_preflight/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "概要 kubeadm reset（重置）前运行启动前检查。\nkubeadm reset phase preflight [flags] 选项 从父命令继承的选项 "
},
{
	"uri": "https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_reset_phase_remove-etcd-member/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "概要 上传关于当前状态的配置，以便 \u0026lsquo;kubeadm upgrade\u0026rsquo; 以后可以知道如何配置升级后的集群。\nkubeadm config upload [flags] 选项 从父命令继承的选项 "
},
{
	"uri": "https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_reset_phase_update-cluster-status/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "概要 如果该节点是控制平面节点，从 ClusterStatus 对象中删除该节点。\nkubeadm reset phase update-cluster-status [flags] 选项 从父命令继承的选项 "
},
{
	"uri": "https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_token/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "概要 此命令管理引导令牌（bootstrap token）。它是可选的，仅适用于高级用例。\n简而言之，引导令牌（bootstrap token）用于在客户端和服务器之间建立双向信任。 当客户端（例如，即将加入集群的节点）需要时，可以使用引导令牌相信正在与之通信的服务器。 然后可以使用具有 “签名” 的引导令牌。\n引导令牌还可以作为一种允许对 API 服务器进行短期身份验证的方法（令牌用作 API 服务器信任客户端的方式），例如用于执行 TLS 引导程序。\n引导令牌准确来说是什么？\n 它是位于 kube-system 命名空间中类型为 “bootstrap.kubernetes.io/token” 的一个 Secret。 引导令牌的格式必须为 “[a-z0-9]{6}.[a-z0-9]{16}”，前一部分是公共令牌 ID，而后者是令牌秘钥，必须在任何情况下都保密！ 必须将 Secret 的名称命名为 “bootstrap-token-(token-id)”。  您可以在此处阅读有关引导令牌（bootstrap token）的更多信息： /docs/admin/bootstrap-tokens/\nkubeadm token [flags] 选项 从父命令继承的选项 "
},
{
	"uri": "https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_token_create/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "概要 这个命令将为你创建一个引导令牌。 您可以设置此令牌的用途，\u0026ldquo;有效时间\u0026rdquo; 和可选的人性化的描述。\n这里的 [token] 是指将要生成的实际令牌。 该令牌应该是一个通过安全机制生成的随机令牌，形式为 \u0026ldquo;[a-z0-9]{6}.[a-z0-9]{16}\u0026quot;。 如果没有给出 [token]，kubeadm 将生成一个随机令牌。\nkubeadm token create [token] 选项 从父命令继承的选项 "
},
{
	"uri": "https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_token_delete/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "概要 这个命令将为你删除指定的引导令牌列表。\n[token-value] 是要删除的 \u0026ldquo;[a-z0-9]{6}.[a-z0-9]{16}\u0026rdquo; 形式的完整令牌或者是 \u0026ldquo;[a-z0-9]{6}\u0026rdquo; 形式的的令牌 ID。\nkubeadm token delete [token-value] ... 选项 从父命令继承的选项 "
},
{
	"uri": "https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_token_generate/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "概要 此命令将打印一个随机生成的可以被 \u0026ldquo;init\u0026rdquo; 和 \u0026ldquo;join\u0026rdquo; 命令使用的引导令牌。 您不必使用此命令来生成令牌。你可以自己设定，只要格式符合 \u0026ldquo;[a-z0-9]{6}.[a-z0-9]{16}\u0026quot;。这个命令提供是为了方便生成规定格式的令牌。 您也可以使用 \u0026ldquo;kubeadm init\u0026rdquo; 并且不指定令牌，该命令会生成一个令牌并打印出来。\nkubeadm token generate [flags] 选项 从父命令继承的选项 "
},
{
	"uri": "https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_token_list/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "概要 此命令将为您列出所有的引导令牌。\nkubeadm token list [flags] 选项 从父命令继承的选项 "
},
{
	"uri": "https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_upgrade/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "概要 此命令能将集群平滑升级到新版本\nkubeadm upgrade [flags] 选项 继承于父命令的选项 "
},
{
	"uri": "https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_upgrade_apply/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "概要 将 Kubernetes 集群升级到指定版本\nkubeadm upgrade apply [version] 选项 从父命令继承的选项 "
},
{
	"uri": "https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_upgrade_diff/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "概述 显示哪些差异将被应用于现有的静态 pod 资源清单。参考: kubeadm upgrade apply \u0026ndash;dry-run\nkubeadm upgrade diff [version] [flags] 选项 从父命令继承的选项 "
},
{
	"uri": "https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_upgrade_node/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "概要 升级集群中某个节点的命令\n\u0026ldquo;node\u0026rdquo; 命令执行以下阶段：\ncontrol-plane 如果存在的话，升级部署在该节点上的管理面实例 kubelet-config 更新该节点上的 kubelet 配置 kubeadm upgrade node [flags] 选项 从父命令继承的选项 "
},
{
	"uri": "https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_upgrade_node_phase/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "概要 使用此命令调用 node 工作流的某个阶段\n选项 从父命令继承的选项 "
},
{
	"uri": "https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_upgrade_node_phase_control-plane/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "概要 升级部署在此节点上的控制平面实例，如果有的话\nkubeadm upgrade node phase control-plane [flags] 选项 从父命令继承的选项 "
},
{
	"uri": "https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_upgrade_node_phase_kubelet-config/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "从群集中 \u0026ldquo;kubelet-config-1.X\u0026rdquo; 的 ConfigMap 下载 kubelet 配置，其中 X 是kubelet 的次要版本。 kubeadm 使用 \u0026ndash;kubelet-version 参数来确定所需的 kubelet 版本。\nkubeadm upgrade node phase kubelet-config [flags] 选项 从父命令继承的选项 "
},
{
	"uri": "https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_upgrade_plan/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "概述 检查可升级到哪些版本，并验证您当前的集群是否可升级。 要跳过互联网检查，请传递可选的 [version] 参数\nkubeadm upgrade plan [version] [flags] 选项 从父命令继承的选项 "
},
{
	"uri": "https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_version/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "概要 打印 kubeadm 的版本\nkubeadm version [flags] 选项 从父命令继承的选项 "
},
{
	"uri": "https://lijun.in/reference/setup-tools/kubeadm/generated/readme/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "此目录下的所有文件都是从其他仓库自动生成的。 不要人工编辑它们。 您必须在上游仓库中编辑它们\n"
},
{
	"uri": "https://lijun.in/tutorials/clusters/apparmor/",
	"title": "AppArmor",
	"tags": [],
	"description": "",
	"content": ". feature-state for_k8s_version=\u0026quot;v1.4\u0026rdquo; state=\u0026quot;beta\u0026rdquo; \u0026gt;}}\nApparmor 是一个 Linux 内核安全模块，它补充了标准的基于 Linux 用户和组的安全模块将程序限制为有限资源集的权限。AppArmor 可以配置为任何应用程序减少潜在的攻击面，并且提供更加深入的防御。AppArmor 是通过配置文件进行配置的，这些配置文件被调整为报名单，列出了特定程序或者容器所需要的访问权限，如 Linux 功能、网络访问、文件权限等。每个配置文件都可以在强制模式(阻止访问不允许的资源)或投诉模式(仅报告冲突)下运行。\n. heading \u0026ldquo;objectives\u0026rdquo; %}}  查看如何在节点上加载配置文件示例 了解如何在 Pod 上强制执行配置文件 了解如何检查配置文件是否已加载 查看违反配置文件时会发生什么情况 查看无法加载配置文件时会发生什么情况  . heading \u0026ldquo;prerequisites\u0026rdquo; %}} 务必：\n Kubernetes 版本至少是 v1.4 \u0026ndash; AppArmor 在 Kubernetes v1.4 版本中才添加了对 AppArmor 的支持。早于 v1.4 版本的 Kubernetes 组件不知道新的 AppArmor 注释，并且将会 默认忽略 提供的任何 AppArmor 设置。为了确保您的 Pods 能够得到预期的保护，必须验证节点的 Kubelet 版本：  kubectl get nodes -o=jsonpath=$\u0026#39;{range .items[*]}{@.metadata.name}: {@.status.nodeInfo.kubeletVersion}\\n{end}\u0026#39; gke-test-default-pool-239f5d02-gyn2: v1.4.0 gke-test-default-pool-239f5d02-x1kf: v1.4.0 gke-test-default-pool-239f5d02-xwux: v1.4.0 AppArmor 内核模块已启用 \u0026ndash; 要使 Linux 内核强制执行 AppArmor 配置文件，必须安装并且启动 AppArmor 内核模块。默认情况下，有几个发行版支持该模块，如 Ubuntu 和 SUSE，还有许多发行版提供可选支持。要检查模块是否已启用，请检查 /sys/module/apparmor/parameters/enabled 文件：  cat /sys/module/apparmor/parameters/enabled Y 如果 Kubelet 包含 AppArmor 支持(\u0026gt;=v1.4)，如果内核模块未启用，它将拒绝运行带有 AppArmor 选项的 Pod。\n. note \u0026gt;}} Ubuntu 携带了许多没有合并到上游 Linux 内核中的 AppArmor 补丁，包括添加附加钩子和特性的补丁。Kubernetes 只在上游版本中测试过，不承诺支持其他特性。 . /note \u0026gt;}}\n Docker 作为容器运行环境 \u0026ndash; 目前，支持 Kubernetes 运行的容器中只有 Docker 也支持 AppArmor。随着更多的运行时添加 AppArmor 的支持，可选项将会增多。您可以使用以下命令验证节点是否正在运行 Docker:\nkubectl get nodes -o=jsonpath=$\u0026#39;{range .items[*]}{@.metadata.name}: {@.status.nodeInfo.containerRuntimeVersion}\\n{end}\u0026#39; gke-test-default-pool-239f5d02-gyn2: docker://1.11.2 gke-test-default-pool-239f5d02-x1kf: docker://1.11.2 gke-test-default-pool-239f5d02-xwux: docker://1.11.2 如果 Kubelet 包含 AppArmor 支持(\u0026gt;=v1.4)，如果运行环境不是 Docker，它将拒绝运行带有 AppArmor 选项的 Pod。\n   配置文件已加载 \u0026ndash; 通过指定每个容器都应使用 AppArmor 配置文件，AppArmor 应用于 Pod。如果指定的任何配置文件尚未加载到内核， Kubelet (\u0026gt;=v1.4) 将拒绝 Pod。通过检查 /sys/kernel/security/apparmor/profiles 文件，可以查看节点加载了哪些配置文件。例如:\nssh gke-test-default-pool-239f5d02-gyn2 \u0026#34;sudo cat /sys/kernel/security/apparmor/profiles | sort\u0026#34; apparmor-test-deny-write (enforce) apparmor-test-audit-write (enforce) docker-default (enforce) k8s-nginx (enforce) 有关在节点上加载配置文件的详细信息，请参见使用配置文件设置节点。\n  只要 Kubelet 版本包含 AppArmor 支持(\u0026gt;=v1.4)，如果不满足任何先决条件，Kubelet 将拒绝带有 AppArmor 选项的 Pod。您还可以通过检查节点就绪状况消息来验证节点上的 AppArmor 支持(尽管这可能会在以后的版本中删除)：\nkubectl get nodes -o=jsonpath=$\u0026#39;{range .items[*]}{@.metadata.name}: {.status.conditions[?(@.reason==\u0026#34;KubeletReady\u0026#34;)].message}\\n{end}\u0026#39; gke-test-default-pool-239f5d02-gyn2: kubelet is posting ready status. AppArmor enabled gke-test-default-pool-239f5d02-x1kf: kubelet is posting ready status. AppArmor enabled gke-test-default-pool-239f5d02-xwux: kubelet is posting ready status. AppArmor enabled 保护 Pod . note \u0026gt;}}\nAppArmor 目前处于测试阶段，因此选项被指定为注释。一旦 AppArmor 被授予支持通用，注释将替换为首要的字段(更多详情参见升级到 GA 的途径)。 . /note \u0026gt;}}\nAppArmor 配置文件被指定为 per-container。要指定要用其运行 Pod 容器的 AppArmor 配置文件，请向 Pod 的元数据添加注释：\ncontainer.apparmor.security.beta.kubernetes.io/\u0026lt;container_name\u0026gt;: \u0026lt;profile_ref\u0026gt; \u0026lt;container_name\u0026gt; 的名称是容器的简称，用以描述简介，并且简称为 \u0026lt;profile_ref\u0026gt; 。\u0026lt;profile_ref\u0026gt; 可以作为其中之一：\n runtime/default 应用运行时的默认配置 localhost/\u0026lt;profile_name\u0026gt; 应用在名为 \u0026lt;profile_name\u0026gt; 的主机上加载的配置文件 unconfined 表示不加载配置文件  有关注释和配置文件名称格式的详细信息，请参阅API 参考。\nKubernetes AppArmor 强制执行方式首先通过检查所有先决条件都已满足，然后将配置文件选择转发到容器运行时进行强制执行。如果未满足先决条件， Pod 将被拒绝，并且不会运行。\n要验证是否应用了配置文件，可以查找容器创建事件中列出的 AppArmor 安全选项：\nkubectl get events | grep Created 22s 22s 1 hello-apparmor Pod spec.containers{hello} Normal Created {kubelet e2e-test-stclair-node-pool-31nt} Created container with docker id 269a53b202d3; Security:[seccomp=unconfined apparmor=k8s-apparmor-example-deny-write] 您还可以通过检查容器的 proc attr，直接验证容器的根进程是否以正确的配置文件运行：\nkubectl exec \u0026lt;pod_name\u0026gt; cat /proc/1/attr/current k8s-apparmor-example-deny-write (enforce) 举例 本例假设您已经使用 AppArmor 支持设置了一个集群。\n首先，我们需要将要使用的配置文件加载到节点上。我们将使用的配置文件仅拒绝所有文件写入：\n#include \u0026lt;tunables/global\u0026gt; profile k8s-apparmor-example-deny-write flags=(attach_disconnected) { #include \u0026lt;abstractions/base\u0026gt; file, # Deny all file writes. deny /** w, } 由于我们不知道 Pod 将被安排在那里，我们需要在所有节点上加载配置文件。在本例中，我们将只使用 SSH 来安装概要文件，但是在使用配置文件设置节点中讨论了其他方法。\nNODES=( # The SSH-accessible domain names of your nodes gke-test-default-pool-239f5d02-gyn2.us-central1-a.my-k8s gke-test-default-pool-239f5d02-x1kf.us-central1-a.my-k8s gke-test-default-pool-239f5d02-xwux.us-central1-a.my-k8s) for NODE in ${NODES[*]}; do ssh $NODE \u0026#39;sudo apparmor_parser -q \u0026lt;\u0026lt;EOF #include \u0026lt;tunables/global\u0026gt; profile k8s-apparmor-example-deny-write flags=(attach_disconnected) { #include \u0026lt;abstractions/base\u0026gt; file, # Deny all file writes. deny /** w, } EOF\u0026#39; done 接下来，我们将运行一个带有拒绝写入配置文件的简单 \u0026ldquo;Hello AppArmor\u0026rdquo; pod：\n. codenew file=\u0026quot;pods/security/hello-apparmor.yaml\u0026rdquo; \u0026gt;}}\nkubectl create -f ./hello-apparmor.yaml 如果我们查看 pod 事件，我们可以看到 pod 容器是用 AppArmor 配置文件 \u0026ldquo;k8s-apparmor-example-deny-write\u0026rdquo; 所创建的：\nkubectl get events | grep hello-apparmor 14s 14s 1 hello-apparmor Pod Normal Scheduled {default-scheduler } Successfully assigned hello-apparmor to gke-test-default-pool-239f5d02-gyn2 14s 14s 1 hello-apparmor Pod spec.containers{hello} Normal Pulling {kubelet gke-test-default-pool-239f5d02-gyn2} pulling image \u0026quot;busybox\u0026quot; 13s 13s 1 hello-apparmor Pod spec.containers{hello} Normal Pulled {kubelet gke-test-default-pool-239f5d02-gyn2} Successfully pulled image \u0026quot;busybox\u0026quot; 13s 13s 1 hello-apparmor Pod spec.containers{hello} Normal Created {kubelet gke-test-default-pool-239f5d02-gyn2} Created container with docker id 06b6cd1c0989; Security:[seccomp=unconfined apparmor=k8s-apparmor-example-deny-write] 13s 13s 1 hello-apparmor Pod spec.containers{hello} Normal Started {kubelet gke-test-default-pool-239f5d02-gyn2} Started container with docker id 06b6cd1c0989 我们可以通过检查该配置文件的 proc attr 来验证容器是否实际使用该配置文件运行：\nkubectl exec hello-apparmor cat /proc/1/attr/current k8s-apparmor-example-deny-write (enforce) 最后，我们可以看到如果试图通过写入文件来违反配置文件，会发生什么情况：\nkubectl exec hello-apparmor touch /tmp/test touch: /tmp/test: Permission denied error: error executing remote command: command terminated with non-zero exit code: Error executing in Docker Container: 1 最后，让我们看看如果我们试图指定一个尚未加载的配置文件会发生什么：\nkubectl create -f /dev/stdin \u0026lt;\u0026lt;EOF apiVersion: v1 kind: Pod metadata: name: hello-apparmor-2 annotations: container.apparmor.security.beta.kubernetes.io/hello: localhost/k8s-apparmor-example-allow-write spec: containers: - name: hello image: busybox command: [ \u0026#34;sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;echo \u0026#39;Hello AppArmor!\u0026#39; \u0026amp;\u0026amp; sleep 1h\u0026#34; ] EOF pod/hello-apparmor-2 created kubectl describe pod hello-apparmor-2 Name: hello-apparmor-2 Namespace: default Node: gke-test-default-pool-239f5d02-x1kf/ Start Time: Tue, 30 Aug 2016 17:58:56 -0700 Labels: \u0026lt;none\u0026gt; Annotations: container.apparmor.security.beta.kubernetes.io/hello=localhost/k8s-apparmor-example-allow-write Status: Pending Reason: AppArmor Message: Pod Cannot enforce AppArmor: profile \u0026quot;k8s-apparmor-example-allow-write\u0026quot; is not loaded IP: Controllers: \u0026lt;none\u0026gt; Containers: hello: Container ID: Image: busybox Image ID: Port: Command: sh -c echo 'Hello AppArmor!' \u0026amp;\u0026amp; sleep 1h State: Waiting Reason: Blocked Ready: False Restart Count: 0 Environment: \u0026lt;none\u0026gt; Mounts: /var/run/secrets/kubernetes.io/serviceaccount from default-token-dnz7v (ro) Conditions: Type Status Initialized True Ready False PodScheduled True Volumes: default-token-dnz7v: Type: Secret (a volume populated by a Secret) SecretName: default-token-dnz7v Optional: false QoS Class: BestEffort Node-Selectors: \u0026lt;none\u0026gt; Tolerations: \u0026lt;none\u0026gt; Events: FirstSeen LastSeen Count From SubobjectPath Type Reason Message --------- -------- ----- ---- ------------- -------- ------ ------- 23s 23s 1 {default-scheduler } Normal Scheduled Successfully assigned hello-apparmor-2 to e2e-test-stclair-node-pool-t1f5 23s 23s 1 {kubelet e2e-test-stclair-node-pool-t1f5} Warning AppArmor Cannot enforce AppArmor: profile \u0026quot;k8s-apparmor-example-allow-write\u0026quot; is not loaded 注意 pod 呈现失败状态，并且显示一条有用的错误信息：Pod Cannot enforce AppArmor: profile \u0026quot;k8s-apparmor-example-allow-write\u0026quot; 未加载。还用相同的消息记录了一个事件。\n管理 使用配置文件设置节点 Kubernetes 目前不提供任何本地机制来将 AppArmor 配置文件加载到节点上。有很多方法可以设置配置文件，例如：\n 通过在每个节点上运行 Pod 的DaemonSet确保加载了正确的配置文件。可以找到一个示例实现这里。 在节点初始化时，使用节点初始化脚本(例如 Salt 、Ansible 等)或镜像。 通过将配置文件复制到每个节点并通过 SSH 加载它们，如示例。  调度程序不知道哪些配置文件加载到哪个节点上，因此必须将全套配置文件加载到每个节点上。另一种方法是为节点上的每个配置文件(或配置文件类)添加节点标签，并使用[节点选择器](/zh/docs/concepts/configuration/assign pod node/)确保 Pod 在具有所需配置文件的节点上运行。\n使用 PodSecurityPolicy 限制配置文件 如果启用了 PodSecurityPolicy 扩展，则可以应用群集范围的 AppArmor 限制。要启用 PodSecurityPolicy，必须在“apiserver”上设置以下标志：\n--enable-admission-plugins=PodSecurityPolicy[,others...] AppArmor 选项可以指定为 PodSecurityPolicy 上的注释：\napparmor.security.beta.kubernetes.io/defaultProfileName: \u0026lt;profile_ref\u0026gt; apparmor.security.beta.kubernetes.io/allowedProfileNames: \u0026lt;profile_ref\u0026gt;[,others...] 默认配置文件名选项指定默认情况下在未指定任何配置文件时应用于容器的配置文件。节点允许配置文件名选项指定允许 Pod 容器运行时的配置文件列表。配置文件的指定格式与容器上的相同。完整规范见API 参考。\n禁用 AppArmor 如果您不希望 AppArmor 在集群上可用，可以通过命令行标志禁用它：\n--feature-gates=AppArmor=false 禁用时，任何包含 AppArmor 配置文件的 Pod 都将因 \u0026ldquo;Forbidden\u0026rdquo; 错误而导致验证失败。注意，默认情况下，docker 总是在非特权 pods 上启用 \u0026ldquo;docker-default\u0026rdquo; 配置文件(如果 AppArmor 内核模块已启用)，并且即使功能门已禁用，也将继续启用该配置文件。当 AppArmor 应用于通用(GA)时，禁用 Apparmor 的选项将被删除。\n使用 AppArmor 升级到 Kubernetes v1.4 不需要对 AppArmor 执行任何操作即可将集群升级到 v1.4。但是，如果任何现有的 pods 有一个 AppArmor 注释，它们将不会通过验证(或 PodSecurityPolicy 认证)。如果节点上加载了许可配置文件，恶意用户可以预先应用许可配置文件，将 pod 权限提升到 docker-default 权限之上。如果存在这个问题，建议清除包含 apparmor.security.beta.kubernetes.io 注释的任何 pods 的集群。\n升级到一般可用性的途径 当 Apparmor 准备升级到通用(GA)时，当前指定的选项通过注释将转换为字段。通过转换支持所有升级和降级路径是非常微妙的，并将在转换发生时详细解释。我们将承诺在至少两个版本中同时支持字段和注释，并在之后的至少两个版本中显式拒绝注释。\n编写配置文件 获得正确指定的 AppArmor 配置文件可能是一件棘手的事情。幸运的是，有一些工具可以帮助您做到这一点：\n aa-genprof and aa-logprof 通过监视应用程序的活动和日志并承认它所采取的操作来生成配置文件规则。更多说明由AppArmor 文档提供。 bane是一个用于 Docker的 AppArmor 档案生成器，它使用简化的档案语言。  建议在开发工作站上通过 Docker 运行应用程序以生成配置文件，但是没有什么可以阻止在运行 Pod 的 Kubernetes 节点上运行工具。\n想要调试 AppArmor 的问题，您可以检查系统日志，查看具体拒绝了什么。AppArmor 将详细消息记录到 dmesg ，错误通常可以在系统日志中或通过 journalctl 找到。更多详细信息见AppArmor 失败。\nAPI 参考 Pod 注释 指定容器将使用的配置文件：\n key: container.apparmor.security.beta.kubernetes.io/\u0026lt;container_name\u0026gt; 中的 \u0026lt;container_name\u0026gt; 匹配 Pod 中的容器名称。 可以为 Pod 中的每个容器指定单独的配置文件。 value: 配置文件参考，如下所述  配置文件参考  runtime/default: 指默认运行时配置文件。  等同于不指定配置文件(没有 PodSecurityPolicy 默认值)，除非它仍然需要启用 AppArmor。 对于 Docker，这将解析为非特权容器的Docker default配置文件，特权容器的配置文件为未定义(无配置文件)。   localhost/\u0026lt;profile_name\u0026gt;: 指按名称加载到节点(localhost)上的配置文件。  可能的配置文件名在 核心策略参考。   unconfined: 这有效地禁用了容器上的 AppArmor 。  任何其他配置文件引用格式无效。\nPodSecurityPolicy 注解 指定在未提供容器时应用于容器的默认配置文件：\n key: apparmor.security.beta.kubernetes.io/defaultProfileName value: 如上述文件参考所述  上面描述的指定配置文件， Pod 容器列表的配置文件引用允许指定：\n key: apparmor.security.beta.kubernetes.io/allowedProfileNames value: 配置文件引用的逗号分隔列表(如上所述)  尽管转义逗号是配置文件名中的合法字符，但此处不能显式允许。    . heading \u0026ldquo;whatsnext\u0026rdquo; %}} 其他资源\n Apparmor 配置文件语言快速指南 Apparmor 核心策略参考  "
},
{
	"uri": "https://lijun.in/tasks/debug-application-cluster/audit/",
	"title": "Auditing",
	"tags": [],
	"description": "",
	"content": ". feature-state state=\u0026quot;beta\u0026rdquo; \u0026gt;}}\nKubernetes 审计功能提供了与安全相关的按时间顺序排列的记录集，记录单个用户、管理员或系统其他组件影响系统的活动顺序。 它能帮助集群管理员处理以下问题：\n 发生了什么？ 什么时候发生的？ 谁触发的？ 活动发生在哪个（些）对象上？ 在哪观察到的？ 它从哪触发的？ 活动的后续处理行为是什么？  . toc \u0026gt;}}\nKube-apiserver 执行审计。每个执行阶段的每个请求都会生成一个事件，然后根据特定策略对事件进行预处理并写入后端。 您可以在 设计方案 中找到更多详细信息。 该策略确定记录的内容并且在后端存储记录。当前的后端支持日志文件和 webhook。\n每个请求都可以用相关的 \u0026ldquo;stage\u0026rdquo; 记录。已知的 stage 有：\n  RequestReceived - 事件的 stage 将在审计处理器接收到请求后，并且在委托给其余处理器之前生成。\n  ResponseStarted - 在响应消息的头部发送后，但是响应消息体发送前。这个 stage 仅为长时间运行的请求生成（例如 watch）。\n  ResponseComplete - 当响应消息体完成并且没有更多数据需要传输的时候。\n  Panic - 当 panic 发生时生成。\n  . note \u0026gt;}}\n注意 审计日志记录功能会增加 API server 的内存消耗，因为需要为每个请求存储审计所需的某些上下文。 此外，内存消耗取决于审计日志记录的配置。 . /note \u0026gt;}}\n审计策略 审计政策定义了关于应记录哪些事件以及应包含哪些数据的规则。审计策略对象结构在 [audit.k8s.io API 组][auditing-api] 中定义。 处理事件时，将按顺序与规则列表进行比较。第一个匹配规则设置事件的 [审计级别][auditing-level]。已知的审计级别有：\n None - 符合这条规则的日志将不会记录。 Metadata - 记录请求的 metadata（请求的用户、timestamp、resource、verb 等等），但是不记录请求或者响应的消息体。 Request - 记录事件的 metadata 和请求的消息体，但是不记录响应的消息体。这不适用于非资源类型的请求。 RequestResponse - 记录事件的 metadata，请求和响应的消息体。这不适用于非资源类型的请求。  您可以使用 --audit-policy-file 标志将包含策略的文件传递给 kube-apiserver。如果不设置该标志，则不记录事件。 注意 rules 字段 必须 在审计策略文件中提供。没有（0）规则的策略将被视为非法配置。\n以下是一个审计策略文件的示例：\n. codenew file=\u0026quot;audit/audit-policy.yaml\u0026rdquo; \u0026gt;}}\n您可以使用最低限度的审计策略文件在 Metadata 级别记录所有请求：\n# Log all requests at the Metadata level. apiVersion: audit.k8s.io/v1beta1 kind: Policy rules: - level: Metadata 管理员构建自己的审计配置文件时，应使用 [GCE 使用的审计配置文件][gce-audit-profile] 作为参考。\n审计后端 审计后端实现将审计事件导出到外部存储。 Kube-apiserver 提供两个后端：\n Log 后端，将事件写入到磁盘 Webhook 后端，将事件发送到外部 API  在这两种情况下，审计事件结构均由 audit.k8s.io API 组中的 API 定义。当前版本的 API 是 [v1beta1][auditing-api]。\n. note \u0026gt;}}\n注意： 在 patch 请求的情况下，请求的消息体需要是一个 JSON 串指定 patch 操作，而不是一个完整的 Kubernetes API 对象 JSON 串。 例如，以下的示例是一个合法的 patch 请求消息体，该请求对应 /apis/batch/v1/namespaces/some-namespace/jobs/some-job-name。\n[ { \u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/parallelism\u0026#34;, \u0026#34;value\u0026#34;: 0 }, { \u0026#34;op\u0026#34;: \u0026#34;remove\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/template/spec/containers/0/terminationMessagePolicy\u0026#34; } ] . /note \u0026gt;}}\nLog 后端 Log 后端将审计事件写入 JSON 格式的文件。您可以使用以下 kube-apiserver 标志配置 Log 审计后端：\n --audit-log-path 指定用来写入审计事件的日志文件路径。不指定此标志会禁用日志后端。- 意味着标准化 --audit-log-maxage 定义了保留旧审计日志文件的最大天数 --audit-log-maxbackup 定义了要保留的审计日志文件的最大数量 --audit-log-maxsize 定义审计日志文件的最大大小（兆字节）  Webhook 后端 Webhook 后端将审计事件发送到远程 API，该远程 API 应该暴露与 kube-apiserver 相同的API。 您可以使用如下 kube-apiserver 标志来配置 webhook 审计后端：\n --audit-webhook-config-file webhook 配置文件的路径。Webhook 配置文件实际上是一个 [kubeconfig][kubeconfig]。 --audit-webhook-initial-backoff 指定在第一次失败后重发请求等待的时间。随后的请求将以指数退避重试。  webhook 配置文件使用 kubeconfig 格式指定服务的远程地址和用于连接它的凭据。\nBatching log 和 webhook 后端都支持 batch。以 webhook 为例，以下是可用参数列表。要获取 log 后端的同样参数，请在参数名称中将 webhook 替换为 log。 默认情况下，在 webhook 中启用 batch，在 log 中禁用 batch。同样，默认情况下，在 webhook 中启用限制，在 log 中禁用限制。\n --audit-webhook-mode 定义缓存策略，可选值如下：  batch - 以批处理缓存事件和异步的过程。这是默认值。 blocking - 阻止 API server 处理每个单独事件的响应。    以下参数仅用于 batch 模式。\n --audit-webhook-batch-buffer-size 定义 batch 之前要缓存的事件数。 如果传入事件的速率溢出缓存区，则会丢弃事件。 --audit-webhook-batch-max-size 定义一个 batch 中的最大事件数。 --audit-webhook-batch-max-wait 无条件 batch 队列中的事件前等待的最大事件。 --audit-webhook-batch-throttle-qps 每秒生成的最大 batch 平均值。 --audit-webhook-batch-throttle-burst 在达到允许的 QPS 前，同一时刻允许存在的最大 batch 生成数。  参数调整 需要设置参数以适应 apiserver 上的负载。\n例如，如果 kube-apiserver 每秒收到 100 个请求，并且每个请求仅在 ResponseStarted 和 ResponseComplete 阶段进行审计，则应该考虑每秒生成约 200 个审计事件。 假设批处理中最多有 100 个事件，则应将限制级别设置为至少 2 个 QPS。 假设后端最多需要 5 秒钟来写入事件，您应该设置缓冲区大小以容纳最多 5 秒的事件，即 10 个 batch，即 1000 个事件。\n但是，在大多数情况下，默认参数应该足够了，您不必手动设置它们。您可以查看 kube-apiserver 公开的以下 Prometheus 指标，并在日志中监控审计子系统的状态。\n apiserver_audit_event_total 包含所有暴露的审计事件数量的指标。 apiserver_audit_error_total 在暴露时由于发生错误而被丢弃的事件的数量。  多集群配置 如果您通过 [aggregation layer][kube-aggregator] 对 Kubernetes API 进行扩展，那么您也可以为聚合的 apiserver 设置审计日志。 想要这么做，您需要以上述的格式给聚合的 apiserver 配置参数，并且配置日志管道以采用审计日志。不同的 apiserver 可以配置不同的审计配置和策略。\n日志选择器示例 使用 fluentd 从日志文件中选择并且分发审计日志 [Fluentd][fluentd] 是一个开源的数据采集器，可以从统一的日志层中采集。 在以下示例中，我们将使用 fluentd 来按照命名空间划分审计事件。\n  在 kube-apiserver node 节点上安装 [fluentd, fluent-plugin-forest and fluent-plugin-rewrite-tag-filter][fluentd_install_doc]\n  为 fluentd 创建一个配置文件\n$ cat \u0026lt;\u0026lt;EOF \u0026gt; /etc/fluentd/config # fluentd conf runs in the same host with kube-apiserver \u0026lt;source\u0026gt; @type tail # audit log path of kube-apiserver path /var/log/audit pos_file /var/log/audit.pos format json time_key time time_format %Y-%m-%dT%H:%M:%S.%N%z tag audit \u0026lt;/source\u0026gt; \u0026lt;filter audit\u0026gt; #https://github.com/fluent/fluent-plugin-rewrite-tag-filter/issues/13 type record_transformer enable_ruby \u0026lt;record\u0026gt; namespace ${record[\u0026quot;objectRef\u0026quot;].nil? ? \u0026quot;none\u0026quot;:(record[\u0026quot;objectRef\u0026quot;][\u0026quot;namespace\u0026quot;].nil? ? \u0026quot;none\u0026quot;:record[\u0026quot;objectRef\u0026quot;][\u0026quot;namespace\u0026quot;])} \u0026lt;/record\u0026gt; \u0026lt;/filter\u0026gt; \u0026lt;match audit\u0026gt; # route audit according to namespace element in context @type rewrite_tag_filter rewriterule1 namespace ^(.+) ${tag}.$1 \u0026lt;/match\u0026gt; \u0026lt;filter audit.**\u0026gt; @type record_transformer remove_keys namespace \u0026lt;/filter\u0026gt; \u0026lt;match audit.**\u0026gt; @type forest subtype file remove_prefix audit \u0026lt;template\u0026gt; time_slice_format %Y%m%d%H compress gz path /var/log/audit-${tag}.*.log format json include_time_key true \u0026lt;/template\u0026gt; \u0026lt;/match\u0026gt;     启动 fluentd\n$ fluentd -c /etc/fluentd/config -vv     给 kube-apiserver 配置以下参数并启动：\n--audit-policy-file=/etc/kubernetes/audit-policy.yaml --audit-log-path=/var/log/kube-audit --audit-log-format=json    在 /var/log/audit-*.log 文件中检查不同命名空间的审计事件  使用 logstash 采集并分发 webhook 后端的审计事件 [Logstash][logstash] 是一个开源的、服务器端的数据处理工具。在下面的示例中，我们将使用 logstash 采集 webhook 后端的审计事件，并且将来自不同用户的事件存入不同的文件。\n  安装 [logstash][logstash_install_doc]\n  为 logstash 创建配置文件\n$ cat \u0026lt;\u0026lt;EOF \u0026gt; /etc/logstash/config input{ http{ #TODO, figure out a way to use kubeconfig file to authenticate to logstash #https://www.elastic.co/guide/en/logstash/current/plugins-inputs-http.html#plugins-inputs-http-ssl port=\u0026gt;8888 } } filter{ split{ # Webhook audit backend sends several events together with EventList # split each event here. field=\u0026gt;[items] # We only need event subelement, remove others. remove_field=\u0026gt;[headers, metadata, apiVersion, \u0026quot;@timestamp\u0026quot;, kind, \u0026quot;@version\u0026quot;, host] } mutate{ rename =\u0026gt; {items=\u0026gt;event} } } output{ file{ # Audit events from different users will be saved into different files. path=\u0026gt;\u0026quot;/var/log/kube-audit-%{[event][user][username]}/audit\u0026quot; } }     启动 logstash\n$ bin/logstash -f /etc/logstash/config --path.settings /etc/logstash/     为 kube-apiserver webhook 审计后端创建一个 kubeconfig 文件\n$ cat \u0026lt;\u0026lt;EOF \u0026gt; /etc/kubernetes/audit-webhook-kubeconfig apiVersion: v1 clusters: - cluster: server: http://\u0026lt;ip_of_logstash\u0026gt;:8888 name: logstash contexts: - context: cluster: logstash user: \u0026quot;\u0026quot; name: default-context current-context: default-context kind: Config preferences: {} users: [] EOF     为 kube-apiserver 配置以下参数并启动：\n--audit-policy-file=/etc/kubernetes/audit-policy.yaml --audit-webhook-config-file=/etc/kubernetes/audit-webhook-kubeconfig    在 logstash node 节点的 /var/log/kube-audit-*/audit 目录中检查审计事件  注意到，除了文件输出插件外，logstash 还有其它多种输出可以让用户路由不同的数据。例如，用户可以将审计事件发送给支持全文搜索和分析的 elasticsearch 插件。\n传统的审计 注意： 传统审计已被弃用，自 1.8 版本以后默认禁用，并且将会在 1.12 版本中彻底移除。 如果想要回退到传统的审计功能，请使用 kube-apiserver 中 feature gate 的 AdvancedAuditing 功能来禁用高级审核功能：\n--feature-gates=AdvancedAuditing=false 在传统格式中，每个审计文件条目包含两行：\n 请求行包含唯一 ID 以匹配响应和请求元数据，例如源 IP、请求用户、模拟信息和请求的资源等。 响应行包含与请求行和响应代码相匹配的唯一 ID。  2017-03-21T03:57:09.106841886-04:00 AUDIT: id=\u0026quot;c939d2a7-1c37-4ef1-b2f7-4ba9b1e43b53\u0026quot; ip=\u0026quot;127.0.0.1\u0026quot; method=\u0026quot;GET\u0026quot; user=\u0026quot;admin\u0026quot; groups=\u0026quot;\\\u0026quot;system:masters\\\u0026quot;,\\\u0026quot;system:authenticated\\\u0026quot;\u0026quot; as=\u0026quot;\u0026lt;self\u0026gt;\u0026quot; asgroups=\u0026quot;\u0026lt;lookup\u0026gt;\u0026quot; namespace=\u0026quot;default\u0026quot; uri=\u0026quot;/api/v1/namespaces/default/pods\u0026quot; 2017-03-21T03:57:09.108403639-04:00 AUDIT: id=\u0026quot;c939d2a7-1c37-4ef1-b2f7-4ba9b1e43b53\u0026quot; response=\u0026quot;200\u0026quot; 配置 Kube-apiserver 提供以下选项，负责配置审核日志的位置和处理方式：\n audit-log-path - 使审计日志指向请求被记录到的文件，'-\u0026rsquo; 表示标准输出。 audit-log-maxage - 根据文件名中编码的时间戳指定保留旧审计日志文件的最大天数。 audit-log-maxbackup - 指定要保留的旧审计日志文件的最大数量。 audit-log-maxsize - 指定审核日志文件的最大大小（兆字节）。默认为100MB。  如果审核日志文件已经存在，则 Kubernetes 会将新的审核日志附加到该文件。 否则，Kubernetes 会在您在 audit-log-path 中指定的位置创建一个审计日志文件。 如果审计日志文件超过了您在 audit-log-maxsize 中指定的大小，则 Kubernetes 将通过在文件名（在文件扩展名之前）附加当前时间戳并重新创建一个新的审计日志文件来重命名当前日志文件。 Kubernetes 可能会在创建新的日志文件时删除旧的日志文件; 您可以通过指定 audit-log-maxbackup 和 audit-log-maxage 选项来配置保留多少文件以及它们的保留时间。\n[auditing-api]: https://github.com/kubernetes/kubernetes/blob/. param \u0026ldquo;githubbranch\u0026rdquo; \u0026gt;}}/staging/src/k8s.io/apiserver/pkg/apis/audit/v1beta1/types.go [gce-audit-profile]: https://github.com/kubernetes/kubernetes/blob/. param \u0026ldquo;githubbranch\u0026rdquo; \u0026gt;}}/cluster/gce/gci/configure-helper.sh#L735 [kubeconfig]: /docs/tasks/access-application-cluster/configure-access-multiple-clusters/ [fluentd]: http://www.fluentd.org/ [fluentd_install_doc]: https://docs.fluentd.org/v/0.12/articles/quickstart#step1-installing-fluentd [logstash]: https://www.elastic.co/products/logstash [logstash_install_doc]: https://www.elastic.co/guide/en/logstash/current/installing-logstash.html [kube-aggregator]: /docs/concepts/api-extension/apiserver-aggregation\n"
},
{
	"uri": "https://lijun.in/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lijun.in/credits/",
	"title": "Credits",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lijun.in/setup/production-environment/on-premises-vm/dcos/",
	"title": "DC/OS 上的 Kubernetes",
	"tags": [],
	"description": "",
	"content": "Mesosphere 提供了一个简单的选项来将 Kubernetes 设置到DC/OS上，它提供：\n 纯上游 Kubernetes 集群一键部署 默认情况下高度可用且安全 与快速数据平台 (例如 Akka、Cassandra、Kafka、Spark) 一起运行的 Kubernetes  Mesosphere 官方指南 DC/OS 入门的正式来源位于quickstart 仓库中。\n"
},
{
	"uri": "https://lijun.in/tasks/administer-cluster/dns-debugging-resolution/",
	"title": "Debug DNS 方案",
	"tags": [],
	"description": "",
	"content": "This page provides hints on diagnosing DNS problems.\n\u0026ndash;\u0026gt;\n这篇文章提供了一些关于 DNS 问题诊断的方法。\n\u0026ndash;\u0026gt;\n. heading \u0026ldquo;prerequisites\u0026rdquo; %}}  . include \u0026ldquo;task-tutorial-prereqs.md\u0026rdquo; \u0026gt;}} . version-check \u0026gt;}} Kubernetes 1.6 或者以上版本。 集群必须使用了 coredns (或者 kube-dns)插件。  创建一个简单的 Pod 作为测试环境 新建一个名为 busybox.yaml 的文件并填入下列内容：\n. codenew file=\u0026quot;admin/dns/busybox.yaml\u0026rdquo; \u0026gt;}}\n然后使用这个文件创建一个 Pod 并验证其状态：\nkubectl create -f https://k8s.io/examples/admin/dns/busybox.yaml pod/busybox created kubectl get pods busybox NAME READY STATUS RESTARTS AGE busybox 1/1 Running 0 \u0026lt;some-time\u0026gt; 只要 Pod 处于 running 状态，您就可以在环境里执行 nslookup 。 如果您看到类似下列的内容，则表示 DNS 是正常运行的。\nkubectl exec -ti busybox -- nslookup kubernetes.default Server: 10.0.0.10 Address 1: 10.0.0.10 Name: kubernetes.default Address 1: 10.0.0.1 如果 nslookup 命令执行失败，请检查下列内容：\n先检查本地的 DNS 配置 查看 resolv.conf 文件的内容 (阅读下面的 从节点继承 DNS 配置 和 已知问题 ，获取更多信息)\nkubectl exec busybox cat /etc/resolv.conf 验证 search 和 name server 的配置是否类似下面的配置 (注意 search 根据不同的云提供商可能会有所不同):\nsearch default.svc.cluster.local svc.cluster.local cluster.local google.internal c.gce_project_id.internal nameserver 10.0.0.10 options ndots:5 下列错误表示 coredns/kube-dns 或者相关服务出现了问题：\nkubectl exec -ti busybox -- nslookup kubernetes.default Server: 10.0.0.10 Address 1: 10.0.0.10 nslookup: can't resolve 'kubernetes.default' 或者\nkubectl exec -ti busybox -- nslookup kubernetes.default Server: 10.0.0.10 Address 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.local nslookup: can't resolve 'kubernetes.default' 检查 DNS Pod 是否运行 使用 kubectl get pods 命令来验证 DNS Pod 是否运行。\n对于 CoreDNS 的情况:\nkubectl get pods --namespace=kube-system -l k8s-app=kube-dns NAME READY STATUS RESTARTS AGE ... coredns-7b96bf9f76-5hsxb 1/1 Running 0 1h coredns-7b96bf9f76-mvmmt 1/1 Running 0 1h ... 或者是 kube-dns:\nkubectl get pods --namespace=kube-system -l k8s-app=kube-dns NAME READY STATUS RESTARTS AGE ... kube-dns-v19-ezo1y 3/3 Running 0 1h ... 如果您发现没有 pod 在运行，或者这些 Pod 的状态是 failed 或者 completed， 那可能这个 DNS 插件在您当前的环境里并没有成功部署，您将需要手动去部署它。\n检查 DNS pod 里的错误 使用 kubectl logs 命令来查看 DNS 容器的日志信息。\n对于 CoreDNS:\nfor p in $(kubectl get pods --namespace=kube-system -l k8s-app=kube-dns -o name); do kubectl logs --namespace=kube-system $p; done 下列是一个正常运行的 CoreDNS 日志信息：\n.:53 2018/08/15 14:37:17 [INFO] CoreDNS-1.2.2 2018/08/15 14:37:17 [INFO] linux/amd64, go1.10.3, 2e322f6 CoreDNS-1.2.2 linux/amd64, go1.10.3, 2e322f6 2018/08/15 14:37:17 [INFO] plugin/reload: Running configuration MD5 = 24e6c59e83ce706f07bcc82c31b1ea1c 对于 kube-dns, 总共有三种类型的日志需要查看：\nkubectl logs --namespace=kube-system $(kubectl get pods --namespace=kube-system -l k8s-app=kube-dns -o name | head -1) -c kubedns kubectl logs --namespace=kube-system $(kubectl get pods --namespace=kube-system -l k8s-app=kube-dns -o name | head -1) -c dnsmasq kubectl logs --namespace=kube-system $(kubectl get pods --namespace=kube-system -l k8s-app=kube-dns -o name | head -1) -c sidecar 看日志信息里是否有可疑的错误，对于 kube-dns, 一个 \u0026lsquo;W\u0026rsquo;, \u0026lsquo;E\u0026rsquo; 或者 \u0026lsquo;F\u0026rsquo; 开头的行表示对应的 Warning（警告）， Error（错误）或者 Failure（失败）。请搜索日志等级是否有这样的关键字的日志信息并使用 kubernetes issues 来提交错误报告。\n检查是否启用了 DNS 服务 使用kubectl get service 命令来检查 DNS 服务是否已经启用。\nkubectl get svc --namespace=kube-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE ... kube-dns ClusterIP 10.0.0.10 \u0026lt;none\u0026gt; 53/UDP,53/TCP 1h ... 注意不管是 CoreDNS 还是 kube-dns ， 这个 service 的名字都会是“kube-dns” 。 如果您已经创建了这个 service 或者说在这个例子里它应该是默认自动创建的，但是它并没有出现，请阅读 services 纠错来获取更多信息。\nDNS 的 endpoints 公开了吗？ 您可以使用 kubectl get endpoints命令来验证 DNS 的 endpoint 是否公开了。\nkubectl get ep kube-dns --namespace=kube-system NAME ENDPOINTS AGE kube-dns 10.180.3.17:53,10.180.3.17:53 1h 如果您没看到对应的 endpoints， 请阅读services 纠错的 endpoints 小节。\n若需要更多的 Kubernetes DNS 例子，请在 Kubernetes GitHub 仓库里查看cluster-dns 例子 。\nDNS 查询有被接收或者执行吗？ 您可以通过给 CoreDNS 的配置文件 (也叫 Corefile)添加log插件来判断查询是否被正确接收。 CoreDNS 的 Corefile 被保存在一个叫 coredns 的 ConfigMap 里，使用下列命令来编辑它：\nkubectl -n kube-system edit configmap coredns 然后类似下面的例子给 Corefile 添加 log。\napiVersion: v1 kind: ConfigMap metadata: name: coredns namespace: kube-system data: Corefile: | .:53 { log errors health kubernetes cluster.local in-addr.arpa ip6.arpa { pods insecure upstream fallthrough in-addr.arpa ip6.arpa } prometheus :9153 proxy . /etc/resolv.conf cache 30 loop reload loadbalance } 保存这些更改后，您可能会需要等待一到两分钟让 Kubernetes 把这些更改应用到 CoreDNS 的 pods 里。\n接下来，发起一些查询并依照本文上面章节的内容查看日志信息，如果 CoreDNS 的 pods 接收到这些查询，您将可以在日志信息里看到他们。\n下面是日志信息里的查询例子：\n.:53 2018/08/15 14:37:15 [INFO] CoreDNS-1.2.0 2018/08/15 14:37:15 [INFO] linux/amd64, go1.10.3, 2e322f6 CoreDNS-1.2.0 linux/amd64, go1.10.3, 2e322f6 2018/09/07 15:29:04 [INFO] plugin/reload: Running configuration MD5 = 162475cdf272d8aa601e6fe67a6ad42f 2018/09/07 15:29:04 [INFO] Reloading complete 172.17.0.18:41675 - [07/Sep/2018:15:29:11 +0000] 59925 \u0026quot;A IN kubernetes.default.svc.cluster.local. udp 54 false 512\u0026quot; NOERROR qr,aa,rd,ra 106 0.000066649s 已知问题 有些 Linux 发行版本 (比如 Ubuntu)， 默认使用一个本地的 DNS 解析器 (systemd-resolved)。 Systemd-resolved 会用一个 stub 文件来覆盖 /etc/resolv.conf从而在解析域名的时候导致了重复向 DNS 上游服务器推送请求。 这个问题可以通过手动指定 kubelet 的 --resolv-conf 标签为正确的 resolv.conf (如果是 systemd-resolved，则这个文件路径为 /run/systemd/resolve/resolv.conf) 来解决。 kubeadm (\u0026gt;= 1.11) 会自动检测systemd-resolved并对应的更改 kubelet 的标签。\nKubernetes 的安装并不会默认配置节点的 resolv.conf 文件来使用集群的 DNS 服务，因为这个配置对于不同的发行版本是不一样的。这个问题应该迟早会被解决的。\nLinux 的 libc 会在仅有三个 DNS 的 nameserver 和六个 DNS 的search 记录时会不可思议的卡死 (详情请查阅这个2005年的bug)。Kubernetes 需要占用一个 nameserver 记录和三个search记录。这意味着如果一个本地的安装已经使用了三个nameserver或者使用了超过三个的 search记录，那有些配置很可能会丢失。有一个不完整的解决方案就是在节点上使用dnsmasq来提供更多的nameserver配置，但是无法提供更多的search记录。您也可以使用kubelet 的 --resolv-conf 标签来解决这个问题。\n如果您是使用 Alpine 3.3 或者更早版本作为您的基础镜像，DNS 可能会由于Alpine 一个已知的问题导致无法正常工作，请查看这里获取更多资料。\nKubernetes Federation (支持多区域部署) 自从 1.3 版本支持了多个 Kubernetes 的联邦集群后，集群 DNS 服务在处理 DNS 请求时需要有一些微弱的调整 (这是向下兼容的)，从而可以使用跨越多个 Kubernetes 集群的联邦服务。请看 联邦集群管理向导 获取更多关于联邦集群和多点支持的信息。\n参考  Services 和 Pods 的 DNS 指南 [kube-dns DNS 插件文档](http://releases.k8s.io/. param \u0026ldquo;githubbranch\u0026rdquo; \u0026gt;}}/cluster/addons/dns/kube-dns/README.md)  接下来  集群里自动伸缩 DNS Service.  "
},
{
	"uri": "https://lijun.in/concepts/workloads/pods/init-containers/",
	"title": "Init 容器",
	"tags": [],
	"description": "",
	"content": "本页提供了 Init 容器的概览，它是一种专用的容器，在glossary_tooltip text=\u0026quot;Pod\u0026rdquo; term_id=\u0026quot;pod\u0026rdquo; \u0026gt;}}内的应用容器启动之前运行，并包括一些应用镜像中不存在的实用工具和安装脚本。\n你可以在Pod的规格信息中与containers数组同级的位置指定 Init 容器。\n理解 Init 容器 glossary_tooltip text=\u0026quot;Pod\u0026rdquo; term_id=\u0026quot;pod\u0026rdquo; \u0026gt;}} 可以包含多个容器，应用运行在这些容器里面，同时 Pod 也可以有一个或多个先于应用容器启动的 Init 容器。\nInit 容器与普通的容器非常像，除了如下两点：\n 它们总是运行到完成。 每个都必须在下一个启动之前成功完成。  如果 Pod 的 Init 容器失败，Kubernetes 会不断地重启该 Pod，直到 Init 容器成功为止。然而，如果 Pod 对应的 restartPolicy 值为 Never，它不会重新启动。\n指定容器为 Init 容器，需要在 Pod 的 spec 中添加 initContainers 字段， 该字段內以[Container](/docs/reference/generated/kubernetes-api/\u0026lt; param \u0026ldquo;version\u0026rdquo; \u0026gt;}}/#container-v1-core) 类型对象数组的形式组织，和应用的 containers 数组同级相邻。 Init 容器的状态在 status.initContainerStatuses 字段中以容器状态数组的格式返回（类似 status.containerStatuses 字段）。\n与普通容器的不同之处 Init 容器支持应用容器的全部字段和特性，包括资源限制、数据卷和安全设置。 然而，Init 容器对资源请求和限制的处理稍有不同，在下面 资源 处有说明。\n同时 Init 容器不支持 Readiness Probe，因为它们必须在 Pod 就绪之前运行完成。\n如果为一个 Pod 指定了多个 Init 容器，这些容器会按顺序逐个运行。每个 Init 容器必须运行成功，下一个才能够运行。当所有的 Init 容器运行完成时，Kubernetes 才会为 Pod 初始化应用容器并像平常一样运行。\nInit 容器能做什么？ 因为 Init 容器具有与应用容器分离的单独镜像，其启动相关代码具有如下优势：\n Init 容器可以包含一些安装过程中应用容器中不存在的实用工具或个性化代码。例如，没有必要仅为了在安装过程中使用类似 sed、 awk、 python 或 dig 这样的工具而去FROM 一个镜像来生成一个新的镜像。 Init 容器可以安全地运行这些工具，避免这些工具导致应用镜像的安全性降低。 应用镜像的创建者和部署者可以各自独立工作，而没有必要联合构建一个单独的应用镜像。 Init 容器能以不同于Pod内应用容器的文件系统视图运行。因此，Init容器可具有访问 glossary_tooltip text=\u0026quot;Secrets\u0026rdquo; term_id=\u0026quot;secret\u0026rdquo; \u0026gt;}} 的权限，而应用容器不能够访问。 由于 Init 容器必须在应用容器启动之前运行完成，因此 Init 容器提供了一种机制来阻塞或延迟应用容器的启动，直到满足了一组先决条件。一旦前置条件满足，Pod内的所有的应用容器会并行启动。  示例 下面是一些如何使用 Init 容器的想法：\n  等待一个 Service 完成创建，通过类似如下 shell 命令：\n for i in {1..100}; do sleep 1; if dig myservice; then exit 0; fi; exit 1    注册这个 Pod 到远程服务器，通过在命令中调用 API，类似如下：\n curl -X POST http://$MANAGEMENT_SERVICE_HOST:$MANAGEMENT_SERVICE_PORT/register -d 'instance=$(\u0026lt;POD_NAME\u0026gt;)\u0026amp;ip=$(\u0026lt;POD_IP\u0026gt;)'    在启动应用容器之前等一段时间，使用类似命令：\n sleep 60    克隆 Git 仓库到 glossary_tooltip text=\u0026quot;Volume\u0026rdquo; term_id=\u0026quot;volume\u0026rdquo; \u0026gt;}}。\n  将配置值放到配置文件中，运行模板工具为主应用容器动态地生成配置文件。例如，在配置文件中存放 POD_IP 值，并使用 Jinja 生成主应用配置文件。\n  使用 Init 容器 下面的例子定义了一个具有 2 个 Init 容器的简单 Pod。 第一个等待 myservice 启动，第二个等待 mydb 启动。 一旦这两个 Init容器 都启动完成，Pod 将启动spec区域中的应用容器。\napiVersion: v1 kind: Pod metadata: name: myapp-pod labels: app: myapp spec: containers: - name: myapp-container image: busybox:1.28 command: [\u0026#39;sh\u0026#39;, \u0026#39;-c\u0026#39;, \u0026#39;echo The app is running! \u0026amp;\u0026amp; sleep 3600\u0026#39;] initContainers: - name: init-myservice image: busybox:1.28 command: [\u0026#39;sh\u0026#39;, \u0026#39;-c\u0026#39;, \u0026#34;until nslookup myservice.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for myservice; sleep 2; done\u0026#34;] - name: init-mydb image: busybox:1.28 command: [\u0026#39;sh\u0026#39;, \u0026#39;-c\u0026#39;, \u0026#34;until nslookup mydb.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for mydb; sleep 2; done\u0026#34;] 下面的 yaml 文件展示了 mydb 和 myservice 两个 Service：\nkind: Service apiVersion: v1 metadata: name: myservice spec: ports: - protocol: TCP port: 80 targetPort: 9376 --- kind: Service apiVersion: v1 metadata: name: mydb spec: ports: - protocol: TCP port: 80 targetPort: 9377 要启动这个 Pod，可以执行如下命令：\nkubectl apply -f myapp.yaml pod/myapp-pod created 要检查其状态：\nkubectl get -f myapp.yaml NAME READY STATUS RESTARTS AGE myapp-pod 0/1 Init:0/2 0 6m 如需更详细的信息：\nkubectl describe -f myapp.yaml Name: myapp-pod Namespace: default [...] Labels: app=myapp Status: Pending [...] Init Containers: init-myservice: [...] State: Running [...] init-mydb: [...] State: Waiting Reason: PodInitializing Ready: False [...] Containers: myapp-container: [...] State: Waiting Reason: PodInitializing Ready: False [...] Events: FirstSeen LastSeen Count From SubObjectPath Type Reason Message --------- -------- ----- ---- ------------- -------- ------ ------- 16s 16s 1 {default-scheduler } Normal Scheduled Successfully assigned myapp-pod to 172.17.4.201 16s 16s 1 {kubelet 172.17.4.201} spec.initContainers{init-myservice} Normal Pulling pulling image \u0026quot;busybox\u0026quot; 13s 13s 1 {kubelet 172.17.4.201} spec.initContainers{init-myservice} Normal Pulled Successfully pulled image \u0026quot;busybox\u0026quot; 13s 13s 1 {kubelet 172.17.4.201} spec.initContainers{init-myservice} Normal Created Created container with docker id 5ced34a04634; Security:[seccomp=unconfined] 13s 13s 1 {kubelet 172.17.4.201} spec.initContainers{init-myservice} Normal Started Started container with docker id 5ced34a04634 如需查看Pod内 Init 容器的日志，请执行:\n$ kubectl logs myapp-pod -c init-myservice # Inspect the first init container $ kubectl logs myapp-pod -c init-mydb # Inspect the second init container 在这一刻，Init 容器将会等待至发现名称为mydb和myservice的 Service。\n如下为创建这些 Service 的配置文件：\n--- apiVersion: v1 kind: Service metadata: name: myservice spec: ports: - protocol: TCP port: 80 targetPort: 9376 --- apiVersion: v1 kind: Service metadata: name: mydb spec: ports: - protocol: TCP port: 80 targetPort: 9377 创建mydb和myservice的 service 命令：\n$ kubectl create -f services.yaml service \u0026quot;myservice\u0026quot; created service \u0026quot;mydb\u0026quot; created 这样你将能看到这些 Init容器 执行完毕，随后my-app的Pod转移进入 Running 状态：\n$ kubectl get -f myapp.yaml NAME READY STATUS RESTARTS AGE myapp-pod 1/1 Running 0 9m 一旦我们启动了 mydb 和 myservice 这两个 Service，我们能够看到 Init 容器完成，并且 myapp-pod 被创建：\n这个简单的例子应该能为你创建自己的 Init 容器提供一些启发。 What\u0026rsquo;s next 部分提供了更详细例子的链接。\n具体行为 在 Pod 启动过程中，每个Init 容器在网络和数据卷初始化之后会按顺序启动。每个 Init容器 成功退出后才会启动下一个 Init容器。 如果因为运行或退出时失败引发容器启动失败，它会根据 Pod 的 restartPolicy 策略进行重试。 然而，如果 Pod 的 restartPolicy 设置为 Always，Init 容器失败时会使用 restartPolicy 的 OnFailure 策略。\n在所有的 Init 容器没有成功之前，Pod 将不会变成 Ready 状态。 Init 容器的端口将不会在 Service 中进行聚集。 正在初始化中的 Pod 处于 Pending 状态，但会将条件 Initializing 设置为 true。\n如果 Pod 重启，所有 Init 容器必须重新执行。\n对 Init 容器 spec 的修改仅限于容器的 image 字段。 更改 Init 容器的 image 字段，等同于重启该 Pod。\n因为 Init 容器可能会被重启、重试或者重新执行，所以 Init 容器的代码应该是幂等的。 特别地，基于 EmptyDirs 写文件的代码，应该对输出文件可能已经存在做好准备。\nInit 容器具有应用容器的所有字段。 然而 Kubernetes 禁止使用 readinessProbe，因为 Init 容器不能定义不同于完成（completion）的就绪（readiness）。 这一点会在校验时强制执行。\n在 Pod 上使用 activeDeadlineSeconds和在容器上使用 livenessProbe 可以避免 Init 容器一直重复失败。 activeDeadlineSeconds 时间包含了 Init 容器启动的时间。\n在 Pod 中的每个应用容器和 Init 容器的名称必须唯一；与任何其它容器共享同一个名称，会在校验时抛出错误。\n资源 给定Init 容器的执行顺序下，资源使用适用于如下规则：\n 所有 Init 容器上定义的任何特定资源的 limit 或 request 的最大值，作为 Pod 有效初始 request/limit Pod 对资源的 有效 limit/request 是如下两者的较大者：  所有应用容器对某个资源的 limit/request 之和 对某个资源的有效初始 limit/request   基于有效 limit/request 完成调度，这意味着 Init 容器能够为初始化过程预留资源，这些资源在 Pod 生命周期过程中并没有被使用。 Pod 的 有效 QoS 层 ，与 Init 容器和应用容器的一样。  配额和限制适用于有效 Pod的 limit/request。 Pod 级别的 cgroups 是基于有效 Pod 的 limit/request，和调度器相同。\nPod 重启的原因 Pod重启导致 Init 容器重新执行，主要有如下几个原因：\n 用户更新 Pod 的 Spec 导致 Init 容器镜像发生改变。Init 容器镜像的变更会引起 Pod 重启. 应用容器镜像的变更仅会重启应用容器。 Pod 的基础设施容器 (译者注：如 pause 容器) 被重启。 这种情况不多见，必须由具备 root 权限访问 Node 的人员来完成。 当 restartPolicy 设置为 Always，Pod 中所有容器会终止而强制重启，由于垃圾收集导致 Init 容器的完成记录丢失。  heading \u0026ldquo;whatsnext\u0026rdquo;  阅读创建包含 Init 容器的 Pod 学习如何调测 Init 容器  "
},
{
	"uri": "https://lijun.in/tasks/administer-cluster/ip-masq-agent/",
	"title": "IP Masquerade Agent 用户指南",
	"tags": [],
	"description": "",
	"content": "此页面展示如何配置和启用 ip-masq-agent。\n. heading \u0026ldquo;prerequisites\u0026rdquo; %}} . include \u0026ldquo;task-tutorial-prereqs.md\u0026rdquo; \u0026gt;}} . version-check \u0026gt;}}\nIP Masquerade Agent 用户指南 ip-masq-agent 配置 iptables 规则以隐藏位于集群节点 IP 地址后面的 pod 的 IP 地址。 这通常在将流量发送到集群的 pod CIDR 范围之外的目的地时使用。\n关键词  NAT (网络地址解析) 是一种通过修改 IP 地址头中的源和/或目标地址信息将一个 IP 地址重新映射到另一个 IP 地址的方法。通常由执行 IP 路由的设备执行。   伪装 NAT 的一种形式，通常用于执行多对一地址转换，其中多个源 IP 地址被隐藏在单个地址后面，该地址通常是执行 IP 路由的设备。在 Kubernetes 中，这是节点的 IP 地址。   CIDR (无类别域间路由) 基于可变长度子网掩码，允许指定任意长度的前缀。CIDR 引入了一种新的 IP 地址表示方法，现在通常称为CIDR表示法，其中地址或路由前缀后添加一个后缀，用来表示前缀的位数，例如 192.168.2.0/24。   本地链路 本地链路是仅对网段或主机所连接的广播域内的通信有效的网络地址。IPv4的本地链路地址在 CIDR 表示法的地址块 169.254.0.0/16 中定义。  ip-masq-agent 配置 iptables 规则，以便在将流量发送到集群节点的IP和集群IP范围之外的目标时处理伪装节点/pod 的 IP 地址。这基本上隐藏了集群节点 IP 地址后面的pod IP地址。在某些环境中，去往“外部”地址的流量必须从已知的机器地址发出。例如，在 Google Cloud 中，任何到互联网的流量都必须来自 VM 的 IP。使用容器时，如 Google Kubernetes Engine，从Pod IP 发出的流量将被拒绝出出站。为了避免这种情况，我们必须将 Pod IP 隐藏在 VM 自己的IP地址后面 - 通常称为“伪装”。默认情况下，代理配置为将RFC 1918指定的三个私有IP范围视为非伪装CIDR。这些范围是 10.0.0.0/8,172.16.0.0/12 和 192.168.0.0/16。默认情况下，代理还将链路本地地址(169.254.0.0/16)视为非伪装 CIDR。代理程序配置为每隔60秒从*/etc/config/ip-masq-agent*重新加载其配置，这也是可修改的。\n代理配置文件必须使用 YAML 或 JSON 语法编写，并且可能包含三个可选值：\n nonMasqueradeCIDRs: CIDR 表示法中的字符串列表，用于指定不需伪装的地址范围。   masqLinkLocal: 布尔值 (true / false)，表示是否将流量伪装到本地链路前缀169.254.0.0/16。 默认为 false。   resyncInterval: 代理尝试从磁盘重新加载配置的时间间隔。 例如 \u0026rsquo;30s\u0026rsquo;，其中 \u0026rsquo;s\u0026rsquo; 是秒，\u0026lsquo;ms\u0026rsquo; 是毫秒等\u0026hellip;  10.0.0.0/8,172.16.0.0/12和192.168.0.0/16）范围内的流量不会被伪装。任何其他流量（假设是互联网）将被伪装。pod 访问本地目的地的例子，可以是其节点的 IP 地址、另一节点的地址或集群的 IP 地址范围内的一个 IP 地址。默认情况下，任何其他流量都将伪装。以下条目展示了 ip-masq-agent 的默认使用的规则：\niptables -t nat -L IP-MASQ-AGENT RETURN all -- anywhere 169.254.0.0/16 /* ip-masq-agent: 集群本地流量不被 MASQUERADE 控制 */ ADDRTYPE match dst-type !LOCAL RETURN all -- anywhere 10.0.0.0/8 /* ip-masq-agent: 集群本地流量不被 MASQUERADE 控制 */ ADDRTYPE match dst-type !LOCAL RETURN all -- anywhere 172.16.0.0/12 /* ip-masq-agent: 集群本地流量不被 MASQUERADE 控制 */ ADDRTYPE match dst-type !LOCAL RETURN all -- anywhere 192.168.0.0/16 /* ip-masq-agent: 集群本地流量不被 MASQUERADE 控制 */ ADDRTYPE match dst-type !LOCAL MASQUERADE all -- anywhere anywhere /* ip-masq-agent: 出站流量应受 MASQUERADE 控制 (此规则必须在集群本地 CIDR 规则之后) */ ADDRTYPE match dst-type !LOCAL 默认情况下，从 Kubernetes 1.7.0 版本开始的 GCE/Google Kubernetes Engine 中，如果启用了网络策略，或者您使用的集群 CIDR 不在 10.0.0.0/8 范围内，则 ip-masq-agent 将在您的集群中运行。如果您在其他环境中运行，则可以将 ip-masq-agent DaemonSet 添加到您的集群：\n创建 ip-masq-agent 通过运行以下 kubectl 指令创建 ip-masq-agent:\nkubectl apply -f https://raw.githubusercontent.com/kubernetes-incubator/ip-masq-agent/master/ip-masq-agent.yaml\n您必须同时将适当的节点标签应用于集群中希望代理运行的任何节点。\nkubectl label nodes my-node beta.kubernetes.io/masq-agent-ds-ready=true\n更多信息可以通过 ip-masq-agent 文档 这里 找到\n在大多数情况下，默认的规则集应该足够；但是，如果您的群集不是这种情况，则可以创建并应用 ConfigMap 来自定义受影响的 IP 范围。 例如，要允许 ip-masq-agent 仅作用于 10.0.0.0/8，您可以一个名为 “config” 的文件中创建以下 ConfigMap 。\n. note \u0026gt;}}\n重要的是，该文件之所以被称为 config，因为默认情况下，该文件将被用作 ip-masq-agent 查找的关键：\nnonMasqueradeCIDRs: - 10.0.0.0/8 resyncInterval: 60s . /note \u0026gt;}}\n运行以下命令将配置映射添加到您的集群：\nkubectl create configmap ip-masq-agent --from-file=config --namespace=kube-system 这将更新位于 /etc/config/ip-masq-agent 的一个文件，该文件以 resyncInterval 为周期定期检查并应用于集群节点。 重新同步间隔到期后，您应该看到您的更改在 iptables 规则中体现：\niptables -t nat -L IP-MASQ-AGENT Chain IP-MASQ-AGENT (1 references) target prot opt source destination RETURN all -- anywhere 169.254.0.0/16 /* ip-masq-agent: 集群本地流量不被 MASQUERADE 控制 */ ADDRTYPE match dst-type !LOCAL RETURN all -- anywhere 10.0.0.0/8 /* ip-masq-agent: cluster-local MASQUERADE all -- anywhere anywhere /* ip-masq-agent: 出站流量应受 MASQUERADE 控制 (此规则必须在集群本地 CIDR 规则之后) */ ADDRTYPE match dst-type !LOCAL 默认情况下，本地链路范围 (169.254.0.0/16) 也由 ip-masq agent 处理，该代理设置适当的 iptables 规则。 要使 ip-masq-agent 忽略本地链路，可以在配置映射中将 masqLinkLocal 设置为true。\nnonMasqueradeCIDRs: - 10.0.0.0/8 resyncInterval: 60s masqLinkLocal: true "
},
{
	"uri": "https://lijun.in/reference/command-line-tools-reference/kube-apiserver/",
	"title": "kube-apiserver",
	"tags": [],
	"description": "",
	"content": "kube-apiserver 概要 Kubernetes API server 为 api 对象验证并配置数据，包括 pods、 services、 replicationcontrollers 和其它 api 对象。API Server 提供 REST 操作和到集群共享状态的前端，所有其他组件通过它进行交互。\nkube-apiserver 选项  --admission-control stringSlice 控制资源进入集群的准入控制插件的顺序列表。逗号分隔的 NamespaceLifecycle 列表。（默认值 [AlwaysAdmit]） --admission-control-config-file string 包含准入控制配置的文件。 --advertise-address ip 向集群成员通知 apiserver 消息的 IP 地址。这个地址必须能够被集群中其他成员访问。如果 IP 地址为空，将会使用 --bind-address，如果未指定 --bind-address，将会使用主机的默认接口地址。 --allow-privileged 如果为 true, 将允许特权容器。 --anonymous-auth 启用到 API server 的安全端口的匿名请求。未被其他认证方法拒绝的请求被当做匿名请求。匿名请求的用户名为 system:anonymous，用户组名为 system:unauthenticated。（默认值 true） --apiserver-count int 集群中运行的 apiserver 数量，必须为正数。（默认值 1） --audit-log-maxage int 基于文件名中的时间戳，旧审计日志文件的最长保留天数。 --audit-log-maxbackup int 旧审计日志文件的最大保留个数。 --audit-log-maxsize int 审计日志被轮转前的最大兆字节数。 --audit-log-path string 如果设置该值，所有到 apiserver 的请求都将会被记录到这个文件。'-' 表示记录到标准输出。 --audit-policy-file string 定义审计策略配置的文件的路径。需要打开 'AdvancedAuditing' 特性开关。AdvancedAuditing 需要一个配置来启用审计功能。 --audit-webhook-config-file string 一个具有 kubeconfig 格式文件的路径，该文件定义了审计的 webhook 配置。需要打开 'AdvancedAuditing' 特性开关。 --audit-webhook-mode string 发送审计事件的策略。 Blocking 模式表示正在发送事件时应该阻塞服务器的响应。 Batch 模式使 webhook 异步缓存和发送事件。 Known 模式为 batch,blocking。（默认值 \u0026quot;batch\u0026quot;) --authentication-token-webhook-cache-ttl duration 从 webhook 令牌认证者获取的响应的缓存时长。( 默认值 2m0s) --authentication-token-webhook-config-file string 包含 webhook 配置的文件，用于令牌认证，具有 kubeconfig 格式。API server 将查询远程服务来决定对 bearer 令牌的认证。 --authorization-mode string 在安全端口上进行权限验证的插件的顺序列表。以逗号分隔的列表，包括：AlwaysAllow,AlwaysDeny,ABAC,Webhook,RBAC,Node.（默认值 \u0026quot;AlwaysAllow\u0026quot;） --authorization-policy-file string 包含权限验证策略的 csv 文件，和 --authorization-mode=ABAC 一起使用，作用在安全端口上。 --authorization-webhook-cache-authorized-ttl duration 从 webhook 授权者获得的 'authorized' 响应的缓存时长。（默认值 5m0s） --authorization-webhook-cache-unauthorized-ttl duration 从 webhook 授权者获得的 'unauthorized' 响应的缓存时长。（默认值 30s） --authorization-webhook-config-file string 包含 webhook 配置的 kubeconfig 格式文件，和 --authorization-mode=Webhook 一起使用。API server 将查询远程服务来决定对 API server 安全端口的访问。 --azure-container-registry-config string 包含 Azure 容器注册表配置信息的文件的路径。 --basic-auth-file string 如果设置该值，这个文件将会被用于准许通过 http 基本认证到 API server 安全端口的请求。 --bind-address ip 监听 --seure-port 的 IP 地址。被关联的接口必须能够被集群其它节点和 CLI/web 客户端访问。如果为空，则将使用所有接口（0.0.0.0）。（默认值 0.0.0.0） --cert-dir string 存放 TLS 证书的目录。如果提供了 --tls-cert-file 和 --tls-private-key-file 选项，该标志将被忽略。（默认值 \u0026quot;/var/run/kubernetes\u0026quot;） --client-ca-file string 如果设置此标志，对于任何请求，如果存包含 client-ca-file 中的 authorities 签名的客户端证书，将会使用客户端证书中的 CommonName 对应的身份进行认证。 --cloud-config string 云服务提供商配置文件路径。空字符串表示无配置文件 . --cloud-provider string 云服务提供商，空字符串表示无提供商。 --contention-profiling 如果已经启用 profiling，则启用锁竞争 profiling。 --cors-allowed-origins stringSlice CORS 的域列表，以逗号分隔。合法的域可以是一个匹配子域名的正则表达式。如果这个列表为空则不会启用 CORS. --delete-collection-workers int 用于 DeleteCollection 调用的工作者数量。这被用于加速 namespace 的清理。( 默认值 1) --deserialization-cache-size int 在内存中缓存的反序列化 json 对象的数量。 --enable-aggregator-routing 打开到 endpoints IP 的 aggregator 路由请求，替换 cluster IP。 --enable-garbage-collector 启用通用垃圾回收器 . 必须与 kube-controller-manager 对应的标志保持同步。 （默认值 true） --enable-logs-handler 如果为 true，则为 apiserver 日志功能安装一个 /logs 处理器。（默认值 true） --enable-swagger-ui 在 apiserver 的 /swagger-ui 路径启用 swagger ui。 --etcd-cafile string 用于保护 etcd 通信的 SSL CA 文件。 --etcd-certfile string 用于保护 etcd 通信的的 SSL 证书文件。 --etcd-keyfile string 用于保护 etcd 通信的 SSL 密钥文件 . --etcd-prefix string 附加到所有 etcd 中资源路径的前缀。 （默认值 \u0026quot;/registry\u0026quot;） --etcd-quorum-read 如果为 true, 启用 quorum 读。 --etcd-servers stringSlice 连接的 etcd 服务器列表 , 形式为（scheme://ip:port)，使用逗号分隔。 --etcd-servers-overrides stringSlice 针对单个资源的 etcd 服务器覆盖配置 , 以逗号分隔。 单个配置覆盖格式为 : group/resource#servers, 其中 servers 形式为 http://ip:port, 以分号分隔。 --event-ttl duration 事件驻留时间。（默认值 1h0m0s) --enable-bootstrap-token-auth 启用此选项以允许 'kube-system' 命名空间中的 'bootstrap.kubernetes.io/token' 类型密钥可以被用于 TLS 的启动认证。 --experimental-encryption-provider-config string 包含加密提供程序的配置的文件，该加密提供程序被用于在 etcd 中保存密钥。 --external-hostname string 为此 master 生成外部 URL 时使用的主机名 ( 例如 Swagger API 文档 )。 --feature-gates mapStringBool 一个描述 alpha/experimental 特性开关的键值对列表。 选项包括 : Accelerators=true|false (ALPHA - default=false) AdvancedAuditing=true|false (ALPHA - default=false) AffinityInAnnotations=true|false (ALPHA - default=false) AllAlpha=true|false (ALPHA - default=false) AllowExtTrafficLocalEndpoints=true|false (default=true) AppArmor=true|false (BETA - default=true) DynamicKubeletConfig=true|false (ALPHA - default=false) DynamicVolumeProvisioning=true|false (ALPHA - default=true) ExperimentalCriticalPodAnnotation=true|false (ALPHA - default=false) ExperimentalHostUserNamespaceDefaulting=true|false (BETA - default=false) LocalStorageCapacityIsolation=true|false (ALPHA - default=false) PersistentLocalVolumes=true|false (ALPHA - default=false) RotateKubeletClientCertificate=true|false (ALPHA - default=false) RotateKubeletServerCertificate=true|false (ALPHA - default=false) StreamingProxyRedirects=true|false (BETA - default=true) TaintBasedEvictions=true|false (ALPHA - default=false) --google-json-key string 用于认证的 Google Cloud Platform 服务账号的 JSON 密钥。 --insecure-allow-any-token username/group1,group2 如果设置该值 , 你的服务将处于非安全状态。任何令牌都将会被允许，并将从令牌中把用户信息解析成为 username/group1,group2。 --insecure-bind-address ip 用于监听 --insecure-port 的 IP 地址 ( 设置成 0.0.0.0 表示监听所有接口 )。（默认值 127.0.0.1) --insecure-port int 用于监听不安全和为认证访问的端口。这个配置假设你已经设置了防火墙规则，使得这个端口不能从集群外访问。对集群的公共地址的 443 端口的访问将被代理到这个端口。默认设置中使用 nginx 实现。（默认值 8080） --kubelet-certificate-authority string 证书 authority 的文件路径。 --kubelet-client-certificate string 用于 TLS 的客户端证书文件路径。 --kubelet-client-key string 用于 TLS 的客户端证书密钥文件路径 . --kubelet-https 为 kubelet 启用 https。 （默认值 true） --kubelet-preferred-address-types stringSlice 用于 kubelet 连接的首选 NodeAddressTypes 列表。 ( 默认值[Hostname,InternalDNS,InternalIP,ExternalDNS,ExternalIP]) --kubelet-read-only-port uint 已废弃 : kubelet 端口 . （默认值 10255） --kubelet-timeout duration kubelet 操作超时时间。（默认值 5s） --kubernetes-service-node-port int 如果不为 0，Kubernetes master 服务（用于创建 / 管理 apiserver）将会使用 NodePort 类型，并将这个值作为端口号。如果为 0，Kubernetes master 服务将会使用 ClusterIP 类型。 --master-service-namespace string 已废弃 : 注入到 pod 中的 kubernetes master 服务的命名空间。（默认值 \u0026quot;default\u0026quot;） --max-connection-bytes-per-sec int 如果不为 0，每个用户连接将会被限速为该值（bytes/sec）。当前只应用于长时间运行的请求。 --max-mutating-requests-inflight int 在给定时间内进行中可变请求的最大数量。当超过该值时，服务将拒绝所有请求。0 值表示没有限制。（默认值 200） --max-requests-inflight int 在给定时间内进行中不可变请求的最大数量。当超过该值时，服务将拒绝所有请求。0 值表示没有限制。（默认值 400） --min-request-timeout int 一个可选字段，表示一个 handler 在一个请求超时前，必须保持它处于打开状态的最小秒数。当前只对监听请求 handler 有效，它基于这个值选择一个随机数作为连接超时值，以达到分散负载的目的（默认值 1800）。 --oidc-ca-file string 如果设置该值，将会使用 oidc-ca-file 中的任意一个 authority 对 OpenID 服务的证书进行验证，否则将会使用主机的根 CA 对其进行验证。 --oidc-client-id string 使用 OpenID 连接的客户端的 ID，如果设置了 oidc-issuer-url，则必须设置这个值。 --oidc-groups-claim string 如果提供该值，这个自定义 OpenID 连接名将指定给特定的用户组。该声明值需要是一个字符串或字符串数组。此标志为实验性的，请查阅验证相关文档进一步了解详细信息。 --oidc-issuer-url string OpenID 颁发者 URL，只接受 HTTPS 方案。如果设置该值，它将被用于验证 OIDC JSON Web Token(JWT)。 --oidc-username-claim string 用作用户名的 OpenID 声明值。注意，不保证除默认 ('sub') 外的其他声明值的唯一性和不变性。此标志为实验性的，请查阅验证相关文档进一步了解详细信息。 --profiling 在 web 接口 host:port/debug/pprof/ 上启用 profiling。（默认值 true） --proxy-client-cert-file string 当必须调用外部程序时，用于证明 aggregator 或者 kube-apiserver 的身份的客户端证书。包括代理到用户 api-server 的请求和调用 webhook 准入控制插件的请求。它期望这个证书包含一个来自于 CA 中的 --requestheader-client-ca-file 标记的签名。该 CA 在 kube-system 命名空间的 'extension-apiserver-authentication' configmap 中发布。从 Kube-aggregator 收到调用的组件应该使用该 CA 进行他们部分的双向 TLS 验证。 --proxy-client-key-file string 当必须调用外部程序时，用于证明 aggregator 或者 kube-apiserver 的身份的客户端证书密钥。包括代理到用户 api-server 的请求和调用 webhook 准入控制插件的请求。 --repair-malformed-updates 如果为 true，服务将会尽力修复更新请求以通过验证，例如：将更新请求 UID 的当前值设置为空。在我们修复了所有发送错误格式请求的客户端后，可以关闭这个标志。 --requestheader-allowed-names stringSlice 使用 --requestheader-username-headers 指定的，允许在头部提供用户名的客户端证书通用名称列表。如果为空，任何通过 --requestheader-client-ca-file 中 authorities 验证的客户端证书都是被允许的。 --requestheader-client-ca-file string 在信任请求头中以 --requestheader-username-headers 指示的用户名之前，用于验证接入请求中客户端证书的根证书捆绑。 --requestheader-extra-headers-prefix stringSlice 用于检查的请求头的前缀列表。建议使用 X-Remote-Extra-。 --requestheader-group-headers stringSlice 用于检查群组的请求头列表。建议使用 X-Remote-Group. --requestheader-username-headers stringSlice 用于检查用户名的请求头列表。建议使用 X-Remote-User。 --runtime-config mapStringString 传递给 apiserver 用于描述运行时配置的键值对集合。 apis/\u0026lt;groupVersion\u0026gt; 键可以被用来打开 / 关闭特定的 api 版本。apis/\u0026lt;groupVersion\u0026gt;/\u0026lt;resource\u0026gt; 键被用来打开 / 关闭特定的资源 . api/all 和 api/legacy 键分别用于控制所有的和遗留的 api 版本 . --secure-port int 用于监听具有认证授权功能的 HTTPS 协议的端口。如果为 0，则不会监听 HTTPS 协议。 （默认值 6443) --service-account-key-file stringArray 包含 PEM 加密的 x509 RSA 或 ECDSA 私钥或公钥的文件，用于验证 ServiceAccount 令牌。如果设置该值，--tls-private-key-file 将会被使用。指定的文件可以包含多个密钥，并且这个标志可以和不同的文件一起多次使用。 --service-cluster-ip-range ipNet CIDR 表示的 IP 范围，服务的 cluster ip 将从中分配。 一定不要和分配给 nodes 和 pods 的 IP 范围产生重叠。 --ssh-keyfile string 如果不为空，在使用安全的 SSH 代理访问节点时，将这个文件作为用户密钥文件。 --storage-backend string 持久化存储后端。 选项为 : 'etcd3' ( 默认 ), 'etcd2'. --storage-media-type string 在存储中保存对象的媒体类型。某些资源或者存储后端可能仅支持特定的媒体类型，并且忽略该配置项。（默认值 \u0026quot;application/vnd.kubernetes.protobuf\u0026quot;) --storage-versions string 按组划分资源存储的版本。 以 \u0026quot;group1/version1,group2/version2,...\u0026quot; 的格式指定。当对象从一组移动到另一组时 , 你可以指定 \u0026quot;group1=group2/v1beta1,group3/v1beta1,...\u0026quot; 的格式。你只需要传入你希望从结果中改变的组的列表。默认为从 KUBE_API_VERSIONS 环境变量集成而来，所有注册组的首选版本列表。 （默认值 \u0026quot;admission.k8s.io/v1alpha1,admissionregistration.k8s.io/v1alpha1,apps/v1beta1,authentication.k8s.io/v1,authorization.k8s.io/v1,autoscaling/v1,batch/v1,certificates.k8s.io/v1beta1,componentconfig/v1alpha1,extensions/v1beta1,federation/v1beta1,imagepolicy.k8s.io/v1alpha1,networking.k8s.io/v1,policy/v1beta1,rbac.authorization.k8s.io/v1beta1,settings.k8s.io/v1alpha1,storage.k8s.io/v1,v1\u0026quot;) --target-ram-mb int apiserver 内存限制，单位为 MB( 用于配置缓存大小等 )。 --tls-ca-file string 如果设置该值，这个证书 authority 将会被用于从 Admission Controllers 过来的安全访问。它必须是一个 PEM 加密的合法 CA 捆绑包。此外 , 该证书 authority 可以被添加到以 --tls-cert-file 提供的证书文件中 . --tls-cert-file string 包含用于 HTTPS 的默认 x509 证书的文件。（如果有 CA 证书，则附加于 server 证书之后）。如果启用了 HTTPS 服务，并且没有提供 --tls-cert-file 和 --tls-private-key-file，则将为公共地址生成一个自签名的证书和密钥并保存于 /var/run/kubernetes 目录。 --tls-private-key-file string 包含匹配 --tls-cert-file 的 x509 证书私钥的文件。 --tls-sni-cert-key namedCertKey 一对 x509 证书和私钥的文件路径 , 可以使用符合正式域名的域形式作为后缀。 如果没有提供域形式后缀 , 则将提取证书名。 非通配符版本优先于通配符版本 , 显示的域形式优先于证书中提取的名字。 对于多个密钥 / 证书对, 请多次使用 --tls-sni-cert-key。例如 : \u0026quot;example.crt,example.key\u0026quot; or \u0026quot;foo.crt,foo.key:*.foo.com,foo.com\u0026quot;. （默认值[]) --token-auth-file string 如果设置该值，这个文件将被用于通过令牌认证来保护 API 服务的安全端口。 --version version[=true] 打印版本信息并退出。 --watch-cache 启用 apiserver 的监视缓存。（默认值 true） --watch-cache-sizes stringSlice 每种资源（pods, nodes 等）的监视缓存大小列表，以逗号分隔。每个缓存配置的形式为：resource#size，size 是一个数字。在 watch-cache 启用时生效。 Auto generated by spf13/cobra on 11-Jul-2017 "
},
{
	"uri": "https://lijun.in/reference/command-line-tools-reference/kube-controller-manager/",
	"title": "kube-controller-manager",
	"tags": [],
	"description": "",
	"content": "kube-controller-manager 概述 Kubernetes 控制器管理器是一个守护进程，嵌入了 Kubernetes 附带的核心控制循环。 在机器人和自动化的应用中，控制回路是一个永不休止的循环，用于调节系统状态。 在 Kubernetes 中，控制器是一个控制循环，它通过 apiserver 监视集群的共享状态，并尝试进行更改以将当前状态转为所需状态。现今，Kubernetes 自带的控制器包括副本控制器，节点控制器，命名空间控制器和serviceaccounts 控制器。\nkube-controller-manager [flags] 选项 \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--allocate-node-cidrs\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt;Should CIDRs for Pods be allocated and set on the cloud provider.\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--alsologtostderr\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt;log to standard error as well as files\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--attach-detach-reconcile-sync-period duration\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;Default: 1m0s\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt;The reconciler sync wait time between volume attach detach. This duration must be larger than one second, and increasing this value from the default may allow for volumes to be mismatched with pods.\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--authentication-kubeconfig string\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt;kubeconfig file pointing at the 'core' kubernetes server with enough rights to create tokenaccessreviews.authentication.k8s.io. This is optional. If empty, all token requests are considered to be anonymous and no client CA is looked up in the cluster.\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--authentication-skip-lookup\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt;If false, the authentication-kubeconfig will be used to lookup missing authentication configuration from the cluster.\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--authentication-token-webhook-cache-ttl duration\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;Default: 10s\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt;The duration to cache responses from the webhook token authenticator.\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--authentication-tolerate-lookup-failure\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt;If true, failures to look up missing authentication configuration from the cluster are not considered fatal. Note that this can result in authentication that treats all requests as anonymous.\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--authorization-always-allow-paths stringSlice\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;Default: [/healthz]\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt;A list of HTTP paths to skip during authorization, i.e. these are authorized without contacting the 'core' kubernetes server.\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--authorization-kubeconfig string\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt;kubeconfig file pointing at the 'core' kubernetes server with enough rights to create subjectaccessreviews.authorization.k8s.io. This is optional. If empty, all requests not skipped by authorization are forbidden.\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--authorization-webhook-cache-authorized-ttl duration\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;Default: 10s\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt;The duration to cache 'authorized' responses from the webhook authorizer.\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--authorization-webhook-cache-unauthorized-ttl duration\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;Default: 10s\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt;The duration to cache 'unauthorized' responses from the webhook authorizer.\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--azure-container-registry-config string\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt;Path to the file containing Azure container registry configuration information.\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--bind-address ip\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;Default: 0.0.0.0\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt;The IP address on which to listen for the --secure-port port. The associated interface(s) must be reachable by the rest of the cluster, and by CLI/web clients. If blank, all interfaces will be used (0.0.0.0 for all IPv4 interfaces and :: for all IPv6 interfaces).\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--cert-dir string\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt;The directory where the TLS certs are located. If --tls-cert-file and --tls-private-key-file are provided, this flag will be ignored.\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--cidr-allocator-type string\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;Default: \u0026quot;RangeAllocator\u0026quot;\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt;Type of CIDR allocator to use\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--client-ca-file string\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt;If set, any request presenting a client certificate signed by one of the authorities in the client-ca-file is authenticated with an identity corresponding to the CommonName of the client certificate.\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--cloud-config string\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt;The path to the cloud provider configuration file. Empty string for no configuration file.\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--cloud-provider string\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt;The provider for cloud services. Empty string for no provider.\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--cluster-cidr string\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt;CIDR Range for Pods in cluster. Requires --allocate-node-cidrs to be true\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--cluster-name string\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;Default: \u0026quot;kubernetes\u0026quot;\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt;The instance prefix for the cluster.\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--cluster-signing-cert-file string\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;Default: \u0026quot;/etc/kubernetes/ca/ca.pem\u0026quot;\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt;Filename containing a PEM-encoded X509 CA certificate used to issue cluster-scoped certificates\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--cluster-signing-key-file string\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;Default: \u0026quot;/etc/kubernetes/ca/ca.key\u0026quot;\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt;Filename containing a PEM-encoded RSA or ECDSA private key used to sign cluster-scoped certificates\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--concurrent-deployment-syncs int32\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;Default: 5\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt;The number of deployment objects that are allowed to sync concurrently. Larger number = more responsive deployments, but more CPU (and network) load\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--concurrent-endpoint-syncs int32\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;Default: 5\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt;The number of endpoint syncing operations that will be done concurrently. Larger number = faster endpoint updating, but more CPU (and network) load\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--concurrent-gc-syncs int32\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;Default: 20\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt;The number of garbage collector workers that are allowed to sync concurrently.\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--concurrent-namespace-syncs int32\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;Default: 10\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt;The number of namespace objects that are allowed to sync concurrently. Larger number = more responsive namespace termination, but more CPU (and network) load\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--concurrent-replicaset-syncs int32\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;Default: 5\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt;The number of replica sets that are allowed to sync concurrently. Larger number = more responsive replica management, but more CPU (and network) load\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--concurrent-resource-quota-syncs int32\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;Default: 5\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt;The number of resource quotas that are allowed to sync concurrently. Larger number = more responsive quota management, but more CPU (and network) load\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--concurrent-service-syncs int32\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;Default: 1\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt;The number of services that are allowed to sync concurrently. Larger number = more responsive service management, but more CPU (and network) load\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--concurrent-serviceaccount-token-syncs int32\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;Default: 5\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt;The number of service account token objects that are allowed to sync concurrently. Larger number = more responsive token generation, but more CPU (and network) load\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--concurrent-ttl-after-finished-syncs int32\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;Default: 5\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt;The number of TTL-after-finished controller workers that are allowed to sync concurrently.\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--concurrent_rc_syncs int32\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;Default: 5\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt;The number of replication controllers that are allowed to sync concurrently. Larger number = more responsive replica management, but more CPU (and network) load\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--configure-cloud-routes\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;Default: true\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt;Should CIDRs allocated by allocate-node-cidrs be configured on the cloud provider.\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--contention-profiling\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt;Enable lock contention profiling, if profiling is enabled\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--controller-start-interval duration\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt;Interval between starting controller managers.\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--controllers stringSlice\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;Default: [*]\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt;A list of controllers to enable. '*' enables all on-by-default controllers, 'foo' enables the controller named 'foo', '-foo' disables the controller named 'foo'.\u0026lt;br/\u0026gt;All controllers: attachdetach, bootstrapsigner, cloud-node-lifecycle, clusterrole-aggregation, cronjob, csrapproving, csrcleaner, csrsigning, daemonset, deployment, disruption, endpoint, garbagecollector, horizontalpodautoscaling, job, namespace, nodeipam, nodelifecycle, persistentvolume-binder, persistentvolume-expander, podgc, pv-protection, pvc-protection, replicaset, replicationcontroller, resourcequota, root-ca-cert-publisher, route, service, serviceaccount, serviceaccount-token, statefulset, tokencleaner, ttl, ttl-after-finished\u0026lt;br/\u0026gt;Disabled-by-default controllers: bootstrapsigner, tokencleaner\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--deployment-controller-sync-period duration\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;Default: 30s\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt;Period for syncing the deployments.\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--disable-attach-detach-reconcile-sync\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt;Disable volume attach detach reconciler sync. Disabling this may cause volumes to be mismatched with pods. Use wisely.\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--enable-dynamic-provisioning\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;Default: true\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt;Enable dynamic provisioning for environments that support it.\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--enable-garbage-collector\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;Default: true\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt;Enables the generic garbage collector. MUST be synced with the corresponding flag of the kube-apiserver.\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--enable-hostpath-provisioner\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt;Enable HostPath PV provisioning when running without a cloud provider. This allows testing and development of provisioning features. HostPath provisioning is not supported in any way, won't work in a multi-node cluster, and should not be used for anything other than testing or development.\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--enable-taint-manager\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;Default: true\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt;WARNING: Beta feature. If set to true enables NoExecute Taints and will evict all not-tolerating Pod running on Nodes tainted with this kind of Taints.\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--experimental-cluster-signing-duration duration\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;Default: 8760h0m0s\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt;The length of duration signed certificates will be given.\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--external-cloud-volume-plugin string\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt;The plugin to use when cloud provider is set to external. Can be empty, should only be set when cloud-provider is external. Currently used to allow node and volume controllers to work for in tree cloud providers.\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--feature-gates mapStringBool\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt;A set of key=value pairs that describe feature gates for alpha/experimental features. Options are:\u0026lt;br/\u0026gt;APIListChunking=true|false (BETA - default=true)\u0026lt;br/\u0026gt;APIResponseCompression=true|false (ALPHA - default=false)\u0026lt;br/\u0026gt;AllAlpha=true|false (ALPHA - default=false)\u0026lt;br/\u0026gt;AppArmor=true|false (BETA - default=true)\u0026lt;br/\u0026gt;AttachVolumeLimit=true|false (BETA - default=true)\u0026lt;br/\u0026gt;BalanceAttachedNodeVolumes=true|false (ALPHA - default=false)\u0026lt;br/\u0026gt;BlockVolume=true|false (BETA - default=true)\u0026lt;br/\u0026gt;BoundServiceAccountTokenVolume=true|false (ALPHA - default=false)\u0026lt;br/\u0026gt;CPUManager=true|false (BETA - default=true)\u0026lt;br/\u0026gt;CRIContainerLogRotation=true|false (BETA - default=true)\u0026lt;br/\u0026gt;CSIBlockVolume=true|false (BETA - default=true)\u0026lt;br/\u0026gt;CSIDriverRegistry=true|false (BETA - default=true)\u0026lt;br/\u0026gt;CSIInlineVolume=true|false (ALPHA - default=false)\u0026lt;br/\u0026gt;CSIMigration=true|false (ALPHA - default=false)\u0026lt;br/\u0026gt;CSIMigrationAWS=true|false (ALPHA - default=false)\u0026lt;br/\u0026gt;CSIMigrationGCE=true|false (ALPHA - default=false)\u0026lt;br/\u0026gt;CSIMigrationOpenStack=true|false (ALPHA - default=false)\u0026lt;br/\u0026gt;CSINodeInfo=true|false (BETA - default=true)\u0026lt;br/\u0026gt;CustomCPUCFSQuotaPeriod=true|false (ALPHA - default=false)\u0026lt;br/\u0026gt;CustomResourcePublishOpenAPI=true|false (ALPHA - default=false)\u0026lt;br/\u0026gt;CustomResourceSubresources=true|false (BETA - default=true)\u0026lt;br/\u0026gt;CustomResourceValidation=true|false (BETA - default=true)\u0026lt;br/\u0026gt;CustomResourceWebhookConversion=true|false (ALPHA - default=false)\u0026lt;br/\u0026gt;DebugContainers=true|false (ALPHA - default=false)\u0026lt;br/\u0026gt;DevicePlugins=true|false (BETA - default=true)\u0026lt;br/\u0026gt;DryRun=true|false (BETA - default=true)\u0026lt;br/\u0026gt;DynamicAuditing=true|false (ALPHA - default=false)\u0026lt;br/\u0026gt;DynamicKubeletConfig=true|false (BETA - default=true)\u0026lt;br/\u0026gt;ExpandCSIVolumes=true|false (ALPHA - default=false)\u0026lt;br/\u0026gt;ExpandInUsePersistentVolumes=true|false (ALPHA - default=false)\u0026lt;br/\u0026gt;ExpandPersistentVolumes=true|false (BETA - default=true)\u0026lt;br/\u0026gt;ExperimentalCriticalPodAnnotation=true|false (ALPHA - default=false)\u0026lt;br/\u0026gt;ExperimentalHostUserNamespaceDefaulting=true|false (BETA - default=false)\u0026lt;br/\u0026gt;HyperVContainer=true|false (ALPHA - default=false)\u0026lt;br/\u0026gt;KubeletPodResources=true|false (ALPHA - default=false)\u0026lt;br/\u0026gt;LocalStorageCapacityIsolation=true|false (BETA - default=true)\u0026lt;br/\u0026gt;MountContainers=true|false (ALPHA - default=false)\u0026lt;br/\u0026gt;NodeLease=true|false (BETA - default=true)\u0026lt;br/\u0026gt;PodShareProcessNamespace=true|false (BETA - default=true)\u0026lt;br/\u0026gt;ProcMountType=true|false (ALPHA - default=false)\u0026lt;br/\u0026gt;QOSReserved=true|false (ALPHA - default=false)\u0026lt;br/\u0026gt;ResourceLimitsPriorityFunction=true|false (ALPHA - default=false)\u0026lt;br/\u0026gt;ResourceQuotaScopeSelectors=true|false (BETA - default=true)\u0026lt;br/\u0026gt;RotateKubeletClientCertificate=true|false (BETA - default=true)\u0026lt;br/\u0026gt;RotateKubeletServerCertificate=true|false (BETA - default=true)\u0026lt;br/\u0026gt;RunAsGroup=true|false (BETA - default=true)\u0026lt;br/\u0026gt;RuntimeClass=true|false (BETA - default=true)\u0026lt;br/\u0026gt;SCTPSupport=true|false (ALPHA - default=false)\u0026lt;br/\u0026gt;ScheduleDaemonSetPods=true|false (BETA - default=true)\u0026lt;br/\u0026gt;ServerSideApply=true|false (ALPHA - default=false)\u0026lt;br/\u0026gt;ServiceNodeExclusion=true|false (ALPHA - default=false)\u0026lt;br/\u0026gt;StorageVersionHash=true|false (ALPHA - default=false)\u0026lt;br/\u0026gt;StreamingProxyRedirects=true|false (BETA - default=true)\u0026lt;br/\u0026gt;SupportNodePidsLimit=true|false (ALPHA - default=false)\u0026lt;br/\u0026gt;SupportPodPidsLimit=true|false (BETA - default=true)\u0026lt;br/\u0026gt;Sysctls=true|false (BETA - default=true)\u0026lt;br/\u0026gt;TTLAfterFinished=true|false (ALPHA - default=false)\u0026lt;br/\u0026gt;TaintBasedEvictions=true|false (BETA - default=true)\u0026lt;br/\u0026gt;TaintNodesByCondition=true|false (BETA - default=true)\u0026lt;br/\u0026gt;TokenRequest=true|false (BETA - default=true)\u0026lt;br/\u0026gt;TokenRequestProjection=true|false (BETA - default=true)\u0026lt;br/\u0026gt;ValidateProxyRedirects=true|false (BETA - default=true)\u0026lt;br/\u0026gt;VolumeSnapshotDataSource=true|false (ALPHA - default=false)\u0026lt;br/\u0026gt;VolumeSubpathEnvExpansion=true|false (ALPHA - default=false)\u0026lt;br/\u0026gt;WinDSR=true|false (ALPHA - default=false)\u0026lt;br/\u0026gt;WinOverlay=true|false (ALPHA - default=false)\u0026lt;br/\u0026gt;WindowsGMSA=true|false (ALPHA - default=false)\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--flex-volume-plugin-dir string\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;Default: \u0026quot;/usr/libexec/kubernetes/kubelet-plugins/volume/exec/\u0026quot;\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt;Full path of the directory in which the flex volume plugin should search for additional third party volume plugins.\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;-h, --help\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt;help for kube-controller-manager\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--horizontal-pod-autoscaler-cpu-initialization-period duration\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;Default: 5m0s\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt;The period after pod start when CPU samples might be skipped.\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--horizontal-pod-autoscaler-downscale-stabilization duration\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;Default: 5m0s\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt;The period for which autoscaler will look backwards and not scale down below any recommendation it made during that period.\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--horizontal-pod-autoscaler-initial-readiness-delay duration\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;Default: 30s\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt;The period after pod start during which readiness changes will be treated as initial readiness.\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--horizontal-pod-autoscaler-sync-period duration\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;Default: 15s\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt;The period for syncing the number of pods in horizontal pod autoscaler.\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--horizontal-pod-autoscaler-tolerance float\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;Default: 0.1\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt;The minimum change (from 1.0) in the desired-to-actual metrics ratio for the horizontal pod autoscaler to consider scaling.\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--http2-max-streams-per-connection int\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt;The limit that the server gives to clients for the maximum number of streams in an HTTP/2 connection. Zero means to use golang's default.\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--kube-api-burst int32\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;Default: 30\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt;Burst to use while talking with kubernetes apiserver.\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--kube-api-content-type string\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;Default: \u0026quot;application/vnd.kubernetes.protobuf\u0026quot;\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt;Content type of requests sent to apiserver.\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--kube-api-qps float32\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;Default: 20\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt;QPS to use while talking with kubernetes apiserver.\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--kubeconfig string\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt;Path to kubeconfig file with authorization and master location information.\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--large-cluster-size-threshold int32\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;Default: 50\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt;Number of nodes from which NodeController treats the cluster as large for the eviction logic purposes. --secondary-node-eviction-rate is implicitly overridden to 0 for clusters this size or smaller.\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--leader-elect\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;Default: true\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt;Start a leader election client and gain leadership before executing the main loop. Enable this when running replicated components for high availability.\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--leader-elect-lease-duration duration\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;Default: 15s\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt;The duration that non-leader candidates will wait after observing a leadership renewal until attempting to acquire leadership of a led but unrenewed leader slot. This is effectively the maximum duration that a leader can be stopped before it is replaced by another candidate. This is only applicable if leader election is enabled.\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--leader-elect-renew-deadline duration\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;Default: 10s\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt;The interval between attempts by the acting master to renew a leadership slot before it stops leading. This must be less than or equal to the lease duration. This is only applicable if leader election is enabled.\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--leader-elect-resource-lock endpoints\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;Default: \u0026quot;endpoints\u0026quot;\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt;The type of resource object that is used for locking during leader election. Supported options are endpoints (default) and `configmaps`.\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--leader-elect-retry-period duration\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;Default: 2s\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt;The duration the clients should wait between attempting acquisition and renewal of a leadership. This is only applicable if leader election is enabled.\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--log-backtrace-at traceLocation\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;Default: :0\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt;when logging hits line file:N, emit a stack trace\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--log-dir string\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt;If non-empty, write log files in this directory\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--log-file string\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt;If non-empty, use this log file\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--log-flush-frequency duration\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;Default: 5s\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt;Maximum number of seconds between log flushes\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--logtostderr\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;Default: true\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt;log to standard error instead of files\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--master string\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt;The address of the Kubernetes API server (overrides any value in kubeconfig).\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--min-resync-period duration\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;Default: 12h0m0s\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt;The resync period in reflectors will be random between MinResyncPeriod and 2*MinResyncPeriod.\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--namespace-sync-period duration\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;Default: 5m0s\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt;The period for syncing namespace life-cycle updates\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--node-cidr-mask-size int32\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;Default: 24\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt;Mask size for node cidr in cluster.\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--node-eviction-rate float32\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;Default: 0.1\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt;Number of nodes per second on which pods are deleted in case of node failure when a zone is healthy (see --unhealthy-zone-threshold for definition of healthy/unhealthy). Zone refers to entire cluster in non-multizone clusters.\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--node-monitor-grace-period duration\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;Default: 40s\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt;Amount of time which we allow running Node to be unresponsive before marking it unhealthy. Must be N times more than kubelet's nodeStatusUpdateFrequency, where N means number of retries allowed for kubelet to post node status.\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--node-monitor-period duration\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;Default: 5s\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt;The period for syncing NodeStatus in NodeController.\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--node-startup-grace-period duration\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;Default: 1m0s\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt;Amount of time which we allow starting Node to be unresponsive before marking it unhealthy.\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--pod-eviction-timeout duration\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;Default: 5m0s\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt;The grace period for deleting pods on failed nodes.\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--profiling\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt;Enable profiling via web interface host:port/debug/pprof/\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--pv-recycler-increment-timeout-nfs int32\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;Default: 30\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt;the increment of time added per Gi to ActiveDeadlineSeconds for an NFS scrubber pod\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--pv-recycler-minimum-timeout-hostpath int32\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;Default: 60\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt;The minimum ActiveDeadlineSeconds to use for a HostPath Recycler pod. This is for development and testing only and will not work in a multi-node cluster.\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--pv-recycler-minimum-timeout-nfs int32\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;Default: 300\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt;The minimum ActiveDeadlineSeconds to use for an NFS Recycler pod\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--pv-recycler-pod-template-filepath-hostpath string\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt;The file path to a pod definition used as a template for HostPath persistent volume recycling. This is for development and testing only and will not work in a multi-node cluster.\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--pv-recycler-pod-template-filepath-nfs string\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt;The file path to a pod definition used as a template for NFS persistent volume recycling\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--pv-recycler-timeout-increment-hostpath int32\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;Default: 30\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt;the increment of time added per Gi to ActiveDeadlineSeconds for a HostPath scrubber pod. This is for development and testing only and will not work in a multi-node cluster.\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--pvclaimbinder-sync-period duration\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;Default: 15s\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt;The period for syncing persistent volumes and persistent volume claims\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--requestheader-allowed-names stringSlice\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt;List of client certificate common names to allow to provide usernames in headers specified by --requestheader-username-headers. If empty, any client certificate validated by the authorities in --requestheader-client-ca-file is allowed.\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--requestheader-client-ca-file string\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt;Root certificate bundle to use to verify client certificates on incoming requests before trusting usernames in headers specified by --requestheader-username-headers. WARNING: generally do not depend on authorization being already done for incoming requests.\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--requestheader-extra-headers-prefix stringSlice\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;Default: [x-remote-extra-]\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt;List of request header prefixes to inspect. X-Remote-Extra- is suggested.\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--requestheader-group-headers stringSlice\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;Default: [x-remote-group]\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt;List of request headers to inspect for groups. X-Remote-Group is suggested.\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--requestheader-username-headers stringSlice\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;Default: [x-remote-user]\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt;List of request headers to inspect for usernames. X-Remote-User is common.\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--resource-quota-sync-period duration\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;Default: 5m0s\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt;The period for syncing quota usage status in the system\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--root-ca-file string\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt;If set, this root certificate authority will be included in service account's token secret. This must be a valid PEM-encoded CA bundle.\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--route-reconciliation-period duration\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;Default: 10s\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt;The period for reconciling routes created for Nodes by cloud provider.\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--secondary-node-eviction-rate float32\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;Default: 0.01\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt;Number of nodes per second on which pods are deleted in case of node failure when a zone is unhealthy (see --unhealthy-zone-threshold for definition of healthy/unhealthy). Zone refers to entire cluster in non-multizone clusters. This value is implicitly overridden to 0 if the cluster size is smaller than --large-cluster-size-threshold.\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--secure-port int\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;Default: 10257\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt;The port on which to serve HTTPS with authentication and authorization.If 0, don't serve HTTPS at all.\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--service-account-private-key-file string\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt;Filename containing a PEM-encoded private RSA or ECDSA key used to sign service account tokens.\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--service-cluster-ip-range string\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt;CIDR Range for Services in cluster. Requires --allocate-node-cidrs to be true\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--skip-headers\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt;If true, avoid header prefixes in the log messages\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--stderrthreshold severity\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;Default: 2\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt;logs at or above this threshold go to stderr\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--terminated-pod-gc-threshold int32\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;Default: 12500\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt;Number of terminated pods that can exist before the terminated pod garbage collector starts deleting terminated pods. If \u0026amp;lt;= 0, the terminated pod garbage collector is disabled.\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--tls-cert-file string\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt;File containing the default x509 Certificate for HTTPS. (CA cert, if any, concatenated after server cert). If HTTPS serving is enabled, and --tls-cert-file and --tls-private-key-file are not provided, a self-signed certificate and key are generated for the public address and saved to the directory specified by --cert-dir.\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--tls-cipher-suites stringSlice\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt;Comma-separated list of cipher suites for the server. If omitted, the default Go cipher suites will be use. Possible values: TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_RC4_128_SHA,TLS_ECDHE_RSA_WITH_3DES_EDE_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_RC4_128_SHA,TLS_RSA_WITH_3DES_EDE_CBC_SHA,TLS_RSA_WITH_AES_128_CBC_SHA,TLS_RSA_WITH_AES_128_CBC_SHA256,TLS_RSA_WITH_AES_128_GCM_SHA256,TLS_RSA_WITH_AES_256_CBC_SHA,TLS_RSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_RC4_128_SHA\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--tls-min-version string\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt;Minimum TLS version supported. Possible values: VersionTLS10, VersionTLS11, VersionTLS12\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--tls-private-key-file string\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt;File containing the default x509 private key matching --tls-cert-file.\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--tls-sni-cert-key namedCertKey\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;Default: []\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt;A pair of x509 certificate and private key file paths, optionally suffixed with a list of domain patterns which are fully qualified domain names, possibly with prefixed wildcard segments. If no domain patterns are provided, the names of the certificate are extracted. Non-wildcard matches trump over wildcard matches, explicit domain patterns trump over extracted names. For multiple key/certificate pairs, use the --tls-sni-cert-key multiple times. Examples: \u0026quot;example.crt,example.key\u0026quot; or \u0026quot;foo.crt,foo.key:*.foo.com,foo.com\u0026quot;.\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--unhealthy-zone-threshold float32\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;Default: 0.55\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt;Fraction of Nodes in a zone which needs to be not Ready (minimum 3) for zone to be treated as unhealthy. \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--use-service-account-credentials\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt;If true, use individual service account credentials for each controller.\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;-v, --v Level\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt;number for the log level verbosity\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--version version[=true]\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt;Print version information and quit\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;--vmodule moduleSpec\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td style=\u0026quot;line-height: 130%; word-wrap: break-word;\u0026quot;\u0026gt;comma-separated list of pattern=N settings for file-filtered logging\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt;  "
},
{
	"uri": "https://lijun.in/reference/kubectl/kubectl-cmds/",
	"title": "kubectl 命令",
	"tags": [],
	"description": "",
	"content": "kubectl 命令参考\n"
},
{
	"uri": "https://lijun.in/reference/kubectl/cheatsheet/",
	"title": "kubectl 备忘单",
	"tags": [],
	"description": "",
	"content": "也可以看下: Kubectl 概述 和 JsonPath 指南。\n本页面是 kubectl 命令的概述。\nkubectl - 备忘单 Kubectl 自动补全 BASH ​```bash source \u0026lt;(kubectl completion bash) # 在 bash 中设置当前 shell 的自动补全，要先安装 bash-completion 包。 echo \u0026ldquo;source \u0026lt;(kubectl completion bash)\u0026rdquo; \u0026raquo; ~/.bashrc # 在您的 bash shell 中永久的添加自动补全\n \u0026lt;!-- You can also use a shorthand alias for `kubectl` that also works with completion: --\u0026gt; 您还可以为 `kubectl` 使用一个速记别名，该别名也可以与 completion 一起使用： ```bash alias k=kubectl complete -F __start_kubectl k ZSH ​```bash source \u0026lt;(kubectl completion zsh) # 在 zsh 中设置当前 shell 的自动补全 echo \u0026ldquo;if [ $commands[kubectl] ]; then source \u0026lt;(kubectl completion zsh); fi\u0026rdquo; \u0026raquo; ~/.zshrc # 在您的 zsh shell 中永久的添加自动补全\n \u0026lt;!-- ## Kubectl Context and Configuration Set which Kubernetes cluster `kubectl` communicates with and modifies configuration information. See [Authenticating Across Clusters with kubeconfig](/docs/tasks/access-application-cluster/configure-access-multiple-clusters/) documentation for detailed config file information. --\u0026gt; ## Kubectl 上下文和配置 设置 `kubectl` 与哪个 Kubernetes 集群进行通信并修改配置信息。查看 [使用 kubeconfig 跨集群授权访问](/docs/tasks/access-application-cluster/configure-access-multiple-clusters/) 文档获取详情配置文件信息。 \u0026lt;!-- ```bash kubectl config view # Show Merged kubeconfig settings. # use multiple kubeconfig files at the same time and view merged config KUBECONFIG=~/.kube/config:~/.kube/kubconfig2 kubectl config view # get the password for the e2e user kubectl config view -o jsonpath='{.users[?(@.name == \u0026quot;e2e\u0026quot;)].user.password}' kubectl config view -o jsonpath='{.users[].name}' # get a list of users kubectl config get-contexts # display list of contexts kubectl config current-context\t# display the current-context kubectl config use-context my-cluster-name # set the default context to my-cluster-name # add a new cluster to your kubeconf that supports basic auth kubectl config set-credentials kubeuser/foo.kubernetes.com --username=kubeuser --password=kubepassword # permanently save the namespace for all subsequent kubectl commands in that context. kubectl config set-context --current --namespace=ggckad-s2 # set a context utilizing a specific username and namespace. kubectl config set-context gce --user=cluster-admin --namespace=foo \\ \u0026amp;\u0026amp; kubectl config use-context gce kubectl config unset users.foo # delete user foo ``` --\u0026gt; ​```bash kubectl config view # 显示合并的 kubeconfig 配置。 # 同时使用多个 kubeconfig 文件并查看合并的配置 KUBECONFIG=~/.kube/config:~/.kube/kubconfig2 kubectl config view # 获取 e2e 用户的密码 kubectl config view -o jsonpath='{.users[?(@.name == \u0026quot;e2e\u0026quot;)].user.password}' kubectl config current-context # 展示当前所处的上下文 kubectl config use-context my-cluster-name # 设置默认的上下文为 my-cluster-name # 添加新的集群配置到 kubeconf 中，使用 basic auth 进行鉴权 kubectl config set-credentials kubeuser/foo.kubernetes.com --username=kubeuser --password=kubepassword # 使用特定的用户名和命名空间设置上下文。 kubectl config set-context gce --user=cluster-admin --namespace=foo \\ \u0026amp;\u0026amp; kubectl config use-context gce Apply apply 通过定义 Kubernetes 资源的文件管理应用程序。它通过运行 kubectl apply 在集群中创建和更新资源。这是在生产中管理 Kubernetes 应用程序的推荐方法。查阅 Kubectl 文档。\n创建对象 Kubernetes 配置可以用 json 或 yaml 定义。可以使用的文件扩展名有 .yaml，.yml 和 .json。\n​```bash kubectl apply -f ./my-manifest.yaml # 创建资源 kubectl apply -f ./my1.yaml -f ./my2.yaml # 使用多个文件创建 kubectl apply -f ./dir # 从目录下的全部配置文件创建资源 kubectl apply -f https://git.io/vPieo # 从 url 中创建资源 kubectl create deployment nginx \u0026ndash;image=nginx # 启动单实例 nginx kubectl explain pods,svc # 获取 pod，svc 配置的文档说明\n从标准输入中的多个 YAML 对象中创建 cat \u0026laquo;EOF | kubectl apply -f - apiVersion: v1 kind: Pod metadata: name: busybox-sleep spec: containers:\n name: busybox image: busybox args:  sleep \u0026ldquo;1000000\u0026rdquo;     apiVersion: v1 kind: Pod metadata: name: busybox-sleep-less spec: containers:\n name: busybox image: busybox args:  sleep \u0026ldquo;1000\u0026rdquo; EOF    创建有多个 key 的 Secret cat \u0026laquo;EOF | kubectl apply -f - apiVersion: v1 kind: Secret metadata: name: mysecret type: Opaque data: password: $(echo -n \u0026ldquo;s33msi4\u0026rdquo; | base64 -w0) username: $(echo -n \u0026ldquo;jane\u0026rdquo; | base64 -w0) EOF\n \u0026lt;!-- ## Viewing, Finding Resources --\u0026gt; ## 获取和查找资源 \u0026lt;!-- ```bash # Get commands with basic output kubectl get services # List all services in the namespace kubectl get pods --all-namespaces # List all pods in all namespaces kubectl get pods -o wide # List all pods in the namespace, with more details kubectl get deployment my-dep # List a particular deployment kubectl get pods --include-uninitialized # List all pods in the namespace, including uninitialized ones kubectl get pod my-pod -o yaml # Get a pod's YAML kubectl get pod my-pod -o yaml --export # Get a pod's YAML without cluster specific information # Describe commands with verbose output kubectl describe nodes my-node kubectl describe pods my-pod kubectl get services --sort-by=.metadata.name # List Services Sorted by Name # List pods Sorted by Restart Count kubectl get pods --sort-by='.status.containerStatuses[0].restartCount' # List pods in test namespace sorted by capacity kubectl get pods -n test --sort-by=.spec.capacity.storage # Get the version label of all pods with label app=cassandra kubectl get pods --selector=app=cassandra -o \\ jsonpath='{.items[*].metadata.labels.version}' # Get all worker nodes (use a selector to exclude results that have a label # named 'node-role.kubernetes.io/master') kubectl get node --selector='!node-role.kubernetes.io/master' # Get all running pods in the namespace kubectl get pods --field-selector=status.phase=Running # Get ExternalIPs of all nodes kubectl get nodes -o jsonpath='{.items[*].status.addresses[?(@.type==\u0026quot;ExternalIP\u0026quot;)].address}' # List Names of Pods that belong to Particular RC # \u0026quot;jq\u0026quot; command useful for transformations that are too complex for jsonpath, it can be found at https://stedolan.github.io/jq/ sel=${$(kubectl get rc my-rc --output=json | jq -j '.spec.selector | to_entries | .[] | \u0026quot;\\(.key)=\\(.value),\u0026quot;')%?} echo $(kubectl get pods --selector=$sel --output=jsonpath={.items..metadata.name}) # Show labels for all pods (or any other Kubernetes object that supports labelling) # Also uses \u0026quot;jq\u0026quot; for item in $( kubectl get pod --output=name); do printf \u0026quot;Labels for %s\\n\u0026quot; \u0026quot;$item\u0026quot; | grep --color -E '[^/]+$' \u0026amp;\u0026amp; kubectl get \u0026quot;$item\u0026quot; --output=json | jq -r -S '.metadata.labels | to_entries | .[] | \u0026quot; \\(.key)=\\(.value)\u0026quot;' 2\u0026gt;/dev/null; printf \u0026quot;\\n\u0026quot;; done # Or this command can be used as well to get all the labels associated with pods kubectl get pods --show-labels # Check which nodes are ready JSONPATH='{range .items[*]}{@.metadata.name}:{range @.status.conditions[*]}{@.type}={@.status};{end}{end}' \\ \u0026amp;\u0026amp; kubectl get nodes -o jsonpath=\u0026quot;$JSONPATH\u0026quot; | grep \u0026quot;Ready=True\u0026quot; # List all Secrets currently in use by a pod kubectl get pods -o json | jq '.items[].spec.containers[].env[]?.valueFrom.secretKeyRef.name' | grep -v null | sort | uniq # List Events sorted by timestamp kubectl get events --sort-by=.metadata.creationTimestamp ``` --\u0026gt; ​```bash # 使用 get 命令获取基本输出 kubectl get services # 列出当前命名空间下的所有 services kubectl get pods --all-namespaces # 列出所有命名空间下的全部的 pods kubectl get pods -o wide # 列出当前命名空间下的全部 pods，有更多的详细信息 kubectl get deployment my-dep # 列出某个特定的 deployment kubectl get pods --include-uninitialized # 列出当前命名空间下的全部 pods，包含未初始化的 kubectl get pod my-pod -o yaml # 获取一个 pod 的 YAML kubectl get pod my-pod -o yaml --export # 获取一个没有集群特定信息的 YAML # 使用 describe 命令获取详细输出 kubectl describe nodes my-node kubectl describe pods my-pod kubectl get services --sort-by=.metadata.name # 列出当前命名空间下所有 services，按照名称排序 # 列出 pods 按照重启次数进行排序 kubectl get pods --sort-by='.status.containerStatuses[0].restartCount' # 列出测试命名空间中的 Pod，按容量排序 kubectl get pods -n test --sort-by=.spec.capacity.storage # 获取包含 app=cassandra 标签全部 pods 的 version 标签 kubectl get pods --selector=app=cassandra -o \\ jsonpath='{.items[*].metadata.labels.version}' # 获取所有工作节点(使用选择器以排除标签名称为 'node-role.kubernetes.io/master' 的结果) kubectl get node --selector='!node-role.kubernetes.io/master' # 获取当前命名空间中正在运行的 pods kubectl get pods --field-selector=status.phase=Running # 获取全部 node 的 ExternalIP 地址 kubectl get nodes -o jsonpath='{.items[*].status.addresses[?(@.type==\u0026quot;ExternalIP\u0026quot;)].address}' # 列出属于某个特定 RC 的 pods 的名称 # \u0026quot;jq\u0026quot; 命令对于 jsonpath 过于复杂的转换非常有用，可以在 https://stedolan.github.io/jq/ 找到它。 sel=${$(kubectl get rc my-rc --output=json | jq -j '.spec.selector | to_entries | .[] | \u0026quot;\\(.key)=\\(.value),\u0026quot;')%?} echo $(kubectl get pods --selector=$sel --output=jsonpath={.items..metadata.name}) # 显示所有 Pod 的标签(或任何其他支持标签的 Kubernetes 对象) # 也可以使用 \u0026quot;jq\u0026quot; for item in $( kubectl get pod --output=name); do printf \u0026quot;Labels for %s\\n\u0026quot; \u0026quot;$item\u0026quot; | grep --color -E '[^/]+$' \u0026amp;\u0026amp; kubectl get \u0026quot;$item\u0026quot; --output=json | jq -r -S '.metadata.labels | to_entries | .[] | \u0026quot; \\(.key)=\\(.value)\u0026quot;' 2\u0026gt;/dev/null; printf \u0026quot;\\n\u0026quot;; done # 或也可以使用此命令来获取与容器关联的所有标签 kubectl get pods --show-labels # 检查哪些节点处于 ready JSONPATH='{range .items[*]}{@.metadata.name}:{range @.status.conditions[*]}{@.type}={@.status};{end}{end}' \\ \u0026amp;\u0026amp; kubectl get nodes -o jsonpath=\u0026quot;$JSONPATH\u0026quot; | grep \u0026quot;Ready=True\u0026quot; # 列出被一个 pod 使用的全部 secret kubectl get pods -o json | jq '.items[].spec.containers[].env[]?.valueFrom.secretKeyRef.name' | grep -v null | sort | uniq # 列出 events，按照创建时间排序 kubectl get events --sort-by=.metadata.creationTimestamp 更新资源 从版本 1.11 开始，rolling-update 已被弃用（参见 CHANGELOG-1.11.md)，请使用 rollout 代替。\n​```bash kubectl set image deployment/frontend www=image:v2 # 滚动更新 \u0026ldquo;frontend\u0026rdquo; deployment 的 \u0026ldquo;www\u0026rdquo; 容器镜像 kubectl rollout history deployment/frontend # 检查部署的历史记录，包括版本 kubectl rollout undo deployment/frontend # 回滚到上次部署版本 kubectl rollout undo deployment/frontend \u0026ndash;to-revision=2 # 回滚到特定部署版本 kubectl rollout status -w deployment/frontend # Watch \u0026ldquo;frontend\u0026rdquo; deployment 的滚动升级状态直到完成\n从 1.11 版本开始弃用 kubectl rolling-update frontend-v1 -f frontend-v2.json # (弃用) 滚动升级 frontend-v1 的 pods kubectl rolling-update frontend-v1 frontend-v2 \u0026ndash;image=image:v2 # (弃用) 修改资源的名称并更新镜像 kubectl rolling-update frontend \u0026ndash;image=image:v2 # (弃用) 更新 frontend 的 pods 的镜像 kubectl rolling-update frontend-v1 frontend-v2 \u0026ndash;rollback # (弃用) 终止已经进行中的 rollout\ncat pod.json | kubectl replace -f - # 通过传入到标准输入的 JSON 来替换 pod\n强制进行替换，会删除然后再创建资源，会导致服务不可用。 kubectl replace \u0026ndash;force -f ./pod.json\n为多副本的 nginx 创建服务，使用 80 端口提供服务，连接到容器的 8000 端口。 kubectl expose rc nginx \u0026ndash;port=80 \u0026ndash;target-port=8000\n更新单容器 pod 的镜像标签到 v4 kubectl get pod mypod -o yaml | sed \u0026rsquo;s/(image: myimage):.*$/\\1:v4/\u0026rsquo; | kubectl replace -f -\nkubectl label pods my-pod new-label=awesome # 添加标签 kubectl annotate pods my-pod icon-url=http://goo.gl/XXBTWq # 添加注解 kubectl autoscale deployment foo \u0026ndash;min=2 \u0026ndash;max=10 # 使 \u0026ldquo;foo\u0026rdquo; deployment 自动伸缩容\n \u0026lt;!-- ## Patching Resources --\u0026gt; ## 局部更新资源 \u0026lt;!-- ```bash kubectl patch node k8s-node-1 -p '{\u0026quot;spec\u0026quot;:{\u0026quot;unschedulable\u0026quot;:true}}' # Partially update a node # Update a container's image; spec.containers[*].name is required because it's a merge key kubectl patch pod valid-pod -p '{\u0026quot;spec\u0026quot;:{\u0026quot;containers\u0026quot;:[{\u0026quot;name\u0026quot;:\u0026quot;kubernetes-serve-hostname\u0026quot;,\u0026quot;image\u0026quot;:\u0026quot;new image\u0026quot;}]}}' # Update a container's image using a json patch with positional arrays kubectl patch pod valid-pod --type='json' -p='[{\u0026quot;op\u0026quot;: \u0026quot;replace\u0026quot;, \u0026quot;path\u0026quot;: \u0026quot;/spec/containers/0/image\u0026quot;, \u0026quot;value\u0026quot;:\u0026quot;new image\u0026quot;}]' # Disable a deployment livenessProbe using a json patch with positional arrays kubectl patch deployment valid-deployment --type json -p='[{\u0026quot;op\u0026quot;: \u0026quot;remove\u0026quot;, \u0026quot;path\u0026quot;: \u0026quot;/spec/template/spec/containers/0/livenessProbe\u0026quot;}]' # Add a new element to a positional array kubectl patch sa default --type='json' -p='[{\u0026quot;op\u0026quot;: \u0026quot;add\u0026quot;, \u0026quot;path\u0026quot;: \u0026quot;/secrets/1\u0026quot;, \u0026quot;value\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;whatever\u0026quot; } }]' ``` --\u0026gt; ​```bash kubectl patch node k8s-node-1 -p '{\u0026quot;spec\u0026quot;:{\u0026quot;unschedulable\u0026quot;:true}}' # 部分更新 node #更新容器的镜像；spec.containers[*].name 是必须的。因为它是一个合并 key。 kubectl patch pod valid-pod -p '{\u0026quot;spec\u0026quot;:{\u0026quot;containers\u0026quot;:[{\u0026quot;name\u0026quot;:\u0026quot;kubernetes-serve-hostname\u0026quot;,\u0026quot;image\u0026quot;:\u0026quot;new image\u0026quot;}]}}' # 使用带位置数组的 json patch 更新容器的镜像 kubectl patch pod valid-pod --type='json' -p='[{\u0026quot;op\u0026quot;: \u0026quot;replace\u0026quot;, \u0026quot;path\u0026quot;: \u0026quot;/spec/containers/0/image\u0026quot;, \u0026quot;value\u0026quot;:\u0026quot;new image\u0026quot;}]' # 使用带位置数组的 json patch 禁用 deployment 的 livenessProbe kubectl patch deployment valid-deployment --type json -p='[{\u0026quot;op\u0026quot;: \u0026quot;remove\u0026quot;, \u0026quot;path\u0026quot;: \u0026quot;/spec/template/spec/containers/0/livenessProbe\u0026quot;}]' # 在带位置数组中添加元素 kubectl patch sa default --type='json' -p='[{\u0026quot;op\u0026quot;: \u0026quot;add\u0026quot;, \u0026quot;path\u0026quot;: \u0026quot;/secrets/1\u0026quot;, \u0026quot;value\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;whatever\u0026quot; } }]' 编辑资源 在编辑器中编辑任何 API 资源\n​```bash kubectl edit svc/docker-registry # 编辑名为 docker-registry 的 service KUBE_EDITOR=\u0026quot;nano\u0026rdquo; kubectl edit svc/docker-registry # 使用其他编辑器\n \u0026lt;!-- ## Scaling Resources --\u0026gt; ## 对资源进行伸缩 \u0026lt;!-- ```bash kubectl scale --replicas=3 rs/foo # Scale a replicaset named 'foo' to 3 kubectl scale --replicas=3 -f foo.yaml # Scale a resource specified in \u0026quot;foo.yaml\u0026quot; to 3 kubectl scale --current-replicas=2 --replicas=3 deployment/mysql # If the deployment named mysql's current size is 2, scale mysql to 3 kubectl scale --replicas=5 rc/foo rc/bar rc/baz # Scale multiple replication controllers ``` --\u0026gt; ​```bash kubectl scale --replicas=3 rs/foo # 将名为 'foo' 的副本集伸缩到 3 副本 kubectl scale --replicas=3 -f foo.yaml # 将在 \u0026quot;foo.yaml\u0026quot; 中的特定资源伸缩到 3 个副本 kubectl scale --current-replicas=2 --replicas=3 deployment/mysql # 如果名为 mysql 的 deployment 的副本当前是 2，那么将它伸缩到 3 kubectl scale --replicas=5 rc/foo rc/bar rc/baz # 伸缩多个 replication controllers 删除资源 ​```bash kubectl delete -f ./pod.json # 删除在 pod.json 中指定的类型和名称的 pod kubectl delete pod,service baz foo # 删除名称为 \u0026ldquo;baz\u0026rdquo; 和 \u0026ldquo;foo\u0026rdquo; 的 pod 和 service kubectl delete pods,services -l name=myLabel # 删除包含 name=myLabel 标签的 pods 和 services kubectl delete pods,services -l name=myLabel \u0026ndash;include-uninitialized # 删除包含 label name=myLabel 标签的 pods 和 services，包括未初始化的 kubectl -n my-ns delete po,svc \u0026ndash;all # 删除在 my-ns 命名空间中全部的 pods 和 services ，包括未初始化的\n删除所有与 pattern1 或 pattern2 匹配的 pod kubectl get pods -n mynamespace \u0026ndash;no-headers=true | awk \u0026lsquo;/pattern1|pattern2/{print $1}\u0026rsquo; | xargs kubectl delete -n mynamespace pod\n \u0026lt;!-- ## Interacting with running Pods --\u0026gt; ## 与运行中的 Pods 进行交互 \u0026lt;!-- ```bash kubectl logs my-pod # dump pod logs (stdout) kubectl logs -l name=myLabel # dump pod logs, with label name=myLabel (stdout) kubectl logs my-pod --previous # dump pod logs (stdout) for a previous instantiation of a container kubectl logs my-pod -c my-container # dump pod container logs (stdout, multi-container case) kubectl logs -l name=myLabel -c my-container # dump pod logs, with label name=myLabel (stdout) kubectl logs my-pod -c my-container --previous # dump pod container logs (stdout, multi-container case) for a previous instantiation of a container kubectl logs -f my-pod # stream pod logs (stdout) kubectl logs -f my-pod -c my-container # stream pod container logs (stdout, multi-container case) kubectl logs -f -l name=myLabel --all-containers # stream all pods logs with label name=myLabel (stdout) kubectl run -i --tty busybox --image=busybox -- sh # Run pod as interactive shell kubectl attach my-pod -i # Attach to Running Container kubectl port-forward my-pod 5000:6000 # Listen on port 5000 on the local machine and forward to port 6000 on my-pod kubectl exec my-pod -- ls / # Run command in existing pod (1 container case) kubectl exec my-pod -c my-container -- ls / # Run command in existing pod (multi-container case) kubectl top pod POD_NAME --containers # Show metrics for a given pod and its containers ``` --\u0026gt; ​```bash kubectl logs my-pod # 获取 pod 日志(标准输出) kubectl logs -l name=myLabel # 获取 pod label name=myLabel 日志(标准输出) kubectl logs my-pod --previous # 获取上个容器实例的 pod 日志(标准输出) kubectl logs my-pod -c my-container # 获取 pod 的容器日志 (标准输出, 多容器的场景) kubectl logs -l name=myLabel -c my-container # 获取 label name=myLabel pod 的容器日志 (标准输出, 多容器的场景) kubectl logs my-pod -c my-container --previous # 获取 pod 的上个容器实例日志 (标准输出, 多容器的场景) kubectl logs -f my-pod # 流式输出 pod 的日志 (标准输出) kubectl logs -f my-pod -c my-container # 流式输出 pod 容器的日志 (标准输出, 多容器的场景) kubectl logs -f -l name=myLabel --all-containers # 流式输出 label name=myLabel pod 的日志 (标准输出) kubectl run -i --tty busybox --image=busybox -- sh # 以交互式 shell 运行 pod kubectl attach my-pod -i # 进入到一个运行中的容器中 kubectl port-forward my-pod 5000:6000 # 在本地计算机上侦听端口 5000 并转发到 my-pod 上的端口 6000 kubectl exec my-pod -- ls / # 在已有的 pod 中运行命令(单容器的场景) kubectl exec my-pod -c my-container -- ls / # 在已有的 pod 中运行命令(多容器的场景) kubectl top pod POD_NAME --containers # 显示给定 pod 和容器的监控数据 与节点和集群进行交互 ​```bash kubectl cordon my-node # 设置 my-node 节点为不可调度 kubectl drain my-node # 对 my-node 节点进行驱逐操作，为节点维护做准备 kubectl uncordon my-node # 设置 my-node 节点为可以调度 kubectl top node my-node # 显示给定 node 的指标 kubectl cluster-info # 显示 master 和 services 的地址 kubectl cluster-info dump # 将当前集群状态输出到标准输出 kubectl cluster-info dump \u0026ndash;output-directory=/path/to/cluster-state # 将当前集群状态输出到 /path/to/cluster-state\n如果已存在具有该键和效果的污点，则其值将按指定替换 kubectl taint nodes foo dedicated=special-user:NoSchedule\n \u0026lt;!-- ### Resource types --\u0026gt; ### 资源类型 \u0026lt;!-- List all supported resource types along with their shortnames, [API group](/docs/concepts/overview/kubernetes-api/#api-groups), whether they are [namespaced](/docs/concepts/overview/working-with-objects/namespaces), and [Kind](/docs/concepts/overview/working-with-objects/kubernetes-objects): --\u0026gt; 列出全部支持的资源类型和它们的简称, [API group](/docs/concepts/overview/kubernetes-api/#api-groups), 无论它们是否是 [namespaced](/docs/concepts/overview/working-with-objects/namespaces), [Kind](/docs/concepts/overview/working-with-objects/kubernetes-objects)。 ```bash kubectl api-resources 用于探索 API 资源的其他操作：\n​```bash kubectl api-resources \u0026ndash;namespaced=true # 所有在命名空间中的资源 kubectl api-resources \u0026ndash;namespaced=false # 所有不在命名空间中的资源 kubectl api-resources -o name # 输出简单的所有资源（只是资源名称） kubectl api-resources -o wide # 具有扩展（又称 \u0026ldquo;wide\u0026rdquo;）输出的所有资源 kubectl api-resources \u0026ndash;verbs=list,get # 支持 \u0026ldquo;list\u0026rdquo; 和 \u0026ldquo;get\u0026rdquo; 请求动词的所有资源 kubectl api-resources \u0026ndash;api-group=extensions # \u0026ldquo;extensions\u0026rdquo; API 组中的所有资源\n \u0026lt;!-- ### Formatting output --\u0026gt; ### 格式化输出 \u0026lt;!-- To output details to your terminal window in a specific format, you can add either the `-o` or `--output` flags to a supported `kubectl` command. --\u0026gt; 要以特定格式将详细信息输出到终端窗口，可以将 `-o` 或 `--output` 参数添加到支持的 `kubectl` 命令。 \u0026lt;!-- Output format | Description --------------| ----------- `-o=custom-columns=\u0026lt;spec\u0026gt;` | Print a table using a comma separated list of custom columns `-o=custom-columns-file=\u0026lt;filename\u0026gt;` | Print a table using the custom columns template in the `\u0026lt;filename\u0026gt;` file `-o=json` | Output a JSON formatted API object `-o=jsonpath=\u0026lt;template\u0026gt;` | Print the fields defined in a [jsonpath](/docs/reference/kubectl/jsonpath) expression `-o=jsonpath-file=\u0026lt;filename\u0026gt;` | Print the fields defined by the [jsonpath](/docs/reference/kubectl/jsonpath) expression in the `\u0026lt;filename\u0026gt;` file `-o=name` | Print only the resource name and nothing else `-o=wide` | Output in the plain-text format with any additional information, and for pods, the node name is included `-o=yaml` | Output a YAML formatted API object --\u0026gt; 输出格式 | 描述 --------------| ----------- `-o=custom-columns=\u0026lt;spec\u0026gt;` | 使用逗号分隔的自定义列列表打印表格 `-o=custom-columns-file=\u0026lt;filename\u0026gt;` | 使用 `\u0026lt;filename\u0026gt;` 文件中的自定义列模板打印表格 `-o=json` | 输出 JSON 格式的 API 对象 `-o=jsonpath=\u0026lt;template\u0026gt;` | 打印 [jsonpath](/docs/reference/kubectl/jsonpath) 表达式中定义的字段 `-o=jsonpath-file=\u0026lt;filename\u0026gt;` | 在 `\u0026lt;filename\u0026gt;` 文件中打印由 [jsonpath](/docs/reference/kubectl/jsonpath) 表达式定义的字段。 `-o=name` | 仅打印资源名称而不打印任何其他内容 `-o=wide` | 使用任何其他信息以纯文本格式输出，对于 pod 来说，包含了节点名称 `-o=yaml` | 输出 YAML 格式的 API 对象 \u0026lt;!-- ### Kubectl output verbosity and debugging --\u0026gt; ### Kubectl 日志输出详细程度和调试 \u0026lt;!-- Kubectl verbosity is controlled with the `-v` or `--v` flags followed by an integer representing the log level. General Kubernetes logging conventions and the associated log levels are described [here](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-instrumentation/logging.md). --\u0026gt; Kubectl 日志输出详细程度是通过 `-v` 或者 `--v` 来控制的，参数后跟了一个数字表示日志的级别。Kubernetes 通用的日志习惯和相关的日志级别在 [这里](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-instrumentation/logging.md) 有相应的描述。 \u0026lt;!-- Verbosity | Description --------------| ----------- `--v=0` | Generally useful for this to *always* be visible to a cluster operator. `--v=1` | A reasonable default log level if you don't want verbosity. `--v=2` | Useful steady state information about the service and important log messages that may correlate to significant changes in the system. This is the recommended default log level for most systems. `--v=3` | Extended information about changes. `--v=4` | Debug level verbosity. `--v=6` | Display requested resources. `--v=7` | Display HTTP request headers. `--v=8` | Display HTTP request contents. `--v=9` | Display HTTP request contents without truncation of contents. --\u0026gt; 详细程度 | 描述 --------------| ----------- `--v=0` | 通常对此有用，*始终*对运维人员可见。 `--v=1` | 如果您不想要详细程度，则为合理的默认日志级别。 `--v=2` | 有关服务的有用稳定状态信息以及可能与系统中的重大更改相关的重要日志消息。这是大多数系统的建议默认日志级别。 `--v=3` | 有关更改的扩展信息。 `--v=4` | Debug 级别。 `--v=6` | 显示请求的资源。 `--v=7` | 显示 HTTP 请求头。 `--v=8` | 显示 HTTP 请求内容。 `--v=9` | 显示 HTTP 请求内容而不截断内容。 ## whatsnext \u0026lt;!-- * Learn more about [Overview of kubectl](/docs/reference/kubectl/overview/). * See [kubectl](/docs/reference/kubectl/kubectl/) options. * Also [kubectl Usage Conventions](/docs/reference/kubectl/conventions/) to understand how to use it in reusable scripts. * See more community [kubectl cheatsheets](https://github.com/dennyzhang/cheatsheet-kubernetes-A4). --\u0026gt; * 学习更多关于 [kubectl 概述](/docs/reference/kubectl/overview/)。 * 查看 [kubectl](/docs/reference/kubectl/kubectl/) 选项. * 也可以查看 [kubectl 使用约定](/docs/reference/kubectl/conventions/) 来理解如果在可以复用的脚本中使用它。 * 查看更多社区 [kubectl 备忘单](https://github.com/dennyzhang/cheatsheet-kubernetes-A4)。 "
},
{
	"uri": "https://lijun.in/reference/kubectl/conventions/",
	"title": "kubectl 的用法约定",
	"tags": [],
	"description": "",
	"content": "kubectl 的推荐用法约定\n在可重用脚本中使用 kubectl 对于脚本中的稳定输出：\n 请求一个面向机器的输出格式，例如 -o name、-o json、-o yaml、-o go template 或 -o jsonpath。 完全限定版本。例如 jobs.v1.batch/myjob。这将确保 kubectl 不会使用其默认版本，该版本会随着时间的推移而更改。 在使用基于生成器的命令（例如 kubectl run 或者 kubectl expose）时，指定 --generator 参数以固定到特定行为。 不要依赖上下文、首选项或其他隐式状态。  最佳实践 kubectl run 若希望 kubectl run 满足基础设施即代码的要求：\n 使用特定版本的标签标记镜像，不要将该标签移动到新版本。例如，使用 :v1234、v1.2.3、r03062016-1-4，而不是 :latest（有关详细信息，请参阅配置的最佳实践)。 固定到特定的生成器版本，例如 kubectl run --generator=run-pod/v1。 使用基于版本控制的脚本来记录所使用的参数，或者至少使用 --record 参数以便为所创建的对象添加注解，在使用轻度参数化的镜像时，记录下所使用的命令行。 使用基于版本控制的脚本来运行包含大量参数的镜像。 对于无法通过 kubectl run 参数来表示的功能特性，使用基于源码控制的配置文件，以记录要使用的功能特性。  生成器 您可以使用带有 --generator 参数的 kubectl run 命令创建如下资源：\ntable caption=\u0026quot;可以使用 kubectl run 创建的资源\u0026rdquo; \u0026gt;}} | 资源 | API 组 | kubectl 命令 | |\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;-|\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash;|\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash; | | Pod | v1 | kubectl run --generator=run-pod/v1 | | ReplicationController (已弃用) | v1 | kubectl run --generator=run/v1 | | Deployment (已弃用) | extensions/v1beta1 | kubectl run --generator=deployment/v1beta1 | | Deployment (已弃用) | apps/v1beta1 | kubectl run --generator=deployment/apps.v1beta1 | | Job (已弃用) | batch/v1 | kubectl run --generator=job/v1 | | CronJob (已弃用) | batch/v2alpha1 | kubectl run --generator=cronjob/v2alpha1 | | CronJob (已弃用) | batch/v1beta1 | kubectl run --generator=cronjob/v1beta1 | /table \u0026gt;}}\nnote \u0026gt;}}\n/note \u0026gt;}}\n如果您显式设置了 --generator 参数，kubectl 将使用您指定的生成器。如果使用 kubectl run 命令但是未指定生成器，kubectl 会根据您设置的其他参数自动选择要使用的生成器。下表列出了如果您自己未指定参数自动使用与之相匹配的生成器：\ntable caption=\u0026quot;kubectl run 参数及其对应的资源\u0026rdquo; \u0026gt;}} | 参数 | 相匹配的资源 | |\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;-|\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash;| | --schedule=\u0026lt;schedule\u0026gt; | CronJob | | --restart=Always | Deployment | | --restart=OnFailure | Job | | --restart=Never | Pod | /table \u0026gt;}}\n如果不指定生成器，kubectl 将按以下顺序考虑其他参数：\n --schedule --restart  您可以使用 --dry-run 参数预览要发送到集群的对象，而无需真正提交。\nkubectl apply  您可以使用 kubectl apply 命令创建或更新资源。有关使用 kubectl apply 更新资源的详细信息，请参阅 Kubectl 文档。  "
},
{
	"uri": "https://lijun.in/reference/command-line-tools-reference/kubelet-authentication-authorization/",
	"title": "Kubelet authentication/authorization",
	"tags": [],
	"description": "",
	"content": ". toc \u0026gt;}}\nOverview A kubelet\u0026rsquo;s HTTPS endpoint exposes APIs which give access to data of varying sensitivity, and allow you to perform operations with varying levels of power on the node and within containers.\nThis document describes how to authenticate and authorize access to the kubelet\u0026rsquo;s HTTPS endpoint.\nKubelet authentication By default, requests to the kubelet\u0026rsquo;s HTTPS endpoint that are not rejected by other configured authentication methods are treated as anonymous requests, and given a username of system:anonymous and a group of system:unauthenticated.\nTo disable anonymous access and send 401 Unauthorized responses to unauthenticated requests:\n start the kubelet with the --anonymous-auth=false flag  To enable X509 client certificate authentication to the kubelet\u0026rsquo;s HTTPS endpoint:\n start the kubelet with the --client-ca-file flag, providing a CA bundle to verify client certificates with start the apiserver with --kubelet-client-certificate and --kubelet-client-key flags see the apiserver authentication documentation for more details  To enable API bearer tokens (including service account tokens) to be used to authenticate to the kubelet\u0026rsquo;s HTTPS endpoint:\n ensure the authentication.k8s.io/v1beta1 API group is enabled in the API server start the kubelet with the --authentication-token-webhook and the --kubeconfig flags the kubelet calls the TokenReview API on the configured API server to determine user information from bearer tokens  Kubelet authorization Any request that is successfully authenticated (including an anonymous request) is then authorized. The default authorization mode is AlwaysAllow, which allows all requests.\nThere are many possible reasons to subdivide access to the kubelet API:\n anonymous auth is enabled, but anonymous users\u0026rsquo; ability to call the kubelet API should be limited bearer token auth is enabled, but arbitrary API users\u0026rsquo; (like service accounts) ability to call the kubelet API should be limited client certificate auth is enabled, but only some of the client certificates signed by the configured CA should be allowed to use the kubelet API  To subdivide access to the kubelet API, delegate authorization to the API server:\n ensure the authorization.k8s.io/v1beta1 API group is enabled in the API server start the kubelet with the --authorization-mode=Webhook and the --kubeconfig flags the kubelet calls the SubjectAccessReview API on the configured API server to determine whether each request is authorized  The kubelet authorizes API requests using the same request attributes approach as the apiserver.\nThe verb is determined from the incoming request\u0026rsquo;s HTTP verb:\n   HTTP verb request verb     POST create   GET, HEAD get   PUT update   PATCH patch   DELETE delete    The resource and subresource is determined from the incoming request\u0026rsquo;s path:\n   Kubelet API resource subresource     /stats/* nodes stats   /metrics/* nodes metrics   /logs/* nodes log   /spec/* nodes spec   all others nodes proxy    The namespace and API group attributes are always an empty string, and the resource name is always the name of the kubelet\u0026rsquo;s Node API object.\nWhen running in this mode, ensure the user identified by the --kubelet-client-certificate and --kubelet-client-key flags passed to the apiserver is authorized for the following attributes:\n verb=*, resource=nodes, subresource=proxy verb=*, resource=nodes, subresource=stats verb=*, resource=nodes, subresource=log verb=*, resource=nodes, subresource=spec verb=*, resource=nodes, subresource=metrics  "
},
{
	"uri": "https://lijun.in/reference/access-authn-authz/controlling-access/",
	"title": "Kubernetes API 访问控制",
	"tags": [],
	"description": "",
	"content": "用户通过 kubectl、客户端库或者通过发送 REST 请求访问 API。 用户（自然人）和 Kubernetes 服务账户 都可以被授权进行 API 访问。 请求到达 API 服务器后会经过几个阶段，具体说明如图：\n传输层安全 在典型的 Kubernetes 集群中，API 通过 443 端口提供服务。 API 服务器会提供一份证书。 该证书一般是自签名的， 所以用户机器上的 $USER/.kube/config 目录通常 包含该 API 服务器证书的根证书，用来代替系统默认根证书。 当用户使用 kube-up.sh 创建集群时，该证书通常会被自动写入用户的 $USER/.kube/config。 如果集群中存在多个用户，则创建者需要与其他用户共享证书。\n认证 一旦 TLS 连接建立，HTTP 请求就进入到了认证的步骤。即图中的步骤 1 。 集群创建脚本或集群管理员会为 API 服务器配置一个或多个认证模块。 更具体的认证相关的描述详见这里。\n认证步骤的输入是整个 HTTP 请求，但这里通常只是检查请求头和 / 或客户端证书。\n认证模块支持客户端证书，密码和 Plain Tokens， Bootstrap Tokens，以及 JWT Tokens（用于服务账户）。\n（管理员）可以同时设置多种认证模块，在设置了多个认证模块的情况下，每个模块会依次尝试认证， 直到其中一个认证成功。\n在 GCE 平台中，客户端证书，密码和 Plain Tokens，Bootstrap Tokens，以及 JWT Tokens 同时被启用。\n如果请求认证失败，则请求被拒绝，返回 401 状态码。 如果认证成功，则被认证为具体的 username，该用户名可供随后的步骤中使用。一些认证模块还提供了用户的组成员关系，另一些则没有。\n尽管 Kubernetes 使用“用户名”来进行访问控制和请求记录，但它实际上并没有 user 对象，也不存储用户名称或其他相关信息。\n授权 当请求被认证为来自某个特定的用户后，该请求需要被授权。 即图中的步骤 2 。\n请求须包含请求者的用户名，请求动作，以及该动作影响的对象。 如果存在相应策略，声明该用户具有进行相应操作的权限，则该请求会被授权。\n例如，如果 Bob 有如下策略，那么他只能够读取 projectCaribou 命名空间下的 pod 资源：\n{ \u0026#34;apiVersion\u0026#34;: \u0026#34;abac.authorization.kubernetes.io/v1beta1\u0026#34;, \u0026#34;kind\u0026#34;: \u0026#34;Policy\u0026#34;, \u0026#34;spec\u0026#34;: { \u0026#34;user\u0026#34;: \u0026#34;bob\u0026#34;, \u0026#34;namespace\u0026#34;: \u0026#34;projectCaribou\u0026#34;, \u0026#34;resource\u0026#34;: \u0026#34;pods\u0026#34;, \u0026#34;readonly\u0026#34;: true } } 如果 Bob 发起以下请求，那么请求能够通过授权，因为 Bob 被允许访问 projectCaribou 命名空间下的对象：\n{ \u0026#34;apiVersion\u0026#34;: \u0026#34;authorization.k8s.io/v1beta1\u0026#34;, \u0026#34;kind\u0026#34;: \u0026#34;SubjectAccessReview\u0026#34;, \u0026#34;spec\u0026#34;: { \u0026#34;resourceAttributes\u0026#34;: { \u0026#34;namespace\u0026#34;: \u0026#34;projectCaribou\u0026#34;, \u0026#34;verb\u0026#34;: \u0026#34;get\u0026#34;, \u0026#34;group\u0026#34;: \u0026#34;unicorn.example.org\u0026#34;, \u0026#34;resource\u0026#34;: \u0026#34;pods\u0026#34; } } } 如果 Bob 对 projectCaribou 命名空间下的对象发起一个写（create 或者 update）请求，那么它的授权会被拒绝。 如果 Bob 请求读取 （get）其他命名空间，例如 projectFish 下的对象，其授权也会被拒绝。\nKubernetes 的授权要求使用通用的 REST 属性与现有的组织或云服务提供商的访问控制系统进行交互。 采用 REST 格式是必要的，因为除 Kubernetes 外，这些访问控制系统还可能与其他的 API 进行交互。\nKubernetes 支持多种授权模块，例如 ABAC 模式，RBAC 模式和 Webhook 模式。 管理员创建集群时，会配置 API 服务器应用的授权模块。 如果多种授权模式同时被启用，Kubernetes 将检查所有模块，如果其中一种通过授权，则请求授权通过。 如果所有的模块全部拒绝，则请求被拒绝（HTTP 状态码 403）。\n要了解更多的 Kubernetes 授权相关信息，包括使用授权模块创建策略的具体说明等，可参考授权概述。\n准入控制 准入控制模块是能够修改或拒绝请求的软件模块。 作为授权模块的补充，准入控制模块会访问被创建或更新的对象的内容。 它们作用于对象的创建，删除，更新和连接（proxy）阶段，但不包括对象的读取。\n可以同时配置多个准入控制器，它们会按顺序依次被调用。\n即图中的步骤 3 。\n与认证和授权模块不同的是，如果任一个准入控制器拒绝请求，那么整个请求会立即被拒绝。\n除了拒绝请求外，准入控制器还可以为对象设置复杂的默认值。\n可用的准入控制模块描述 如下。\n一旦请求通过所有准入控制器，将使用对应 API 对象的验证流程对其进行验证，然后写入对象存储 （如步骤 4）。\nAPI 的端口和 IP 上述讨论适用于发送请求到 API 服务器的安全端口（典型情况）。\n实际上 API 服务器可以通过两个端口提供服务：\n默认情况下，API 服务器在 2 个端口上提供 HTTP 服务：\n  Localhost Port:\n - 用于测试和启动，以及管理节点的其他组件 （scheduler, controller-manager）与 API 的交互 - 没有 TLS - 默认值为 8080，可以通过 `--insecure-port` 标记来修改。 - 默认的 IP 地址为 localhost， 可以通过 `--insecure-bind-address` 标记来修改。 - 请求会 **绕过** 认证和鉴权模块。 - 请求会被准入控制模块处理。 - 其访问需要主机访问的权限。    Secure Port:\n - 尽可能使用该端口访问 - 应用 TLS。 可以通过 `--tls-cert-file` 设置证书， 通过 `--tls-private-key-file` 设置私钥。 - 默认值为 6443，可以通过 `--secure-port` 标记来修改。 - 默认 IP 是首个非本地的网络接口地址，可以通过 `--bind-address` 标记来修改。 - 请求会经过认证和鉴权模块处理。 - 请求会被准入控制模块处理。 - 要求认证和授权模块正常运行。    通过 kube-up.sh 创建集群时， 对 Google Compute Engine（GCE） 和一些其他的云供应商来说， API 通过 443 端口提供服务。 对 GCE 而言，项目上配置了防火墙规则，允许外部的 HTTPS 请求访问 API，其他（厂商的）集群设置方法各不相同。\n"
},
{
	"uri": "https://lijun.in/concepts/cluster-administration/proxies/",
	"title": "Kubernetes 中的代理",
	"tags": [],
	"description": "",
	"content": "本文讲述了 Kubernetes 中所使用的代理。\n代理 用户在使用 Kubernetes 的过程中可能遇到几种不同的代理（proxy）：\n  kubectl proxy：\n 运行在用户的桌面或 pod 中 从本机地址到 Kubernetes apiserver 的代理 客户端到代理使用 HTTP 协议 代理到 apiserver 使用 HTTPS 协议 指向 apiserver 添加认证头信息    apiserver proxy：\n 是一个建立在 apiserver 内部的“堡垒” 将集群外部的用户与群集 IP 相连接，这些IP是无法通过其他方式访问的 运行在 apiserver 进程内 客户端到代理使用 HTTPS 协议 (如果配置 apiserver 使用 HTTP 协议，则使用 HTTP 协议) 通过可用信息进行选择，代理到目的地可能使用 HTTP 或 HTTPS 协议 可以用来访问 Node、 Pod 或 Service 当用来访问 Service 时，会进行负载均衡    kube proxy：\n 在每个节点上运行 代理 UDP 和 TCP 不支持 HTTP 提供负载均衡能力 只用来访问 Service    apiserver 之前的代理/负载均衡器：\n 在不同集群间的存在形式和实现不同 (如 nginx) 位于所有客户端和一个或多个 apiserver 之间 存在多个 apiserver 时，扮演负载均衡器的角色    外部服务的云负载均衡器：\n 由一些云供应商提供 (如AWS ELB、 Google Cloud Load Balancer) Kubernetes service 为 LoadBalancer 类型时自动创建 只使用 UDP/TCP 协议 不同云供应商的实现不同。    Kubernetes 用户通常只需要关心前两种类型的代理，集群管理员通常需要确保后面几种类型的代理设置正确。\n请求重定向 代理已经取代重定向功能，重定向已被弃用。\n"
},
{
	"uri": "https://lijun.in/tasks/administer-cluster/running-cloud-controller/",
	"title": "Kubernetes 云管理控制器",
	"tags": [],
	"description": "",
	"content": ". feature-state state=\u0026quot;beta\u0026rdquo; \u0026gt;}}\nKubernetes v1.6 包含一个新的二进制文件，叫做 cloud-controller-manager。cloud-controller-manager 是一个嵌入了特定云服务控制循环逻辑的守护进程。这些特定云服务控制循环逻辑最初存在于 kube-controller-manager 中。由于云服务提供商开发和发布的速度与 Kubernetes 项目不同，将服务提供商专用代码从 cloud-controller-manager 二进制中抽象出来有助于云服务厂商在 Kubernetes 核心代码之外独立进行开发。\ncloud-controller-manager 可以被链接到任何满足 cloudprovider.Interface 约束的云服务提供商。为了兼容旧版本，Kubernetes 核心项目中提供的 cloud-controller-manager 使用和 kube-controller-manager 相同的云服务类库。已经在 Kubernetes 核心项目中支持的云服务提供商预计将通过使用 in-tree 的 cloud-controller-manager 过渡到 Kubernetes 核心之外。在将来的 Kubernetes 发布中，所有的云管理控制器将在 Kubernetes 核心项目之外，由 sig 领导者或者云服务厂商进行开发。\n管理 需求 每个云服务都有一套各自的需求用于系统平台的集成，这不应与运行 kube-controller-manager 的需求有太大差异。作为经验法则，你需要：\n 云服务认证 / 授权：您的云服务可能需要使用令牌或者 IAM 规则以允许对其 API 的访问 kubernetes 认证 / 授权：cloud-controller-manager 可能需要 RBAC 规则以访问 kubernetes apiserver 高可用：类似于 kube-controller-manager，您可能希望通过主节点选举（默认开启）配置一个高可用的云管理控制器。  运行云管理控制器 您需要对集群配置做适当的修改以成功地运行云管理控制器：\n 一定不要为 kube-apiserver 和 kube-controller-manager 指定 --cloud-provider 标志。这将保证它们不会运行任何云服务专用循环逻辑，这将会由云管理控制器运行。未来这个标记将被废弃并去除。 kubelet 必须使用 --cloud-provider=external 运行。这是为了保证让 kubelet 知道在执行任何任务前，它必须被云管理控制器初始化。  请记住，设置群集使用云管理控制器将用多种方式更改群集行为：\n 指定了 --cloud-provider=external 的 kubelet 将被添加一个 node.cloudprovider.kubernetes.io/uninitialized 的污点，导致其在初始化过程中不可调度（NoSchedule）。这将标记该节点在能够正常调度前，需要外部的控制器进行二次初始化。请注意，如果云管理控制器不可用，集群中的新节点会一直处于不可调度的状态。这个污点很重要，因为调度器可能需要关于节点的云服务特定的信息，比如他们的区域或类型（high cpu, gpu, high memory, spot instance 等）。 集群中节点的云服务信息将不再能够从本地元数据中获取，取而代之的是所有获取节点信息的 API 调用都将通过云管理控制器。这意味着你可以通过限制到 kubelet 云服务 API 的访问来提升安全性。在更大的集群中您可能需要考虑云管理控制器是否会遇到速率限制，因为它现在负责集群中几乎所有到云服务的 API 调用。  对于 v1.8 版本，云管理控制器可以实现：\n node 控制器 - 负责使用云服务 API 更新 kubernetes 节点并删除在云服务上已经删除的 kubernetes 节点。 service 控制器 - 负责在云服务上为类型为 LoadBalancer 的 service 提供负载均衡器。 route 控制器 - 负责在云服务上配置网络路由。 如果您使用的是 out-of-tree 提供商，请按需实现其余任意特性。  示例 如果当前 Kubernetes 内核支持您使用的云服务，并且想要采用云管理控制器，请参见 kubernetes 内核中的云管理控制器。\n对于不在 Kubernetes 内核中的云管理控制器，您可以在云服务厂商或 sig 领导者的源中找到对应的项目。\n DigitalOcean keepalived Oracle Cloud Infrastructure Rancher  对于已经存在于 Kubernetes 内核中的提供商，您可以在集群中将 in-tree 云管理控制器作为守护进程运行。请使用如下指南：\n. codenew file=\u0026quot;admin/cloud/ccm-example.yaml\u0026rdquo; \u0026gt;}}\n限制 运行云管理控制器会有一些可能的限制。虽然以后的版本将处理这些限制，但是知道这些生产负载的限制很重要。\n对 Volume 的支持 云管理控制器未实现 kube-controller-manager 中的任何 volume 控制器，因为和 volume 的集成还需要与 kubelet 协作。由于我们引入了 CSI (容器存储接口，container storage interface) 并对弹性 volume 插件添加了更强大的支持，云管理控制器将添加必要的支持，以使云服务同 volume 更好的集成。请在 这里 了解更多关于 out-of-tree CSI volume 插件的信息。\n可扩展性 在以前为云服务提供商提供的架构中，我们依赖 kubelet 的本地元数据服务来获取关于它本身的节点信息。通过这个新的架构，现在我们完全依赖云管理控制器来获取所有节点的信息。对于非常大的集群，您需要考虑可能的瓶颈，例如资源需求和 API 速率限制。\n鸡和蛋的问题 云管理控制器的目标是将云服务特性的开发从 Kubernetes 核心项目中解耦。不幸的是，Kubernetes 项目的许多方面都假设云服务提供商的特性同项目紧密结合。因此，这种新架构的采用可能导致某些场景下，当一个请求需要从云服务提供商获取信息时，在该请求没有完成的情况下云管理控制器不能返回那些信息。\nKubelet 中的 TLS 引导特性是一个很好的例子。目前，TLS 引导认为 Kubelet 有能力从云提供商（或本地元数据服务）获取所有的地址类型（私有、公用等），但在被初始化之前，云管理控制器不能设置节点地址类型，而这需要 kubelet 拥有 TLS 证书以和 apiserver 通信。\n随着这种措施的演进，将来的发行版中将作出改变来解决这些问题。\n开发自己的云管理控制器 要构建和开发您自己的云管理控制器，请阅读 开发云管理控制器 文档。\n"
},
{
	"uri": "https://lijun.in/tutorials/online-training/overview/",
	"title": "Kubernetes 在线培训概述",
	"tags": [],
	"description": "",
	"content": "以下是提供 Kubernetes 在线培训的一些网站:\n [AIOps Essentials (使用 Prometheus 度量标准自动缩放 Kubernetes) 和动手实验室 (Linux 学院)] (https://linuxacademy.com/devops/training/course/name/using-machine-learning-to-scale-kubernetes-clusters)   [带有动手实验室的 Amazon EKS 深入研究(Linux 学院)] (https://linuxacademy.com/amazon-web-services/training/course/name/amazon-eks-deep-dive)   [具有动手实验和实践考试的 Cloud Native 认证 Kubernetes 管理员 (CKA)(Linux 学院)] (https://linuxacademy.com/linux/training/course/name/cloud-native-certified-kubernetes-administrator-cka)   带有实践测试的认证 Kubernetes 管理员准备课程 (KodeKloud)   [经过实践实验室和实践考试认证的 Kubernetes 应用程序开发人员(CKAD) (Linux 学院)] (https://linuxacademy.com/containers/training/course/name/certified-kubernetes-application-developer-ckad/)   经过认证的带有实践测试的 Kubernetes 应用程序开发人员准备课程 (KodeKloud)   Google Kubernetes 引擎入门(Coursera)   [Kubernetes 入门 (Pluralsight)] (https://www.pluralsight.com/courses/getting-started-kubernetes)   OCI Oracle Kubernetes 引擎(OKE)上的 Kubernetes集群入门 (学习库)   [Google Kubernetes 引擎深度学习 (Linux 学院)] (https://linuxacademy.com/google-cloud-platform/training/course/name/google-kubernetes-engine-deep-dive)   [通过动手实验室进行深度学习(Linux 学院)] (https://linuxacademy.com/linux/training/course/name/helm-deep-dive-part-1)   [Kubernetes 的动手入门 (Instruqt)] (https://play.instruqt.com/public/topics/getting-started-with-kubernetes)   IBM Cloud: 使用 Kubernetes 部署微服务 (Coursera)   Kubernetes (edX) 简介   [带动手实验室的 Kubernetes Essentials (Linux 学院)] (https://linuxacademy.com/linux/training/course/name/kubernetes-essentials)   通过动手实验室为绝对初学者提供 Kubernetes (KodeKloud)   Kubernetes 基础 (LFS258) (Linux 基金会)   [Kubernetes 动手实验室快速入门 (Linux 学院)] (https://linuxacademy.com/linux/training/course/name/kubernetes-quick-start)   Kubernetes 通过动手实验室解决难题 (Linux 学院)   [通过动手实验室实现 Kubernetes 安全性(Linux 学院)] (https://linuxacademy.com/linux/training/course/name/kubernetes-security)   [通过动手实验室启动您的第一个 OpenShift 操作 (Linux 学院)] (https://linuxacademy.com/containers/training/course/name/red-hat-open-shift)   [边干边学 Kubernetes - 100% 动手体验 (Linux 学院)] (https://linuxacademy.com/linux/training/course/name/learn-kubernetes-by-doing)   使用交互式动手场景学习 Kubernetes (Katacoda)   [Kubernetes中的微服务应用程序 - 100% 动手体验 (Linux Academy)] (https://linuxacademy.com/devops/training/course/name/learn-microservices-by-doing)   [通过动手实验室使用 Prometheus 监控 Kubernetes (Linux 学院)] (https://linuxacademy.com/linux/training/course/name/kubernetes-and-prometheus)   [带动手实验室的 Istio 服务网络 (Linux 学院)] (https://linuxacademy.com/linux/training/course/name/service-mesh-with-istio-part-1)   [使用 Kubernetes 的可扩展微服务(Udacity)] (https://www.udacity.com/course/scalable-microservices-with-kubernetes--ud615)   [自定进度的 Kubernetes 在线课程 (Learnk8s 学院)] (https://learnk8s.io/academy)  "
},
{
	"uri": "https://lijun.in/concepts/architecture/master-node-communication/",
	"title": "Master 节点通信",
	"tags": [],
	"description": "",
	"content": "概览 本文对 Master 节点（确切说是 apiserver）和 Kubernetes 集群之间的通信路径进行了分类。目的是为了让用户能够自定义他们的安装，对网络配置进行加固，使得集群能够在不可信的网络上（或者在一个云服务商完全公共的 IP 上）运行。\nCluster -\u0026gt; Master 所有从集群到 master 的通信路径都终止于 apiserver（其它 master 组件没有被设计为可暴露远程服务）。在一个典型的部署中，apiserver 被配置为在一个安全的 HTTPS 端口（443）上监听远程连接并启用一种或多种形式的客户端身份认证机制。一种或多种客户端身份认证机制应该被启用，特别是在允许使用 匿名请求 或 service account tokens 的时候。\n应该使用集群的公共根证书开通节点，如此它们就能够基于有效的客户端凭据安全的连接 apiserver。例如：在一个默认的 GCE 部署中，客户端凭据以客户端证书的形式提供给 kubelet。请查看 kubelet TLS bootstrapping 获取如何自动提供 kubelet 客户端证书。\n想要连接到 apiserver 的 Pods 可以使用一个 service account 安全的进行连接。这种情况下，当 Pods 被实例化时 Kubernetes 将自动的把公共根证书和一个有效的不记名令牌注入到 pod 里。kubernetes service （所有 namespaces 中）都配置了一个虚拟 IP 地址，用于转发（通过 kube-proxy）请求到 apiserver 的 HTTPS endpoint。\nMaster 组件通过非安全（没有加密或认证）端口和集群的 apiserver 通信。这个端口通常只在 master 节点的 localhost 接口暴露，这样，所有在相同机器上运行的 master 组件就能和集群的 apiserver 通信。一段时间以后，master 组件将变为使用带身份认证和权限验证的安全端口（查看#13598）。\n这样的结果使得从集群（在节点上运行的 nodes 和 pods）到 master 的缺省连接操作模式默认被保护，能够在不可信或公网中运行。\nMaster -\u0026gt; Cluster 从 master（apiserver）到集群有两种主要的通信路径。第一种是从 apiserver 到集群中每个节点上运行的 kubelet 进程。第二种是从 apiserver 通过它的代理功能到任何 node、pod 或者 service。\napiserver -\u0026gt; kubelet 从 apiserver 到 kubelet 的连接用于获取 pods 日志、连接（通过 kubectl）运行中的 pods，以及使用 kubelet 的端口转发功能。这些连接终止于 kubelet 的 HTTPS endpoint。\n默认的，apiserver 不会验证 kubelet 的服务证书，这会导致连接遭到中间人攻击，因而在不可信或公共网络上是不安全的。\n为了对这个连接进行认证，请使用 --kubelet-certificate-authority 标记给 apiserver 提供一个根证书捆绑，用于 kubelet 的服务证书。\n如果这样不可能，又要求避免在不可信的或公共的网络上进行连接，请在 apiserver 和 kubelet 之间使用 SSH 隧道。\n最后，应该启用 Kubelet 用户认证和/或权限认证来保护 kubelet API。\napiserver -\u0026gt; nodes, pods, and services 从 apiserver 到 node、pod 或者 service 的连接默认为纯 HTTP 方式，因此既没有认证，也没有加密。他们能够通过给 API URL 中的 node、pod 或 service 名称添加前缀 https: 来运行在安全的 HTTPS 连接上。但他们即不会认证 HTTPS endpoint 提供的证书，也不会提供客户端证书。这样虽然连接是加密的，但它不会提供任何完整性保证。这些连接目前还不能安全的在不可信的或公共的网络上运行。\nSSH 隧道 Google Kubernetes Engine 使用 SSH 隧道保护 Master -\u0026gt; Cluster 通信路径。在这种配置下，apiserver 发起一个到集群中每个节点的 SSH 隧道（连接到在 22 端口监听的 ssh 服务）并通过这个隧道传输所有到 kubelet、node、pod 或者 service 的流量。这个隧道保证流量不会在集群运行的私有 GCE 网络之外暴露。\n"
},
{
	"uri": "https://lijun.in/setup/production-environment/on-premises-vm/ovirt/",
	"title": "oVirt",
	"tags": [],
	"description": "",
	"content": "oVirt 是一个虚拟数据中心管理器，可以对多个主机上的多个虚拟机进行强大的管理。 使用 KVM 和 libvirt ，可以将 oVirt 安装在 Fedora、CentOS 或者 Red Hat Enterprise Linux 主机上，以部署和管理您的虚拟数据中心。\noVirt 云驱动的部署 oVirt 云驱动可以轻松发现新 VM 实例并自动将其添加为 Kubernetes 集群的节点。 目前，包括 Kubernetes 在内，尚无社区支持或预加载的 VM 镜像，但可以在 VM 中 导入 或 安装 Project Atomic（或 Fedora）来 生成模版。 包括 Kubernetes 的任何其他 Linux 发行版也可能可行。\n必须在寄宿系统中 安装 ovirt-guest-agent，才能将 VM 的 IP 地址和主机名报给 ovirt-engine 并最终报告给 Kubernetes。\n一旦 Kubernetes 模版可用，就可以开始创建可由云驱动发现的 VM。\n使用 oVirt 云驱动 oVirt 云驱动需要访问 oVirt REST-API 来收集正确的信息，所需的凭据应在 ovirt-cloud.conf 文件中设定：\n[connection] uri = https://localhost:8443/ovirt-engine/api username = admin@internal password = admin 在同一文件中，可以指定（使用 filters 节区）搜索查询，用于辨识要报告给 Kubernetes 的 VM：\n[filters] # Search query used to find nodes vms = tag=kubernetes 在上面的示例中，所有带有 kubernetes 标签的虚拟机都将作为节点报告给 Kubernetes。\n然后必须向 kube-controller-manager 提供 ovirt-cloud.conf 文件：\nkube-controller-manager ... --cloud-provider=ovirt --cloud-config=/path/to/ovirt-cloud.conf ... oVirt 云驱动截屏视频 这段简短的截屏视频演示了如何使用 oVirt 云提供商将 VM 动态添加到 Kubernetes 集群。\n\n支持级别    IaaS 提供商 配置管理 OS 联网 文件 遵从性 支持级别     oVirt    文件  社区 (@simon3z)    "
},
{
	"uri": "https://lijun.in/concepts/policy/pod-security-policy/",
	"title": "Pod 安全策略",
	"tags": [],
	"description": "",
	"content": "PodSecurityPolicy 类型的对象能够控制，是否可以向 Pod 发送请求，该 Pod 能够影响被应用到 Pod 和容器的 SecurityContext。 查看 Pod 安全策略建议 获取更多信息。\n什么是 Pod 安全策略？ Pod 安全策略 是集群级别的资源，它能够控制 Pod 运行的行为，以及它具有访问什么的能力。 PodSecurityPolicy 对象定义了一组条件，指示 Pod 必须按系统所能接受的顺序运行。 它们允许管理员控制如下方面：\n   控制面 字段名称     已授权容器的运行 privileged   为容器添加默认的一组能力 defaultAddCapabilities   为容器去掉某些能力 requiredDropCapabilities   容器能够请求添加某些能力 allowedCapabilities   控制卷类型的使用 volumes   主机网络的使用 hostNetwork   主机端口的使用 hostPorts   主机 PID namespace 的使用 hostPID   主机 IPC namespace 的使用 hostIPC   主机路径的使用 allowedHostPaths   容器的 SELinux 上下文 seLinux   用户 ID runAsUser   配置允许的补充组 supplementalGroups   分配拥有 Pod 数据卷的 FSGroup fsGroup   必须使用一个只读的 root 文件系统 readOnlyRootFilesystem    Pod 安全策略 由设置和策略组成，它们能够控制 Pod 访问的安全特征。这些设置分为如下三类：\n 基于布尔值控制 ：这种类型的字段默认为最严格限制的值。 基于被允许的值集合控制 ：这种类型的字段会与这组值进行对比，以确认值被允许。 基于策略控制 ：设置项通过一种策略提供的机制来生成该值，这种机制能够确保指定的值落在被允许的这组值中。  RunAsUser  MustRunAs - 必须配置一个 range。使用该范围内的第一个值作为默认值。验证是否不在配置的该范围内。 MustRunAsNonRoot - 要求提交的 Pod 具有非零 runAsUser 值，或在镜像中定义了 USER 环境变量。不提供默认值。 RunAsAny - 没有提供默认值。允许指定任何 runAsUser 。  SELinux  MustRunAs - 如果没有使用预分配的值，必须配置 seLinuxOptions。默认使用 seLinuxOptions。验证 seLinuxOptions。 RunAsAny - 没有提供默认值。允许任意指定的 seLinuxOptions ID。  SupplementalGroups  MustRunAs - 至少需要指定一个范围。默认使用第一个范围的最小值。验证所有范围的值。 RunAsAny - 没有提供默认值。允许任意指定的 supplementalGroups ID。  FSGroup  MustRunAs - 至少需要指定一个范围。默认使用第一个范围的最小值。验证在第一个范围内的第一个 ID。 RunAsAny - 没有提供默认值。允许任意指定的 fsGroup ID。  控制卷 通过设置 PSP 卷字段，能够控制具体卷类型的使用。当创建一个卷的时候，与该字段相关的已定义卷可以允许设置如下值：\n azureFile azureDisk flocker flexVolume hostPath emptyDir gcePersistentDisk awsElasticBlockStore gitRepo secret nfs iscsi glusterfs persistentVolumeClaim rbd cinder cephFS downwardAPI fc configMap vsphereVolume quobyte projected portworxVolume scaleIO storageos * (allow all volumes)  对新的 PSP，推荐允许的卷的最小集合包括：configMap、downwardAPI、emptyDir、persistentVolumeClaim、secret 和 projected。\n主机网络  HostPorts ， 默认为 empty。HostPortRange 列表通过 min(包含) and max(包含) 来定义，指定了被允许的主机端口。  允许的主机路径  AllowedHostPaths 是一个被允许的主机路径前缀的白名单。空值表示所有的主机路径都可以使用。  许可 包含 PodSecurityPolicy 的 许可控制，允许控制集群资源的创建和修改，基于这些资源在集群范围内被许可的能力。\n许可使用如下的方式为 Pod 创建最终的安全上下文：\n 检索所有可用的 PSP。 生成在请求中没有指定的安全上下文设置的字段值。 基于可用的策略，验证最终的设置。  如果某个策略能够匹配上，该 Pod 就被接受。如果请求与 PSP 不匹配，则 Pod 被拒绝。\nPod 必须基于 PSP 验证每个字段。\n创建一个策略和一个 Pod 在一个文件中定义 PodSecurityPolicy 对象实例。这里的策略只是用来禁止创建有特权 要求的 Pods。\nfile=\u0026quot;policy/example-psp.yaml\u0026rdquo; \u0026gt;}}\n使用 kubectl 执行创建操作：\nkubectl-admin create -f example-psp.yaml 获取 Pod 安全策略列表 获取已存在策略列表，使用 kubectl get：\n$ kubectl get psp NAME PRIV CAPS SELINUX RUNASUSER FSGROUP SUPGROUP READONLYROOTFS VOLUMES permissive false [] RunAsAny RunAsAny RunAsAny RunAsAny false [*] privileged true [] RunAsAny RunAsAny RunAsAny RunAsAny false [*] restricted false [] RunAsAny MustRunAsNonRoot RunAsAny RunAsAny false [emptyDir secret downwardAPI configMap persistentVolumeClaim projected] 修改 Pod 安全策略 通过交互方式修改策略，使用 kubectl edit：\n$ kubectl edit psp permissive 该命令将打开一个默认文本编辑器，在这里能够修改策略。\n删除 Pod 安全策略 一旦不再需要一个策略，很容易通过 kubectl 删除它：\n$ kubectl delete psp permissive podsecuritypolicy \u0026#34;permissive\u0026#34; deleted 启用 Pod 安全策略 为了能够在集群中使用 Pod 安全策略，必须确保满足如下条件：\n 已经启用 API 类型 extensions/v1beta1/podsecuritypolicy（仅对 1.6 之前的版本） 已经启用许可控制器 PodSecurityPolicy 已经定义了自己的策略  使用 RBAC 在 Kubernetes 1.5 或更新版本，可以使用 PodSecurityPolicy 来控制，对基于用户角色和组的已授权容器的访问。访问不同的 PodSecurityPolicy 对象，可以基于认证来控制。基于 Deployment、ReplicaSet 等创建的 Pod，限制访问 PodSecurityPolicy 对象，Controller Manager 必须基于安全 API 端口运行，并且不能够具有超级用户权限。\nPodSecurityPolicy 认证使用所有可用的策略，包括创建 Pod 的用户，Pod 上指定的服务账户（Service Account）。当 Pod 基于 Deployment、ReplicaSet 创建时，它是创建 Pod 的 Controller Manager，所以如果基于非安全 API 端口运行，允许所有的 PodSecurityPolicy 对象，并且不能够有效地实现细分权限。用户访问给定的 PSP 策略有效，仅当是直接部署 Pod 的情况。更多详情，查看 PodSecurityPolicy RBAC 示例，当直接部署 Pod 时，应用 PodSecurityPolicy 控制基于角色和组的已授权容器的访问 。\n"
},
{
	"uri": "https://lijun.in/concepts/workloads/pods/pod-lifecycle/",
	"title": "Pod 的生命周期",
	"tags": [],
	"description": "",
	"content": "该页面将描述 Pod 的生命周期。\nPod phase Pod 的 status 定义在 PodStatus 对象中，其中有一个 phase 字段。\nPod 的运行阶段（phase）是 Pod 在其生命周期中的简单宏观概述。该阶段并不是对容器或 Pod 的综合汇总，也不是为了做为综合状态机。\nPod 相位的数量和含义是严格指定的。除了本文档中列举的内容外，不应该再假定 Pod 有其他的 phase 值。\n下面是 phase 可能的值：\n 挂起（Pending）：Pod 已被 Kubernetes 系统接受，但有一个或者多个容器镜像尚未创建。等待时间包括调度 Pod 的时间和通过网络下载镜像的时间，这可能需要花点时间。 运行中（Running）：该 Pod 已经绑定到了一个节点上，Pod 中所有的容器都已被创建。至少有一个容器正在运行，或者正处于启动或重启状态。 成功（Succeeded）：Pod 中的所有容器都被成功终止，并且不会再重启。 失败（Failed）：Pod 中的所有容器都已终止了，并且至少有一个容器是因为失败终止。也就是说，容器以非0状态退出或者被系统终止。 未知（Unknown）：因为某些原因无法取得 Pod 的状态，通常是因为与 Pod 所在主机通信失败。  Pod 状态 Pod 有一个 PodStatus 对象，其中包含一个 PodCondition 数组。 PodCondition 数组的每个元素都有一个 type 字段和一个 status 字段。type 字段是字符串，可能的值有 PodScheduled、Ready、Initialized 和 Unschedulable。status 字段是一个字符串，可能的值有 True、False 和 Unknown。\n容器探针 探针 是由 kubelet 对容器执行的定期诊断。要执行诊断，kubelet 调用由容器实现的 Handler。有三种类型的处理程序：\n ExecAction：在容器内执行指定命令。如果命令退出时返回码为 0 则认为诊断成功。 TCPSocketAction：对指定端口上的容器的 IP 地址进行 TCP 检查。如果端口打开，则诊断被认为是成功的。 HTTPGetAction：对指定的端口和路径上的容器的 IP 地址执行 HTTP Get 请求。如果响应的状态码大于等于200 且小于 400，则诊断被认为是成功的。  每次探测都将获得以下三种结果之一：\n 成功：容器通过了诊断。 失败：容器未通过诊断。 未知：诊断失败，因此不会采取任何行动。  Kubelet 可以选择是否执行在容器上运行的三种探针执行和做出反应：\n livenessProbe：指示容器是否正在运行。如果存活探测失败，则 kubelet 会杀死容器，并且容器将受到其 重启策略 的影响。如果容器不提供存活探针，则默认状态为 Success。 readinessProbe：指示容器是否准备好服务请求。如果就绪探测失败，端点控制器将从与 Pod 匹配的所有 Service 的端点中删除该 Pod 的 IP 地址。初始延迟之前的就绪状态默认为 Failure。如果容器不提供就绪探针，则默认状态为 Success。 startupProbe: 指示容器中的应用是否已经启动。如果提供了启动探测(startup probe)，则禁用所有其他探测，直到它成功为止。如果启动探测失败，kubelet 将杀死容器，容器服从其重启策略进行重启。如果容器没有提供启动探测，则默认状态为成功Success。  该什么时候使用存活（liveness）和就绪（readiness）探针? 如果容器中的进程能够在遇到问题或不健康的情况下自行崩溃，则不一定需要存活探针; kubelet 将根据 Pod 的restartPolicy 自动执行正确的操作。\n如果您希望容器在探测失败时被杀死并重新启动，那么请指定一个存活探针，并指定restartPolicy 为 Always 或 OnFailure。\n如果要仅在探测成功时才开始向 Pod 发送流量，请指定就绪探针。在这种情况下，就绪探针可能与存活探针相同，但是 spec 中的就绪探针的存在意味着 Pod 将在没有接收到任何流量的情况下启动，并且只有在探针探测成功后才开始接收流量。\n如果您希望容器能够自行维护，您可以指定一个就绪探针，该探针检查与存活探针不同的端点。\n请注意，如果您只想在 Pod 被删除时能够排除请求，则不一定需要使用就绪探针；在删除 Pod 时，Pod 会自动将自身置于未完成状态，无论就绪探针是否存在。当等待 Pod 中的容器停止时，Pod 仍处于未完成状态。\nPod 和容器状态 有关 Pod 容器状态的详细信息，请参阅 PodStatus 和 ContainerStatus。请注意，报告的 Pod 状态信息取决于当前的 ContainerState。\n重启策略 PodSpec 中有一个 restartPolicy 字段，可能的值为 Always、OnFailure 和 Never。默认为 Always。 restartPolicy 适用于 Pod 中的所有容器。restartPolicy 仅指通过同一节点上的 kubelet 重新启动容器。失败的容器由 kubelet 以五分钟为上限的指数退避延迟（10秒，20秒，40秒\u0026hellip;）重新启动，并在成功执行十分钟后重置。如 Pod 文档 中所述，一旦绑定到一个节点，Pod 将永远不会重新绑定到另一个节点。\nPod 的生命 一般来说，Pod 不会消失，直到人为销毁他们。这可能是一个人或控制器。这个规则的唯一例外是成功或失败的 phase 超过一段时间（由 master 确定）的Pod将过期并被自动销毁。\n有三种可用的控制器：\n  使用 Job 运行预期会终止的 Pod，例如批量计算。Job 仅适用于重启策略为 OnFailure 或 Never 的 Pod。\n  对预期不会终止的 Pod 使用 ReplicationController、ReplicaSet 和 Deployment ，例如 Web 服务器。 ReplicationController 仅适用于具有 restartPolicy 为 Always 的 Pod。\n  提供特定于机器的系统服务，使用 DaemonSet 为每台机器运行一个 Pod 。\n  所有这三种类型的控制器都包含一个 PodTemplate。建议创建适当的控制器，让它们来创建 Pod，而不是直接自己创建 Pod。这是因为单独的 Pod 在机器故障的情况下没有办法自动复原，而控制器却可以。\n如果节点死亡或与集群的其余部分断开连接，则 Kubernetes 将应用一个策略将丢失节点上的所有 Pod 的 phase 设置为 Failed。\n示例 高级 liveness 探针示例 存活探针由 kubelet 来执行，因此所有的请求都在 kubelet 的网络命名空间中进行。\napiVersion: v1 kind: Pod metadata: labels: test: liveness name: liveness-http spec: containers: - args: - /server image: k8s.gcr.io/liveness livenessProbe: httpGet: # 当没有定义 \u0026#34;host\u0026#34; 时，使用 \u0026#34;PodIP\u0026#34; # host: my-host # 当没有定义 \u0026#34;scheme\u0026#34; 时，使用 \u0026#34;HTTP\u0026#34; scheme 只允许 \u0026#34;HTTP\u0026#34; 和 \u0026#34;HTTPS\u0026#34; # scheme: HTTPS path: /healthz port: 8080 httpHeaders: - name: X-Custom-Header value: Awesome initialDelaySeconds: 15 timeoutSeconds: 1 name: liveness 状态示例  Pod 中只有一个容器并且正在运行。容器成功退出。  记录完成事件。 如果 restartPolicy 为：  Always：重启容器；Pod phase 仍为 Running。 OnFailure：Pod phase 变成 Succeeded。 Never：Pod phase 变成 Succeeded。     Pod 中只有一个容器并且正在运行。容器退出失败。  记录失败事件。 如果 restartPolicy 为：  Always：重启容器；Pod phase 仍为 Running。 OnFailure：重启容器；Pod phase 仍为 Running。 Never：Pod phase 变成 Failed。     Pod 中有两个容器并且正在运行。有一个容器退出失败。  记录失败事件。 如果 restartPolicy 为：  Always：重启容器；Pod phase 仍为 Running。 OnFailure：重启容器；Pod phase 仍为 Running。 Never：不重启容器；Pod phase 仍为 Running。   如果有一个容器没有处于运行状态，并且两个容器退出：  记录失败事件。 如果 restartPolicy 为：  Always：重启容器；Pod phase 仍为 Running。 OnFailure：重启容器；Pod phase 仍为 Running。 Never：Pod phase 变成 Failed。       Pod 中只有一个容器并处于运行状态。容器运行时内存超出限制：  容器以失败状态终止。 记录 OOM 事件。 如果 restartPolicy 为：  Always：重启容器；Pod phase 仍为 Running。 OnFailure：重启容器；Pod phase 仍为 Running。 Never: 记录失败事件；Pod phase 仍为 Failed。     Pod 正在运行，磁盘故障：  杀掉所有容器。 记录适当事件。 Pod phase 变成 Failed。 如果使用控制器来运行，Pod 将在别处重建。   Pod 正在运行，其节点被分段。  节点控制器等待直到超时。 节点控制器将 Pod phase 设置为 Failed。 如果是用控制器来运行，Pod 将在别处重建。    "
},
{
	"uri": "https://lijun.in/showcase/",
	"title": "Showcase",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lijun.in/tasks/debug-application-cluster/events-stackdriver/",
	"title": "StackDriver 中的事件",
	"tags": [],
	"description": "",
	"content": "Kubernetes 事件是一种对象，它为用户提供了洞察集群内发生的事情的能力，例如调度程序做出了什么决定，或者为什么某些 Pod 被逐出节点。 您可以在应用程序自检和调试中阅读有关使用事件调试应用程序的更多信息。\n因为事件是 API 对象，所以它们存储在主节点上的 apiserver 中。 为了避免主节点磁盘空间被填满，将强制执行保留策略：事件在最后一次发生的一小时后将会被删除。 为了提供更长的历史记录和聚合能力，应该安装第三方解决方案来捕获事件。\n本文描述了一个将 Kubernetes 事件导出为 Stackdriver Logging 的解决方案，在这里可以对它们进行处理和分析。\n. note \u0026gt;}}\n不能保证集群中发生的所有事件都将导出到 Stackdriver。 事件不能导出的一种可能情况是事件导出器没有运行（例如，在重新启动或升级期间）。 在大多数情况下，可以将事件用于设置 metrics 和 alerts 等目的，但您应该注意潜在的不准确性。 . /note \u0026gt;}}\n部署 Google Kubernetes Engine 在 Google Kubernetes Engine 中，如果启用了云日志，那么事件导出器默认部署在主节点运行版本为 1.7 及更高版本的集群中。 为了防止干扰您的工作负载，事件导出器没有设置资源，并且处于尽力而为的 QoS 类型中，这意味着它将在资源匮乏的情况下第一个被杀死。 如果要导出事件，请确保有足够的资源给事件导出器 Pod 使用。 这可能会因为工作负载的不同而有所不同，但平均而言，需要大约 100MB 的内存和 100m 的 CPU。\n部署到现有集群 使用下面的命令将事件导出器部署到您的集群：\nkubectl create -f https://k8s.io/examples/debug/event-exporter.yaml 由于事件导出器访问 Kubernetes API，因此它需要权限才能访问。 以下的部署配置为使用 RBAC 授权。 它设置服务帐户和集群角色绑定，以允许事件导出器读取事件。 为了确保事件导出器 Pod 不会从节点中退出，您可以另外设置资源请求。 如前所述，100MB 内存和 100m CPU 应该就足够了。\n. codenew file=\u0026quot;debug/event-exporter.yaml\u0026rdquo; \u0026gt;}}\n用户指南 事件在 Stackdriver Logging 中被导出到 GKE Cluster 资源。 您可以通过从可用资源的下拉菜单中选择适当的选项来找到它们：\n您可以使用 Stackdriver Logging 的过滤机制基于事件对象字段进行过滤。 例如，下面的查询将显示调度程序中有关 Deployment nginx-deployment 中的 Pod 的事件：\nresource.type=\u0026quot;gke_cluster\u0026quot; jsonPayload.kind=\u0026quot;Event\u0026quot; jsonPayload.source.component=\u0026quot;default-scheduler\u0026quot; jsonPayload.involvedObject.name:\u0026quot;nginx-deployment\u0026quot; . figure src=\u0026rdquo;/images/docs/stackdriver-event-exporter-filter.png\u0026rdquo; alt=\u0026quot;Filtered events in the Stackdriver Logging interface\u0026rdquo; width=\u0026quot;500\u0026rdquo; \u0026gt;}}\n"
},
{
	"uri": "https://lijun.in/tutorials/stateful-application/basic-stateful-set/",
	"title": "StatefulSet 基础",
	"tags": [],
	"description": "",
	"content": "本教程介绍如何了使用 StatefulSets 来管理应用。演示了如何创建、删除、扩容/缩容和更新 StatefulSets 的 Pods。\n. heading \u0026ldquo;prerequisites\u0026rdquo; %}} 在开始本教程之前，你应该熟悉以下 Kubernetes 的概念：\n Pods Cluster DNS Headless Services PersistentVolumes [PersistentVolume Provisioning](https://github.com/kubernetes/examples/tree/. param \u0026ldquo;githubbranch\u0026rdquo; \u0026gt;}}/staging/persistent-volume-provisioning/) StatefulSets kubectl CLI  本教程假设你的集群被配置为动态的提供 PersistentVolumes。如果没有这样配置，在开始本教程之前，你需要手动准备 2 个 1 GiB 的存储卷。\n. heading \u0026ldquo;objectives\u0026rdquo; %}} StatefulSets 旨在与有状态的应用及分布式系统一起使用。然而在 Kubernetes 上管理有状态应用和分布式系统是一个宽泛而复杂的话题。为了演示 StatefulSet 的基本特性，并且不使前后的主题混淆，你将会使用 StatefulSet 部署一个简单的 web 应用。\n在阅读本教程后，你将熟悉以下内容：\n 如何创建 StatefulSet StatefulSet 怎样管理它的 Pods 如何删除 StatefulSet 如何对 StatefulSet 进行扩容/缩容 如何更新一个 StatefulSet 的 Pods  创建 StatefulSet 作为开始，使用如下示例创建一个 StatefulSet。它和 StatefulSets 概念中的示例相似。它创建了一个 Headless Service nginx 用来发布 StatefulSet web 中的 Pod 的 IP 地址。\n. codenew file=\u0026quot;application/web/web.yaml\u0026rdquo; \u0026gt;}}\n下载上面的例子并保存为文件 web.yaml。\n你需要使用两个终端窗口。在第一个终端中，使用 [kubectl get](/zh/docs/user-guide/kubectl/. param \u0026ldquo;version\u0026rdquo; \u0026gt;}}/#get) 来查看 StatefulSet 的 Pods 的创建情况。\nkubectl get pods -w -l app=nginx 在另一个终端中，使用 kubectl apply来创建定义在 web.yaml 中的 Headless Service 和 StatefulSet。\nkubectl apply -f web.yaml service/nginx created statefulset.apps/web created 上面的命令创建了两个 Pod，每个都运行了一个 NGINX web 服务器。获取 nginx Service 和 web StatefulSet 来验证是否成功的创建了它们。\nkubectl get service nginx NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE nginx ClusterIP None \u0026lt;none\u0026gt; 80/TCP 12s kubectl get statefulset web NAME DESIRED CURRENT AGE web 2 1 20s 顺序创建 Pod 对于一个拥有 N 个副本的 StatefulSet，Pod 被部署时是按照 {0 …… N-1} 的序号顺序创建的。在第一个终端中使用 kubectl get 检查输出。这个输出最终将看起来像下面的样子。\nkubectl get pods -w -l app=nginx NAME READY STATUS RESTARTS AGE web-0 0/1 Pending 0 0s web-0 0/1 Pending 0 0s web-0 0/1 ContainerCreating 0 0s web-0 1/1 Running 0 19s web-1 0/1 Pending 0 0s web-1 0/1 Pending 0 0s web-1 0/1 ContainerCreating 0 0s web-1 1/1 Running 0 18s 请注意在 web-0 Pod 处于 Running和Ready 状态后 web-1 Pod 才会被启动。\nStatefulSet 中的 Pod StatefulSet 中的 Pod 拥有一个唯一的顺序索引和稳定的网络身份标识。\n检查 Pod 的顺序索引 获取 StatefulSet 的 Pod。\nkubectl get pods -l app=nginx NAME READY STATUS RESTARTS AGE web-0 1/1 Running 0 1m web-1 1/1 Running 0 1m 如同 StatefulSets 概念中所提到的，StatefulSet 中的 Pod 拥有一个具有黏性的、独一无二的身份标志。这个标志基于 StatefulSet 控制器分配给每个 Pod 的唯一顺序索引。Pod 的名称的形式为\u0026lt;statefulset name\u0026gt;-\u0026lt;ordinal index\u0026gt;。webStatefulSet 拥有两个副本，所以它创建了两个 Pod：web-0和web-1。\n使用稳定的网络身份标识 每个 Pod 都拥有一个基于其顺序索引的稳定的主机名。使用kubectl exec在每个 Pod 中执行hostname。\nfor i in 0 1; do kubectl exec web-$i -- sh -c \u0026#39;hostname\u0026#39;; done web-0 web-1 使用 kubectl run 运行一个提供 nslookup 命令的容器，该命令来自于 dnsutils 包。通过对 Pod 的主机名执行 nslookup，你可以检查他们在集群内部的 DNS 地址。\nkubectl run -i --tty --image busybox:1.28 dns-test --restart=Never --rm nslookup web-0.nginx Server: 10.0.0.10 Address 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.local Name: web-0.nginx Address 1: 10.244.1.6 nslookup web-1.nginx Server: 10.0.0.10 Address 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.local Name: web-1.nginx Address 1: 10.244.2.6 headless service 的 CNAME 指向 SRV 记录（记录每个 Running 和 Ready 状态的 Pod）。SRV 记录指向一个包含 Pod IP 地址的记录表项。\n在一个终端中查看 StatefulSet 的 Pod。\nkubectl get pod -w -l app=nginx 在另一个终端中使用 kubectl delete 删除 StatefulSet 中所有的 Pod。\nkubectl delete pod -l app=nginx pod \u0026#34;web-0\u0026#34; deleted pod \u0026#34;web-1\u0026#34; deleted 等待 StatefulSet 重启它们，并且两个 Pod 都变成 Running 和 Ready 状态。\nkubectl get pod -w -l app=nginx NAME READY STATUS RESTARTS AGE web-0 0/1 ContainerCreating 0 0s NAME READY STATUS RESTARTS AGE web-0 1/1 Running 0 2s web-1 0/1 Pending 0 0s web-1 0/1 Pending 0 0s web-1 0/1 ContainerCreating 0 0s web-1 1/1 Running 0 34s 使用 kubectl exec 和 kubectl run 查看 Pod 的主机名和集群内部的 DNS 表项。\nfor i in 0 1; do kubectl exec web-$i -- sh -c \u0026#39;hostname\u0026#39;; done web-0 web-1 kubectl run -i --tty --image busybox:1.28 dns-test --restart=Never --rm /bin/sh nslookup web-0.nginx Server: 10.0.0.10 Address 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.local Name: web-0.nginx Address 1: 10.244.1.7 nslookup web-1.nginx Server: 10.0.0.10 Address 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.local Name: web-1.nginx Address 1: 10.244.2.8 Pod 的序号、主机名、SRV 条目和记录名称没有改变，但和 Pod 相关联的 IP 地址可能发生了改变。在本教程中使用的集群中它们就改变了。这就是为什么不要在其他应用中使用 StatefulSet 中的 Pod 的 IP 地址进行连接，这点很重要。\n如果你需要查找并连接一个 StatefulSet 的活动成员，你应该查询 Headless Service 的 CNAME。和 CNAME 相关联的 SRV 记录只会包含 StatefulSet 中处于 Running 和 Ready 状态的 Pod。\n如果你的应用已经实现了用于测试 liveness 和 readiness 的连接逻辑，你可以使用 Pod 的 SRV 记录（web-0.nginx.default.svc.cluster.local， web-1.nginx.default.svc.cluster.local）。因为他们是稳定的，并且当你的 Pod 的状态变为 Running 和 Ready 时，你的应用就能够发现它们的地址。\n写入稳定的存储 获取 web-0 和 web-1 的 PersistentVolumeClaims。\nkubectl get pvc -l app=nginx NAME STATUS VOLUME CAPACITY ACCESSMODES AGE www-web-0 Bound pvc-15c268c7-b507-11e6-932f-42010a800002 1Gi RWO 48s www-web-1 Bound pvc-15c79307-b507-11e6-932f-42010a800002 1Gi RWO 48s StatefulSet 控制器创建了两个 PersistentVolumeClaims，绑定到两个 PersistentVolumes。由于本教程使用的集群配置为动态提供 PersistentVolume，所有的 PersistentVolume 都是自动创建和绑定的。\nNGINX web 服务器默认会加载位于 /usr/share/nginx/html/index.html 的 index 文件。StatefulSets spec 中的 volumeMounts 字段保证了 /usr/share/nginx/html 文件夹由一个 PersistentVolume 支持。\n将 Pod 的主机名写入它们的index.html文件并验证 NGINX web 服务器使用该主机名提供服务。\nfor i in 0 1; do kubectl exec web-$i -- sh -c \u0026#39;echo $(hostname) \u0026gt; /usr/share/nginx/html/index.html\u0026#39;; done for i in 0 1; do kubectl exec -it web-$i -- curl localhost; done web-0 web-1 . note \u0026gt;}}\n请注意，如果你看见上面的 curl 命令返回了 403 Forbidden 的响应，你需要像这样修复使用 volumeMounts（due to a bug when using hostPath volumes）挂载的目录的权限：\nfor i in 0 1; do kubectl exec web-$i -- chmod 755 /usr/share/nginx/html; done 在你重新尝试上面的 curl 命令之前。 . /note \u0026gt;}}\n在一个终端查看 StatefulSet 的 Pod。\nkubectl get pod -w -l app=nginx 在另一个终端删除 StatefulSet 所有的 Pod。\nkubectl delete pod -l app=nginx pod \u0026#34;web-0\u0026#34; deleted pod \u0026#34;web-1\u0026#34; deleted 在第一个终端里检查 kubectl get 命令的输出，等待所有 Pod 变成 Running 和 Ready 状态。\nkubectl get pod -w -l app=nginx NAME READY STATUS RESTARTS AGE web-0 0/1 ContainerCreating 0 0s NAME READY STATUS RESTARTS AGE web-0 1/1 Running 0 2s web-1 0/1 Pending 0 0s web-1 0/1 Pending 0 0s web-1 0/1 ContainerCreating 0 0s web-1 1/1 Running 0 34s 验证所有 web 服务器在继续使用它们的主机名提供服务。\nfor i in 0 1; do kubectl exec -it web-$i -- curl localhost; done web-0 web-1 虽然 web-0 和 web-1 被重新调度了，但它们仍然继续监听各自的主机名，因为和它们的 PersistentVolumeClaim 相关联的 PersistentVolume 被重新挂载到了各自的 volumeMount 上。不管 web-0 和 web-1 被调度到了哪个节点上，它们的 PersistentVolumes 将会被挂载到合适的挂载点上。\n扩容/缩容 StatefulSet 扩容/缩容 StatefulSet 指增加或减少它的副本数。这通过更新 replicas 字段完成。你可以使用[kubectl scale](/zh/docs/user-guide/kubectl/. param \u0026ldquo;version\u0026rdquo; \u0026gt;}}/#scale) 或者[kubectl patch](/zh/docs/user-guide/kubectl/. param \u0026ldquo;version\u0026rdquo; \u0026gt;}}/#patch)来扩容/缩容一个 StatefulSet。\n扩容 在一个终端窗口观察 StatefulSet 的 Pod。\nkubectl get pods -w -l app=nginx 在另一个终端窗口使用 kubectl scale 扩展副本数为 5。\nkubectl scale sts web --replicas=5 statefulset.apps/web scaled 在第一个 终端中检查 kubectl get 命令的输出，等待增加的 3 个 Pod 的状态变为 Running 和 Ready。\nkubectl get pods -w -l app=nginx NAME READY STATUS RESTARTS AGE web-0 1/1 Running 0 2h web-1 1/1 Running 0 2h NAME READY STATUS RESTARTS AGE web-2 0/1 Pending 0 0s web-2 0/1 Pending 0 0s web-2 0/1 ContainerCreating 0 0s web-2 1/1 Running 0 19s web-3 0/1 Pending 0 0s web-3 0/1 Pending 0 0s web-3 0/1 ContainerCreating 0 0s web-3 1/1 Running 0 18s web-4 0/1 Pending 0 0s web-4 0/1 Pending 0 0s web-4 0/1 ContainerCreating 0 0s web-4 1/1 Running 0 19s StatefulSet 控制器扩展了副本的数量。如同创建 StatefulSet 所述，StatefulSet 按序号索引顺序的创建每个 Pod，并且会等待前一个 Pod 变为 Running 和 Ready 才会启动下一个 Pod。\n缩容 在一个终端观察 StatefulSet 的 Pod。\nkubectl get pods -w -l app=nginx 在另一个终端使用 kubectl patch 将 StatefulSet 缩容回三个副本。\nkubectl patch sts web -p \u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;replicas\u0026#34;:3}}\u0026#39; statefulset.apps/web patched 等待 web-4 和 web-3 状态变为 Terminating。\nkubectl get pods -w -l app=nginx NAME READY STATUS RESTARTS AGE web-0 1/1 Running 0 3h web-1 1/1 Running 0 3h web-2 1/1 Running 0 55s web-3 1/1 Running 0 36s web-4 0/1 ContainerCreating 0 18s NAME READY STATUS RESTARTS AGE web-4 1/1 Running 0 19s web-4 1/1 Terminating 0 24s web-4 1/1 Terminating 0 24s web-3 1/1 Terminating 0 42s web-3 1/1 Terminating 0 42s 顺序终止 Pod 控制器会按照与 Pod 序号索引相反的顺序每次删除一个 Pod。在删除下一个 Pod 前会等待上一个被完全关闭。\n获取 StatefulSet 的 PersistentVolumeClaims。\nkubectl get pvc -l app=nginx NAME STATUS VOLUME CAPACITY ACCESSMODES AGE www-web-0 Bound pvc-15c268c7-b507-11e6-932f-42010a800002 1Gi RWO 13h www-web-1 Bound pvc-15c79307-b507-11e6-932f-42010a800002 1Gi RWO 13h www-web-2 Bound pvc-e1125b27-b508-11e6-932f-42010a800002 1Gi RWO 13h www-web-3 Bound pvc-e1176df6-b508-11e6-932f-42010a800002 1Gi RWO 13h www-web-4 Bound pvc-e11bb5f8-b508-11e6-932f-42010a800002 1Gi RWO 13h 五个 PersistentVolumeClaims 和五个 PersistentVolumes 仍然存在。查看 Pod 的 稳定存储，我们发现当删除 StatefulSet 的 Pod 时，挂载到 StatefulSet 的 Pod 的 PersistentVolumes 不会被删除。当这种删除行为是由 StatefulSet 缩容引起时也是一样的。\n更新 StatefulSet Kubernetes 1.7 版本的 StatefulSet 控制器支持自动更新。更新策略由 StatefulSet API Object 的spec.updateStrategy 字段决定。这个特性能够用来更新一个 StatefulSet 中的 Pod 的 container images，resource requests，以及 limits，labels 和 annotations。RollingUpdate滚动更新是 StatefulSets 默认策略。\nRolling Update 策略 RollingUpdate 更新策略会更新一个 StatefulSet 中所有的 Pod，采用与序号索引相反的顺序并遵循 StatefulSet 的保证。\nPatch web StatefulSet 来执行 RollingUpdate 更新策略。\nkubectl patch statefulset web -p \u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;updateStrategy\u0026#34;:{\u0026#34;type\u0026#34;:\u0026#34;RollingUpdate\u0026#34;}}}\u0026#39; statefulset.apps/web patched 在一个终端窗口中 patch web StatefulSet 来再次的改变容器镜像。\nkubectl patch statefulset web --type=\u0026#39;json\u0026#39; -p=\u0026#39;[{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/template/spec/containers/0/image\u0026#34;, \u0026#34;value\u0026#34;:\u0026#34;gcr.io/google_containers/nginx-slim:0.8\u0026#34;}]\u0026#39; statefulset.apps/web patched 在另一个终端监控 StatefulSet 中的 Pod。\nkubectl get po -l app=nginx -w NAME READY STATUS RESTARTS AGE web-0 1/1 Running 0 7m web-1 1/1 Running 0 7m web-2 1/1 Running 0 8m web-2 1/1 Terminating 0 8m web-2 1/1 Terminating 0 8m web-2 0/1 Terminating 0 8m web-2 0/1 Terminating 0 8m web-2 0/1 Terminating 0 8m web-2 0/1 Terminating 0 8m web-2 0/1 Pending 0 0s web-2 0/1 Pending 0 0s web-2 0/1 ContainerCreating 0 0s web-2 1/1 Running 0 19s web-1 1/1 Terminating 0 8m web-1 0/1 Terminating 0 8m web-1 0/1 Terminating 0 8m web-1 0/1 Terminating 0 8m web-1 0/1 Pending 0 0s web-1 0/1 Pending 0 0s web-1 0/1 ContainerCreating 0 0s web-1 1/1 Running 0 6s web-0 1/1 Terminating 0 7m web-0 1/1 Terminating 0 7m web-0 0/1 Terminating 0 7m web-0 0/1 Terminating 0 7m web-0 0/1 Terminating 0 7m web-0 0/1 Terminating 0 7m web-0 0/1 Pending 0 0s web-0 0/1 Pending 0 0s web-0 0/1 ContainerCreating 0 0s web-0 1/1 Running 0 10s StatefulSet 里的 Pod 采用和序号相反的顺序更新。在更新下一个 Pod 前，StatefulSet 控制器终止每个 Pod 并等待它们变成 Running 和 Ready。请注意，虽然在顺序后继者变成 Running 和 Ready 之前 StatefulSet 控制器不会更新下一个 Pod，但它仍然会重建任何在更新过程中发生故障的 Pod，使用的是它们当前的版本。已经接收到更新请求的 Pod 将会被恢复为更新的版本，没有收到请求的 Pod 则会被恢复为之前的版本。像这样，控制器尝试继续使应用保持健康并在出现间歇性故障时保持更新的一致性。\n获取 Pod 来查看他们的容器镜像。\nfor p in 0 1 2; do kubectl get po web-$p --template \u0026#39;{{range $i, $c := .spec.containers}}{{$c.image}}{{end}}\u0026#39;; echo; done k8s.gcr.io/nginx-slim:0.8 k8s.gcr.io/nginx-slim:0.8 k8s.gcr.io/nginx-slim:0.8 StatefulSet 中的所有 Pod 现在都在运行之前的容器镜像。\n小窍门：你还可以使用 kubectl rollout status sts/\u0026lt;name\u0026gt; 来查看 rolling update 的状态。\n分段更新 你可以使用 RollingUpdate 更新策略的 partition 参数来分段更新一个 StatefulSet。分段的更新将会使 StatefulSet 中的其余所有 Pod 保持当前版本的同时仅允许改变 StatefulSet 的 .spec.template。\nPatch web StatefulSet 来对 updateStrategy 字段添加一个分区。\nkubectl patch statefulset web -p \u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;updateStrategy\u0026#34;:{\u0026#34;type\u0026#34;:\u0026#34;RollingUpdate\u0026#34;,\u0026#34;rollingUpdate\u0026#34;:{\u0026#34;partition\u0026#34;:3}}}}\u0026#39; statefulset.apps/web patched 再次 Patch StatefulSet 来改变容器镜像。\nkubectl patch statefulset web --type=\u0026#39;json\u0026#39; -p=\u0026#39;[{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/template/spec/containers/0/image\u0026#34;, \u0026#34;value\u0026#34;:\u0026#34;k8s.gcr.io/nginx-slim:0.7\u0026#34;}]\u0026#39; statefulset.apps/web patched 删除 StatefulSet 中的 Pod。\nkubectl delete po web-2 pod \u0026#34;web-2\u0026#34; deleted 等待 Pod 变成 Running 和 Ready。\nkubectl get po -lapp=nginx -w NAME READY STATUS RESTARTS AGE web-0 1/1 Running 0 4m web-1 1/1 Running 0 4m web-2 0/1 ContainerCreating 0 11s web-2 1/1 Running 0 18s 获取 Pod 的容器。\nkubectl get po web-2 --template \u0026#39;{{range $i, $c := .spec.containers}}{{$c.image}}{{end}}\u0026#39; k8s.gcr.io/nginx-slim:0.8 请注意，虽然更新策略是 RollingUpdate，StatefulSet 控制器还是会使用原始的容器恢复 Pod。这是因为 Pod 的序号比 updateStrategy 指定的 partition 更小。\n灰度扩容 你可以通过减少 上文指定的 partition 来进行灰度扩容，以此来测试你的程序的改动。\nPatch StatefulSet 来减少分区。\nkubectl patch statefulset web -p \u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;updateStrategy\u0026#34;:{\u0026#34;type\u0026#34;:\u0026#34;RollingUpdate\u0026#34;,\u0026#34;rollingUpdate\u0026#34;:{\u0026#34;partition\u0026#34;:2}}}}\u0026#39; statefulset.apps/web patched 等待 web-2 变成 Running 和 Ready。\nkubectl get po -lapp=nginx -w NAME READY STATUS RESTARTS AGE web-0 1/1 Running 0 4m web-1 1/1 Running 0 4m web-2 0/1 ContainerCreating 0 11s web-2 1/1 Running 0 18s 获取 Pod 的容器。\nkubectl get po web-2 --template \u0026#39;{{range $i, $c := .spec.containers}}{{$c.image}}{{end}}\u0026#39; k8s.gcr.io/nginx-slim:0.7 当你改变 partition 时，StatefulSet 会自动的更新 web-2 Pod，这是因为 Pod 的序号小于或等于 partition。\n删除 web-1 Pod。\nkubectl delete po web-1 pod \u0026#34;web-1\u0026#34; deleted 等待 web-1 变成 Running 和 Ready。\nkubectl get po -lapp=nginx -w NAME READY STATUS RESTARTS AGE web-0 1/1 Running 0 6m web-1 0/1 Terminating 0 6m web-2 1/1 Running 0 2m web-1 0/1 Terminating 0 6m web-1 0/1 Terminating 0 6m web-1 0/1 Terminating 0 6m web-1 0/1 Pending 0 0s web-1 0/1 Pending 0 0s web-1 0/1 ContainerCreating 0 0s web-1 1/1 Running 0 18s 获取 web-1 Pod 的容器。\nkubectl get po web-1 --template \u0026#39;{{range $i, $c := .spec.containers}}{{$c.image}}{{end}}\u0026#39; k8s.gcr.io/nginx-slim:0.8 web-1 被按照原来的配置恢复，因为 Pod 的序号小于分区。当指定了分区时，如果更新了 StatefulSet 的 .spec.template，则所有序号大于或等于分区的 Pod 都将被更新。如果一个序号小于分区的 Pod 被删除或者终止，它将被按照原来的配置恢复。\n分阶段的扩容 你可以使用类似灰度扩容的方法执行一次分阶段的扩容（例如一次线性的、等比的或者指数形式的扩容）。要执行一次分阶段的扩容，你需要设置 partition 为希望控制器暂停更新的序号。\n分区当前为2。请将分区设置为0。\nkubectl patch statefulset web -p \u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;updateStrategy\u0026#34;:{\u0026#34;type\u0026#34;:\u0026#34;RollingUpdate\u0026#34;,\u0026#34;rollingUpdate\u0026#34;:{\u0026#34;partition\u0026#34;:0}}}}\u0026#39; statefulset.apps/web patched 等待 StatefulSet 中的所有 Pod 变成 Running 和 Ready。\nkubectl get po -lapp=nginx -w NAME READY STATUS RESTARTS AGE web-0 1/1 Running 0 3m web-1 0/1 ContainerCreating 0 11s web-2 1/1 Running 0 2m web-1 1/1 Running 0 18s web-0 1/1 Terminating 0 3m web-0 1/1 Terminating 0 3m web-0 0/1 Terminating 0 3m web-0 0/1 Terminating 0 3m web-0 0/1 Terminating 0 3m web-0 0/1 Terminating 0 3m web-0 0/1 Pending 0 0s web-0 0/1 Pending 0 0s web-0 0/1 ContainerCreating 0 0s web-0 1/1 Running 0 3s 获取 Pod 的容器。\nfor p in 0 1 2; do kubectl get po web-$p --template \u0026#39;{{range $i, $c := .spec.containers}}{{$c.image}}{{end}}\u0026#39;; echo; done k8s.gcr.io/nginx-slim:0.7 k8s.gcr.io/nginx-slim:0.7 k8s.gcr.io/nginx-slim:0.7 将 partition 改变为 0 以允许 StatefulSet 控制器继续更新过程。\nOn Delete 策略 OnDelete 更新策略实现了传统（1.7 之前）行为，它也是默认的更新策略。当你选择这个更新策略并修改 StatefulSet 的 .spec.template 字段时，StatefulSet 控制器将不会自动的更新 Pod。\n删除 StatefulSet StatefulSet 同时支持级联和非级联删除。使用非级联方式删除 StatefulSet 时，StatefulSet 的 Pod 不会被删除。使用级联删除时，StatefulSet 和它的 Pod 都会被删除。\n非级联删除 在一个终端窗口查看 StatefulSet 中的 Pod。\nkubectl get pods -w -l app=nginx 使用 kubectl delete 删除 StatefulSet。请确保提供了 --cascade=false 参数给命令。这个参数告诉 Kubernetes 只删除 StatefulSet 而不要删除它的任何 Pod。\nkubectl delete statefulset web --cascade=false statefulset.apps \u0026#34;web\u0026#34; deleted 获取 Pod 来检查他们的状态。\nkubectl get pods -l app=nginx NAME READY STATUS RESTARTS AGE web-0 1/1 Running 0 6m web-1 1/1 Running 0 7m web-2 1/1 Running 0 5m 虽然 web 已经被删除了，但所有 Pod 仍然处于 Running 和 Ready 状态。 删除 web-0。\nkubectl delete pod web-0 pod \u0026#34;web-0\u0026#34; deleted 获取 StatefulSet 的 Pod。\nkubectl get pods -l app=nginx NAME READY STATUS RESTARTS AGE web-1 1/1 Running 0 10m web-2 1/1 Running 0 7m 由于 web StatefulSet 已经被删除，web-0没有被重新启动。\n在一个终端监控 StatefulSet 的 Pod。\nkubectl get pods -w -l app=nginx 在另一个终端里重新创建 StatefulSet。请注意，除非你删除了 nginx Service （你不应该这样做），你将会看到一个错误，提示 Service 已经存在。\nkubectl apply -f web.yaml statefulset.apps/web created service/nginx unchanged 请忽略这个错误。它仅表示 kubernetes 进行了一次创建 nginx Headless Service 的尝试，尽管那个 Service 已经存在。\n在第一个终端中运行并检查 kubectl get 命令的输出。\nkubectl get pods -w -l app=nginx NAME READY STATUS RESTARTS AGE web-1 1/1 Running 0 16m web-2 1/1 Running 0 2m NAME READY STATUS RESTARTS AGE web-0 0/1 Pending 0 0s web-0 0/1 Pending 0 0s web-0 0/1 ContainerCreating 0 0s web-0 1/1 Running 0 18s web-2 1/1 Terminating 0 3m web-2 0/1 Terminating 0 3m web-2 0/1 Terminating 0 3m web-2 0/1 Terminating 0 3m 当重新创建 web StatefulSet 时，web-0被第一个重新启动。由于 web-1 已经处于 Running 和 Ready 状态，当 web-0 变成 Running 和 Ready 时，StatefulSet 会直接接收这个 Pod。由于你重新创建的 StatefulSet 的 replicas 等于 2，一旦 web-0 被重新创建并且 web-1 被认为已经处于 Running 和 Ready 状态时，web-2将会被终止。\n让我们再看看被 Pod 的 web 服务器加载的 index.html 的内容。\nfor i in 0 1; do kubectl exec -it web-$i -- curl localhost; done web-0 web-1 尽管你同时删除了 StatefulSet 和 web-0 Pod，但它仍然使用最初写入 index.html 文件的主机名进行服务。这是因为 StatefulSet 永远不会删除和一个 Pod 相关联的 PersistentVolumes。当你重建这个 StatefulSet 并且重新启动了 web-0 时，它原本的 PersistentVolume 会被重新挂载。\n级联删除 在一个终端窗口观察 StatefulSet 里的 Pod。\nkubectl get pods -w -l app=nginx 在另一个窗口中再次删除这个 StatefulSet。这次省略 --cascade=false 参数。\nkubectl delete statefulset web statefulset.apps \u0026#34;web\u0026#34; deleted 在第一个终端检查 kubectl get 命令的输出，并等待所有的 Pod 变成 Terminating 状态。\nkubectl get pods -w -l app=nginx NAME READY STATUS RESTARTS AGE web-0 1/1 Running 0 11m web-1 1/1 Running 0 27m NAME READY STATUS RESTARTS AGE web-0 1/1 Terminating 0 12m web-1 1/1 Terminating 0 29m web-0 0/1 Terminating 0 12m web-0 0/1 Terminating 0 12m web-0 0/1 Terminating 0 12m web-1 0/1 Terminating 0 29m web-1 0/1 Terminating 0 29m web-1 0/1 Terminating 0 29m 如同你在缩容一节看到的，Pod 按照和他们序号索引相反的顺序每次终止一个。在终止一个 Pod 前，StatefulSet 控制器会等待 Pod 后继者被完全终止。\n请注意，虽然级联删除会删除 StatefulSet 和它的 Pod，但它并不会删除和 StatefulSet 关联的 Headless Service。你必须手动删除nginx Service。\nkubectl delete service nginx service \u0026#34;nginx\u0026#34; deleted 再一次重新创建 StatefulSet 和 Headless Service。\nkubectl apply -f web.yaml service/nginx created statefulset.apps/web created 当 StatefulSet 所有的 Pod 变成 Running 和 Ready 时，获取它们的 index.html 文件的内容。\nfor i in 0 1; do kubectl exec -it web-$i -- curl localhost; done web-0 web-1 即使你已经删除了 StatefulSet 和它的全部 Pod，这些 Pod 将会被重新创建并挂载它们的 PersistentVolumes，并且 web-0 和 web-1 将仍然使用它们的主机名提供服务。\n最后删除 web StatefulSet 和 nginx service。\nkubectl delete service nginx service \u0026#34;nginx\u0026#34; deleted kubectl delete statefulset web statefulset \u0026#34;web\u0026#34; deleted Pod 管理策略 对于某些分布式系统来说，StatefulSet 的顺序性保证是不必要和/或者不应该的。这些系统仅仅要求唯一性和身份标志。为了解决这个问题，在 Kubernetes 1.7 中我们针对 StatefulSet API Object 引入了 .spec.podManagementPolicy。\nOrderedReady Pod 管理策略 OrderedReady pod 管理策略是 StatefulSets 的默认选项。它告诉 StatefulSet 控制器遵循上文展示的顺序性保证。\nParallel Pod 管理策略 Parallel pod 管理策略告诉 StatefulSet 控制器并行的终止所有 Pod，在启动或终止另一个 Pod 前，不必等待这些 Pod 变成 Running 和 Ready 或者完全终止状态。\n. codenew file=\u0026quot;application/web/web-parallel.yaml\u0026rdquo; \u0026gt;}}\n下载上面的例子并保存为 web-parallel.yaml。\n这份清单和你在上文下载的完全一样，只是 web StatefulSet 的 .spec.podManagementPolicy 设置成了 Parallel。\n在一个终端窗口查看 StatefulSet 中的 Pod。\nkubectl get po -lapp=nginx -w 在另一个终端窗口创建清单中的 StatefulSet 和 Service。\nkubectl apply -f web-parallel.yaml service/nginx created statefulset.apps/web created 查看你在第一个终端中运行的 kubectl get 命令的输出。\nkubectl get po -lapp=nginx -w NAME READY STATUS RESTARTS AGE web-0 0/1 Pending 0 0s web-0 0/1 Pending 0 0s web-1 0/1 Pending 0 0s web-1 0/1 Pending 0 0s web-0 0/1 ContainerCreating 0 0s web-1 0/1 ContainerCreating 0 0s web-0 1/1 Running 0 10s web-1 1/1 Running 0 10s StatefulSet 控制器同时启动了 web-0 和 web-1。\n保持第二个终端打开，并在另一个终端窗口中扩容 StatefulSet。\nkubectl scale statefulset/web --replicas=4 statefulset.apps/web scaled 在 kubectl get 命令运行的终端里检查它的输出。\nweb-3 0/1 Pending 0 0s web-3 0/1 Pending 0 0s web-3 0/1 Pending 0 7s web-3 0/1 ContainerCreating 0 7s web-2 1/1 Running 0 10s web-3 1/1 Running 0 26s StatefulSet 控制器启动了两个新的 Pod，而且在启动第二个之前并没有等待第一个变成 Running 和 Ready 状态。\n保持这个终端打开，并在另一个终端删除 web StatefulSet。\nkubectl delete sts web 在另一个终端里再次检查 kubectl get 命令的输出。\nweb-3 1/1 Terminating 0 9m web-2 1/1 Terminating 0 9m web-3 1/1 Terminating 0 9m web-2 1/1 Terminating 0 9m web-1 1/1 Terminating 0 44m web-0 1/1 Terminating 0 44m web-0 0/1 Terminating 0 44m web-3 0/1 Terminating 0 9m web-2 0/1 Terminating 0 9m web-1 0/1 Terminating 0 44m web-0 0/1 Terminating 0 44m web-2 0/1 Terminating 0 9m web-2 0/1 Terminating 0 9m web-2 0/1 Terminating 0 9m web-1 0/1 Terminating 0 44m web-1 0/1 Terminating 0 44m web-1 0/1 Terminating 0 44m web-0 0/1 Terminating 0 44m web-0 0/1 Terminating 0 44m web-0 0/1 Terminating 0 44m web-3 0/1 Terminating 0 9m web-3 0/1 Terminating 0 9m web-3 0/1 Terminating 0 9m StatefulSet 控制器将并发的删除所有 Pod，在删除一个 Pod 前不会等待它的顺序后继者终止。\n关闭 kubectl get 命令运行的终端并删除nginx Service。\nkubectl delete svc nginx . heading \u0026ldquo;cleanup\u0026rdquo; %}} 你需要删除本教程中用到的 PersistentVolumes 的持久化存储介质。基于你的环境、存储配置和提供方式，按照必须的步骤保证回收所有的存储。\n"
},
{
	"uri": "https://lijun.in/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://lijun.in/reference/command-line-tools-reference/kubelet-tls-bootstrapping/",
	"title": "TLS bootstrapping",
	"tags": [],
	"description": "",
	"content": ". toc \u0026gt;}}\nOverview This document describes how to set up TLS client certificate bootstrapping for kubelets. Kubernetes 1.4 introduced an API for requesting certificates from a cluster-level Certificate Authority (CA). The original intent of this API is to enable provisioning of TLS client certificates for kubelets. The proposal can be found here and progress on the feature is being tracked as feature #43.\nkube-apiserver configuration The API server should be configured with an authenticator that can authenticate tokens as a user in the system:bootstrappers group.\nThis group will later be used in the controller-manager configuration to scope approvals in the default approval controller. As this feature matures, you should ensure tokens are bound to a Role-Based Access Control (RBAC) policy which limits requests (using the bootstrap token) strictly to client requests related to certificate provisioning. With RBAC in place, scoping the tokens to a group allows for great flexibility (e.g. you could disable a particular bootstrap group\u0026rsquo;s access when you are done provisioning the nodes).\nWhile any authentication strategy can be used for the kubelet\u0026rsquo;s initial bootstrap credentials, the following two authenticators are recommended for ease of provisioning.\n Bootstrap Tokens - alpha Token authentication file  Using bootstrap tokens is currently alpha and will simplify the management of bootstrap token management especially in a HA scenario.\nToken authentication file Tokens are arbitrary but should represent at least 128 bits of entropy derived from a secure random number generator (such as /dev/urandom on most modern systems). There are multiple ways you can generate a token. For example:\nhead -c 16 /dev/urandom | od -An -t x | tr -d ' '\nwill generate tokens that look like 02b50b05283e98dd0fd71db496ef01e8\nThe token file should look like the following example, where the first three values can be anything and the quoted group name should be as depicted:\n02b50b05283e98dd0fd71db496ef01e8,kubelet-bootstrap,10001,\u0026quot;system:bootstrappers\u0026quot; Add the --token-auth-file=FILENAME flag to the kube-apiserver command (in your systemd unit file perhaps) to enable the token file. See docs here for further details.\nClient certificate CA bundle Add the --client-ca-file=FILENAME flag to the kube-apiserver command to enable client certificate authentication, referencing a certificate authority bundle containing the signing certificate (e.g. --client-ca-file=/var/lib/kubernetes/ca.pem).\nkube-controller-manager configuration The API for requesting certificates adds a certificate-issuing control loop to the Kubernetes Controller Manager. This takes the form of a cfssl local signer using assets on disk. Currently, all certificates issued have one year validity and a default set of key usages.\nSigning assets You must provide a Certificate Authority in order to provide the cryptographic materials necessary to issue certificates. This CA should be trusted by kube-apiserver for authentication with the --client-ca-file=FILENAME flag. The management of the CA is beyond the scope of this document but it is recommended that you generate a dedicated CA for Kubernetes. Both certificate and key are assumed to be PEM-encoded.\nThe kube-controller-manager flags are:\n--cluster-signing-cert-file=\u0026quot;/etc/path/to/kubernetes/ca/ca.crt\u0026quot; --cluster-signing-key-file=\u0026quot;/etc/path/to/kubernetes/ca/ca.key\u0026quot; Approval controller In 1.7 the experimental \u0026ldquo;group auto approver\u0026rdquo; controller is dropped in favor of the new csrapproving controller that ships as part of kube-controller-manager and is enabled by default. The controller uses the SubjectAccessReview API to determine if a given user is authorized to request a CSR, then approves based on the authorization outcome. To prevent conflicts with other approvers, the builtin approver doesn\u0026rsquo;t explicitly deny CSRs, only ignoring unauthorized requests.\nThe controller categorizes CSRs into three subresources:\n nodeclient - a request by a user for a client certificate with O=system:nodes and CN=system:node:(node name). selfnodeclient - a node renewing a client certificate with the same O and CN. selfnodeserver - a node renewing a serving certificate. (ALPHA, requires feature gate)  The checks to determine if a CSR is a selfnodeserver request is currently tied to the kubelet\u0026rsquo;s credential rotation implementation, an alpha feature. As such, the definition of selfnodeserver will likely change in a future and requires the RotateKubeletServerCertificate feature gate on the controller manager. The feature progress can be tracked at kubernetes/features#267.\n--feature-gates=RotateKubeletServerCertificate=true The following RBAC ClusterRoles represent the nodeclient, selfnodeclient, and selfnodeserver capabilities. Similar roles may be automatically created in future releases.\n# A ClusterRole which instructs the CSR approver to approve a user requesting # node client credentials. kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: approve-node-client-csr rules: - apiGroups: [\u0026#34;certificates.k8s.io\u0026#34;] resources: [\u0026#34;certificatesigningrequests/nodeclient\u0026#34;] verbs: [\u0026#34;create\u0026#34;] --- # A ClusterRole which instructs the CSR approver to approve a node renewing its # own client credentials. kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: approve-node-client-renewal-csr rules: - apiGroups: [\u0026#34;certificates.k8s.io\u0026#34;] resources: [\u0026#34;certificatesigningrequests/selfnodeclient\u0026#34;] verbs: [\u0026#34;create\u0026#34;] --- # A ClusterRole which instructs the CSR approver to approve a node requesting a # serving cert matching its client cert. kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: approve-node-server-renewal-csr rules: - apiGroups: [\u0026#34;certificates.k8s.io\u0026#34;] resources: [\u0026#34;certificatesigningrequests/selfnodeserver\u0026#34;] verbs: [\u0026#34;create\u0026#34;] These powers can be granted to credentials, such as bootstrapping tokens. For example, to replicate the behavior provided by the removed auto-approval flag, of approving all CSRs by a single group:\n# REMOVED: This flag no longer works as of 1.7. --insecure-experimental-approve-all-kubelet-csrs-for-group=\u0026quot;system:bootstrappers\u0026quot; An admin would create a ClusterRoleBinding targeting that group.\n# Approve all CSRs for the group \u0026#34;system:bootstrappers\u0026#34; kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: auto-approve-csrs-for-group subjects: - kind: Group name: system:bootstrappers apiGroup: rbac.authorization.k8s.io roleRef: kind: ClusterRole name: approve-node-client-csr apiGroup: rbac.authorization.k8s.io To let a node renew its own credentials, an admin can construct a ClusterRoleBinding targeting that node\u0026rsquo;s credentials:\nkind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: node1-client-cert-renewal subjects: - kind: User name: system:node:node-1 # Let \u0026#34;node-1\u0026#34; renew its client certificate. apiGroup: rbac.authorization.k8s.io roleRef: kind: ClusterRole name: approve-node-client-renewal-csr apiGroup: rbac.authorization.k8s.io Deleting the binding will prevent the node from renewing its client credentials, effectively removing it from the cluster once its certificate expires.\nkubelet configuration To request a client certificate from kube-apiserver, the kubelet first needs a path to a kubeconfig file that contains the bootstrap authentication token. You can use kubectl config set-cluster, set-credentials, and set-context to build this kubeconfig. Provide the name kubelet-bootstrap to kubectl config set-credentials and include --token=\u0026lt;token-value\u0026gt; as follows:\nkubectl config set-credentials kubelet-bootstrap --token=${BOOTSTRAP_TOKEN} --kubeconfig=bootstrap.kubeconfig When starting the kubelet, if the file specified by --kubeconfig does not exist, the bootstrap kubeconfig is used to request a client certificate from the API server. On approval of the certificate request and receipt back by the kubelet, a kubeconfig file referencing the generated key and obtained certificate is written to the path specified by --kubeconfig. The certificate and key file will be placed in the directory specified by --cert-dir.\n. note \u0026gt;}} The following flags are required to enable this bootstrapping when starting the kubelet:\n--bootstrap-kubeconfig=\u0026quot;/path/to/bootstrap/kubeconfig\u0026quot; . /note \u0026gt;}}\nAdditionally, in 1.7 the kubelet implements alpha features for enabling rotation of both its client and/or serving certs. These can be enabled through the respective RotateKubeletClientCertificate and RotateKubeletServerCertificate feature flags on the kubelet, but may change in backward incompatible ways in future releases.\n--feature-gates=RotateKubeletClientCertificate=true,RotateKubeletServerCertificate=true RotateKubeletClientCertificate causes the kubelet to rotate its client certificates by creating new CSRs as its existing credentials expire. RotateKubeletServerCertificate causes the kubelet to both request a serving certificate after bootstrapping its client credentials and rotate the certificate. The serving cert currently does not request DNS or IP SANs.\nkubectl approval The signing controller does not immediately sign all certificate requests. Instead, it waits until they have been flagged with an \u0026ldquo;Approved\u0026rdquo; status by an appropriately-privileged user. This is intended to eventually be an automated process handled by an external approval controller, but for the alpha version of the API it can be done manually by a cluster administrator using kubectl. An administrator can list CSRs with kubectl get csr and describe one in detail with kubectl describe csr \u0026lt;name\u0026gt;. Before the 1.6 release there were no direct approve/deny commands so an approver had to update the Status field directly (rough how-to). Later versions of Kubernetes offer kubectl certificate approve \u0026lt;name\u0026gt; and kubectl certificate deny \u0026lt;name\u0026gt; commands.\n"
},
{
	"uri": "https://lijun.in/tasks/administer-cluster/configure-upgrade-etcd/",
	"title": "为 Kubernetes 运行 etcd 集群",
	"tags": [],
	"description": "",
	"content": ". glossary_definition term_id=\u0026quot;etcd\u0026rdquo; length=\u0026quot;all\u0026rdquo; prepend=\u0026quot;etcd 是一个\u0026quot;\u0026gt;}}\n. heading \u0026ldquo;prerequisites\u0026rdquo; %}} . include \u0026ldquo;task-tutorial-prereqs.md\u0026rdquo; \u0026gt;}} . version-check \u0026gt;}}\n先决条件   运行的 etcd 集群个数成员为奇数。\n  etcd 是一个 leader-based 分布式系统。确保主节点定期向所有从节点发送心跳，以保持集群稳定。\n  确保不发生资源不足。\n集群的性能和稳定性对网络和磁盘 IO 非常敏感。任何资源匮乏都会导致心跳超时，从而导致集群的不稳定。不稳定的情况表明没有选出任何主节点。在这种情况下，集群不能对其当前状态进行任何更改，这意味着不能调度新的 pod。\n  保持稳定的 etcd 集群对 Kubernetes 集群的稳定性至关重要。因此，请在专用机器或隔离环境上运行 etcd 集群，以满足所需资源需求。\n  在生产中运行的 etcd 的最低推荐版本是 3.2.10+。\n  资源要求 使用有限的资源运行 etcd 只适合测试目的。为了在生产中部署，需要先进的硬件配置。在生产中部署 etcd 之前，请查看所需资源参考文档。\n启动 etcd 集群 本节介绍如何启动单节点和多节点 etcd 集群。\n单节点 etcd 集群 只为测试目的使用单节点 etcd 集群。\n  运行以下命令：\n./etcd --listen-client-urls=http://$PRIVATE_IP:2379 --advertise-client-urls=http://$PRIVATE_IP:2379   使用参数 --etcd-servers=$PRIVATE_IP:2379 启动 Kubernetes API 服务器。\n使用您 etcd 客户端 IP 替换 PRIVATE_IP。\n  多节点 etcd 集群 为了耐用性和高可用性，在生产中将以多节点集群的方式运行 etcd，并且定期备份。建议在生产中使用五个成员的集群。有关该内容的更多信息，请参阅常见问题文档。\n可以通过静态成员信息或动态发现的方式配置 etcd 集群。有关集群的详细信息，请参阅 etcd 集群文档。\n例如，考虑运行以下客户端 URL 的五个成员的 etcd 集群：http://$IP1:2379，http://$IP2:2379，http://$IP3:2379，http://$IP4:2379 和 http://$IP5:2379。要启动 Kubernetes API 服务器：\n  运行以下命令：\n./etcd --listen-client-urls=http://$IP1:2379, http://$IP2:2379, http://$IP3:2379, http://$IP4:2379, http://$IP5:2379 --advertise-client-urls=http://$IP1:2379, http://$IP2:2379, http://$IP3:2379, http://$IP4:2379, http://$IP5:2379   使用参数 --etcd-servers=$IP1:2379, $IP2:2379, $IP3:2379, $IP4:2379, $IP5:2379 启动 Kubernetes API 服务器。\n使用您 etcd 客户端 IP 地址替换 IP。\n  使用负载均衡的多节点 etcd 集群 要运行负载均衡的 etcd 集群：\n 建立一个 etcd 集群。 在 etcd 集群前面配置负载均衡器。例如，让负载均衡器的地址为 $LB。 使用参数 --etcd-servers=$LB:2379 启动 Kubernetes API 服务器。  安全的 etcd 集群 对 etcd 的访问相当于集群中的 root 权限，因此理想情况下只有 API 服务器才能访问它。考虑到数据的敏感性，建议只向需要访问 etcd 集群的节点授予权限。\n想要确保 etcd 的安全，可以设置防火墙规则或使用 etcd 提供的安全特性，这些安全特性依赖于 x509 公钥基础设施（PKI）。首先，通过生成密钥和证书对来建立安全的通信通道。 例如，使用密钥对 peer.key 和 peer.cert 来保护 etcd 成员之间的通信，而 client.cert 和 client.cert 用于保护 etcd 与其客户端之间的通信。请参阅 etcd 项目提供的示例脚本，以生成用于客户端身份验证的密钥对和 CA 文件。\n安全通信 若要使用安全对等通信对 etcd 进行配置，请指定参数 --peer-key-file=peer.key 和 --peer-cert-file=peer.cert，并使用 https 作为 URL 模式。\n类似地，要使用安全客户端通信对 etcd 进行配置，请指定参数 --key-file=k8sclient.key 和 --cert-file=k8sclient.cert，并使用 https 作为 URL 模式。\n限制 etcd 集群的访问 配置安全通信后，将 etcd 集群的访问限制在 Kubernetes API 服务器上。使用 TLS 身份验证来完成此任务。\n例如，考虑由 CA etcd.ca 信任的密钥对 k8sclient.key 和 k8sclient.cert。当 etcd 配置为 --client-cert-auth 和 TLS 时，它使用系统 CA 或由 --trusted-ca-file 参数传入的 CA 验证来自客户端的证书。 指定参数 --client-cert-auth=true 和 --trusted-ca-file=etcd.ca 将限制对具有证书 k8sclient.cert 的客户端的访问。\n一旦正确配置了 etcd，只有具有有效证书的客户端才能访问它。要让 Kubernetes API 服务器访问，可以使用参数 --etcd-certfile=k8sclient.cert,--etcd-keyfile=k8sclient.key 和 --etcd-cafile=ca.cert 配置它。\n. note \u0026gt;}} Kubernetes 目前不支持 etcd 身份验证。想要了解更多信息，请参阅相关的问题支持 etcd v2 的基本认证。 . /note \u0026gt;}}\n替换失败的 etcd 成员 etcd 集群通过容忍少数成员故障实现高可用性。但是，要改善集群的整体健康状况，请立即替换失败的成员。当多个成员失败时，逐个替换它们。替换失败成员需要两个步骤：删除失败成员和添加新成员。\n虽然 etcd 在内部保留唯一的成员 ID，但建议为每个成员使用唯一的名称，以避免人为错误。例如，考虑一个三成员的 etcd 集群。让 URL 为：member1=http://10.0.0.1， member2=http://10.0.0.2 和 member3=http://10.0.0.3。当 member1 失败时，将其替换为 member4=http://10.0.0.4。\n  获取失败的 member1 的成员 ID：\netcdctl --endpoints=http://10.0.0.2,http://10.0.0.3 member list\n显示以下信息：\n 8211f1d0f64f3269, started, member1, http://10.0.0.1:2380, http://10.0.0.1:2379 91bc3c398fb3c146, started, member2, http://10.0.0.2:2380, http://10.0.0.2:2379 fd422379fda50e48, started, member3, http://10.0.0.3:2380, http://10.0.0.3:2379    移除失败的成员\netcdctl member remove 8211f1d0f64f3269\n显示以下信息：\nRemoved member 8211f1d0f64f3269 from cluster    增加新成员：\n./etcdctl member add member4 --peer-urls=http://10.0.0.4:2380\n显示以下信息：\nMember 2be1eb8f84b7f63e added to cluster ef37ad9dc622a7c4    在 IP 为 10.0.0.4 的机器上启动新增加的成员：\n export ETCD_NAME=\u0026quot;member4\u0026quot; export ETCD_INITIAL_CLUSTER=\u0026quot;member2=http://10.0.0.2:2380,member3=http://10.0.0.3:2380,member4=http://10.0.0.4:2380\u0026quot; export ETCD_INITIAL_CLUSTER_STATE=existing etcd [flags]    做以下事情之一：\n 更新其 --etcd-servers 参数，使 Kubernetes 知道配置进行了更改，然后重新启动 Kubernetes API 服务器。 如果在 deployment 中使用了负载均衡，更新负载均衡配置。    有关集群重新配置的详细信息，请参阅 etcd 重构文档。\n备份 etcd 集群 所有 Kubernetes 对象都存储在 etcd 上。定期备份 etcd 集群数据对于在灾难场景（例如丢失所有主节点）下恢复 Kubernetes 集群非常重要。快照文件包含所有 Kubernetes 状态和关键信息。为了保证敏感的 Kubernetes 数据的安全，可以对快照文件进行加密。\n备份 etcd 集群可以通过两种方式完成：etcd 内置快照和卷快照。\n内置快照 etcd 支持内置快照，因此备份 etcd 集群很容易。快照可以从使用 etcdctl snapshot save 命令的活动成员中获取，也可以通过从 etcd 数据目录复制 member/snap/db 文件，该 etcd 数据目录目前没有被 etcd 进程使用。获取快照通常不会影响成员的性能。\n下面是一个示例，用于获取 $ENDPOINT 所提供的键空间的快照到文件 snapshotdb：\nETCDCTL_API=3 etcdctl --endpoints $ENDPOINT snapshot save snapshotdb # exit 0 # verify the snapshot ETCDCTL_API=3 etcdctl --write-out=table snapshot status snapshotdb +----------+----------+------------+------------+ | HASH | REVISION | TOTAL KEYS | TOTAL SIZE | +----------+----------+------------+------------+ | fe01cf57 | 10 | 7 | 2.1 MB | +----------+----------+------------+------------+ 卷快照 如果 etcd 运行在支持备份的存储卷（如 Amazon Elastic Block 存储）上，则可以通过获取存储卷的快照来备份 etcd 数据。\n扩展 etcd 集群 通过交换性能，扩展 etcd 集群可以提高可用性。缩放不会提高集群性能和能力。一般情况下不要扩大或缩小 etcd 集群的集合。不要为 etcd 集群配置任何自动缩放组。强烈建议始终在任何官方支持的规模上运行生产 Kubernetes 集群时使用静态的五成员 etcd 集群。\n合理的扩展是在需要更高可靠性的情况下，将三成员集群升级为五成员集群。请参阅 etcd 重新配置文档以了解如何将成员添加到现有集群中的信息。\n恢复 etcd 集群 etcd 支持从 major.minor 或其他不同 patch 版本的 etcd 进程中获取的快照进行恢复。还原操作用于恢复失败的集群的数据。\n在启动还原操作之前，必须有一个快照文件。它可以是来自以前备份操作的快照文件，也可以是来自剩余数据目录的快照文件。 有关从快照文件还原集群的详细信息和示例，请参阅 etcd 灾难恢复文档。\n如果还原的集群的访问 URL 与前一个集群不同，则必须相应地重新配置 Kubernetes API 服务器。 在本例中，使用参数 --etcd-servers=$NEW_ETCD_CLUSTER 而不是参数 --etcd-servers=$OLD_ETCD_CLUSTER 重新启动 Kubernetes API 服务器。 用相应的 IP 地址替换 $NEW_ETCD_CLUSTER 和 $OLD_ETCD_CLUSTER。如果在 etcd 集群前面使用负载平衡，则可能需要更新负载均衡器。\n如果大多数 etcd 成员永久失败，则认为 etcd 集群失败。在这种情况下，Kubernetes 不能对其当前状态进行任何更改。 虽然已调度的 pod 可能继续运行，但新的 pod 无法调度。在这种情况下，恢复 etcd 集群并可能需要重新配置 Kubernetes API 服务器以修复问题。\n升级和回滚 etcd 集群 从 Kubernetes v1.13.0 开始，不在支持 etcd2 作为新的或现有 Kubernetes 集群的后端。Kubernetes 支持 etcd2 和 etcd3 的时间表如下：\n Kubernetes v1.0: 仅限 etcd2 Kubernetes v1.5.1: 添加了 etcd3 支持，新的集群仍默认为 etcd2 Kubernetes v1.6.0: 使用 kube-up.sh 创建的新集群默认为 etcd3，而 kube-apiserver 默认为 etcd3 Kubernetes v1.9.0: 宣布弃用 etcd2 存储后端 Kubernetes v1.13.0: 删除了 etcd2 存储后端，kube-apiserver 将拒绝以 --storage-backend = etcd2 开头，消息 etcd2 不再是支持的存储后端  在使用 --storage-backend = etcd2 升级 v1.12.x kube-apiserver 到 v1.13.x 之前，etcd v2 数据必须迁移到 v3 存储后端，并且 kube-apiserver 调用改为使用 --storage-backend=etcd3。\n从 etcd2 迁移到 etcd3 的过程在很大程度上取决于部署和配置 etcd 集群的方式，以及如何部署和配置 Kubernetes 集群。 我们建议您查阅集群提供商的文档，以了解是否存在预定义的解决方案。\n如果您的集群是通过 kube-up.sh 创建的并且仍然使用 etcd2 作为其存储后端，请参阅 Kubernetes v1.12 etcd 集群升级文档\n已知问题：具有安全端点的 etcd 客户端均衡器 在 etcd v3.3.13 或更早版本的 etcd v3 客户端有一个严重的错误，会影响 kube-apiserver 和 HA 部署。etcd 客户端平衡器故障转移不适用于安全端点。结果是，etcd 服务器可能会失败或短暂地与 kube-apiserver 断开连接。这会影响 kube-apiserver HA 的部署。\n该修复程序是在 etcd v3.4 中进行的（并反向移植到 v3.3.14 或更高版本）：现在，新客户端将创建自己的凭证捆绑包，以在拨号功能中正确设置授权目标。\n因为此修复程序要求将 gRPC 依赖升级（到 v1.23.0 ），因此，下游 Kubernetes 未反向移植 etcd 升级。这意味着只能从 Kubernetes 1.16 获得 kube-apiserver 中的 etcd 修复。\n要紧急修复 Kubernetes 1.15 或更早版本的此错误，请构建一个自定义的 kube-apiserver 。 您可以使用vendor/google.golang.org/grpc/credentials/credentials.go 和 etcd@db61ee106 来进行本地更改。\n请看 \u0026ldquo;kube-apiserver 1.13.x refuses to work when first etcd-server is not available\u0026rdquo;.\n"
},
{
	"uri": "https://lijun.in/tasks/inject-data-application/define-environment-variable-container/",
	"title": "为容器设置环境变量",
	"tags": [],
	"description": "",
	"content": "本页将展示如何为 kubernetes Pod 下的容器设置环境变量。\n. heading \u0026ldquo;prerequisites\u0026rdquo; %}} . include \u0026ldquo;task-tutorial-prereqs.md\u0026rdquo; \u0026gt;}} . version-check \u0026gt;}}\n为容器设置一个环境变量 创建 Pod 时，可以为其下的容器设置环境变量。通过配置文件的 env 或者 envFrom 字段来设置环境变量。\n本示例中，将创建一个只包含单个容器的 Pod。Pod 的配置文件中设置环境变量的名称为 DEMO_GREETING， 其值为 \u0026quot;Hello from the environment\u0026quot;。下面是 Pod 的配置文件内容：\n. codenew file=\u0026quot;pods/inject/envars.yaml\u0026rdquo; \u0026gt;}}\n  基于 YAML 文件创建一个 Pod：\nkubectl apply -f https://k8s.io/examples/pods/inject/envars.yaml     获取一下当前正在运行的 Pods 信息：\nkubectl get pods -l purpose=demonstrate-envars 查询结果应为：\nNAME READY STATUS RESTARTS AGE envar-demo 1/1 Running 0 9s     进入该 Pod 下的容器并打开一个命令终端：\nkubectl exec -it envar-demo -- /bin/bash     在命令终端中通过执行 printenv 打印出环境变量。\nroot@envar-demo:/# printenv 打印结果应为：\nNODE_VERSION=4.4.2 EXAMPLE_SERVICE_PORT_8080_TCP_ADDR=10.3.245.237 HOSTNAME=envar-demo ... DEMO_GREETING=Hello from the environment DEMO_FAREWELL=Such a sweet sorrow    通过键入 exit 退出命令终端。  . note \u0026gt;}} 通过 env 或 envFrom 字段设置的环境变量将覆盖容器镜像中指定的所有环境变量。 . /note \u0026gt;}}\n. note \u0026gt;}} 环境变量之间可能出现互相依赖或者循环引用的情况，使用之前需注意引用顺序 . /note \u0026gt;}}\n在配置中使用环境变量 您在 Pod 的配置中定义的环境变量可以在配置的其他地方使用，例如可用在为 Pod 的容器设置的命令和参数中。在下面的示例配置中，环境变量 GREETING ，HONORIFIC 和 NAME 分别设置为 Warm greetings to ，The Most Honorable 和 Kubernetes。然后这些环境变量在传递给容器 env-print-demo 的 CLI 参数中使用。\napiVersion: v1 kind: Pod metadata: name: print-greeting spec: containers: - name: env-print-demo image: bash env: - name: GREETING value: \u0026#34;Warm greetings to\u0026#34; - name: HONORIFIC value: \u0026#34;The Most Honorable\u0026#34; - name: NAME value: \u0026#34;Kubernetes\u0026#34; command: [\u0026#34;echo\u0026#34;] args: [\u0026#34;$(GREETING) $(HONORIFIC) $(NAME)\u0026#34;] 创建后，命令 echo Warm greetings to The Most Honorable Kubernetes 将在容器中运行。\n. heading \u0026ldquo;whatsnext\u0026rdquo; %}}  有关环境变量的更多信息，请参阅这里。 有关如何通过环境变量来使用 Secret，请参阅这里。 关于 [EnvVarSource](/docs/reference/generated/kubernetes-api/. param \u0026ldquo;version\u0026rdquo; \u0026gt;}}/#envvarsource-v1-core) 资源的信息。  "
},
{
	"uri": "https://lijun.in/tasks/administer-cluster/reserve-compute-resources/",
	"title": "为系统守护进程预留计算资源",
	"tags": [],
	"description": "",
	"content": "Kubernetes 的节点可以按照 Capacity 调度。默认情况下 pod 能够使用节点全部可用容量。这是个问题，因为节点自己通常运行了不少驱动 OS 和 Kubernetes 的系统守护进程。除非为这些系统守护进程留出资源，否则它们将与 pod 争夺资源并导致节点资源短缺问题。\nkubelet 公开了一个名为 Node Allocatable 的特性，有助于为系统守护进程预留计算资源。Kubernetes 推荐集群管理员按照每个节点上的工作负载密度配置 Node Allocatable。\n. heading \u0026ldquo;prerequisites\u0026rdquo; %}} . include \u0026ldquo;task-tutorial-prereqs.md\u0026rdquo; \u0026gt;}} . version-check \u0026gt;}}\n可分配的节点 Node Capacity --------------------------- | kube-reserved | |-------------------------| | system-reserved | |-------------------------| | eviction-threshold | |-------------------------| | | | allocatable | | (available for pods) | | | | | --------------------------- Kubernetes 节点上的 Allocatable 被定义为 pod 可用计算资源量。调度器不会超额申请 Allocatable。目前支持 CPU, memory 和 ephemeral-storage 这几个参数。\n可分配的节点暴露为 API 中 v1.Node 对象的一部分，也是 CLI 中 kubectl describe node 的一部分。\n在 kubelet 中，可以为两类系统守护进程预留资源。\n启用 QoS 和 Pod 级别的 cgroups 为了恰当的在节点范围实施 node allocatable，您必须通过 --cgroups-per-qos 标志启用新的 cgroup 层次结构。这个标志是默认启用的。启用后，kubelet 将在其管理的 cgroup 层次结构中创建所有终端用户的 pod。\n配置 cgroup 驱动 kubelet 支持在主机上使用 cgroup 驱动操作 cgroup 层次结构。驱动通过 --cgroup-driver 标志配置。\n支持的参数值如下：\n cgroupfs 是默认的驱动，在主机上直接操作 cgroup 文件系统以对 cgroup 沙箱进行管理。 systemd 是可选的驱动，使用 init 系统支持的资源的瞬时切片管理 cgroup 沙箱。  取决于相关容器运行时的配置，操作员可能需要选择一个特定的 cgroup 驱动来保证系统正常运行。例如如果操作员使用 docker 运行时提供的 systemd cgroup 驱动时，必须配置 kubelet 使用 systemd cgroup 驱动。\nKube 预留值  Kubelet Flag: --kube-reserved=[cpu=100m][,][memory=100Mi][,][ephemeral-storage=1Gi][,][pid=1000] Kubelet Flag: --kube-reserved-cgroup=  kube-reserved 是为了给诸如 kubelet、container runtime、node problem detector 等 kubernetes 系统守护进程争取资源预留。 这并不代表要给以 pod 形式运行的系统守护进程保留资源。kube-reserved 通常是节点上的一个 pod 密度 功能。 这个性能仪表盘 从 pod 密度的多个层面展示了 kubelet 和 docker engine 的 cpu 和 内存 使用情况。 这个博文解释了如何仪表板以提出合适的 kube-reserved 预留。\n除了 cpu，内存 和 ephemeral-storage 之外，pid 可能是指定为 kubernetes 系统守护进程预留指定数量的进程 ID。\n要选择性的在系统守护进程上执行 kube-reserved，需要把 kubelet 的 --kube-reserved-cgroup 标志的值设置为 kube 守护进程的父控制组。\n推荐将 kubernetes 系统守护进程放置于顶级控制组之下（例如 systemd 机器上的 runtime.slice）。理想情况下每个系统守护进程都应该在其自己的子控制组中运行。请参考这篇文档，获取更过关于推荐控制组层次结构的细节。\n请注意，如果 --kube-reserved-cgroup 不存在，Kubelet 将不会创建它。如果指定了一个无效的 cgroup，Kubelet 将会失败。\n系统预留值  Kubelet Flag: --system-reserved=[cpu=100m][,][memory=100Mi][,][ephemeral-storage=1Gi][,][pid=1000] Kubelet Flag: --system-reserved-cgroup=  system-reserved 用于为诸如 sshd、udev 等系统守护进程争取资源预留。 system-reserved 也应该为 kernel 预留 内存，因为目前 kernel 使用的内存并不记在 Kubernetes 的 pod 上。 同时还推荐为用户登录会话预留资源（systemd 体系中的 user.slice）。\n除了 cpu，内存 和 ephemeral-storage 之外，pid 可能是指定为 kubernetes 系统守护进程预留指定数量的进程 ID。\n要想在系统守护进程上可选地执行 system-reserved，请指定 --system-reserved-cgroup kubelet 标志的值为 OS 系统守护进程的父级控制组。\n推荐将 OS 系统守护进程放在一个顶级控制组之下（例如 systemd 机器上的 system.slice）。\n请注意，如果 --system-reserved-cgroup 不存在，Kubelet 不会创建它。如果指定了无效的 cgroup，Kubelet 将会失败。\n明确保留的 CPU 列表 . feature-state for_k8s_version=\u0026quot;v1.17\u0026rdquo; state=\u0026quot;stable\u0026rdquo; \u0026gt;}}\n Kubelet Flag: --reserved-cpus=0-3  reserved-cpus 旨在为操作系统守护程序和 kubernetes 系统守护程序定义一个显式 cpuset。此选项在 kubernetes 1.17 版本中添加。 reserved-cpus 适用于不打算针对 cpuset 资源为操作系统守护程序和 kubernetes 系统守护程序定义单独的顶级 cgroups 的系统。 如果 Kubelet 没有 指定参数 --system-reserved-cgroup 和 --kube-reserved-cgroup，则 reserved-cpus 提供的显式 cpuset 将优先于 --kube-reserved 和 --system-reserved 选项定义的 cpuset。\n此选项是专门为 Telco 或 NFV 用例设计的，在这些用例中不受控制的中断或计时器可能会影响其工作负载性能。 可以使用此选项为系统或 kubernetes 守护程序以及中断或计时器定义显式的 cpuset，因此系统上的其余 CPU 可以专门用于工作负载，而不受不受控制的中断或计时器的影响较小。要将系统守护程序、kubernetes 守护程序和中断或计时器移动到此选项定义的显式 cpuset 上，应使用 Kubernetes 之外的其他机制。 例如：在 Centos 系统中，可以使用 tuned 工具集来执行此操作。\n驱逐阈值  Kubelet Flag: --eviction-hard=[memory.available\u0026lt;500Mi]  节点级别的内存压力将导致系统内存不足，这将影响到整个节点及其上运行的所有 pod。节点可以暂时离线直到内存已经回收为止。 为了防止（或减少可能性）系统内存不足，kubelet 提供了资源不足管理。驱逐操作只支持 memory 和 ephemeral-storage。 通过 --eviction-hard 标志预留一些内存后，当节点上的可用内存降至保留值以下时，kubelet 将尝试驱逐 pod。 假设，如果节点上不存在系统守护进程，pod 将不能使用超过 capacity-eviction-hard 的资源。因此，为驱逐而预留的资源对 pod 是不可用的。\n执行节点分配  Kubelet Flag: --enforce-node-allocatable=pods[,][system-reserved][,][kube-reserved]  调度器将 Allocatable 按 pod 的可用 capacity 对待。\nkubelet 默认在 pod 中执行 Allocatable。无论何时，如果所有 pod 的总用量超过了 Allocatable，驱逐 pod 的措施将被执行。有关驱逐策略的更多细节可以在这里找到。请通过设置 kubelet --enforce-node-allocatable 标志值为 pods 控制这个措施。\n可选的，通过在相同标志中同时指定 kube-reserved 和 system-reserved 值能够使 kubelet 执行 kube-reserved 和 system-reserved。请注意，要想执行 kube-reserved 或者 system-reserved 时，需要分别指定 --kube-reserved-cgroup 或者 --system-reserved-cgroup。\n一般原则 系统守护进程期望被按照类似 Guaranteed pod 一样对待。系统守护进程可以在其范围控制组中爆发式增长，您需要将这个行为作为 kubernetes 部署的一部分进行管理。 例如，kubelet 应该有它自己的控制组并和容器运行时共享 Kube-reserved 资源。然而，如果执行了 kube-reserved，则 kubelet 不能突然爆发并耗尽节点的所有可用资源。\n在执行 system-reserved 预留操作时请加倍小心，因为它可能导致节点上的关键系统服务 CPU 资源短缺或因为内存不足而被终止。 建议只有当用户详尽地描述了他们的节点以得出精确的估计时才强制执行 system-reserved，并且如果该组中的任何进程都是 oom_killed，则对他们恢复的能力充满信心。\n 在 pods 上执行 Allocatable 作为开始。 一旦足够用于追踪系统守护进程的监控和告警的机制到位，请尝试基于用量探索方式执行 kube-reserved。 随着时间推进，如果绝对必要，可以执行 system-reserved。  随着时间的增长以及越来越多特性的加入，kube 系统守护进程对资源的需求可能也会增加。以后 kubernetes 项目将尝试减少对节点系统守护进程的利用，但目前那并不是优先事项。所以，请期待在将来的发布中将 Allocatable 容量降低。\n示例场景 这是一个用于说明节点分配计算方式的示例：\n 节点拥有 32Gi 内存，16 核 CPU 和 100Gi 存储 --kube-reserved 设置为 cpu=1,memory=2Gi,ephemeral-storage=1Gi --system-reserved 设置为 cpu=500m,memory=1Gi,ephemeral-storage=1Gi --eviction-hard 设置为 memory.available\u0026lt;500Mi,nodefs.available\u0026lt;10%  在这个场景下，Allocatable 将会是 14.5 CPUs、28.5Gi 内存以及 88Gi 本地存储。 调度器保证这个节点上的所有 pod 请求的内存总量不超过 28.5Gi，存储不超过 88Gi。 当 pod 的内存使用总量超过 28.5Gi 或者磁盘使用总量超过 88Gi 时，Kubelet 将会驱逐它们。如果节点上的所有进程都尽可能多的使用 CPU，则 pod 加起来不能使用超过 14.5 CPUs 的资源。\n当没有执行 kube-reserved 和/或 system-reserved 且系统守护进程使用量超过其预留时，如果节点内存用量高于 31.5Gi 或存储大于 90Gi，kubelet 将会驱逐 pod。\n可用特性 截至 Kubernetes 1.2 版本，已经可以可选的指定 kube-reserved 和 system-reserved 预留。当在相同的发布中都可用时，调度器将转为使用 Allocatable 替代 Capacity。\n截至 Kubernetes 1.6 版本，eviction-thresholds 是通过计算 Allocatable 进行考虑。要使用旧版本的行为，请设置 --experimental-allocatable-ignore-eviction kubelet 标志为 true。\n截至 Kubernetes 1.6 版本，kubelet 使用控制组在 pod 上执行 Allocatable。要使用旧版本行为，请取消设置 --enforce-node-allocatable kubelet 标志。请注意，除非 --kube-reserved 或者 --system-reserved 或者 --eviction-hard 标志没有默认参数，否则 Allocatable 的实施不会影响已经存在的 deployment。\n截至 Kubernetes 1.6 版本，kubelet 在 pod 自己的 cgroup 沙箱中启动它们，这个 cgroup 沙箱在 kubelet 管理的 cgroup 层次结构中的一个独占部分中。在从前一个版本升级 kubelet 之前，要求操作员 drain 节点，以保证 pod 及其关联的容器在 cgroup 层次结构中合适的部分中启动。\n截至 Kubernetes 1.7 版本，kubelet 支持指定 storage 为 kube-reserved 和 system-reserved 的资源。\n截至 Kubernetes 1.8 版本，对于 alpha 版本，storage 键值名称已更改为 ephemeral-storage。\n从 Kubernetes 1.17 版本开始，可以选择将 reserved-cpus 显式 cpuset 指定为操作系统守护程序、中断、计时器和 Kubernetes 守护程序保留的 CPU。\n"
},
{
	"uri": "https://lijun.in/tasks/administer-cluster/extended-resource-node/",
	"title": "为节点发布扩展资源",
	"tags": [],
	"description": "",
	"content": "本文展示了如何为节点指定扩展资源。 扩展资源允许集群管理员发布节点级别的资源，这些资源在不进行发布的情况下无法被 Kubernetes 感知。\n. heading \u0026ldquo;prerequisites\u0026rdquo; %}} . include \u0026ldquo;task-tutorial-prereqs.md\u0026rdquo; \u0026gt;}} . version-check \u0026gt;}}\n获取您的节点名称 kubectl get nodes 选择您的一个节点用于此练习。\n在您的一个节点上发布一种新的扩展资源 为在一个节点上发布一种新的扩展资源，需要发送一个 HTTP PATCH 请求到 Kubernetes API server。 例如：假设您的一个节点上带有四个 dongle 资源。下面是一个 PATCH 请求的示例， 该请求为您的节点发布四个 dongle 资源。\nPATCH /api/v1/nodes/\u0026lt;your-node-name\u0026gt;/status HTTP/1.1 Accept: application/json Content-Type: application/json-patch+json Host: k8s-master:8080 [ { \u0026#34;op\u0026#34;: \u0026#34;add\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/status/capacity/example.com~1dongle\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;4\u0026#34; } ] 注意：Kubernetes 不需要了解 dongle 资源的含义和用途。 前面的 PATCH 请求仅仅告诉 Kubernetes 您的节点拥有四个您称之为 dongle 的东西。\n启动一个代理（proxy），以便您可以很容易地向 Kubernetes API server 发送请求：\nkubectl proxy 在另一个命令窗口中，发送 HTTP PATCH 请求。 用您的节点名称替换 \u0026lt;your-node-name\u0026gt;：\ncurl --header \u0026#34;Content-Type: application/json-patch+json\u0026#34; \\ --request PATCH \\ --data \u0026#39;[{\u0026#34;op\u0026#34;: \u0026#34;add\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/status/capacity/example.com~1dongle\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;4\u0026#34;}]\u0026#39; \\ http://localhost:8001/api/v1/nodes/\u0026lt;your-node-name\u0026gt;/status . note \u0026gt;}} 在前面的请求中，~1 为 patch 路径中 “/” 符号的编码。JSON-Patch 中的操作路径值被解析为 JSON 指针。 更多细节，请查看 IETF RFC 6901 的第 3 部分。 . /note \u0026gt;}}\n输出显示该节点的 dongle 资源容量（capacity）为 4：\n\u0026quot;capacity\u0026quot;: { \u0026quot;cpu\u0026quot;: \u0026quot;2\u0026quot;, \u0026quot;memory\u0026quot;: \u0026quot;2049008Ki\u0026quot;, \u0026quot;example.com/dongle\u0026quot;: \u0026quot;4\u0026quot;, 描述您的节点：\nkubectl describe node \u0026lt;your-node-name\u0026gt; 输出再次展示了 dongle 资源：\nCapacity: cpu: 2 memory: 2049008Ki example.com/dongle: 4 现在，应用开发者可以创建请求一定数量 dongle 资源的 Pod 了。 参见将扩展资源分配给容器。\n讨论 扩展资源类似于内存和 CPU 资源。 例如，正如一个节点拥有一定数量的内存和 CPU 资源， 它们被节点上运行的所有组件共享，该节点也可以拥有一定数量的 dongle 资源， 这些资源同样被节点上运行的所有组件共享。 此外，正如应用开发者可以创建请求一定数量的内存和 CPU 资源的 Pod， 他们也可以创建请求一定数量 dongle 资源的 Pod。\n扩展资源对 Kubernetes 是不透明的。 Kubernetes 不知道扩展资源含义相关的任何信息。 Kubernetes 只了解一个节点拥有一定数量的扩展资源。 扩展资源必须以整形数量进行发布。 例如，一个节点可以发布 4 个 dongle 资源，但是不能发布 4.5 个。\n存储示例 假设一个节点拥有一种特殊类型的磁盘存储，其容量为 800 GiB。 您可以为该特殊存储创建一个名称， 如 example.com/special-storage。 然后您就可以按照一定规格的块（如 100 GiB）对其进行发布。 在这种情况下，您的节点将会通知它拥有八个 example.com/special-storage 类型的资源。\nCapacity: ... example.com/special-storage: 8 如果您想要允许针对特殊存储任意（数量）的请求，您可以按照 1 byte 大小的块来发布特殊存储。 在这种情况下，您将会发布 800Gi 数量的 example.com/special-storage 类型的资源。\nCapacity: ... example.com/special-storage: 800Gi 然后，容器就能够请求任意数量（多达 800Gi）字节的特殊存储。\n清理 这里是一个从节点移除 dongle 资源发布的 PATCH 请求。\nPATCH /api/v1/nodes/\u0026lt;your-node-name\u0026gt;/status HTTP/1.1 Accept: application/json Content-Type: application/json-patch+json Host: k8s-master:8080 [ { \u0026quot;op\u0026quot;: \u0026quot;remove\u0026quot;, \u0026quot;path\u0026quot;: \u0026quot;/status/capacity/example.com~1dongle\u0026quot;, } ] 启动一个代理，以便您可以很容易地向 Kubernetes API server 发送请求：\nkubectl proxy 在另一个命令窗口中，发送 HTTP PATCH 请求。 用您的节点名称替换 \u0026lt;your-node-name\u0026gt;：\ncurl --header \u0026#34;Content-Type: application/json-patch+json\u0026#34; \\ --request PATCH \\ --data \u0026#39;[{\u0026#34;op\u0026#34;: \u0026#34;remove\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/status/capacity/example.com~1dongle\u0026#34;}]\u0026#39; \\ http://localhost:8001/api/v1/nodes/\u0026lt;your-node-name\u0026gt;/status 验证 dongle 资源的发布已经被移除：\nkubectl describe node \u0026lt;your-node-name\u0026gt; | grep dongle . heading \u0026ldquo;whatsnext\u0026rdquo; %}} (你不应该看到任何输出)\n针对应用开发人员  将扩展资源分配给容器  针对集群管理员  为 Namespace 配置最小和最大内存约束 为 Namespace 配置最小和最大 CPU 约束  "
},
{
	"uri": "https://lijun.in/reference/access-authn-authz/abac/",
	"title": "使用 ABAC 鉴权",
	"tags": [],
	"description": "",
	"content": "基于属性的访问控制（Attribute-based access control - ABAC）定义了访问控制范例，其中通过使用将属性组合在一起的策略来向用户授予访问权限。\n策略文件格式 基于 ABAC 模式，可以这样指定策略文件 --authorization-policy-file=SOME_FILENAME。\n此文件格式是 JSON Lines，不应存在封闭的列表或映射，每行一个映射。\n每一行都是一个策略对象，策略对象是具有以下属性的映射：\n 版本控制属性：  apiVersion，字符串类型：有效值为abac.authorization.kubernetes.io/v1beta1，允许对策略格式进行版本控制和转换。 kind，字符串类型：有效值为 Policy，允许对策略格式进行版本控制和转换。   spec 配置为具有以下映射的属性：  主体匹配属性：  user，字符串类型；来自 --token-auth-file 的用户字符串，如果你指定 user，它必须与验证用户的用户名匹配。 group，字符串类型；如果指定 group，它必须与经过身份验证的用户的一个组匹配，system:authenticated匹配所有经过身份验证的请求。system:unauthenticated匹配所有未经过身份验证的请求。     资源匹配属性：  apiGroup，字符串类型；一个 API 组。  例： extensions 通配符：*匹配所有 API 组。   namespace，字符串类型；一个命名空间。  例如：kube-system 通配符：*匹配所有资源请求。   resource，字符串类型；资源类型。  例：pods 通配符：*匹配所有资源请求。     非资源匹配属性：  nonResourcePath，字符串类型；非资源请求路径。  例如：/version或 /apis 通配符：  * 匹配所有非资源请求。 /foo/* 匹配 /foo/ 的所有子路径。       readonly，键入布尔值，如果为 true，则表示该策略仅适用于 get、list 和 watch 操作。  . note \u0026gt;}}\n属性未设置等效于属性被设置为对应类型的零值( 例如空字符串、0、false)，然而，出于可读性考虑，应尽量选择不设置这类属性。\n在将来，策略可能以 JSON 格式表示，并通过 REST 界面进行管理。\n. /note \u0026gt;}}\n鉴权算法 请求具有与策略对象的属性对应的属性。\n当接收到请求时，确定属性。未知属性设置为其类型的零值（例如：空字符串，0，false）。\n设置为 \u0026quot;*\u0026quot; 的属性将匹配相应属性的任何值。\n检查属性的元组，以匹配策略文件中的每个策略。如果至少有一行匹配请求属性，则请求被鉴权（但仍可能无法通过稍后的合法性检查）。\n要允许任何经过身份验证的用户执行某些操作，请将策略组属性设置为 \u0026quot;system:authenticated\u0026quot;。\n要允许任何未经身份验证的用户执行某些操作，请将策略组属性设置为 \u0026quot;system:authentication\u0026quot;。\n要允许用户执行任何操作，请使用 apiGroup，命名空间， 资源和 nonResourcePath 属性设置为 \u0026quot;*\u0026quot; 的策略。\n要允许用户执行任何操作，请使用设置为 \u0026quot;*\u0026quot; 的 apiGroup，namespace，resource 和 nonResourcePath 属性编写策略。\nKubectl Kubectl 使用 api-server 的 /api 和 /apis 端点来发现服务资源类型，并使用位于 /openapi/v2 的模式信息来验证通过创建/更新操作发送到 API 的对象。\n当使用 ABAC 鉴权时，这些特殊资源必须显式地通过策略中的 nonResourcePath 属性暴露出来（参见下面的 示例）：\n /api，/api/*，/apis和 /apis/* 用于 API 版本协商。 /version 通过 kubectl version 检索服务器版本。 /swaggerapi/* 用于创建 / 更新操作。  要检查涉及到特定 kubectl 操作的 HTTP 调用，您可以调整详细程度： kubectl \u0026ndash;v=8 version\n例子   Alice 可以对所有资源做任何事情：\n{\u0026#34;apiVersion\u0026#34;: \u0026#34;abac.authorization.kubernetes.io/v1beta1\u0026#34;, \u0026#34;kind\u0026#34;: \u0026#34;Policy\u0026#34;, \u0026#34;spec\u0026#34;: {\u0026#34;user\u0026#34;: \u0026#34;alice\u0026#34;, \u0026#34;namespace\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;resource\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;apiGroup\u0026#34;: \u0026#34;*\u0026#34;}}   Kubelet 可以读取任何 pod：\n{\u0026#34;apiVersion\u0026#34;: \u0026#34;abac.authorization.kubernetes.io/v1beta1\u0026#34;, \u0026#34;kind\u0026#34;: \u0026#34;Policy\u0026#34;, \u0026#34;spec\u0026#34;: {\u0026#34;user\u0026#34;: \u0026#34;kubelet\u0026#34;, \u0026#34;namespace\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;resource\u0026#34;: \u0026#34;pods\u0026#34;, \u0026#34;readonly\u0026#34;: true}}   Kubelet 可以读写事件：\n{\u0026#34;apiVersion\u0026#34;: \u0026#34;abac.authorization.kubernetes.io/v1beta1\u0026#34;, \u0026#34;kind\u0026#34;: \u0026#34;Policy\u0026#34;, \u0026#34;spec\u0026#34;: {\u0026#34;user\u0026#34;: \u0026#34;kubelet\u0026#34;, \u0026#34;namespace\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;resource\u0026#34;: \u0026#34;events\u0026#34;}}    Bob 可以在命名空间 projectCaribou 中读取 pod：\n{\u0026#34;apiVersion\u0026#34;: \u0026#34;abac.authorization.kubernetes.io/v1beta1\u0026#34;, \u0026#34;kind\u0026#34;: \u0026#34;Policy\u0026#34;, \u0026#34;spec\u0026#34;: {\u0026#34;user\u0026#34;: \u0026#34;bob\u0026#34;, \u0026#34;namespace\u0026#34;: \u0026#34;projectCaribou\u0026#34;, \u0026#34;resource\u0026#34;: \u0026#34;pods\u0026#34;, \u0026#34;readonly\u0026#34;: true}}   任何人都可以对所有非资源路径进行只读请求：\n{\u0026#34;apiVersion\u0026#34;: \u0026#34;abac.authorization.kubernetes.io/v1beta1\u0026#34;, \u0026#34;kind\u0026#34;: \u0026#34;Policy\u0026#34;, \u0026#34;spec\u0026#34;: {\u0026#34;group\u0026#34;: \u0026#34;system:authenticated\u0026#34;, \u0026#34;readonly\u0026#34;: true, \u0026#34;nonResourcePath\u0026#34;: \u0026#34;*\u0026#34;}} {\u0026#34;apiVersion\u0026#34;: \u0026#34;abac.authorization.kubernetes.io/v1beta1\u0026#34;, \u0026#34;kind\u0026#34;: \u0026#34;Policy\u0026#34;, \u0026#34;spec\u0026#34;: {\u0026#34;group\u0026#34;: \u0026#34;system:unauthenticated\u0026#34;, \u0026#34;readonly\u0026#34;: true, \u0026#34;nonResourcePath\u0026#34;: \u0026#34;*\u0026#34;}}   [完整文件示例](http://releases.k8s.io/. param \u0026ldquo;githubbranch\u0026rdquo; \u0026gt;}}/pkg/auth/authorizer/abac/example_policy_file.jsonl)\n服务帐户的快速说明 服务帐户自动生成用户。用户名是根据命名约定生成的：\nsystem:serviceaccount:\u0026lt;namespace\u0026gt;:\u0026lt;serviceaccountname\u0026gt; 创建新的命名空间也会导致创建一个新的服务帐户：\nsystem:serviceaccount:\u0026lt;namespace\u0026gt;:default 例如，如果要将 API 的 kube-system 完整权限中的默认服务帐户授予，则可以将此行添加到策略文件中：\n{\u0026#34;apiVersion\u0026#34;:\u0026#34;abac.authorization.kubernetes.io/v1beta1\u0026#34;,\u0026#34;kind\u0026#34;:\u0026#34;Policy\u0026#34;,\u0026#34;spec\u0026#34;:{\u0026#34;user\u0026#34;:\u0026#34;system:serviceaccount:kube-system:default\u0026#34;,\u0026#34;namespace\u0026#34;:\u0026#34;*\u0026#34;,\u0026#34;resource\u0026#34;:\u0026#34;*\u0026#34;,\u0026#34;apiGroup\u0026#34;:\u0026#34;*\u0026#34;}} 需要重新启动 apiserver 以获取新的策略行。\n"
},
{
	"uri": "https://lijun.in/tutorials/configuration/configure-redis-using-configmap/",
	"title": "使用 ConfigMap 来配置 Redis",
	"tags": [],
	"description": "",
	"content": "这篇文档基于使用 ConfigMap 来配置 Containers 这个任务，提供了一个使用 ConfigMap 来配置 Redis 的真实案例。\n. heading \u0026ldquo;objectives\u0026rdquo; %}}    创建一个包含以下内容的 kustomization.yaml 文件： 一个 ConfigMap 生成器 一个使用 ConfigMap 的 Pod 资源配置   使用 kubectl apply -k ./ 应用整个路径的配置 验证配置已经被正确应用。  . heading \u0026ldquo;prerequisites\u0026rdquo; %}}  . include \u0026ldquo;task-tutorial-prereqs.md\u0026rdquo; \u0026gt;}} . version-check \u0026gt;}}   此页面上显示的示例适用于 kubectl 1.14和在其以上的版本。 理解使用ConfigMap来配置Containers。  真实世界的案例：使用 ConfigMap 来配置 Redis 按照下面的步骤，您可以使用ConfigMap中的数据来配置Redis缓存。\n 根据docs/user-guide/configmap/redis/redis-config来创建一个ConfigMap：  . codenew file=\u0026quot;pods/config/redis-config\u0026rdquo; \u0026gt;}}\ncurl -OL https://k8s.io/examples/pods/config/redis-config cat \u0026lt;\u0026lt;EOF \u0026gt;./kustomization.yaml configMapGenerator: - name: example-redis-config files: - redis-config EOF 将 pod 的资源配置添加到 kustomization.yaml 文件中：\n. codenew file=\u0026quot;pods/config/redis-pod.yaml\u0026rdquo; \u0026gt;}}\ncurl -OL https://raw.githubusercontent.com/kubernetes/website/master/content/en/examples/pods/config/redis-pod.yaml cat \u0026lt;\u0026lt;EOF \u0026gt;\u0026gt;./kustomization.yaml resources: - redis-pod.yaml EOF 应用整个 kustomization 文件夹以创建 ConfigMap 和 Pod 对象：\nkubectl apply -k . 使用以下命令检查创建的对象\n\u0026gt; kubectl get -k . NAME DATA AGE configmap/example-redis-config-dgh9dg555m 1 52s NAME READY STATUS RESTARTS AGE pod/redis 1/1 Running 0 52s 在示例中，配置卷挂载在 /redis-master 下。 它使用 path 将 redis-config 密钥添加到名为 redis.conf 的文件中。 因此，redis配置的文件路径为 /redis-master/redis.conf。 这是镜像将在其中查找 redis master 的配置文件的位置。\n使用 kubectl exec 进入 pod 并运行 redis-cli 工具来验证配置已正确应用：\nkubectl exec -it redis redis-cli 127.0.0.1:6379\u0026gt; CONFIG GET maxmemory 1) \u0026#34;maxmemory\u0026#34; 2) \u0026#34;2097152\u0026#34; 127.0.0.1:6379\u0026gt; CONFIG GET maxmemory-policy 1) \u0026#34;maxmemory-policy\u0026#34; 2) \u0026#34;allkeys-lru\u0026#34; 删除创建的 pod：\nkubectl delete pod redis . heading \u0026ldquo;whatsnext\u0026rdquo; %}}  了解有关 ConfigMaps的更多信息。  "
},
{
	"uri": "https://lijun.in/tasks/administer-cluster/coredns/",
	"title": "使用 CoreDNS 进行服务发现",
	"tags": [],
	"description": "",
	"content": "此页面介绍了 CoreDNS 升级过程以及如何安装 CoreDNS 而不是 kube-dns。\n. heading \u0026ldquo;prerequisites\u0026rdquo; %}} . include \u0026ldquo;task-tutorial-prereqs.md\u0026rdquo; \u0026gt;}} . version-check \u0026gt;}}\n关于 CoreDNS CoreDNS 是一个灵活可扩展的 DNS 服务器，可以作为 Kubernetes 集群 DNS。与 Kubernetes 一样，CoreDNS 项目由 . glossary_tooltip text=\u0026quot;CNCF\u0026rdquo; term_id=\u0026quot;cncf\u0026rdquo; \u0026gt;}} 托管。\n通过在现有的集群中替换 kube-dns，可以在集群中使用 CoreDNS 代替 kube-dns 部署，或者使用 kubeadm 等工具来为您部署和升级集群。\n安装 CoreDNS 有关手动部署或替换 kube-dns，请参阅 CoreDNS GitHub 工程。\n迁移到 CoreDNS 使用 kubeadm 升级现有集群 在 Kubernetes 1.10 及更高版本中，当您使用 kubeadm 升级使用 kube-dns 的集群时，您还可以迁移到 CoreDNS。 在本例中 kubeadm 将生成 CoreDNS 配置（\u0026ldquo;Corefile\u0026rdquo;）基于 kube-dns ConfigMap，保存联邦、存根域和上游名称服务器的配置。\n如果您正在从 kube-dns 迁移到 CoreDNS，请确保在升级期间将 CoreDNS 特性门设置为 true。例如，v1.11.0 升级应该是这样的:\nkubeadm upgrade apply v1.11.0 --feature-gates=CoreDNS=true 在 Kubernetes 版本 1.13 和更高版本中，CoreDNS特性门已经删除，CoreDNS 在默认情况下使用。 如果您想升级集群以使用 kube-dns，请遵循 此处 。\n在 1.11 之前的版本中，核心文件将被升级过程中创建的文件覆盖。 如果已对其进行自定义，则应保存现有的 ConfigMap。 在新的 ConfigMap 启动并运行后，您可以重新应用自定义。\n如果您在 Kubernetes 1.11 及更高版本中运行 CoreDNS，则在升级期间，将保留现有的 Corefile。\n使用 kubeadm 安装 kube-dns 而不是 CoreDNS . note \u0026gt;}}\n在 Kubernetes 1.11 中，CoreDNS 已经升级到通用可用性(GA)，并默认安装。\n. /note \u0026gt;}}\n若要在1.13之前到版本上安装 kube-dns，请将 CoreDNS 特性门值设置为 false：\nkubeadm init --feature-gates=CoreDNS=false 对于 1.13 版和更高版本，请遵循此处概述到指南。\n升级 CoreDNS 从 v1.9 起，Kubernetes 提供了 CoreDNS。 您可以在此处检查 Kubernetes 随附的 CoreDNS 版本以及对 CoreDNS 所做的更改。\n如果您只想升级 CoreDNS 或使用自己的自定义镜像，则可以手动升级 CoreDNS。\nCoreDNS 调优 当涉及到资源利用时，优化内核的配置可能是有用的。有关详细信息，请参阅 关于扩展 CoreDNS 的文档。\n. heading \u0026ldquo;whatsnext\u0026rdquo; %}} 您可以通过修改 Corefile 来配置 CoreDNS，以支持比 ku-dns 更多的用例。有关更多信息，请参考 CoreDNS 网站。\n"
},
{
	"uri": "https://lijun.in/tasks/debug-application-cluster/crictl/",
	"title": "使用 crictl 对 Kubernetes 节点进行调试",
	"tags": [],
	"description": "",
	"content": ". feature-state for_k8s_version=\u0026quot;v1.11\u0026rdquo; state=\u0026quot;stable\u0026rdquo; \u0026gt;}}\ncrictl 是 CRI 兼容的容器运行时命令行接口。 您可以使用它来检查和调试 Kubernetes 节点上的容器运行时和应用程序。 crictl和它的源代码在 cri-tools 代码库。\n. heading \u0026ldquo;prerequisites\u0026rdquo; %}} crictl 需要带有 CRI 运行时的 Linux 操作系统。\n安装 crictl 您可以从 cri-tools 发布页面下载一个压缩的 crictl 归档文件，用于几种不同的架构。 下载与您的 kubernetes 版本相对应的版本。 提取它并将其移动到系统路径上的某个位置，例如/usr/local/bin/。\n一般用法 crictl 命令有几个子命令和运行时参数。 有关详细信息，请使用 crictl help 或 crictl \u0026lt;subcommand\u0026gt; help 获取帮助信息。\ncrictl 默认连接到 unix:///var/run/dockershim.sock。 对于其他的运行时，您可以用多种不同的方法设置端点：\n 通过设置参数 --runtime-endpoint 和 --image-endpoint 通过设置环境变量 CONTAINER_RUNTIME_ENDPOINT 和 IMAGE_SERVICE_ENDPOINT 通过在配置文件中设置端点 --config=/etc/crictl.yaml  您还可以在连接到服务器并启用或禁用调试时指定超时值，方法是在配置文件中指定 timeout 或 debug 值，或者使用 --timeout 和 --debug 命令行参数。\n要查看或编辑当前配置，请查看或编辑 /etc/crictl.yaml 的内容。\ncat /etc/crictl.yaml runtime-endpoint: unix:///var/run/dockershim.sock image-endpoint: unix:///var/run/dockershim.sock timeout: 10 debug: true crictl 命令示例 . warning \u0026gt;}}\n如果使用 crictl 在正在运行的 Kubernetes 集群上创建 Pod 沙盒或容器，kubelet 最终将删除它们。 crictl不是一个通用的工作流工具，而是一个对调试有用的工具。\n. /warning \u0026gt;}}\n打印 Pod 清单 打印所有 Pod 的清单：\ncrictl pods POD ID CREATED STATE NAME NAMESPACE ATTEMPT 926f1b5a1d33a About a minute ago Ready sh-84d7dcf559-4r2gq default 0 4dccb216c4adb About a minute ago Ready nginx-65899c769f-wv2gp default 0 a86316e96fa89 17 hours ago Ready kube-proxy-gblk4 kube-system 0 919630b8f81f1 17 hours ago Ready nvidia-device-plugin-zgbbv kube-system 0 根据名称打印 Pod 清单：\ncrictl pods --name nginx-65899c769f-wv2gp POD ID CREATED STATE NAME NAMESPACE ATTEMPT 4dccb216c4adb 2 minutes ago Ready nginx-65899c769f-wv2gp default 0 根据标签打印 Pod 清单：\ncrictl pods --label run=nginx POD ID CREATED STATE NAME NAMESPACE ATTEMPT 4dccb216c4adb 2 minutes ago Ready nginx-65899c769f-wv2gp default 0 打印镜像清单 打印所有镜像清单：\ncrictl images IMAGE TAG IMAGE ID SIZE busybox latest 8c811b4aec35f 1.15MB k8s-gcrio.azureedge.net/hyperkube-amd64 v1.10.3 e179bbfe5d238 665MB k8s-gcrio.azureedge.net/pause-amd64 3.1 da86e6ba6ca19 742kB nginx latest cd5239a0906a6 109MB 根据仓库打印镜像清单：\ncrictl images nginx IMAGE TAG IMAGE ID SIZE nginx latest cd5239a0906a6 109MB 只打印镜像 ID：\ncrictl images -q sha256:8c811b4aec35f259572d0f79207bc0678df4c736eeec50bc9fec37ed936a472a sha256:e179bbfe5d238de6069f3b03fccbecc3fb4f2019af741bfff1233c4d7b2970c5 sha256:da86e6ba6ca197bf6bc5e9d900febd906b133eaa4750e6bed647b0fbe50ed43e sha256:cd5239a0906a6ccf0562354852fae04bc5b52d72a2aff9a871ddb6bd57553569 打印容器清单 打印所有容器清单：\ncrictl ps -a CONTAINER ID IMAGE CREATED STATE NAME ATTEMPT 1f73f2d81bf98 busybox@sha256:141c253bc4c3fd0a201d32dc1f493bcf3fff003b6df416dea4f41046e0f37d47 7 minutes ago Running sh 1 9c5951df22c78 busybox@sha256:141c253bc4c3fd0a201d32dc1f493bcf3fff003b6df416dea4f41046e0f37d47 8 minutes ago Exited sh 0 87d3992f84f74 nginx@sha256:d0a8828cccb73397acb0073bf34f4d7d8aa315263f1e7806bf8c55d8ac139d5f 8 minutes ago Running nginx 0 1941fb4da154f k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:00d814b1f7763f4ab5be80c58e98140dfc69df107f253d7fdd714b30a714260a 18 hours ago Running kube-proxy 0 打印正在运行的容器清单：\ncrictl ps CONTAINER ID IMAGE CREATED STATE NAME ATTEMPT 1f73f2d81bf98 busybox@sha256:141c253bc4c3fd0a201d32dc1f493bcf3fff003b6df416dea4f41046e0f37d47 6 minutes ago Running sh 1 87d3992f84f74 nginx@sha256:d0a8828cccb73397acb0073bf34f4d7d8aa315263f1e7806bf8c55d8ac139d5f 7 minutes ago Running nginx 0 1941fb4da154f k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:00d814b1f7763f4ab5be80c58e98140dfc69df107f253d7fdd714b30a714260a 17 hours ago Running kube-proxy 0 在正在运行的容器上执行命令 crictl exec -i -t 1f73f2d81bf98 ls bin dev etc home proc root sys tmp usr var 获取容器日志 获取容器的所有日志：\ncrictl logs 87d3992f84f74 10.240.0.96 - - [06/Jun/2018:02:45:49 +0000] \u0026quot;GET / HTTP/1.1\u0026quot; 200 612 \u0026quot;-\u0026quot; \u0026quot;curl/7.47.0\u0026quot; \u0026quot;-\u0026quot; 10.240.0.96 - - [06/Jun/2018:02:45:50 +0000] \u0026quot;GET / HTTP/1.1\u0026quot; 200 612 \u0026quot;-\u0026quot; \u0026quot;curl/7.47.0\u0026quot; \u0026quot;-\u0026quot; 10.240.0.96 - - [06/Jun/2018:02:45:51 +0000] \u0026quot;GET / HTTP/1.1\u0026quot; 200 612 \u0026quot;-\u0026quot; \u0026quot;curl/7.47.0\u0026quot; \u0026quot;-\u0026quot; 获取最近的 N 行日志：\ncrictl logs --tail=1 87d3992f84f74 10.240.0.96 - - [06/Jun/2018:02:45:51 +0000] \u0026quot;GET / HTTP/1.1\u0026quot; 200 612 \u0026quot;-\u0026quot; \u0026quot;curl/7.47.0\u0026quot; \u0026quot;-\u0026quot; 运行 Pod 沙盒 用 crictl 运行 Pod 沙盒对容器运行时排错很有帮助。 在运行的 Kubernetes 集群中，沙盒会随机地被 kubelet 停止和删除。\n  编写下面的 JSON 文件：\n{ \u0026#34;metadata\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;nginx-sandbox\u0026#34;, \u0026#34;namespace\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;attempt\u0026#34;: 1, \u0026#34;uid\u0026#34;: \u0026#34;hdishd83djaidwnduwk28bcsb\u0026#34; }, \u0026#34;logDirectory\u0026#34;: \u0026#34;/tmp\u0026#34;, \u0026#34;linux\u0026#34;: { } }   使用 crictl runp 命令应用 JSON 文件并运行沙盒。\ncrictl runp pod-config.json 返回了沙盒的 ID。\n  创建容器 用 crictl 创建容器对容器运行时排错很有帮助。 在运行的 Kubernetes 集群中，沙盒会随机的被 kubelet 停止和删除。\n  拉取 busybox 镜像\ncrictl pull busybox Image is up to date for busybox@sha256:141c253bc4c3fd0a201d32dc1f493bcf3fff003b6df416dea4f41046e0f37d47   创建 Pod 和容器的配置：\nPod 配置：\n{ \u0026#34;metadata\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;nginx-sandbox\u0026#34;, \u0026#34;namespace\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;attempt\u0026#34;: 1, \u0026#34;uid\u0026#34;: \u0026#34;hdishd83djaidwnduwk28bcsb\u0026#34; }, \u0026#34;log_directory\u0026#34;: \u0026#34;/tmp\u0026#34;, \u0026#34;linux\u0026#34;: { } } 容器配置：\n{ \u0026#34;metadata\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;busybox\u0026#34; }, \u0026#34;image\u0026#34;:{ \u0026#34;image\u0026#34;: \u0026#34;busybox\u0026#34; }, \u0026#34;command\u0026#34;: [ \u0026#34;top\u0026#34; ], \u0026#34;log_path\u0026#34;:\u0026#34;busybox.log\u0026#34;, \u0026#34;linux\u0026#34;: { } }   创建容器，传递先前创建的 Pod 的 ID、容器配置文件和 Pod 配置文件。返回容器的 ID。\ncrictl create f84dd361f8dc51518ed291fbadd6db537b0496536c1d2d6c05ff943ce8c9a54f container-config.json pod-config.json   查询所有容器并确认新创建的容器状态为 Created。\ncrictl ps -a CONTAINER ID IMAGE CREATED STATE NAME ATTEMPT 3e025dd50a72d busybox 32 seconds ago Created busybox 0   启动容器 要启动容器，要将容器 ID 传给 crictl start：\ncrictl start 3e025dd50a72d956c4f14881fbb5b1080c9275674e95fb67f965f6478a957d60 3e025dd50a72d956c4f14881fbb5b1080c9275674e95fb67f965f6478a957d60 确认容器的状态为 Running。\ncrictl ps CONTAINER ID IMAGE CREATED STATE NAME ATTEMPT 3e025dd50a72d busybox About a minute ago Running busybox 0 更多信息请参考 kubernetes-incubator/cri-tools。\n"
},
{
	"uri": "https://lijun.in/tasks/debug-application-cluster/logging-elasticsearch-kibana/",
	"title": "使用 ElasticSearch 和 Kibana 进行日志管理",
	"tags": [],
	"description": "",
	"content": "在 Google Compute Engine (GCE) 平台上，默认的日志管理支持目标是 Stackdriver Logging，在 使用 Stackdriver Logging 管理日志中详细描述了这一点。\n本文介绍了如何设置一个集群，将日志导入Elasticsearch，并使用 Kibana 查看日志，作为在 GCE 上运行应用时使用 Stackdriver Logging 管理日志的替代方案。\n. note \u0026gt;}}\n您不能在 Google Kubernetes Engine 平台运行的 Kubernetes 集群上自动的部署 Elasticsearch 和 Kibana。您必须手动部署它们。 . /note \u0026gt;}}\n要使用 Elasticsearch 和 Kibana 处理集群日志，您应该在使用 kube-up.sh 脚本创建集群时设置下面所示的环境变量：\nKUBE_LOGGING_DESTINATION=elasticsearch 您还应该确保设置了 KUBE_ENABLE_NODE_LOGGING=true （这是 GCE 平台的默认设置）。\n现在，当您创建集群时，将有一条消息将指示每个节点上运行的 Fluentd 日志收集守护进程以 ElasticSearch 为日志输出目标：\n$ cluster/kube-up.sh ... Project: kubernetes-satnam Zone: us-central1-b ... calling kube-up Project: kubernetes-satnam Zone: us-central1-b +++ Staging server tars to Google Storage: gs://kubernetes-staging-e6d0e81793/devel +++ kubernetes-server-linux-amd64.tar.gz uploaded (sha1 = 6987c098277871b6d69623141276924ab687f89d) +++ kubernetes-salt.tar.gz uploaded (sha1 = bdfc83ed6b60fa9e3bff9004b542cfc643464cd0) Looking for already existing resources Starting master and configuring firewalls Created [https://www.googleapis.com/compute/v1/projects/kubernetes-satnam/zones/us-central1-b/disks/kubernetes-master-pd]. NAME ZONE SIZE_GB TYPE STATUS kubernetes-master-pd us-central1-b 20 pd-ssd READY Created [https://www.googleapis.com/compute/v1/projects/kubernetes-satnam/regions/us-central1/addresses/kubernetes-master-ip]. +++ Logging using Fluentd to elasticsearch 每个节点的 Fluentd pod、Elasticsearch pod 和 Kibana pod 都应该在集群启动后不久运行在 kube-system 命名空间中。\n$ kubectl get pods --namespace=kube-system NAME READY STATUS RESTARTS AGE elasticsearch-logging-v1-78nog 1/1 Running 0 2h elasticsearch-logging-v1-nj2nb 1/1 Running 0 2h fluentd-elasticsearch-kubernetes-node-5oq0 1/1 Running 0 2h fluentd-elasticsearch-kubernetes-node-6896 1/1 Running 0 2h fluentd-elasticsearch-kubernetes-node-l1ds 1/1 Running 0 2h fluentd-elasticsearch-kubernetes-node-lz9j 1/1 Running 0 2h kibana-logging-v1-bhpo8 1/1 Running 0 2h kube-dns-v3-7r1l9 3/3 Running 0 2h monitoring-heapster-v4-yl332 1/1 Running 1 2h monitoring-influx-grafana-v1-o79xf 2/2 Running 0 2h fluentd-elasticsearch pod 从每个节点收集日志并将其发送到 elasticsearch-logging pods，该 pod 是名为 elasticsearch-logging 的服务的一部分。 这些 ElasticSearch pod 存储日志，并通过 REST API 将其公开。 kibana-logging pod 提供了一个用于读取 ElasticSearch 中存储的日志的 Web UI，它是名为 kibana-logging 的服务的一部分。\nElasticsearch 和 Kibana 服务都位于 kube-system 命名空间中，并且没有通过可公开访问的 IP 地址直接暴露。 要访问它们，请参照访问集群中运行的服务的说明进行操作。\n如果你想在浏览器中访问 elasticsearch-logging 服务，你将看到类似下面的状态页面：\n现在你可以直接在浏览器中输入 Elasticsearch 查询，如果你愿意的话。 请参考 Elasticsearch 的文档 以了解这样做的更多细节。\n或者，您可以使用 Kibana 查看集群的日志（再次使用访问集群中运行的服务的说明）。 第一次访问 Kibana URL 时，将显示一个页面，要求您配置所接收日志的视图。 选择时间序列值的选项，然后选择 @timestamp。 在下面的页面中选择 Discover 选项卡，然后您应该能够看到所摄取的日志。 您可以将刷新间隔设置为 5 秒，以便定期刷新日志。\n以下是从 Kibana 查看器中摄取日志的典型视图：\n. heading \u0026ldquo;whatsnext\u0026rdquo; %}} Kibana 为浏览您的日志提供了各种强大的选项！有关如何深入研究它的一些想法，请查看 Kibana 的文档。\n"
},
{
	"uri": "https://lijun.in/tasks/debug-application-cluster/falco/",
	"title": "使用 Falco 审计",
	"tags": [],
	"description": "",
	"content": "使用 Falco 采集审计事件 Falco是一个开源项目，用于为云原生平台提供入侵和异常检测。本节介绍如何设置 Falco、如何将审计事件发送到 Falco 公开的 Kubernetes Audit 端点、以及 Falco 如何应用一组规则来自动检测可疑行为。\n安装 Falco 使用以下方法安装 Falco ：\n [独立安装 Falco][falco_installation] [Kubernetes DaemonSet][falco_installation] [Falco Helm Chart][falco_helm_chart]  安装完成 Falco 后，请确保将其配置为公开 Audit Webhook。为此，请使用以下配置：\nwebserver: enabled: true listen_port: 8765 k8s_audit_endpoint: /k8s_audit ssl_enabled: false ssl_certificate: /etc/falco/falco.pem 此配置通常位于 /etc/falco/falco.yaml 文件中。如果 Falco 作为 Kubernetes DaemonSet 安装，请编辑 falco-config ConfigMap 并添加此配置。\n配置 Kubernetes 审计   为 kube-apiserver webhook 审计后端创建一个kubeconfig文件。\n cat \u0026lt;\u0026lt;EOF \u0026gt; /etc/kubernetes/audit-webhook-kubeconfig apiVersion: v1 kind: Config clusters: - cluster: server: http://\u0026lt;ip_of_falco\u0026gt;:8765/k8s_audit name: falco contexts: - context: cluster: falco user: \u0026quot;\u0026quot; name: default-context current-context: default-context preferences: {} users: [] EOF     使用以下选项启动 kube-apiserver：\n--audit-policy-file=/etc/kubernetes/audit-policy.yaml --audit-webhook-config-file=/etc/kubernetes/audit-webhook-kubeconfig   审计规则 专门用于 Kubernetes 审计事件的规则可以在 [k8s_audit_rules.yaml][falco_k8s_audit_rules] 中找到。如果审计规则是作为本机软件包安装或使用官方 Docker 镜像安装的，则 Falco 会将规则文件复制到 /etc/falco/ 中以便使用。\n共有三类规则。\n第一类规则用于查找可疑或异常活动，例如：\n-未经授权或匿名用户的任何活动。 -创建使用未知或不允许的镜像的 pod。 -创建特权 Pod，从主机安装敏感文件系统的 Pod 或使用主机网络的 Pod。 -创建 NodePort 服务。 -创建包含私有证书（例如密码和云提供商 secrets ）的 ConfigMap。 -在正在运行的 Pod 上附加或执行命令。 -在一组允许的名称空间之外创建一个名称空间。 -在 kube-system 或 kube-public 命名空间中创建 pod 或服务帐户。 -尝试修改或删除系统 ClusterRole。 -创建一个 ClusterRoleBinding 到 cluster-admin 角色。 -创建 ClusterRole 时在动词或资源中使用通配符。 例如，过度赋权。 -创建具有写权限的 ClusterRole 或可以在 Pod 上执行命令的 ClusterRole。\n第二类规则跟踪正在创建或销毁的资源，包括：\n Deployments Services ConfigMaps Namespaces Service accounts Role/ClusterRoles Role/ClusterRoleBindings  最后一类规则仅负责显示 Falco 收到的所有审核事件。默认情况下，此规则是禁用的，因为它可能会很吵。\n有关更多详细信息，请参阅 Falco 文档中的[Kubernetes审计事件][falco_ka_docs]。\n[auditing-api]: https://github.com/kubernetes/kubernetes/blob/. param \u0026ldquo;githubbranch\u0026rdquo; \u0026gt;}}/staging/src/k8s.io/apiserver/pkg/apis/audit/v1/types.go [gce-audit-profile]: https://github.com/kubernetes/kubernetes/blob/. param \u0026ldquo;githubbranch\u0026rdquo; \u0026gt;}}/cluster/gce/gci/configure-helper.sh#L735 [kubeconfig]: /docs/tasks/access-application-cluster/configure-access-multiple-clusters/ [fluentd]: http://www.fluentd.org/ [fluentd_install_doc]: https://docs.fluentd.org/v1.0/articles/quickstart#step-1:-installing-fluentd [fluentd_plugin_management_doc]: https://docs.fluentd.org/v1.0/articles/plugin-management [logstash]: https://www.elastic.co/products/logstash [logstash_install_doc]: https://www.elastic.co/guide/en/logstash/current/installing-logstash.html [kube-aggregator]: /docs/concepts/api-extension/apiserver-aggregation [falco_website]: https://www.falco.org [falco_k8s_audit_rules]: https://github.com/falcosecurity/falco/blob/master/rules/k8s_audit_rules.yaml [falco_ka_docs]: https://falco.org/docs/event-sources/kubernetes-audit [falco_installation]: https://falco.org/docs/installation [falco_helm_chart]: https://github.com/helm/charts/tree/master/stable/falco\n"
},
{
	"uri": "https://lijun.in/tasks/service-catalog/install-service-catalog-using-helm/",
	"title": "使用 Helm 安装 Service Catalog",
	"tags": [],
	"description": "",
	"content": ". glossary_definition term_id=\u0026quot;service-catalog\u0026rdquo; length=\u0026quot;all\u0026rdquo; prepend=\u0026quot;Service Catalog is\u0026rdquo; \u0026gt;}}\n使用 Helm 在 Kubernetes 集群上安装 Service Catalog。 要获取有关此过程的最新信息，请浏览 kubernetes-incubator/service-catalog 仓库。\n. heading \u0026ldquo;prerequisites\u0026rdquo; %}}  理解 Service Catalog 的关键概念。 Service Catalog 需要 Kubernetes 集群版本在 1.7 或更高版本。 您必须启用 Kubernetes 集群的 DNS 功能。  如果使用基于云的 Kubernetes 集群或 . glossary_tooltip text=\u0026quot;Minikube\u0026rdquo; term_id=\u0026quot;minikube\u0026rdquo; \u0026gt;}}，则可能已经启用了集群 DNS。 如果您正在使用 hack/local-up-cluster.sh，请确保设置了 KUBE_ENABLE_CLUSTER_DNS 环境变量，然后运行安装脚本。   安装和设置 v1.7 或更高版本的 kubectl，确保将其配置为连接到 Kubernetes 集群。 安装 v2.7.0 或更高版本的 Helm。  遵照 Helm 安装说明。 如果已经安装了适当版本的 Helm，请执行 helm init 来安装 Helm 的服务器端组件 Tiller。    添加 service-catalog Helm 仓库 安装 Helm 后，通过执行以下命令将 service-catalog Helm 存储库添加到本地计算机：\nhelm repo add svc-cat https://svc-catalog-charts.storage.googleapis.com 通过执行以下命令进行检查，以确保安装成功：\nhelm search service-catalog 如果安装成功，该命令应输出以下内容：\nNAME VERSION DESCRIPTION svc-cat/catalog 0.0.1 service-catalog API server and controller-manag... 启用 RBAC 您的 Kubernetes 集群必须启用 RBAC，这需要您的 Tiller Pod 具有 cluster-admin 访问权限。\n如果您使用的是 Minikube，请使用以下参数运行 minikube start 命令：\nminikube start --extra-config=apiserver.Authorization.Mode=RBAC 如果您使用 hack/local-up-cluster.sh，请使用以下值设置 AUTHORIZATION_MODE 环境变量：\nAUTHORIZATION_MODE=Node,RBAC hack/local-up-cluster.sh -O 默认情况下，helm init 将 Tiller Pod 安装到 kube-system 命名空间，Tiller 配置为使用 default 服务帐户。\n. note \u0026gt;}}\n如果在运行 helm init 时使用了 --tiller-namespace 或 --service-account 参数，则需要调整以下命令中的 --serviceaccount 参数以引用相应的 namespace 和 ServiceAccount 名称。 . /note \u0026gt;}}\n配置 Tiller 以获得 cluster-admin 访问权限：\nkubectl create clusterrolebinding tiller-cluster-admin \\  --clusterrole=cluster-admin \\  --serviceaccount=kube-system:default 在 Kubernetes 集群中安装 Service Catalog 使用以下命令从 Helm 存储库的根目录安装 Service Catalog：\n. tabs name=\u0026quot;helm-versions\u0026rdquo; \u0026gt;}} . tab name=\u0026quot;Helm version 3\u0026rdquo; %}}\nhelm install catalog svc-cat/catalog --namespace catalog . /tab %}} . tab name=\u0026quot;Helm version 2\u0026rdquo; %}}\nhelm install svc-cat/catalog --name catalog --namespace catalog . heading \u0026ldquo;whatsnext\u0026rdquo; %}}  查看示例服务代理。 探索 kubernetes-incubator/service-catalog 项目。  "
},
{
	"uri": "https://lijun.in/setup/production-environment/turnkey/icp/",
	"title": "使用 IBM Cloud Private 在多个云上运行 Kubernetes",
	"tags": [],
	"description": "",
	"content": "IBM® Cloud Private 是一个 一站式云解决方案并且是一个本地的一站式云解决方案。 IBM Cloud Private 提供纯上游 Kubernetes，以及运行实际企业工作负载所需的典型管理组件。这些工作负载包括健康管理、日志管理、审计跟踪以及用于跟踪平台上工作负载使用情况的计量。\nIBM Cloud Private 提供了社区版和全支持的企业版。可从 Docker Hub 免费获得社区版本。企业版支持高可用性拓扑，并包括 IBM 对 Kubernetes 和 IBM Cloud Private 管理平台的商业支持。如果您想尝试 IBM Cloud Private，您可以使用托管试用版、教程或自我指导演示。您也可以尝试免费的社区版。有关详细信息，请参阅 IBM Cloud Private 入门。\n有关更多信息，请浏览以下资源：\n IBM Cloud Private IBM Cloud Private 参考架构 IBM Cloud Private 文档  IBM Cloud Private 和 Terraform 您可以利用一下模块使用 Terraform 部署 IBM Cloud Private：\n AWS：将 IBM Cloud Private 部署到 AWS Azure：将 IBM Cloud Private 部署到 Azure IBM Cloud：将 IBM Cloud Private 集群部署到 IBM Cloud OpenStack：将IBM Cloud Private 部署到 OpenStack Terraform 模块：在任何支持的基础架构供应商上部署 IBM Cloud Private VMware：将 IBM Cloud Private 部署到 VMware  AWS 上的 IBM Cloud Private 您可以使用 AWS CloudFormation 或 Terraform 在 Amazon Web Services（AWS）上部署 IBM Cloud Private 集群。\nIBM Cloud Private 快速入门可以自动将 IBM Cloud Private 部署到 AWS Cloud 上的新虚拟私有云（VPC）中。常规部署大约需要60分钟，而高可用性（HA）部署大约需要75分钟。快速入门包括 AWS CloudFormation 模板和部署指南。\n这个快速入门适用于希望探索应用程序现代化并希望通过使用 IBM Cloud Private 和 IBM 工具加速实现其数字化转换目标的用户。快速入门可帮助用户在 AWS 上快速部署高可用性（HA）、生产级的 IBM Cloud Private 参考架构。有关所有详细信息和部署指南，请参阅 IBM Cloud Private 在 AWS 上的快速入门 。\nIBM Cloud Private 也可以通过使用 Terraform 在 AWS 云平台上运行。要在 AWS EC2 环境中部署 IBM Cloud Private，请参阅在 AWS 上安装 IBM Cloud Private。\nAzure 上的 IBM Cloud Private 您可以启用 Microsoft Azure 作为 IBM Cloud Private 部署的云提供者，并利用 Azure 公共云上的所有 IBM Cloud Private 功能。有关更多信息，请参阅 Azure 上的 IBM Cloud Private。\n带有 Red Hat OpenShift 的 IBM Cloud Private 您可以将在 IBM Cloud Private 上运行的 IBM 认证的软件容器部署到 Red Hat OpenShift 上。\n整合能力：\n 在仅脱机安装模式下支持 Linux®64 位平台 单主控节点配置 集成的 IBM Cloud Private 集群管理控制台和目录 集成的核心平台服务，例如监控、计量和日志 IBM Cloud Private 使用 OpenShift 镜像仓库  有关更多信息，请参阅 OpenShift 上的 IBM Cloud Private。\nVirtualBox 上的 IBM Cloud Private 要将 IBM Cloud Private 安装到 VirtualBox 环境，请参阅在 VirtualBox 上安装 IBM Cloud Private。\nVMware 上的 IBM Cloud Private 您可以使用 Ubuntu 或 RHEL 镜像在 VMware 上安装 IBM Cloud Private。有关详细信息，请参见以下项目：\n 使用 Ubuntu 安装IBM Cloud Private 使用 Red Hat Enterprise 安装 IBM Cloud Private  IBM Cloud Private Hosted 服务会自动在您的 VMware vCenter Server 实例上部署 IBM Cloud Private Hosted。此服务将微服务和容器的功能带到 IBM Cloud上的VMware 环境中。使用此服务，您可以将同样熟悉的 VMware 和 IBM Cloud Private 操作模型和工具从本地扩展到 IBM Cloud。\n有关更多信息，请参阅 IBM Cloud Private Hosted 服务。\n"
},
{
	"uri": "https://lijun.in/tasks/administer-cluster/kms-provider/",
	"title": "使用 KMS 提供商进行数据加密",
	"tags": [],
	"description": "",
	"content": "本页展示了如何配置秘钥管理服务—— Key Management Service (KMS) 提供商和插件以启用数据加密。\n. heading \u0026ldquo;prerequisites\u0026rdquo; %}}  . include \u0026ldquo;task-tutorial-prereqs.md\u0026rdquo; \u0026gt;}} . version-check \u0026gt;}}   需要 Kubernetes 1.10.0 或更新版本   需要 etcd v3 或更新版本  . feature-state for_k8s_version=\u0026quot;v1.12\u0026rdquo; state=\u0026quot;beta\u0026rdquo; \u0026gt;}}\nKMS 加密提供商使用封套加密模型来加密 etcd 中的数据。数据使用数据加密秘钥（DEK）加密；每次加密都生成一个新的 DEK。这些 DEK 经一个秘钥加密秘钥（KEK）加密后在一个远端的 KMS 中存储和管理。KMS 提供商使用 gRPC 与一个特定的 KMS 插件通信。这个 KMS 插件作为一个 gRPC 服务器被部署在 Kubernetes 主服务器的同一个主机上，负责与远端 KMS 的通信。\n配置 KMS 提供商 为了在 API 服务器上配置 KMS 提供商，在加密配置文件中的提供商数组中加入一个类型为 kms 的提供商，并设置下列属性：\n name: KMS 插件的显示名称。 endpoint: gRPC 服务器（KMS 插件）的监听地址。该端点是一个 UNIX 的套接字。 cachesize: 以明文缓存的数据加密秘钥（DEKs）的数量。一旦被缓存，就可以直接使用 DEKs 而无需另外调用 KMS；而未被缓存的 DEKs 需要调用一次 KMS 才能解包。 timeout: 在返回一个错误之前，kube-apiserver 等待 kms-plugin 响应的时间（默认是 3 秒）。  参见 理解静态数据加密配置\n实现 KMS 插件 为实现一个 KMS 插件，您可以开发一个新的插件 gRPC 服务器或启用一个由您的云服务提供商提供的 KMS 插件。您可以将这个插件与远程 KMS 集成，并把它部署到 Kubernetes 的主服务器上。\n启用由云服务提供商支持的 KMS 有关启用云服务提供商特定的 KMS 插件的说明，请咨询您的云服务提供商。\n开发 KMS 插件 gRPC 服务器 您可以使用 Go 语言的存根文件开发 KMS 插件 gRPC 服务器。对于其他语言，您可以用 proto 文件创建可以用于开发 gRPC 服务器代码的存根文件。\n 使用 Go：使用存根文件 service.pb.go 中的函数和数据结构开发 gRPC 服务器代码。   使用 Go 以外的其他语言：用 protoc 编译器编译 proto 文件： service.proto 为指定语言生成存根文件。  然后使用存根文件中的函数和数据结构开发服务器代码。\n注意：\n kms 插件版本：v1beta1  作为对过程调用 Version 的响应，兼容的 KMS 插件应把 v1beta1 作为 VersionResponse.version 返回\n 消息版本：v1beta1  所有来自 KMS 提供商的消息都把 version 字段设置为当前版本 v1beta1\n 协议：UNIX 域套接字 (unix)  gRPC 服务器应监听 UNIX 域套接字\n将 KMS 插件与远程 KMS 整合 KMS 插件可以用任何受 KMS 支持的协议与远程 KMS 通信。 所有的配置数据，包括 KMS 插件用于与远程 KMS 通信的认证凭据，都由 KMS 插件独立地存储和管理。KMS 插件可以用额外的元数据对密文进行编码，这些元数据是在把它发往 KMS 进行解密之前可能要用到的。\n部署 KMS 插件 确保 KMS 插件与 Kubernetes 主服务器运行在同一主机上。\n使用 KMS 提供商加密数据 为了加密数据：\n 使用 kms 提供商的相应的属性创建一个新的加密配置文件：  kind: EncryptionConfiguration apiVersion: apiserver.config.k8s.io/v1 resources: - resources: - secrets providers: - kms: name: myKmsPlugin endpoint: unix:///tmp/socketfile.sock cachesize: 100 timeout: 3s - identity: {} 设置 kube-apiserver 的 --encryption-provider-config 参数指向配置文件的位置。  重启 API 服务器。  注意： 在 1.13 之前的加密功能的 alpha 版本需要一个带有 kind: EncryptionConfig 和 apiVersion: v1 的配置文件，并使用 --experimental-encryption-provider-config 标志。\n验证数据是否已加密 写入 etcd 时数据被加密。重启 kube-apiserver 后，任何新建或更新的秘密信息在存储时应该已被加密。要验证这点，您可以用 etcdctl 命令行程序获取秘密信息内容。\n 在默认的命名空间里创建一个名为 secret1 的秘密信息：  kubectl create secret generic secret1 -n default --from-literal=mykey=mydata 用 etcdctl 命令行，从 etcd 读取出秘密信息：  ETCDCTL_API=3 etcdctl get /kubernetes.io/secrets/default/secret1 [...] | hexdump -C 其中 [...] 是用于连接 etcd 服务器的额外参数。\n验证保存的秘密信息是否是以 k8s:enc:kms:v1: 开头的，这表明 kms 提供商已经对结果数据加密。  验证秘密信息在被 API 获取时已被正确解密：  kubectl describe secret secret1 -n default 应该符合 mykey: mydata 格式\n确保所有秘密信息都已被加密 因为秘密信息是在写入时被加密的，所以在更新秘密信息时会加密该内容。\n下列命令读取所有秘密信息并更新他们以便应用服务器端加密。如果因为写入冲突导致错误发生，请重试此命令。对较大的集群，您可能希望根据命名空间或脚本更新去细分秘密内容。\nkubectl get secrets --all-namespaces -o json | kubectl replace -f - 从本地加密提供商切换到 KMS 提供商 为了从本地加密提供商切换到 kms 提供商并重新加密所有秘密内容：\n 在配置文件中加入 kms 提供商作为第一个条目，如下列样例所示  kind: EncryptionConfiguration apiVersion: apiserver.config.k8s.io/v1 resources: - resources: - secrets providers: - kms: name : myKmsPlugin endpoint: unix:///tmp/socketfile.sock cachesize: 100 - aescbc: keys: - name: key1 secret: \u0026lt;BASE 64 ENCODED SECRET\u0026gt; 重启所有 kube-apiserver 进程。  运行下列命令使用 kms 提供商强制重新加密所有秘密信息。  kubectl get secrets --all-namespaces -o json| kubectl replace -f - 禁用静态数据加密 要禁用静态数据加密：\n 将 identity 提供商作为配置文件中的第一个条目：  kind: EncryptionConfiguration apiVersion: apiserver.config.k8s.io/v1 resources: - resources: - secrets providers: - identity: {} - kms: name : myKmsPlugin endpoint: unix:///tmp/socketfile.sock cachesize: 100 重启所有 kube-apiserver 进程。  运行下列命令强制重新加密所有秘密信息。  kubectl get secrets --all-namespaces -o json | kubectl replace -f - "
},
{
	"uri": "https://lijun.in/tasks/administer-cluster/kubeadm/kubeadm-certs/",
	"title": "使用 kubeadm 进行证书管理",
	"tags": [],
	"description": "",
	"content": ". feature-state for_k8s_version=\u0026quot;v1.15\u0026rdquo; state=\u0026quot;stable\u0026rdquo; \u0026gt;}}\n由 kubeadm 生成的客户端证书在 1 年后到期。 本页说明如何使用 kubeadm 管理证书续订。\n. heading \u0026ldquo;prerequisites\u0026rdquo; %}} 熟悉 Kubernetes 中的 PKI证书和要求。\n您应该熟悉Kubernetes 中的 PKI 证书和要求。\n检查证书是否过期 check-expiration 能被用来检查证书是否过期\nkubeadm alpha certs check-expiration 输出类似于以下内容：\nCERTIFICATE EXPIRES RESIDUAL TIME EXTERNALLY MANAGED admin.conf May 15, 2020 13:03 UTC 364d false apiserver May 15, 2020 13:00 UTC 364d false apiserver-etcd-client May 15, 2020 13:00 UTC 364d false apiserver-kubelet-client May 15, 2020 13:00 UTC 364d false controller-manager.conf May 15, 2020 13:03 UTC 364d false etcd-healthcheck-client May 15, 2020 13:00 UTC 364d false etcd-peer May 15, 2020 13:00 UTC 364d false etcd-server May 15, 2020 13:00 UTC 364d false front-proxy-client May 15, 2020 13:00 UTC 364d false scheduler.conf May 15, 2020 13:03 UTC 364d false 该命令显示 /etc/kubernetes/pki 文件夹中的客户端证书以及 kubeadm 使用的 KUBECONFIG 文件中嵌入的客户端证书的到期时间/剩余时间。\n另外， kubeadm 会通知用户证书是否由外部管理； 在这种情况下，用户应该小心的手动/使用其他工具来管理证书更新。\n. warning \u0026gt;}}\n. /warning \u0026gt;}}\n. note \u0026gt;}}\n. /note \u0026gt;}}\n自动更新证书 kubeadm 会在控制面板升级的时候更新所有证书\n这个功能旨在解决最简单的用例；如果您对此类证书的更新没有特殊要求，并且定期执行 Kubernetes 版本升级（每次升级之间的间隔时间少于 1 年），则 kubeadm 将确保您的集群保持最新状态并保持合理的安全性。\n. note \u0026gt;}}\n. /note \u0026gt;}}\n如果您对证书更新有更复杂的需求，则可通过将 --certificate-renewal=false 传递给 kubeadm upgrade apply 或者 kubeadm upgrade node ，从而选择不采用默认行为。\n手动更新证书 您能随时通过 kubeadm alpha certs renew 命令手动更新您的证书。\n这个命令用 CA （或者 front-proxy-CA ）证书和存储在 /etc/kubernetes/pki 中的密钥执行更新。\n. warning \u0026gt;}}\n. /warning \u0026gt;}}\n. note \u0026gt;}}\nalpha certs renew 使用现有的证书作为属性 (Common Name、Organization、SAN 等) 的权威来源，而不是 kubeadm-config ConfigMap 。强烈建议使它们保持同步。 . /note \u0026gt;}}\nkubeadm alpha certs renew 提供下列选项\nKubernetes 证书通常在一年后到期。\n --csr-only 可用于经过一个外部 CA 生成的证书签名请求来更新证书（无需实际替换更新证书）；更多信息请参见下一段。   也可以更新单个证书而不是全部证书。  用 Kubernetes 证书 API 更新证书 本节提供有关如何使用 Kubernetes 证书 API 执行手动证书更新的更多详细信息。\n. caution \u0026gt;}}\n这些是针对需要将其组织的证书基础结构集成到 kubeadm 构建的集群中的用户的高级主题。如果默认的 kubeadm 配置满足了您的需求，则应让 kubeadm 管理证书。 . /caution \u0026gt;}}\n设置一个签名者 Kubernetes 证书颁发机构不是开箱即用。 您可以配置外部签名者，例如 cert-manager ，也可以使用内置签名者。 内置签名者是 kube-controller-manager 的一部分。 要激活内置签名者，请传递--cluster-signing-cert-file 和 --cluster-signing-key-file参数。\n这个内置签名者是 kube-controller-manager 的一部分。\n要激活内置签名者，必须传递 --cluster-signing-cert-file 和 --cluster-signing-key-file 参数。\n如果您正在正在创建一个新的集群，您可以使用 kubeadm 的 配置文件\napiVersion: kubeadm.k8s.io/v1beta2 kind: ClusterConfiguration controllerManager: extraArgs: cluster-signing-cert-file: /etc/kubernetes/pki/ca.crt cluster-signing-key-file: /etc/kubernetes/pki/ca.key 创建证书签名请求 (CSR) 您能用 kubeadm alpha certs renew --use-api 为 Kubernetes 证书 API 创建一个证书签名请求。\n如果您设置例如 [cert-manager][cert-manager] 等外部签名者，则会自动批准证书签名请求（CSRs）。 否者，您必须使用 kubectl certificate 命令手动批准证书。 以下 kubeadm 命令输出要批准的证书名称，然后缓慢等待批准发生：\nsudo kubeadm alpha certs renew apiserver --use-api \u0026amp; 输出类似于以下内容：\n[1] 2890 [certs] certificate request \u0026quot;kubeadm-cert-kube-apiserver-ld526\u0026quot; created 批准证书签名请求 (CSR) 如果您设置了一个外部签名者， 证书签名请求 (CSRs) 会自动被批准。\n否则，您必须用 kubectl certificate 命令手动批准证书，例如\nkubectl certificate approve kubeadm-cert-kube-apiserver-ld526 输出类似于以下内容：\ncertificatesigningrequest.certificates.k8s.io/kubeadm-cert-kube-apiserver-ld526 approved 您可以使用 kubectl get csr 查看待处理证书列表。\n通过外部 CA 更新证书 本节提供有关如何使用外部 CA 执行手动更新证书的更多详细信息。\n为了更好的与外部 CA 集成，kubeadm 还可以生成证书签名请求（CSR）。 CSR 表示向 CA 请求客户的签名证书。 在 kubeadm 术语中，通常由磁盘 CA 签名的任何证书都可以作为 CSR 生成。但是，CA 不能作为 CSR 生成。\n创建证书签名请求 (CSR) 您可以传入一个带有 --csr-dir 的目录，将 CRS 输出到指定位置。 如果未指定 --csr-dir ，则使用默认证书目录( /etc/kubernetes/pki )。 CSR 和随附的私钥都在输出中给出。签署证书后，必须将证书和私钥复制到 PKI 目录（默认情况下为 /etc/kubernetes/pki）。\nCSR 代表对 CA 的请求，要求获得客户端的签名证书。\n您能用 kubeadm alpha certs renew --csr-only 创建一个证书签名请求。\n输出中给出了 CSR 和随附的私钥；您可以输入目录名和 --csr-dir ，以将 CSR 输出到指定位置。\n证书能用 kubeadm alpha certs renew --csr-only 更新。\n在 kubeadm init ,通过 --csr-dir 能指定输出文件夹。\n要使用新证书，请将签名的证书和私钥复制到 PKI 目录（默认情况下为 /etc/kubernetes/pki ）\n一个 CSR 包含一个证书的名字，域和 IP， 但是未指定用法\n颁发证书时， CA 有责任指定正确的证书用法 。\n 在 openssl 中，这是通过 openssl ca 命令 完成的。 在 cfssl 中，这是通过 在配置文件中指定用法 来完成的。  使用首选方法对证书签名后，必须将证书和私钥复制到 PKI 目录（默认为 /etc/kubernetes/pki ）。\n"
},
{
	"uri": "https://lijun.in/tasks/administer-cluster/access-cluster-api/",
	"title": "使用 Kubernetes API 访问集群",
	"tags": [],
	"description": "",
	"content": "本页展示了如何使用 Kubernetes API 访问集群\n. heading \u0026ldquo;prerequisites\u0026rdquo; %}} . include \u0026ldquo;task-tutorial-prereqs.md\u0026rdquo; \u0026gt;}} . version-check \u0026gt;}}\n访问集群 API 使用 kubectl 进行首次访问 首次访问 Kubernetes API 时，请使用 Kubernetes 命令行工具 kubectl 。\n要访问集群，您需要知道集群位置并拥有访问它的凭证。通常，当您完成入门指南时，这会自动设置完成，或者由其他人设置好集群并将凭证和位置提供给您。\n使用此命令检查 kubectl 已知的位置和凭证：\nkubectl config view 许多[样例](https://github.com/kubernetes/examples/tree/. param \u0026ldquo;githubbranch\u0026rdquo; \u0026gt;}}/)提供了使用 kubectl 的介绍。完整文档请见 kubectl 手册。\n直接访问 REST API kubectl 处理对 API 服务器的定位和身份验证。如果您想通过 http 客户端（如 curl 或 wget，或浏览器）直接访问 REST API，您可以通过多种方式对 API 服务器进行定位和身份验证：\n 以代理模式运行 kubectl（推荐）。 推荐使用此方法，因为它用存储的 apiserver 位置并使用自签名证书验证 API 服务器的标识。使用这种方法无法进行中间人（MITM）攻击。 另外，您可以直接为 http 客户端提供位置和身份认证。这适用于被代理混淆的客户端代码。为防止中间人攻击，您需要将根证书导入浏览器。  使用 Go 或 Python 客户端库可以在代理模式下访问 kubectl。\n使用 kubectl 代理 下列命令使 kubectl 运行在反向代理模式下。它处理 API 服务器的定位和身份认证。\n像这样运行它：\nkubectl proxy --port=8080 \u0026amp; 参见 kubectl 代理 获取更多细节。\n然后您可以通过 curl，wget，或浏览器浏览 API，像这样：\ncurl http://localhost:8080/api/ 输出类似如下：\n{ \u0026#34;versions\u0026#34;: [ \u0026#34;v1\u0026#34; ], \u0026#34;serverAddressByClientCIDRs\u0026#34;: [ { \u0026#34;clientCIDR\u0026#34;: \u0026#34;0.0.0.0/0\u0026#34;, \u0026#34;serverAddress\u0026#34;: \u0026#34;10.0.1.149:443\u0026#34; } ] } 不使用 kubectl 代理 通过将身份认证令牌直接传给 API 服务器，可以避免使用 kubectl 代理，像这样：\n使用 grep/cut 方式：\n# Check all possible clusters, as you .KUBECONFIG may have multiple contexts: kubectl config view -o jsonpath=\u0026#39;{\u0026#34;Cluster name\\tServer\\n\u0026#34;}{range .clusters[*]}{.name}{\u0026#34;\\t\u0026#34;}{.cluster.server}{\u0026#34;\\n\u0026#34;}{end}\u0026#39; # Select name of cluster you want to interact with from above output: export CLUSTER_NAME=\u0026#34;some_server_name\u0026#34; # Point to the API server refering the cluster name APISERVER=$(kubectl config view -o jsonpath=\u0026#34;{.clusters[?(@.name==\\\u0026#34;$CLUSTER_NAME\\\u0026#34;)].cluster.server}\u0026#34;) # Gets the token value TOKEN=$(kubectl get secrets -o jsonpath=\u0026#34;{.items[?(@.metadata.annotations[\u0026#39;kubernetes\\.io/service-account\\.name\u0026#39;]==\u0026#39;default\u0026#39;)].data.token}\u0026#34;|base64 -d) # Explore the API with TOKEN curl -X GET $APISERVER/api --header \u0026#34;Authorization: Bearer $TOKEN\u0026#34; --insecure 输出类似如下：\n{ \u0026#34;kind\u0026#34;: \u0026#34;APIVersions\u0026#34;, \u0026#34;versions\u0026#34;: [ \u0026#34;v1\u0026#34; ], \u0026#34;serverAddressByClientCIDRs\u0026#34;: [ { \u0026#34;clientCIDR\u0026#34;: \u0026#34;0.0.0.0/0\u0026#34;, \u0026#34;serverAddress\u0026#34;: \u0026#34;10.0.1.149:443\u0026#34; } ] } 使用 jsonpath 方式：\n$ APISERVER=$(kubectl config view --minify -o jsonpath='{.clusters[0].cluster.server}') $ TOKEN=$(kubectl get secret $(kubectl get serviceaccount default -o jsonpath='{.secrets[0].name}') -o jsonpath='{.data.token}' | base64 --decode ) $ curl $APISERVER/api --header \u0026quot;Authorization: Bearer $TOKEN\u0026quot; --insecure { \u0026quot;kind\u0026quot;: \u0026quot;APIVersions\u0026quot;, \u0026quot;versions\u0026quot;: [ \u0026quot;v1\u0026quot; ], \u0026quot;serverAddressByClientCIDRs\u0026quot;: [ { \u0026quot;clientCIDR\u0026quot;: \u0026quot;0.0.0.0/0\u0026quot;, \u0026quot;serverAddress\u0026quot;: \u0026quot;10.0.1.149:443\u0026quot; } ] } 上面例子使用了 --insecure 标志位。这使它易受到 MITM 攻击。当 kubectl 访问集群时，它使用存储的根证书和客户端证书访问服务器。（已安装在 ~/.kube 目录下）。由于集群认证通常是自签名的，因此可能需要特殊设置才能让你的 http 客户端使用根证书。\n在一些集群中，API 服务器不需要身份认证；它运行在本地，或由防火墙保护着。对此并没有一个标准。配置对 API 的访问 阐述了一个集群管理员如何对此进行配置。这种方法可能与未来的高可用性支持发生冲突。\n编程方式访问 API Kubernetes 官方支持 Go 和 Python 的客户端库.\nGo 客户端  要获取库，运行下列命令：go get k8s.io/client-go/\u0026lt;version number\u0026gt;/kubernetes 参见 https://github.com/kubernetes/client-go 查看受支持的版本。 基于 client-go 客户端编写应用程序。注意 client-go 定义了自己的 API 对象，因此如果需要，请从 client-go 而不是主仓库导入 API 定义，例如 import \u0026quot;k8s.io/client-go/1.4/pkg/api/v1\u0026quot; 是正确做法。  Go 客户端可以使用与 kubectl 命令行工具相同的 kubeconfig 文件 定位和验证 API 服务器。参见这个 例子：\nimport ( \u0026#34;fmt\u0026#34; \u0026#34;k8s.io/client-go/1.4/kubernetes\u0026#34; \u0026#34;k8s.io/client-go/1.4/pkg/api/v1\u0026#34; \u0026#34;k8s.io/client-go/1.4/tools/clientcmd\u0026#34; ) ... // uses the current context in kubeconfig  config, _ := clientcmd.BuildConfigFromFlags(\u0026#34;\u0026#34;, \u0026#34;path to kubeconfig\u0026#34;) // creates the clientset  clientset, _:= kubernetes.NewForConfig(config) // access the API to list pods  pods, _:= clientset.CoreV1().Pods(\u0026#34;\u0026#34;).List(v1.ListOptions{}) fmt.Printf(\u0026#34;There are %d pods in the cluster\\n\u0026#34;, len(pods.Items)) ... 如果该应用程序部署为集群中的一个 Pod，请参阅 下一节。\nPython 客户端 要使用 Python 客户端，运行下列命令：pip install kubernetes 参见 Python 客户端库主页 查看更多安装选项。\nPython 客户端可以使用与 kubectl 命令行工具相同的 kubeconfig 文件 定位和验证 API 服务器。参见这个 例子：\nfrom kubernetes import client, config config.load_kube_config() v1=client.CoreV1Api() print(\u0026#34;Listing pods with their IPs:\u0026#34;) ret = v1.list_pod_for_all_namespaces(watch=False) for i in ret.items: print(\u0026#34;%s\\t%s\\t%s\u0026#34; % (i.status.pod_ip, i.metadata.namespace, i.metadata.name)) 其他语言 有许多 客户端库 可以用于从其他语言访问 API。请参阅其他库的文档了解它们的身份验证方式。\n从 Pod 中访问 API 从 Pod 访问 API 时，对 API 服务器的定位和身份验证会有所不同。\n从 Pod 使用 Kubernetes API 的最简单的方法就是使用一个官方的 客户端库。这些库可以自动发现 API 服务器并进行身份验证。\n在运行在 Pod 中时，可以通过 default 命名空间中的名为 kubernetes 的服务访问 Kubernetes apiserver。也就是说，Pods 可以使用 kubernetes.default.svc 主机名来查询 API 服务器。官方客户端库自动完成这个工作。\n从一个 Pod 内，向 API 服务器进行身份认证的推荐的做法是使用 服务账号 凭证。默认的，一个 Pod 与一个服务账号关联，该服务账户的凭证（令牌）放置在此 Pod 中每个容器的文件系统树中的 /var/run/secrets/kubernetes.io/serviceaccount/token 处。\n如果可用，凭证包被放入每个容器的文件系统树中的 /var/run/secrets/kubernetes.io/serviceaccount/ca.crt 处，并且将被用于验证 API 服务器的服务证书。\n最后，用于命名空间 API 操作的默认的命名空间放置在每个容器中的 /var/run/secrets/kubernetes.io/serviceaccount/namespace 文件中。\n从一个 Pod 内，连接 Kubernetes API 的推荐方法是：\n 使用官方的 客户端库 因为他们会自动地完成 API 主机发现和身份认证。以 Go 客户端来说，rest.InClusterConfig() 可以帮助解决这个问题。参见 这里的一个例子。   如果您想要在没有官方客户端库的情况下查询 API，可以在 Pod 里以一个新的边车容器的 命令的方式运行 kubectl proxy 。此方式下，kubectl proxy 将对 API 进行身份验证并将其公开在 Pod 的 localhost 接口上，以便 Pod 中的其他容器可以直接使用它。  在每种情况下，Pod 的服务账号凭证被用于与 API 服务器的安全通信。\n"
},
{
	"uri": "https://lijun.in/setup/learning-environment/minikube/",
	"title": "使用 Minikube 安装 Kubernetes",
	"tags": [],
	"description": "",
	"content": "Minikube 是一种可以让您在本地轻松运行 Kubernetes 的工具。Minikube 在笔记本电脑上的虚拟机（VM）中运行单节点 Kubernetes 集群，供那些希望尝试 Kubernetes 或进行日常开发的用户使用。\nMinikube 功能 Minikube 支持以下 Kubernetes 功能：\n DNS NodePorts ConfigMaps 和 Secrets Dashboards 容器运行时: Docker、CRI-O 以及 containerd 启用 CNI （容器网络接口） Ingress  安装 请参阅安装 Minikube。\n快速开始 这个简短的演示将指导您如何在本地启动、使用和删除 Minikube。请按照以下步骤开始探索 Minikube。\n  启动 Minikube 并创建一个集群：\nminikube start 输出类似于：\nStarting local Kubernetes cluster... Running pre-create checks... Creating machine... Starting local Kubernetes cluster... 有关使用特定 Kubernetes 版本、VM 或容器运行时启动集群的详细信息，请参阅启动集群。\n  现在，您可以使用 kubectl 与集群进行交互。有关详细信息，请参阅与集群交互。\n让我们使用名为 echoserver 的镜像创建一个 Kubernetes Deployment，并使用 --port 在端口 8080 上暴露服务。echoserver 是一个简单的 HTTP 服务器。\nkubectl create deployment hello-minikube --image=k8s.gcr.io/echoserver:1.10 输出类似于：\ndeployment.apps/hello-minikube created   要访问 hello-minikube Deployment，需要将其作为 Service 公开：\nkubectl expose deployment hello-minikube --type=NodePort --port=8080 选项 --type = NodePort 指定 Service 的类型。\n输出类似于：\nservice/hello-minikube exposed   现在 hello-minikube Pod 已经启动，但是您必须等到 Pod 启动完全才能通过暴露的 Service 访问它。\n检查 Pod 是否启动并运行：\nkubectl get pod ``\n如果输出显示 STATUS 为 ContainerCreating，则表明 Pod 仍在创建中：\nNAME READY STATUS RESTARTS AGE hello-minikube-3383150820-vctvh 0/1 ContainerCreating 0 3s ``\n如果输出显示 STATUS 为 Running，则 Pod 现在正在运行：\nNAME READY STATUS RESTARTS AGE hello-minikube-3383150820-vctvh 1/1 Running 0 13s ``\n  获取暴露 Service 的 URL 以查看 Service 的详细信息：\nminikube service hello-minikube --url ``\n  要查看本地集群的详细信息，请在浏览器中复制粘贴并访问上一步骤输出的 URL。\n输出类似于：\nHostname: hello-minikube-7c77b68cff-8wdzq Pod Information: -no pod information available- Server values: server_version=nginx: 1.13.3 - lua: 10008 Request Information: client_address=172.17.0.1 method=GET real path=/ query= request_version=1.1 request_scheme=http request_uri=http://192.168.99.100:8080/ Request Headers: accept=*/* host=192.168.99.100:30674 user-agent=curl/7.47.0 Request Body: -no body in request- 如果您不再希望运行 Service 和集群，则可以删除它们。\n  删除 hello-minikube Service：\nkubectl delete services hello-minikube 输出类似于：\nservice \u0026quot;hello-minikube\u0026quot; deleted   删除 hello-minikube Deployment：\nkubectl delete deployment hello-minikube 输出类似于：\ndeployment.extensions \u0026quot;hello-minikube\u0026quot; deleted   停止本地 Minikube 集群：\nminikube stop 输出类似于：\nStopping \u0026quot;minikube\u0026quot;... \u0026quot;minikube\u0026quot; stopped. 有关更多信息，请参阅停止集群。\n  删除本地 Minikube 集群：\nminikube delete 输出类似于：\nDeleting \u0026quot;minikube\u0026quot; ... The \u0026quot;minikube\u0026quot; cluster has been deleted. 有关更多信息，请参阅删除集群。\n  管理您的集群 启动集群 minikube start 命令可用于启动集群。\n此命令将创建并配置一台虚拟机，使其运行单节点 Kubernetes 集群。\n此命令还会配置您的 kubectl 安装，以便使其能与您的 Kubernetes 集群正确通信。\n. note \u0026gt;}} 如果您启用了 web 代理，则需要将此信息传递给 minikube start 命令：\nhttps_proxy=\u0026lt;my proxy\u0026gt; minikube start --docker-env http_proxy=\u0026lt;my proxy\u0026gt; --docker-env https_proxy=\u0026lt;my proxy\u0026gt; --docker-env no_proxy=192.168.99.0/24 不幸的是，单独设置环境变量不起作用。\nMinikube 还创建了一个 minikube 上下文，并将其设置为 kubectl 的默认上下文。\n要切换回此上下文，请运行以下命令：kubectl config use-context minikube。 . /note \u0026gt;}}\n指定 Kubernetes 版本 您可以通过将 --kubernetes-version 字符串添加到 minikube start 命令来指定要用于 Minikube 的 Kubernetes 版本。例如，要运行版本 . param \u0026ldquo;fullversion\u0026rdquo; \u0026gt;}}，您可以运行以下命令：\nminikube start --kubernetes-version . param \u0026quot;fullversion\u0026quot; \u0026gt;}} 指定 VM 驱动程序 您可以通过将 --vm-driver=\u0026lt;enter_driver_name\u0026gt; 参数添加到 minikube start 来更改 VM 驱动程序。\n例如命令：\nminikube start --vm-driver=\u0026lt;driver_name\u0026gt; Minikube 支持以下驱动程序：\n. note \u0026gt;}} 有关支持的驱动程序以及如何安装插件的详细信息，请参阅驱动程序。 . /note \u0026gt;}}\n virtualbox vmwarefusion kvm2 (驱动安装) hyperkit (驱动安装) hyperv (驱动安装)  请注意，下面的 IP 是动态的，可以更改。可以使用 minikube ip 检索。\n vmware (驱动安装) （VMware 统一驱动） none (在主机上运行Kubernetes组件，而不是在 VM 中。使用该驱动依赖 Docker (安装 Docker) 和 Linux 环境)  通过别的容器运行时启动集群 您可以通过以下容器运行时启动 Minikube。\n. tabs name=\u0026quot;container_runtimes\u0026rdquo; \u0026gt;}} . tab name=\u0026quot;containerd\u0026rdquo; %}}\n要使用 containerd 作为容器运行时，请运行：\nminikube start \\  --network-plugin=cni \\  --enable-default-cni \\  --container-runtime=containerd \\  --bootstrapper=kubeadm 或者您可以使用扩展版本：\nminikube start \\  --network-plugin=cni \\  --enable-default-cni \\  --extra-config=kubelet.container-runtime=remote \\  --extra-config=kubelet.container-runtime-endpoint=unix:///run/containerd/containerd.sock \\  --extra-config=kubelet.image-service-endpoint=unix:///run/containerd/containerd.sock \\  --bootstrapper=kubeadm . /tab %}} . tab name=\u0026quot;CRI-O\u0026rdquo; %}}\n要使用 CRI-O 作为容器运行时，请运行：\nminikube start \\  --network-plugin=cni \\  --enable-default-cni \\  --container-runtime=cri-o \\  --bootstrapper=kubeadm 或者您可以使用扩展版本：\nminikube start \\  --network-plugin=cni \\  --enable-default-cni \\  --extra-config=kubelet.container-runtime=remote \\  --extra-config=kubelet.container-runtime-endpoint=/var/run/crio.sock \\  --extra-config=kubelet.image-service-endpoint=/var/run/crio.sock \\  --bootstrapper=kubeadm . /tab %}} . /tabs \u0026gt;}}\n通过重用 Docker 守护进程使用本地镜像 当为 Kubernetes 使用单个 VM 时，重用 Minikube 的内置 Docker 守护程序非常有用。重用内置守护程序意味着您不必在主机上构建 Docker 镜像仓库并将镜像推入其中。相反，您可以在与 Minikube 相同的 Docker 守护进程内部构建，这可以加速本地实验。\n. note \u0026gt;}}\n一定要用非 latest 的标签来标记你的 Docker 镜像，并使用该标签来拉取镜像。因为 :latest 标记的镜像，其默认镜像拉取策略是 Always，如果在默认的 Docker 镜像仓库（通常是 DockerHub）中没有找到你的 Docker 镜像，最终会导致一个镜像拉取错误（ErrImagePull）。 . /note \u0026gt;}}\n要在 Mac/Linux 主机上使用 Docker 守护程序，请在 shell 中运行 docker-env command：\neval $(minikube docker-env) 您现在可以在 Mac/Linux 机器的命令行中使用 Docker 与 Minikube VM 内的 Docker 守护程序进行通信：\ndocker ps . note \u0026gt;}}\n在 Centos 7 上，Docker 可能会报如下错误：\nCould not read CA certificate \u0026quot;/etc/docker/ca.pem\u0026quot;: open /etc/docker/ca.pem: no such file or directory 您可以通过更新 /etc/sysconfig/docker 来解决此问题，以确保 Minikube 的环境更改得到遵守：\n\u0026lt; DOCKER_CERT_PATH=/etc/docker --- \u0026gt; if [ -z \u0026#34;${DOCKER_CERT_PATH}\u0026#34; ]; then \u0026gt; DOCKER_CERT_PATH=/etc/docker \u0026gt; fi . /note \u0026gt;}}\n配置 Kubernetes Minikube 有一个 \u0026ldquo;configurator\u0026rdquo; 功能，允许用户使用任意值配置 Kubernetes 组件。\n要使用此功能，可以在 minikube start 命令中使用 --extra-config 参数。\n此参数允许重复，因此您可以使用多个不同的值多次传递它以设置多个选项。\n此参数采用 component.key=value 形式的字符串，其中 component 是下面列表中的一个字符串，key 是配置项名称，value 是要设置的值。\n通过检查每个组件的 Kubernetes componentconfigs 的文档，可以找到有效的 key。\n下面是每个组件所支持的配置的介绍文档：\n kubelet apiserver proxy controller-manager etcd scheduler  例子 要在 Kubelet 上将 MaxPods 设置更改为 5，请传递此参数：--extra-config=kubelet.MaxPods=5。\n此功能还支持嵌套结构。要在调度程序上将 LeaderElection.LeaderElect 设置更改为 true，请传递此参数：--extra-config=scheduler.LeaderElection.LeaderElect=true。\n要将 apiserver 的 AuthorizationMode 设置为 RBAC，您可以使用：--extra-config=apiserver.authorization-mode=RBAC。\n停止集群 minikube stop 命令可用于停止集群。\n此命令关闭 Minikube 虚拟机，但保留所有集群状态和数据。\n再次启动集群会将其恢复到以前的状态。\n删除集群 minikube delete 命令可用于删除集群。\n此命令将关闭并删除 Minikube 虚拟机，不保留任何数据或状态。\n与集群交互 Kubectl minikube start 命令创建一个名为 minikube 的 kubectl 上下文。\n此上下文包含与 Minikube 集群通信的配置。\nMinikube 会自动将此上下文设置为默认值，但如果您以后需要切换回它，请运行：\nkubectl config use-context minikube，\n或者像这样，每个命令都附带其执行的上下文：kubectl get pods --context=minikube。\n仪表盘 要访问 Kubernetes Dashboard，请在启动 Minikube 后在 shell 中运行此命令以获取地址：\nminikube dashboard Service 要访问通过节点（Node）端口公开的 Service，请在启动 Minikube 后在 shell 中运行此命令以获取地址：\nminikube service [-n NAMESPACE] [--url] NAME 网络 Minikube VM 通过 host-only IP 暴露给主机系统，可以通过 minikube ip 命令获得该 IP。\n在 NodePort 上，可以通过该 IP 地址访问任何类型为 NodePort 的服务。\n要确定服务的 NodePort，可以像这样使用 kubectl 命令：\nkubectl get service $SERVICE --output='jsonpath=\u0026quot;{.spec.ports[0].nodePort}\u0026quot;'\n持久卷（PersistentVolume） Minikube 支持 hostPath 类型的 持久卷。\n这些持久卷会映射为 Minikube VM 内的目录。\nMinikube VM 引导到 tmpfs，因此大多数目录不会在重新启动（minikube stop）之后保持不变。\n但是，Minikube 被配置为保存存储在以下主机目录下的文件：\n /data /var/lib/minikube /var/lib/docker  下面是一个持久卷配置示例，用于在 /data 目录中保存数据：\napiVersion: v1 kind: PersistentVolume metadata: name: pv0001 spec: accessModes: - ReadWriteOnce capacity: storage: 5Gi hostPath: path: /data/pv0001/ 挂载宿主机文件夹 一些驱动程序将在 VM 中挂载一个主机文件夹，以便您可以轻松地在 VM 和主机之间共享文件。目前这些都是不可配置的，并且根据您正在使用的驱动程序和操作系统的不同而不同。\n. note \u0026gt;}} KVM 驱动程序中尚未实现主机文件夹共享。 . /note \u0026gt;}}\n   驱动 操作系统 宿主机文件夹 VM 文件夹     VirtualBox Linux /home /hosthome   VirtualBox macOS /Users /Users   VirtualBox Windows C://Users /c/Users   VMware Fusion macOS /Users /Users   Xhyve macOS /Users /Users    私有容器镜像仓库 要访问私有容器镜像仓库，请按照本页上的步骤操作。\n我们建议您使用 ImagePullSecrets，但是如果您想在 Minikube VM 上配置访问权限，可以将 .dockercfg 放在 /home/docker 目录中，或将config.json 放在 /home/docker/.docker 目录。\n附加组件 为了让 Minikube 正确启动或重新启动自定义插件，请将您希望用 Minikube 启动的插件放在 ~/.minikube/addons 目录中。此文件夹中的插件将被移动到 Minikube VM 并在每次 Minikube 启动或重新启动时被启动。\n基于 HTTP 代理使用 Minikube Minikube 创建了一个包含 Kubernetes 和 Docker 守护进程的虚拟机。\n当 Kubernetes 尝试使用 Docker 调度容器时，Docker 守护程序可能需要访问外部网络来拉取容器镜像。\n如果您配置了 HTTP 代理，则可能也需要为 Docker 进行代理设置。\n要实现这一点，可以在 minikube start 期间将所需的环境变量作为参数传递给启动命令。\n例如：\nminikube start --docker-env http_proxy=http://$YOURPROXY:PORT \\  --docker-env https_proxy=https://$YOURPROXY:PORT 如果您的虚拟机地址是 192.168.99.100，那么您的代理设置可能会阻止 kubectl 直接访问它。\n要绕过此 IP 地址的代理配置，您应该修改 no_proxy 设置。您可以这样做：\nexport no_proxy=$no_proxy,$(minikube ip) 已知的问题 需要多个节点的功能无法在 Minikube 中使用。\n设计 Minikube 使用 libmachine 配置虚拟机，kubeadm 配置 Kubernetes 集群。\n有关 Minikube 的更多信息，请参阅提案。\n其他链接  目标和非目标: 有关 Minikube 项目的目标和非目标，请参阅我们的 roadmap。 开发指南: 请查阅 CONTRIBUTING.md 获取有关如何提交 Pull Request 的概述。 构建 Minikube: 有关如何从源代码构建/测试 Minikube 的说明，请参阅构建指南。 添加新依赖: 有关如何向 Minikube 添加新依赖的说明，请参阅添加依赖项指南。 添加新插件: 有关如何为 Minikube 添加新插件的说明，请参阅添加插件指南。 MicroK8s: 希望避免运行虚拟机的 Linux 用户可以考虑使用 MicroK8s 作为替代品。  社区 我们欢迎您向社区提交贡献、提出问题以及参与评论！Minikube 开发人员可以在 Slack 的 #minikube 频道上互动交流（点击这里获得邀请）。我们还有 kubernetes-dev Google Groups 邮件列表。如果您要发信到列表中，请在主题前加上 \u0026ldquo;minikube: \u0026ldquo;。\n"
},
{
	"uri": "https://lijun.in/tasks/inject-data-application/podpreset/",
	"title": "使用 PodPreset 将信息注入 Pods",
	"tags": [],
	"description": "",
	"content": "在 pod 创建时，用户可以使用 podpreset 对象将 secrets、卷挂载和环境变量等信息注入其中。 本文展示了一些 PodPreset 资源使用的示例。 用户可以从理解 Pod Presets 中了解 PodPresets 的整体情况。\n. toc \u0026gt;}}\n创建 Pod Preset 简单的 Pod Spec 示例 这里是一个简单的示例，展示了如何通过 Pod Preset 修改 Pod spec 。\n. codenew file=\u0026quot;podpreset/preset.yaml\u0026rdquo; \u0026gt;}}\n创建 PodPreset：\nkubectl apply -f https://k8s.io/examples/podpreset/preset.yaml 检查所创建的 PodPreset：\nkubectl get podpreset NAME AGE allow-database 1m 新的 PodPreset 会对所有具有标签 role: frontend 的 Pods 采取行动。\n用户提交的 pod spec：\n. codenew file=\u0026quot;podpreset/pod.yaml\u0026rdquo; \u0026gt;}}\n创建 Pod：\nkubectl create -f https://k8s.io/examples/podpreset/pod.yaml 列举运行中的 Pods：\nkubectl get pods NAME READY STATUS RESTARTS AGE website 1/1 Running 0 4m 通过准入控制器后的 Pod 规约：\n. codenew file=\u0026quot;podpreset/merged.yaml\u0026rdquo; \u0026gt;}}\n要查看如上输出，运行下面的命令：\nkubectl get pod website -o yaml 带有 ConfigMap 的 Pod Spec 示例 这里的示例展示了如何通过 PodPreset 修改 Pod 规约，PodPreset 中定义了 ConfigMap 作为环境变量取值来源。\n用户提交的 pod spec：\n. codenew file=\u0026quot;podpreset/pod.yaml\u0026rdquo; \u0026gt;}}\n用户提交的 ConfigMap：\n. codenew file=\u0026quot;podpreset/configmap.yaml\u0026rdquo; \u0026gt;}}\nPodPreset 示例：\n. codenew file=\u0026quot;podpreset/allow-db.yaml\u0026rdquo; \u0026gt;}}\n通过准入控制器后的 Pod spec：\n. codenew file=\u0026quot;podpreset/allow-db-merged.yaml\u0026rdquo; \u0026gt;}}\n带有 Pod Spec 的 ReplicaSet 示例 以下示例展示了（通过 ReplicaSet 创建 pod 后）只有 pod spec 会被 Pod Preset 所修改。\n用户提交的 ReplicaSet：\n. codenew file=\u0026quot;podpreset/replicaset.yaml\u0026rdquo; \u0026gt;}}\nPodPreset 示例：\n. codenew file=\u0026quot;podpreset/preset.yaml\u0026rdquo; \u0026gt;}}\n通过准入控制器后的 Pod spec：\n注意 ReplicaSet spec 没有改变，用户必须检查单独的 pod 来验证 PodPreset 已被应用。\n. codenew file=\u0026quot;podpreset/replicaset-merged.yaml\u0026rdquo; \u0026gt;}}\n多 PodPreset 示例 这里的示例展示了如何通过多个 Pod 注入策略修改 Pod spec。\n用户提交的 Pod 规约：\n. codenew file=\u0026quot;podpreset/pod.yaml\u0026rdquo; \u0026gt;}}\nPodPreset 示例：\n. codenew file=\u0026quot;podpreset/preset.yaml\u0026rdquo; \u0026gt;}}\n另一个 Pod Preset 示例：\n. codenew file=\u0026quot;podpreset/proxy.yaml\u0026rdquo; \u0026gt;}}\n通过准入控制器后的 Pod 规约：\n. codenew file=\u0026quot;podpreset/multi-merged.yaml\u0026rdquo; \u0026gt;}}\n冲突示例 这里的示例展示了 PodPreset 与原 Pod 存在冲突时，Pod spec 不会被修改。\n用户提交的 Pod 规约：\n. codenew file=\u0026quot;podpreset/conflict-pod.yaml\u0026rdquo; \u0026gt;}}\nPodPreset 示例：\n. codenew file=\u0026quot;podpreset/conflict-preset.yaml\u0026rdquo; \u0026gt;}}\n因存在冲突，通过准入控制器后的 Pod spec 不会改变：\n. codenew file=\u0026quot;podpreset/conflict-pod.yaml\u0026rdquo; \u0026gt;}}\n如果运行 kubectl describe... 用户会看到以下事件：\n$ kubectl describe ... .... Events: FirstSeen LastSeen Count From SubobjectPath Reason Message Tue, 07 Feb 2017 16:56:12 -0700 Tue, 07 Feb 2017 16:56:12 -0700 1 {podpreset.admission.kubernetes.io/podpreset-allow-database } conflict Conflict on pod preset. Duplicate mountPath /cache. 删除 Pod Preset 一旦用户不再需要 pod preset，可以使用 kubectl 进行删除：\nkubectl delete podpreset allow-database podpreset \u0026quot;allow-database\u0026quot; deleted "
},
{
	"uri": "https://lijun.in/tasks/service-catalog/install-service-catalog-using-sc/",
	"title": "使用 SC 安装服务目录",
	"tags": [],
	"description": "",
	"content": ". glossary_definition term_id=\u0026quot;service-catalog\u0026rdquo; length=\u0026quot;all\u0026rdquo; prepend=\u0026quot;Service Catalog is\u0026rdquo; \u0026gt;}}\n使用服务目录安装程序工具可以轻松地在 Kubernetes 集群上安装或卸载服务目录。 这个 CLI 工具以 sc 命令形式被安装在您的本地环境中。\n. heading \u0026ldquo;prerequisites\u0026rdquo; %}}   了解服务目录的主要概念。\n  安装 Go 1.6+ 以及设置 GOPATH。\n  安装生成 SSL 工件所需的 cfssl 工具。\n  服务目录需要 Kubernetes 1.7+ 版本。\n  安装和设置 kubectl，以便将其配置为连接到 Kubernetes v1.7+ 集群。\n  要安装服务目录，kubectl 用户必须绑定到 cluster-admin 角色。为了确保这是正确的，请运行以下命令：\n kubectl create clusterrolebinding cluster-admin-binding --clusterrole=cluster-admin --user=\u0026lt;user-name\u0026gt;    在本地环境中安装 sc 使用 go get 命令安装 sc CLI 工具：\ngo get github.com/GoogleCloudPlatform/k8s-service-catalog/installer/cmd/sc 执行上述命令后，sc 应被安装在 GOPATH/bin 目录中了。\n在 Kubernetes 集群中安装服务目录 首先，检查是否已经安装了所有依赖项。运行：\nsc check 如检查通过，应输出：\nDependency check passed. You are good to go. 接下来，运行安装命令并指定要用于备份的 storageclass：\nsc install --etcd-backup-storageclass \u0026#34;standard\u0026#34; 卸载服务目录 如果您想使用 sc 工具从 Kubernetes 集群卸载服务目录，请运行：\nsc uninstall . heading \u0026ldquo;whatsnext\u0026rdquo; %}}  查看 服务代理示例。 探索 kubernetes-incubator/service-catalog 项目。  "
},
{
	"uri": "https://lijun.in/tasks/inject-data-application/distribute-credentials-secure/",
	"title": "使用 Secret 安全地分发凭证",
	"tags": [],
	"description": "",
	"content": "本文展示如何安全地将敏感数据（如密码和加密密钥）注入到 Pods 中。\n. heading \u0026ldquo;prerequisites\u0026rdquo; %}} . include \u0026ldquo;task-tutorial-prereqs.md\u0026rdquo; \u0026gt;}} . version-check \u0026gt;}}\n将 secret 数据转换为 base-64 形式 假设用户想要有两条 secret 数据：用户名 my-app 和密码 39528$vdg7Jb。 首先使用 Base64 编码 将用户名和密码转化为 base-64 形式。 这里是一个 Linux 示例：\n```shell echo -n 'my-app' | base64 echo -n '39528$vdg7Jb' | base64 ```  结果显示 base-64 形式的用户名为 bXktYXBw， base-64 形式的密码为 Mzk1MjgkdmRnN0pi。\n创建 Secret 这里是一个配置文件，可以用来创建存有用户名和密码的 Secret:\n. codenew file=\u0026quot;pods/inject/secret.yaml\u0026rdquo; \u0026gt;}}\n  创建 Secret\nkubectl create -f https://k8s.io/examples/pods/inject/secret.yaml . note \u0026gt;}} 如果想要跳过 Base64 编码的步骤，可以使用 kubectl create secret 命令来创建 Secret： . /note \u0026gt;}}\nkubectl create secret generic test-secret --from-literal=username=\u0026#39;my-app\u0026#39; --from-literal=password=\u0026#39;39528$vdg7Jb\u0026#39;   查看 Secret 相关信息：\nkubectl get secret test-secret  输出：\nNAME TYPE DATA AGE test-secret Opaque 2 1m   查看 Secret 相关的更多详细信息：\nkubectl describe secret test-secret  输出：\nName: test-secret Namespace: default Labels: \u0026lt;none\u0026gt; Annotations: \u0026lt;none\u0026gt; Type: Opaque Data ==== password: 13 bytes username: 7 bytes   创建可以通过卷访问 secret 数据的 Pod 这里是一个可以用来创建 pod 的配置文件：\n. codenew file=\u0026quot;pods/inject/secret-pod.yaml\u0026rdquo; \u0026gt;}}\n  创建 Pod：\nkubectl create -f secret-pod.yaml   确认 Pod 正在运行：\nkubectl get pod secret-test-pod 输出：\nNAME READY STATUS RESTARTS AGE secret-test-pod 1/1 Running 0 42m   在 Pod 中运行的容器中获取一个 shell：\nkubectl exec -it secret-test-pod -- /bin/bash   secret 数据通过挂载在 /etc/secret-volume 目录下的卷暴露在容器中。 在 shell 中，进入 secret 数据被暴露的目录：\nroot@secret-test-pod:/# cd /etc/secret-volume   在 shell 中，列出 /etc/secret-volume 目录的文件：\nroot@secret-test-pod:/etc/secret-volume# ls 输出显示了两个文件，每个对应一条 secret 数据：\npassword username   在 shell 中，显示 username 和 password 文件的内容：\nroot@secret-test-pod:/etc/secret-volume# cat username; echo; cat password; echo 输出为用户名和密码：\nmy-app 39528$vdg7Jb   创建通过环境变量访问 secret 数据的 Pod 这里是一个可以用来创建 pod 的配置文件：\n. codenew file=\u0026quot;pods/inject/secret-envars-pod.yaml\u0026rdquo; \u0026gt;}}\n  创建 Pod：\nkubectl create -f https://k8s.io/examples/pods/inject/secret-envars-pod.yaml   确认 Pod 正在运行：\nkubectl get pod secret-envars-test-pod 输出：\nNAME READY STATUS RESTARTS AGE secret-envars-test-pod 1/1 Running 0 4m   在 Pod 中运行的容器中获取一个 shell：\nkubectl exec -it secret-envars-test-pod -- /bin/bash   在 shell 中，显示环境变量：\nroot@secret-envars-test-pod:/# printenv 输出包括用户名和密码：\n... SECRET_USERNAME=my-app ... SECRET_PASSWORD=39528$vdg7Jb   . heading \u0026ldquo;whatsnext\u0026rdquo; %}}  了解更多关于 Secrets。 了解 Volumes。  参考  [Secret](/docs/api-reference/. param \u0026ldquo;version\u0026rdquo; \u0026gt;}}/#secret-v1-core) [Volume](/docs/api-reference/. param \u0026ldquo;version\u0026rdquo; \u0026gt;}}/#volume-v1-core) [Pod](/docs/api-reference/. param \u0026ldquo;version\u0026rdquo; \u0026gt;}}/#pod-v1-core)  "
},
{
	"uri": "https://lijun.in/tasks/access-application-cluster/connecting-frontend-backend/",
	"title": "使用 Service 把前端连接到后端",
	"tags": [],
	"description": "",
	"content": "本任务会描述如何创建前端微服务和后端微服务。后端微服务是一个 hello 欢迎程序。 前端和后端的连接是通过 Kubernetes 服务对象（Service object）完成的。\n. heading \u0026ldquo;objectives\u0026rdquo; %}}  使用部署对象（Deployment object）创建并运行一个微服务 从后端将流量路由到前端 使用服务对象把前端应用连接到后端应用  . heading \u0026ldquo;prerequisites\u0026rdquo; %}}   . include \u0026ldquo;task-tutorial-prereqs.md\u0026rdquo; \u0026gt;}} . version-check \u0026gt;}}\n  本任务使用 外部负载均衡服务， 所以需要对应的可支持此功能的环境。如果你的环境不能支持，你可以使用 NodePort 类型的服务代替。\n  使用部署对象（Deployment）创建后端 后端是一个简单的 hello 欢迎微服务应用。这是后端应用的 Deployment 配置文件：\n. codenew file=\u0026quot;service/access/hello.yaml\u0026rdquo; \u0026gt;}}\n创建后端 Deployment：\nkubectl apply -f https://k8s.io/examples/service/access/hello.yaml 查看后端的 Deployment 信息：\nkubectl describe deployment hello 输出类似于：\nName: hello Namespace: default CreationTimestamp: Mon, 24 Oct 2016 14:21:02 -0700 Labels: app=hello tier=backend track=stable Selector: app=hello,tier=backend,track=stable Replicas: 7 updated | 7 total | 7 available | 0 unavailable StrategyType: RollingUpdate MinReadySeconds: 0 RollingUpdateStrategy: 1 max unavailable, 1 max surge OldReplicaSets: \u0026lt;none\u0026gt; NewReplicaSet: hello-3621623197 (7/7 replicas created) Events: ... 创建后端服务对象（Service object） 前端连接到后端的关键是 Service。Service 创建一个固定 IP 和 DNS 解析名入口， 使得后端微服务可达。Service 使用 selector 标签来寻找目标 Pod。\n首先，浏览 Service 的配置文件：\n. codenew file=\u0026quot;service/access/hello-service.yaml\u0026rdquo; \u0026gt;}}\n配置文件中，你可以看到 Service 将流量路由到包含 app: hello 和 tier: backend 标签的 Pod。\n创建 hello Service：\nkubectl apply -f https://k8s.io/examples/service/access/hello-service.yaml 此时，你已经有了一个在运行的后端 Deployment，你也有了一个 Service 用于路由网络流量。\n创建前端应用 既然你已经有了后端应用，你可以创建一个前端应用连接到后端。前端应用通过 DNS 名连接到后端的工作 Pods。 DNS 名是 \u0026ldquo;hello\u0026rdquo;，也就是 Service 配置文件中 name 字段的值。\n前端 Deployment 中的 Pods 运行一个 nginx 镜像，这个已经配置好镜像去寻找后端的 hello Service。 只是 nginx 的配置文件：\n. codenew file=\u0026quot;service/access/frontend.conf\u0026rdquo; \u0026gt;}}\n与后端类似，前端用包含一个 Deployment 和一个 Service。Service 的配置文件包含了 type: LoadBalancer， 也就是说，Service 会使用你的云服务商的默认负载均衡设备。\n. codenew file=\u0026quot;service/access/frontend.yaml\u0026rdquo; \u0026gt;}}\n创建前端 Deployment 和 Service：\nkubectl apply -f https://k8s.io/examples/service/access/frontend.yaml 通过输出确认两个资源都已经被创建：\ndeployment \u0026quot;frontend\u0026quot; created service \u0026quot;frontend\u0026quot; created 注意：这个 nginx 配置文件是被打包在 容器镜像 里的。 更好的方法是使用 ConfigMap，这样的话你可以更轻易地更改配置。\n与前端 Service 交互 一旦你创建了 LoadBalancer 类型的 Service，你可以使用这条命令查看外部 IP：\nkubectl get service frontend 外部 IP 字段的生成可能需要一些时间。如果是这种情况，外部 IP 会显示为 \u0026lt;pending\u0026gt;。\nNAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE frontend 10.51.252.116 \u0026lt;pending\u0026gt; 80/TCP 10s 使用相同的命令直到它显示外部 IP 地址：\nNAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE frontend 10.51.252.116 XXX.XXX.XXX.XXX 80/TCP 1m 通过前端发送流量 前端和后端已经完成连接了。你可以使用 curl 命令通过你的前端 Service 的外部 IP 访问服务端点。\ncurl http://\u0026lt;EXTERNAL-IP\u0026gt; 后端生成的消息输出如下：\n{\u0026quot;message\u0026quot;:\u0026quot;Hello\u0026quot;} . heading \u0026ldquo;whatsnext\u0026rdquo; %}}  了解更多 Services 了解更多 ConfigMaps  "
},
{
	"uri": "https://lijun.in/tutorials/services/source-ip/",
	"title": "使用 Source IP",
	"tags": [],
	"description": "",
	"content": "Kubernetes 集群中运行的应用通过 Service 抽象来互相查找、通信和与外部世界沟通。本文介绍被发送到不同类型 Services 的数据包源 IP 的变化过程，你可以根据你的需求改变这些行为。\n. heading \u0026ldquo;prerequisites\u0026rdquo; %}} . include \u0026ldquo;task-tutorial-prereqs.md\u0026rdquo; \u0026gt;}} . version-check \u0026gt;}}\n术语表 本文使用了下列术语：\n NAT: 网络地址转换 Source NAT: 替换数据包的源 IP, 通常为节点的 IP Destination NAT: 替换数据包的目的 IP, 通常为 Pod 的 IP VIP: 一个虚拟 IP, 例如分配给每个 Kubernetes Service 的 IP Kube-proxy: 一个网络守护程序，在每个节点上协调 Service VIP 管理  准备工作 你必须拥有一个正常工作的 Kubernetes 1.5 集群来运行此文档中的示例。该示例使用一个简单的 nginx webserver，通过一个HTTP消息头返回它接收到请求的源IP。你可以像下面这样创建它：\nkubectl run source-ip-app --image=k8s.gcr.io/echoserver:1.4 输出结果为\ndeployment.apps/source-ip-app created . heading \u0026ldquo;objectives\u0026rdquo; %}}  通过多种类型的 Services 暴露一个简单应用 理解每种 Service 类型如何处理源 IP NAT 理解保留源IP所涉及的折中  Type=ClusterIP 类型 Services 的 Source IP 如果你的 kube-proxy 运行在 iptables 模式下，从集群内部发送到 ClusterIP 的包永远不会进行源地址 NAT，这从 Kubernetes 1.2 开始是默认选项。Kube-proxy 通过一个 proxyMode endpoint 暴露它的模式。\nkubectl get nodes 输出结果与以下结果类似:\nNAME STATUS ROLES AGE VERSION kubernetes-node-6jst Ready \u0026lt;none\u0026gt; 2h v1.13.0 kubernetes-node-cx31 Ready \u0026lt;none\u0026gt; 2h v1.13.0 kubernetes-node-jj1t Ready \u0026lt;none\u0026gt; 2h v1.13.0 从其中一个节点中得到代理模式\nkubernetes-node-6jst $ curl localhost:10249/proxyMode 输出结果为：\niptables 你可以通过在source IP应用上创建一个Service来测试源IP保留。\nkubectl expose deployment source-ip-app --name=clusterip --port=80 --target-port=8080 输出结果为：\nservice/clusterip exposed kubectl get svc clusterip 输出结果与以下结果类似：\nNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE clusterip ClusterIP 10.0.170.92 \u0026lt;none\u0026gt; 80/TCP 51s 从相同集群中的一个 pod 访问这个 ClusterIP：\nkubectl run busybox -it --image=busybox --restart=Never --rm 输出结果与以下结果类似：\nWaiting for pod default/busybox to be running, status is Pending, pod ready: false If you don't see a command prompt, try pressing enter. # ip addr 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 3: eth0: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1460 qdisc noqueue link/ether 0a:58:0a:f4:03:08 brd ff:ff:ff:ff:ff:ff inet 10.244.3.8/24 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::188a:84ff:feb0:26a5/64 scope link valid_lft forever preferred_lft forever # wget -qO - 10.0.170.92 CLIENT VALUES: client_address=10.244.3.8 command=GET ... 无论客户端 pod 和 服务端 pod 是否在相同的节点上，client_address 始终是客户端 pod 的 IP 地址。\nType=NodePort 类型 Services 的 Source IP 从 Kubernetes 1.5 开始，发送给类型为 Type=NodePort Services 的数据包默认进行源地址 NAT。你可以通过创建一个 NodePort Service 来进行测试：\nkubectl expose deployment source-ip-app --name=nodeport --port=80 --target-port=8080 --type=NodePort 输出结果为：\nservice/nodeport exposed NODEPORT=$(kubectl get -o jsonpath=\u0026quot;{.spec.ports[0].nodePort}\u0026quot; services nodeport) NODES=$(kubectl get nodes -o jsonpath='{ $.items[*].status.addresses[?(@.type==\u0026quot;ExternalIP\u0026quot;)].address }') 如果你的集群运行在一个云服务上，你可能需要为上面报告的 nodes:nodeport 开启一条防火墙规则。 现在，你可以通过上面分配的节点端口从外部访问这个 Service。\nfor node in $NODES; do curl -s $node:$NODEPORT | grep -i client_address; done 输出结果与以下结果类似：\nclient_address=10.180.1.1 client_address=10.240.0.5 client_address=10.240.0.3 请注意，这些并不是正确的客户端 IP，它们是集群的内部 IP。这是所发生的事情：\n 客户端发送数据包到 node2:nodePort node2 使用它自己的 IP 地址替换数据包的源 IP 地址（SNAT） node2 使用 pod IP 地址替换数据包的目的 IP 地址 数据包被路由到 node 1，然后交给 endpoint Pod 的回复被路由回 node2 Pod 的回复被发送回给客户端  用图表示：\n client \\ ^ \\ \\ v \\ node 1 \u0026lt;--- node 2 | ^ SNAT | | ---\u0026gt; v | endpoint 为了防止这种情况发生，Kubernetes 提供了一个特性来保留客户端的源 IP 地址(点击此处查看可用特性)。设置 service.spec.externalTrafficPolicy 的值为 Local，请求就只会被代理到本地 endpoints 而不会被转发到其它节点。这样就保留了最初的源 IP 地址。如果没有本地 endpoints，发送到这个节点的数据包将会被丢弃。这样在应用到数据包的任何包处理规则下，你都能依赖这个正确的 source-ip 使数据包通过并到达 endpoint。\n设置 service.spec.externalTrafficPolicy 字段如下：\nkubectl patch svc nodeport -p '{\u0026quot;spec\u0026quot;:{\u0026quot;externalTrafficPolicy\u0026quot;:\u0026quot;Local\u0026quot;}}' 输出结果为：\nservice/nodeport patched 现在，重新运行测试：\nfor node in $NODES; do curl --connect-timeout 1 -s $node:$NODEPORT | grep -i client_address; done 输出结果为：\nclient_address=104.132.1.79 请注意，你只从 endpoint pod 运行的那个节点得到了一个回复，这个回复有正确的客户端 IP。\n这是发生的事情：\n 客户端发送数据包到 node2:nodePort，它没有任何 endpoints 数据包被丢弃 客户端发送数据包到 node1:nodePort，它有endpoints node1 使用正确的源 IP 地址将数据包路由到 endpoint  用图表示：\n client ^ / \\ / / \\ / v X node 1 node 2 ^ | | | | v endpoint Type=LoadBalancer 类型 Services 的 Source IP 从Kubernetes1.5开始，发送给类型为 Type=LoadBalancer Services 的数据包默认进行源地址 NAT，这是因为所有处于 Ready 状态的可调度 Kubernetes 节点对于负载均衡的流量都是符合条件的。所以如果数据包到达一个没有 endpoint 的节点，系统将把这个包代理到有 endpoint 的节点，并替换数据包的源 IP 为节点的 IP（如前面章节所述）。\n你可以通过在一个 loadbalancer 上暴露这个 source-ip-app 来进行测试。\nkubectl expose deployment source-ip-app --name=loadbalancer --port=80 --target-port=8080 --type=LoadBalancer 输出结果为：\nservice/loadbalancer exposed 打印Service的IPs：\nkubectl get svc loadbalancer 输出结果与以下结果类似：\nNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE loadbalancer LoadBalancer 10.0.65.118 104.198.149.140 80/TCP 5m curl 104.198.149.140 输出结果与以下结果类似：\nCLIENT VALUES: client_address=10.240.0.5 ... 然而，如果你的集群运行在 Google Kubernetes Engine/GCE 上，可以通过设置 service.spec.externalTrafficPolicy 字段值为 Local ，故意导致健康检查失败来强制使没有 endpoints 的节点把自己从负载均衡流量的可选节点列表中删除。\n用图表示：\n client | lb VIP / ^ v / health check ---\u0026gt; node 1 node 2 \u0026lt;--- health check 200 \u0026lt;--- ^ | ---\u0026gt; 500 | V endpoint 你可以设置 annotation 来进行测试：\nkubectl patch svc loadbalancer -p '{\u0026quot;spec\u0026quot;:{\u0026quot;externalTrafficPolicy\u0026quot;:\u0026quot;Local\u0026quot;}}' 你应该能够立即看到 Kubernetes 分配的 service.spec.healthCheckNodePort 字段：\nkubectl get svc loadbalancer -o yaml | grep -i healthCheckNodePort 输出结果与以下结果类似：\n healthCheckNodePort: 32122 service.spec.healthCheckNodePort 字段指向每个节点在 /healthz 路径上提供的用于健康检查的端口。你可以这样测试：\nkubectl get pod -o wide -l run=source-ip-app 输出结果与以下结果类似：\nNAME READY STATUS RESTARTS AGE IP NODE source-ip-app-826191075-qehz4 1/1 Running 0 20h 10.180.1.136 kubernetes-node-6jst 使用 curl 命令发送请求到每个节点的 /healthz 路径。\nkubernetes-node-6jst $ curl localhost:32122/healthz 输出结果与以下结果类似：\n1 Service Endpoints found kubernetes-node-jj1t $ curl localhost:32122/healthz 输出结果与以下结果类似：\nNo Service Endpoints Found 主节点运行的 service 控制器负责分配 cloud loadbalancer。在这样做的同时，它也会分配指向每个节点的 HTTP 健康检查的 port/path。等待大约 10 秒钟之后，没有 endpoints 的两个节点的健康检查会失败，然后 curl 负载均衡器的 ip：\ncurl 104.198.149.140 输出结果与以下结果类似：\nCLIENT VALUES: client_address=104.132.1.79 ... 跨平台支持\n从 Kubernetes 1.5 开始，通过类型为 Type=LoadBalancer 的 Services 进行源 IP 保存的支持仅在一部分 cloudproviders 中实现（GCP and Azure）。你的集群运行的 cloudprovider 可能以某些不同的方式满足 loadbalancer 的要求：\n  使用一个代理终止客户端连接并打开一个到你的 nodes/endpoints 的新连接。在这种情况下，源 IP 地址将永远是云负载均衡器的地址而不是客户端的。\n  使用一个包转发器，因此从客户端发送到负载均衡器 VIP 的请求在拥有客户端源 IP 地址的节点终止，而不被中间代理。\n  第一类负载均衡器必须使用一种它和后端之间约定的协议来和真实的客户端 IP 通信，例如 HTTP X-FORWARDED-FOR 头，或者 proxy 协议。 第二类负载均衡器可以通过简单的在保存于 Service 的 service.spec.healthCheckNodePort 字段上创建一个 HTTP 健康检查点来使用上面描述的特性。\n. heading \u0026ldquo;cleanup\u0026rdquo; %}} 删除服务：\n$ kubectl delete svc -l run=source-ip-app 删除 Deployment、ReplicaSet 和 Pod：\n$ kubectl delete deployment source-ip-app . heading \u0026ldquo;whatsnext\u0026rdquo; %}}  学习更多关于 通过 services 连接应用 学习更多关于 负载均衡  "
},
{
	"uri": "https://lijun.in/tasks/run-application/run-stateless-application-deployment/",
	"title": "使用Deployment运行一个无状态应用",
	"tags": [],
	"description": "",
	"content": "本文介绍通过Kubernetes Deployment对象如何去运行一个应用.\n. heading \u0026ldquo;objectives\u0026rdquo; %}}  创建一个nginx deployment. 使用kubectl列举关于deployment信息. 更新deployment.  . heading \u0026ldquo;prerequisites\u0026rdquo; %}} . include \u0026ldquo;task-tutorial-prereqs.md\u0026rdquo; \u0026gt;}} . version-check \u0026gt;}}\n创建和探究一个nginx deployment 你可以通过创建一个Kubernetes Deployment对象来运行一个应用, 可以在一个YAML文件中描述Deployment. 例如, 下面这个YAML文件描述了一个运行nginx:1.7.9 Docker镜像的Deployment:\n. codenew file=\u0026quot;application/deployment.yaml\u0026rdquo; \u0026gt;}}\n  通过YAML文件创建一个Deployment:\n kubectl apply -f https://k8s.io/examples/application/deployment.yaml    展示Deployment相关信息:\nkubectl describe deployment nginx-deployment user@computer:~/website$ kubectl describe deployment nginx-deployment Name: nginx-deployment Namespace: default CreationTimestamp: Tue, 30 Aug 2016 18:11:37 -0700 Labels: app=nginx Annotations: deployment.kubernetes.io/revision=1 Selector: app=nginx Replicas: 2 desired | 2 updated | 2 total | 2 available | 0 unavailable StrategyType: RollingUpdate MinReadySeconds: 0 RollingUpdateStrategy: 1 max unavailable, 1 max surge Pod Template: Labels: app=nginx Containers: nginx: Image: nginx:1.7.9 Port: 80/TCP Environment: \u0026lt;none\u0026gt; Mounts: \u0026lt;none\u0026gt; Volumes: \u0026lt;none\u0026gt; Conditions: Type Status Reason ---- ------ ------ Available True MinimumReplicasAvailable Progressing True NewReplicaSetAvailable OldReplicaSets: \u0026lt;none\u0026gt; NewReplicaSet: nginx-deployment-1771418926 (2/2 replicas created) No events.    列出deployment创建的pods:\nkubectl get pods -l app=nginx NAME READY STATUS RESTARTS AGE nginx-deployment-1771418926-7o5ns 1/1 Running 0 16h nginx-deployment-1771418926-r18az 1/1 Running 0 16h    展示某一个pod信息:\nkubectl describe pod \u0026lt;pod-name\u0026gt;  该处 \u0026lt;pod-name\u0026gt; 指某一pod的名称.\n  更新deployment 你可以通过更新一个新的YAML文件来更新deployment. 下面的YAML文件指定该deployment镜像更新为nginx 1.8.\n. codenew file=\u0026quot;application/deployment-update.yaml\u0026rdquo; \u0026gt;}}\n  应用新的YAML:\n kubectl apply -f https://k8s.io/examples/application/deployment-update.yaml    查看该deployment创建的pods以新的名称同时删除旧的pods:\n kubectl get pods -l app=nginx    通过增加副本数来弹缩应用 你可以通过应用新的YAML文件来增加Deployment中pods的数量. 该YAML文件将replicas设置为4, 指定该Deployment应有4个pods:\n. codenew file=\u0026quot;application/deployment-scale.yaml\u0026rdquo; \u0026gt;}}\n  应用新的YAML文件:\n kubectl apply -f https://k8s.io/examples/application/deployment-scale.yaml    验证Deployment有4个pods:\n kubectl get pods -l app=nginx  输出的结果类似于:\n NAME READY STATUS RESTARTS AGE nginx-deployment-148880595-4zdqq 1/1 Running 0 25s nginx-deployment-148880595-6zgi1 1/1 Running 0 25s nginx-deployment-148880595-fxcez 1/1 Running 0 2m nginx-deployment-148880595-rwovn 1/1 Running 0 2m    删除deployment 通过名称删除deployment:\nkubectl delete deployment nginx-deployment  ReplicationControllers \u0026ndash; 旧的方式 创建一个多副本应用首选方法是使用Deployment,反过来使用ReplicaSet. 在Deployment和ReplicaSet加入到Kubernetes之前, 多副本应用通过ReplicationController来配置.\n. heading \u0026ldquo;whatsnext\u0026rdquo; %}}  了解更多 Deployment objects.  "
},
{
	"uri": "https://lijun.in/reference/access-authn-authz/bootstrap-tokens/",
	"title": "使用启动引导令牌（Bootstrap Tokens）认证",
	"tags": [],
	"description": "",
	"content": ". toc \u0026gt;}}\n概述 启动引导令牌是一种简单的持有者令牌（Bearer Token），这种令牌是在新建集群或者在现有集群中添加新节点时使用的。 它被设计成能够支持 kubeadm，但是也可以被用在其他的案例中以便用户在 不使用 kubeadm 的情况下启动集群。它也被设计成可以通过 RBAC 策略，结合 Kubelet TLS Bootstrapping 系统进行工作。\n启动引导令牌被定义成一个特定类型的 secrets(bootstrap.kubernetes.io/token)，并存在于 kube-system 命名空间中。然后这些 secrets 会被 API 服务器上的启动引导的认证器读取。 控制器管理器中的控制器 TokenCleaner 能够删除过期的令牌。在节点发现的过程中 Kubernetes 会使用特殊的 ConfigMap 对象。 控制器管理器中的 BootstrapSigner 控制器也会使用启动引导令牌为这类对象生成签名信息。\n目前，启动引导令牌处于 alpha 阶段，但是预期也不会有大的突破性变化。\n令牌格式 启动引导令牌使用 abcdef.0123456789abcdef 的形式。 更加规范地说，它们必须符合正则表达式 [a-z0-9]{6}\\.[a-z0-9]{16}。\n令牌的第一部分是 \u0026ldquo;Token ID\u0026rdquo; ，它是公共信息。用于引用某个令牌，并确保不会泄露认证所使用的秘密信息。 第二部分是“令牌秘密（Token Secret）”，它应该被共享给收信的第三方。\n启用启动引导令牌 所有与启动引导令牌相关的特性在 Kubernetes v1.6 版本中默认都是禁用的。\n你可以在 API 服务器上通过 --enable-bootstrap-token-auth 参数启用启动引导令牌。 你可以设置控制管理器的 --controllers 参数来启用启动引导令牌相关的控制器，例如 --controllers=*,tokencleaner,bootstrapsigner 。 在使用 kubeadm 时，这是自动完成的。\nHTTPS 调用中的令牌是这样使用的：\nAuthorization: Bearer 07401b.f395accd246ae52d 启动引导令牌的密文格式 每个合法的令牌背后对应着 kube-system 命名空间中的某个 Secret 对象。 你可以从 这里 找到完整设计文档。\n这是 secret 看起来的样子。注意，base64(string) 表示应该通过 base64 对值进行编码。 这里使用的是未解码的版本以便于阅读。\napiVersion: v1 kind: Secret metadata: name: bootstrap-token-07401b namespace: kube-system type: bootstrap.kubernetes.io/token data: description: base64(The default bootstrap token generated by \u0026#39;kubeadm init\u0026#39;.) token-id: base64(07401b) token-secret: base64(f395accd246ae52d) expiration: base64(2017-03-10T03:22:11Z) usage-bootstrap-authentication: base64(true) usage-bootstrap-signing: base64(true) secret 的类型必须是 bootstrap.kubernetes.io/token ，而且名字必须是 bootstrap-token-\u0026lt;token id\u0026gt;。 description 是人类可读的描述，而不应该是机器可读的信息。令牌 ID 和 Secret 是包含在数据字典中的。\nusage-bootstrap-* 成员表示这个 secret 的用途。启用时，值必须设置为 true。\nusage-bootstrap-authentication 表示令牌可以用于 API 服务器的认证。认证器会以 system:bootstrap:\u0026lt;Token ID\u0026gt; 认证。它被包含在 system:bootstrappers 组中。 命名和组是故意受限制的，以防止用户在启动引导后再使用这些令牌。\nusage-bootstrap-signing 表示令牌应该被用于 cluster-info ConfigMap 的签名，就像下面描述的那样。\nexpiration 数据成员显示了令牌在失效后到现在的时间。这是遵循 RFC3339 进行编码的 UTC 时间。 TokenCleaner 控制器会删除过期的令牌。\n使用 kubeadm 管理令牌 你可以使用 kubeadm 工具管理正在运行集群的令牌。它会从 kubeadm 创建的集群（/etc/kubernetes/admin.conf） 自动抓取默认管理员密码。你可以通过参数 --kubeconfig 对下面命令指定一个另外的 kubeconfig 文件抓取密码。\n kubeadm token list 列举了令牌，同时显示了它们的过期时间和用途。 kubeadm token create 创建一个新令牌。  --description 设置新令牌的描述。 --ttl duration 设置令牌从“现在”起到过期时间的差值。 默认是 0 ，也就是不过期。 --usages 设置令牌被使用的方式。默认是 signing,authentication。用途在上面已经描述。   kubeadm token delete \u0026lt;token id\u0026gt;|\u0026lt;token id\u0026gt;.\u0026lt;token secret\u0026gt; 删除令牌。 令牌可以只用 ID 来确认，也可以用整个令牌的值。如果只用 ID 的情况下，密文不匹配的令牌也会被删除。  ConfigMap 签名 除了认证之外，令牌可以用于签名 ConfigMap。这在集群启动过程的早期，在客户端信任 API 服务器之前被使用。 被签名的 ConfigMap 可以通过共享令牌被认证。\n被签名的 ConfigMap 是 cluster-info，存在于 kube-public 命名空间中。 典型的工作流中，客户端在未经认证和忽略 TLS 报错的状态下读取这个 ConfigMap。 通过 ConfigMap 中嵌入的签名校验 ConfigMap 的载荷。\nConfigMap 会是这个样子的：\napiVersion: v1 kind: ConfigMap metadata: name: cluster-info namespace: kube-public data: jws-kubeconfig-07401b: eyJhbGciOiJIUzI1NiIsImtpZCI6IjA3NDAxYiJ9..tYEfbo6zDNo40MQE07aZcQX2m3EB2rO3NuXtxVMYm9U kubeconfig: | apiVersion: v1 clusters: - cluster: certificate-authority-data: \u0026lt;really long certificate data\u0026gt; server: https://10.138.0.2:6443 name: \u0026#34;\u0026#34; contexts: [] current-context: \u0026#34;\u0026#34; kind: Config preferences: {} users: [] ConfigMap 的 kubeconfig 成员是一个填好了集群信息的配置文件。 这里主要交换的信息是 certificate-authority-data。在将来可能会有扩展。\n签名是一个 JWS 签名，使用了 \u0026ldquo;detached\u0026rdquo; 模式。为了检验签名，用户应该按照 JWS 规则 （base64 编码而忽略结尾的 =）对 kubeconfig 的载荷进行编码。完成编码的载荷会被通过插入 JWS 并存在于两个点的中间 ，用于形成一个完整的 JWS。可以使用令牌的完整信息（比如 07401b.f395accd246ae52d）作为共享密钥， 通过 HS256 方式 (HMAC-SHA256) 对 JWS 进行校验。 用户 必须 确保使用了 HS256。\n"
},
{
	"uri": "https://lijun.in/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/",
	"title": "关键插件 Pod 的调度保证",
	"tags": [],
	"description": "",
	"content": "除了在主机上运行的 Kubernetes 核心组件（如 api-server 、scheduler 、controller-manager）之外，还有许多插件，由于各种原因， 必须在常规集群节点（而不是 Kubernetes 主节点）上运行。 其中一些插件对于功能完备的群集至关重要，例如 Heapster、DNS 和 UI。 如果关键插件被逐出（手动或作为升级等其他操作的副作用）或者变成挂起状态，群集可能会停止正常工作。 关键插件进入挂起状态的例子有：集群利用率过高；被逐出的关键插件 Pod 释放了空间，但该空间被之前悬决的 Pod 占用；由于其它原因导致节点上可用资源的总量发生变化。\n标记关键 Pod 要将 pod 标记为关键性（critical），pod 必须在 kube-system 命名空间中运行（可通过参数配置）。 同时，需要将 priorityClassName 设置为 system-cluster-critical 或 system-node-critical ，后者是整个群集的最高级别。 或者，也可以为 Pod 添加名为 scheduler.alpha.kubernetes.io/critical-pod、值为空字符串的注解。 不过，这一注解从 1.13 版本开始不再推荐使用，并将在 1.14 中删除。\n"
},
{
	"uri": "https://lijun.in/tasks/administer-cluster/kubeadm/kubeadm-upgrade/",
	"title": "升级 kubeadm 集群",
	"tags": [],
	"description": "",
	"content": "本页介绍了如何将 kubeadm 创建的 Kubernetes 集群从 1.16.x 版本升级到 1.17.x 版本，以及从版本 1.17.x 升级到 1.17.y ，其中 y \u0026gt; x。\n要查看 kubeadm 创建的有关旧版本集群升级的信息，请参考以下页面：\n 将 kubeadm 集群从 1.15 升级到 1.16 将 kubeadm 集群从 1.14 升级到 1.15 将 kubeadm 集群从 1.13 升级到 1.14  高版本升级工作流如下：\n 升级主控制平面节点。 升级其他控制平面节点。 升级工作节点。  . heading \u0026ldquo;prerequisites\u0026rdquo; %}}  您需要有一个由 kubeadm 创建并运行着 1.16.0 或更高版本的 Kubernetes 集群。 禁用 Swap。 集群应使用静态的控制平面和 etcd pod 或者 外部 etcd。 务必仔细认真阅读[发行说明](. latest-release-notes \u0026gt;}})。 务必备份所有重要组件，例如存储在数据库中应用层面的状态。 kubeadm upgrade 不会影响您的工作负载，只会涉及 Kubernetes 内部的组件，但备份终究是好的。  附加信息  升级后，因为容器 spec 哈希值已更改，所以所有容器都会重新启动。 您只能从一个次版本升级到下一个次版本，或者同样次版本的补丁版。也就是说，升级时无法跳过版本。 例如，您只能从 1.y 升级到 1.y+1，而不能从 from 1.y 升级到 1.y+2。  确定要升级到哪个版本   找到最新的稳定版 1.17:\n. tabs name=\u0026quot;k8s_install_versions\u0026rdquo; \u0026gt;}} . tab name=\u0026quot;Ubuntu, Debian or HypriotOS\u0026rdquo; %}} apt update apt-cache policy kubeadm\n在列表中查找最新的 1.17 版本 它看起来应该是 1.17.x-00 ，其中 x 是最新的补丁 . /tab %}} . tab name=\u0026quot;CentOS, RHEL or Fedora\u0026rdquo; %}} yum list \u0026ndash;showduplicates kubeadm \u0026ndash;disableexcludes=kubernetes\n在列表中查找最新的 1.17 版本 它看起来应该是 1.17.x-0 ，其中 x 是最新的补丁 . /tab %}} . /tabs \u0026gt;}}\n  升级第一个控制平面节点   在第一个控制平面节点上，升级 kubeadm :\n. tabs name=\u0026quot;k8s_install_kubeadm_first_cp\u0026rdquo; \u0026gt;}} . tab name=\u0026quot;Ubuntu, Debian or HypriotOS\u0026rdquo; %}}\n用最新的修补程序版本替换 1.17.x-00 中的 x apt-mark unhold kubeadm \u0026amp;\u0026amp;\napt-get update \u0026amp;\u0026amp; apt-get install -y kubeadm=1.17.x-00 \u0026amp;\u0026amp;\napt-mark hold kubeadm . /tab %}} . tab name=\u0026quot;CentOS, RHEL or Fedora\u0026rdquo; %}}\n用最新的修补程序版本替换 1.17.x-0 中的 x yum install -y kubeadm-1.17.x-0 \u0026ndash;disableexcludes=kubernetes . /tab %}} . /tabs \u0026gt;}}\n    验证 kubeadm 版本：\nkubeadm version     腾空控制平面节点：\nkubectl drain $CP_NODE --ignore-daemonsets     在主节点上，运行:\nsudo kubeadm upgrade plan 您应该可以看到与下面类似的输出：\n[preflight] Running pre-flight checks. [upgrade] Making sure the cluster is healthy: [upgrade/config] Making sure the configuration is correct: [upgrade/config] Reading configuration from the cluster... [upgrade/config] FYI: You can look at this config file with \u0026#39;kubectl -n kube-system get cm kubeadm-config -oyaml\u0026#39; [upgrade] Fetching available versions to upgrade to [upgrade/versions] Cluster version: v1.16.0 [upgrade/versions] kubeadm version: v1.17.0 Components that must be upgraded manually after you have upgraded the control plane with \u0026#39;kubeadm upgrade apply\u0026#39;: COMPONENT CURRENT AVAILABLE Kubelet 1 x v1.16.0 v1.17.0 Upgrade to the latest version in the v1.13 series: COMPONENT CURRENT AVAILABLE API Server v1.16.0 v1.17.0 Controller Manager v1.16.0 v1.17.0 Scheduler v1.16.0 v1.17.0 Kube Proxy v1.16.0 v1.17.0 CoreDNS 1.6.2 1.6.5 Etcd 3.3.15 3.4.3-0 You can now apply the upgrade by executing the following command: kubeadm upgrade apply v1.17.0 _____________________________________________________________________ 此命令检查您的集群是否可以升级，并可以获取到升级的版本。\n    选择要升级到的版本，然后运行相应的命令。例如:\nsudo kubeadm upgrade apply v1.17.x  将 x 替换为您为此升级选择的修补程序版本。  您应该可以看见与下面类似的输出：\n[preflight] Running pre-flight checks. [upgrade] Making sure the cluster is healthy: [upgrade/config] Making sure the configuration is correct: [upgrade/config] Reading configuration from the cluster... [upgrade/config] FYI: You can look at this config file with \u0026#39;kubectl -n kube-system get cm kubeadm-config -oyaml\u0026#39; [upgrade/version] You have chosen to change the cluster version to \u0026#34;v1.17.0\u0026#34; [upgrade/versions] Cluster version: v1.16.0 [upgrade/versions] kubeadm version: v1.17.0 [upgrade/confirm] Are you sure you want to proceed with the upgrade? [y/N]: y [upgrade/prepull] Will prepull images for components [kube-apiserver kube-controller-manager kube-scheduler etcd] [upgrade/prepull] Prepulling image for component etcd. [upgrade/prepull] Prepulling image for component kube-scheduler. [upgrade/prepull] Prepulling image for component kube-apiserver. [upgrade/prepull] Prepulling image for component kube-controller-manager. [apiclient] Found 0 Pods for label selector k8s-app=upgrade-prepull-etcd [apiclient] Found 0 Pods for label selector k8s-app=upgrade-prepull-kube-scheduler [apiclient] Found 0 Pods for label selector k8s-app=upgrade-prepull-kube-controller-manager [apiclient] Found 0 Pods for label selector k8s-app=upgrade-prepull-kube-apiserver [apiclient] Found 1 Pods for label selector k8s-app=upgrade-prepull-etcd [apiclient] Found 1 Pods for label selector k8s-app=upgrade-prepull-kube-controller-manager [apiclient] Found 1 Pods for label selector k8s-app=upgrade-prepull-kube-scheduler [apiclient] Found 1 Pods for label selector k8s-app=upgrade-prepull-kube-apiserver [upgrade/prepull] Prepulled image for component etcd. [upgrade/prepull] Prepulled image for component kube-apiserver. [upgrade/prepull] Prepulled image for component kube-scheduler. [upgrade/prepull] Prepulled image for component kube-controller-manager. [upgrade/prepull] Successfully prepulled the images for all the control plane components [upgrade/apply] Upgrading your Static Pod-hosted control plane to version \u0026#34;v1.17.0\u0026#34;... Static pod: kube-apiserver-myhost hash: 6436b0d8ee0136c9d9752971dda40400 Static pod: kube-controller-manager-myhost hash: 8ee730c1a5607a87f35abb2183bf03f2 Static pod: kube-scheduler-myhost hash: 4b52d75cab61380f07c0c5a69fb371d4 [upgrade/etcd] Upgrading to TLS for etcd Static pod: etcd-myhost hash: 877025e7dd7adae8a04ee20ca4ecb239 [upgrade/staticpods] Moved new manifest to \u0026#34;/etc/kubernetes/manifests/etcd.yaml\u0026#34; and backed up old manifest to \u0026#34;/etc/kubernetes/tmp/kubeadm-backup-manifests-2019-03-14-20-52-44/etcd.yaml\u0026#34; [upgrade/staticpods] Waiting for the kubelet to restart the component [upgrade/staticpods] This might take a minute or longer depending on the component/version gap (timeout 5m0s) Static pod: etcd-myhost hash: 877025e7dd7adae8a04ee20ca4ecb239 Static pod: etcd-myhost hash: 877025e7dd7adae8a04ee20ca4ecb239 Static pod: etcd-myhost hash: 64a28f011070816f4beb07a9c96d73b6 [apiclient] Found 1 Pods for label selector component=etcd [upgrade/staticpods] Component \u0026#34;etcd\u0026#34; upgraded successfully! [upgrade/etcd] Waiting for etcd to become available [upgrade/staticpods] Writing new Static Pod manifests to \u0026#34;/etc/kubernetes/tmp/kubeadm-upgraded-manifests043818770\u0026#34; [upgrade/staticpods] Moved new manifest to \u0026#34;/etc/kubernetes/manifests/kube-apiserver.yaml\u0026#34; and backed up old manifest to \u0026#34;/etc/kubernetes/tmp/kubeadm-backup-manifests-2019-03-14-20-52-44/kube-apiserver.yaml\u0026#34; [upgrade/staticpods] Waiting for the kubelet to restart the component [upgrade/staticpods] This might take a minute or longer depending on the component/version gap (timeout 5m0s) Static pod: kube-apiserver-myhost hash: 6436b0d8ee0136c9d9752971dda40400 Static pod: kube-apiserver-myhost hash: 6436b0d8ee0136c9d9752971dda40400 Static pod: kube-apiserver-myhost hash: 6436b0d8ee0136c9d9752971dda40400 Static pod: kube-apiserver-myhost hash: b8a6533e241a8c6dab84d32bb708b8a1 [apiclient] Found 1 Pods for label selector component=kube-apiserver [upgrade/staticpods] Component \u0026#34;kube-apiserver\u0026#34; upgraded successfully! [upgrade/staticpods] Moved new manifest to \u0026#34;/etc/kubernetes/manifests/kube-controller-manager.yaml\u0026#34; and backed up old manifest to \u0026#34;/etc/kubernetes/tmp/kubeadm-backup-manifests-2019-03-14-20-52-44/kube-controller-manager.yaml\u0026#34; [upgrade/staticpods] Waiting for the kubelet to restart the component [upgrade/staticpods] This might take a minute or longer depending on the component/version gap (timeout 5m0s) Static pod: kube-controller-manager-myhost hash: 8ee730c1a5607a87f35abb2183bf03f2 Static pod: kube-controller-manager-myhost hash: 6f77d441d2488efd9fc2d9a9987ad30b [apiclient] Found 1 Pods for label selector component=kube-controller-manager [upgrade/staticpods] Component \u0026#34;kube-controller-manager\u0026#34; upgraded successfully! [upgrade/staticpods] Moved new manifest to \u0026#34;/etc/kubernetes/manifests/kube-scheduler.yaml\u0026#34; and backed up old manifest to \u0026#34;/etc/kubernetes/tmp/kubeadm-backup-manifests-2019-03-14-20-52-44/kube-scheduler.yaml\u0026#34; [upgrade/staticpods] Waiting for the kubelet to restart the component [upgrade/staticpods] This might take a minute or longer depending on the component/version gap (timeout 5m0s) Static pod: kube-scheduler-myhost hash: 4b52d75cab61380f07c0c5a69fb371d4 Static pod: kube-scheduler-myhost hash: a24773c92bb69c3748fcce5e540b7574 [apiclient] Found 1 Pods for label selector component=kube-scheduler [upgrade/staticpods] Component \u0026#34;kube-scheduler\u0026#34; upgraded successfully! [upload-config] storing the configuration used in ConfigMap \u0026#34;kubeadm-config\u0026#34; in the \u0026#34;kube-system\u0026#34; Namespace [kubelet] Creating a ConfigMap \u0026#34;kubelet-config-1.17\u0026#34; in namespace kube-system with the configuration for the kubelets in the cluster [kubelet-start] Downloading configuration for the kubelet from the \u0026#34;kubelet-config-1.17\u0026#34; ConfigMap in the kube-system namespace [kubelet-start] Writing kubelet configuration to file \u0026#34;/var/lib/kubelet/config.yaml\u0026#34; [bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials [bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token [bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster [addons] Applied essential addon: CoreDNS [addons] Applied essential addon: kube-proxy [upgrade/successful] SUCCESS! Your cluster was upgraded to \u0026#34;v1.17.0\u0026#34;. Enjoy! [upgrade/kubelet] Now that your control plane is upgraded, please proceed with upgrading your kubelets if you haven\u0026#39;t already done so.     手动升级你的 CNI 供应商插件。\n您的容器网络接口（CNI）应该提供了程序自身的升级说明。 检查插件页面查找您 CNI 所提供的程序，并查看是否需要其他升级步骤。\n如果 CNI 提供程序作为 DaemonSet 运行，则在其他控制平面节点上不需要此步骤。\n    取消对控制面节点的保护\nkubectl uncordon $CP_NODE    升级控制平面节点上的 kubelet 和 kubectl ： . tabs name=\u0026quot;k8s_install_kubelet\u0026rdquo; \u0026gt;}} . tab name=\u0026quot;Ubuntu, Debian or HypriotOS\u0026rdquo; %}} 用最新的修补程序版本替换 1.17.x-00 中的 x apt-mark unhold kubelet kubectl \u0026amp;\u0026amp;\napt-get update \u0026amp;\u0026amp; apt-get install -y kubelet=1.17.x-00 kubectl=1.17.x-00 \u0026amp;\u0026amp;\napt-mark hold kubelet kubectl . /tab %}} . tab name=\u0026quot;CentOS, RHEL or Fedora\u0026rdquo; %}}\n用最新的修补程序版本替换 1.17.x-00 中的 x yum install -y kubelet-1.17.x-0 kubectl-1.17.x-0 \u0026ndash;disableexcludes=kubernetes . /tab %}} . /tabs \u0026gt;}}\n    重启 kubelet\nsudo systemctl restart kubelet   升级其他控制平面节点  与第一个控制平面节点相同，但使用：  sudo kubeadm upgrade node experimental-control-plane 而不是：\nsudo kubeadm upgrade apply 也不需要 sudo kubeadm upgrade plan 。\n升级工作节点 工作节点上的升级过程应该一次执行一个节点，或者一次执行几个节点，以不影响运行工作负载所需的最小容量。\n升级 kubeadm   在所有工作节点升级 kubeadm :\n. tabs name=\u0026quot;k8s_install_kubeadm_worker_nodes\u0026rdquo; \u0026gt;}} . tab name=\u0026quot;Ubuntu, Debian or HypriotOS\u0026rdquo; %}}\n用最新的修补程序版本替换 1.17.x-00 中的 x apt-mark unhold kubeadm \u0026amp;\u0026amp;\napt-get update \u0026amp;\u0026amp; apt-get install -y kubeadm=1.17.x-00 \u0026amp;\u0026amp;\napt-mark hold kubeadm . /tab %}} . tab name=\u0026quot;CentOS, RHEL or Fedora\u0026rdquo; %}}\n用最新的修补程序版本替换 1.17.x-00 中的 x yum install -y kubeadm-1.17.x-0 \u0026ndash;disableexcludes=kubernetes . /tab %}} . /tabs \u0026gt;}}\n  保护节点   通过将节点标记为不可调度并逐出工作负载，为维护做好准备。运行：\nkubectl drain $NODE --ignore-daemonsets 您应该可以看见与下面类似的输出：\nnode/ip-172-31-85-18 cordoned WARNING: ignoring DaemonSet-managed Pods: kube-system/kube-proxy-dj7d7, kube-system/weave-net-z65qx node/ip-172-31-85-18 drained   升级 kubelet 配置   升级 kubelet 配置:\nsudo kubeadm upgrade node config --kubelet-version v1.14.x 用最新的修补程序版本替换 1.14.x-00 中的 x\n  升级 kubelet 与 kubectl   通过运行适用于您的 Linux 发行版包管理器升级 Kubernetes 软件包版本：\n. tabs name=\u0026quot;k8s_kubelet_and_kubectl\u0026rdquo; \u0026gt;}} . tab name=\u0026quot;Ubuntu, Debian or HypriotOS\u0026rdquo; %}}\n用最新的修补程序版本替换 1.17.x-00 中的 xs apt-mark unhold kubelet kubectl \u0026amp;\u0026amp;\napt-get update \u0026amp;\u0026amp; apt-get install -y kubelet=1.17.x-00 kubectl=1.17.x-00 \u0026amp;\u0026amp;\napt-mark hold kubelet kubectl . /tab %}} . tab name=\u0026quot;CentOS, RHEL or Fedora\u0026rdquo; %}}\n用最新的修补程序版本替换 1.17.x-00 中的 x yum install -y kubelet-1.17.x-0 kubectl-1.17.x-0 \u0026ndash;disableexcludes=kubernetes . /tab %}} . /tabs \u0026gt;}}\n    重启 kubelet\nsudo systemctl restart kubelet   取消对节点的保护   通过将节点标记为可调度，让节点重新上线:\nkubectl uncordon $NODE   验证集群的状态 在所有节点上升级 kubelet 后，通过从 kubectl 可以访问集群的任何位置运行以下命令，验证所有节点是否再次可用：\nkubectl get nodes STATUS 应显示所有节点为 Ready 状态，并且版本号已经被更新。\n从故障状态恢复 如果 kubeadm upgrade 失败并且没有回滚，例如由于执行期间意外关闭，您可以再次运行 kubeadm upgrade。 此命令是幂等的，并最终确保实际状态是您声明的所需状态。 要从故障状态恢复，您还可以运行 kubeadm upgrade --force 而不去更改集群正在运行的版本。\n它是怎么工作的 kubeadm upgrade apply 做了以下工作：\n 检查您的集群是否处于可升级状态:  API 服务器是可访问的 所有节点处于 Ready 状态 控制平面是健康的   强制执行版本 skew 策略。 确保控制平面的镜像是可用的或可拉取到服务器上。 升级控制平面组件或回滚（如果其中任何一个组件无法启动）。 应用新的 kube-dns 和 kube-proxy 清单，并强制创建所有必需的 RBAC 规则。 如果旧文件在 180 天后过期，将创建 API 服务器的新证书和密钥文件并备份旧文件。  kubeadm upgrade node experimental-control-plane 在其他控制平面节点上执行以下操作：\n 从集群中获取 kubeadm ClusterConfiguration。 可选地备份 kube-apiserver 证书。 升级控制平面组件的静态 Pod 清单。  "
},
{
	"uri": "https://lijun.in/tasks/access-application-cluster/communicate-containers-same-pod-shared-volume/",
	"title": "同 Pod 内的容器使用共享卷通信",
	"tags": [],
	"description": "",
	"content": "本文旨在说明如何让一个 Pod 内的两个容器使用一个卷（Volume）进行通信。\n. heading \u0026ldquo;prerequisites\u0026rdquo; %}} . include \u0026ldquo;task-tutorial-prereqs.md\u0026rdquo; \u0026gt;}} . version-check \u0026gt;}}\n创建一个包含两个容器的 Pod 在这个练习中，你会创建一个包含两个容器的 Pod。两个容器共享一个卷用于他们之间的通信。 Pod 的配置文件如下：\n. codenew file=\u0026quot;pods/two-container-pod.yaml\u0026rdquo; \u0026gt;}}\n在配置文件中，你可以看到 Pod 有一个共享卷，名为 shared-data。\n配置文件中的第一个容器运行了一个 nginx 服务器。共享卷的挂载路径是 /usr/share/nginx/html。 第二个容器是基于 debian 镜像的，有一个 /pod-data 的挂载路径。第二个容器运行了下面的命令然后终止。\necho Hello from the debian container \u0026gt; /pod-data/index.html  注意，第二个容器在 nginx 服务器的根目录下写了 index.html 文件。\n创建一个包含两个容器的 Pod：\nkubectl apply -f https://k8s.io/examples/pods/two-container-pod.yaml  查看 Pod 和容器的信息：\nkubectl get pod two-containers --output=yaml  这是输出的一部分：\napiVersion: v1 kind: Pod metadata: ... name: two-containers namespace: default ... spec: ... containerStatuses: - containerID: docker://c1d8abd1 ... image: debian ... lastState: terminated: ... name: debian-container ... - containerID: docker://96c1ff2c5bb ... image: nginx ... name: nginx-container ... state: running: ...  你可以看到 debian 容器已经被终止了，而 nginx 服务器依然在运行。\n进入 nginx 容器的 shell：\nkubectl exec -it two-containers -c nginx-container -- /bin/bash  在 shell 中，确认 nginx 还在运行。\nroot@two-containers:/# ps aux  输出类似于这样：\nUSER PID ... STAT START TIME COMMAND root 1 ... Ss 21:12 0:00 nginx: master process nginx -g daemon off;  回忆一下，debian 容器在 nginx 的根目录下创建了 index.html 文件。 使用 curl 向 nginx 服务器发送一个 GET 请求：\nroot@two-containers:/# apt-get update root@two-containers:/# apt-get install curl root@two-containers:/# curl localhost  输出表示 nginx 提供了 debian 容器写的页面：\nHello from the debian container  讨论 Pod 能有多个容器的主要原因是为了支持辅助应用（helper applications），以协助主应用（primary application）。 辅助应用的典型例子是数据抽取，数据推送和代理。辅助应用和主应用经常需要相互通信。 就如这个练习所示，通信通常是通过共享文件系统完成的，或者，也通过回环网络接口 localhost 完成。 举个网络接口的例子，web 服务器带有一个协助程序用于拉取 Git 仓库的更新。\n在本练习中的卷为 Pod 生命周期中的容器相互通信提供了一种方法。如果 Pod 被删除或者重建了， 任何共享卷中的数据都会丢失。\n. heading \u0026ldquo;whatsnext\u0026rdquo; %}}   更多学习内容 混合容器的方式。\n  学习 模块化架构的混合容器。\n  参见 配置一个使用存储卷的 Pod。\n  参见 [卷](/docs/api-reference/. param \u0026ldquo;version\u0026rdquo; \u0026gt;}}/#volume-v1-core)。\n  参见 [Pod](/docs/api-reference/. param \u0026ldquo;version\u0026rdquo; \u0026gt;}}/#pod-v1-core).\n  "
},
{
	"uri": "https://lijun.in/tasks/administer-cluster/enabling-endpointslices/",
	"title": "启用端点切片",
	"tags": [],
	"description": "",
	"content": "本页提供启用 Kubernetes 端点切片的总览\n. heading \u0026ldquo;prerequisites\u0026rdquo; %}} . include \u0026ldquo;task-tutorial-prereqs.md\u0026rdquo; \u0026gt;}} . version-check \u0026gt;}}\n介绍 端点切片为 Kubernetes 端点提供了可伸缩和可扩展的替代方案。它们建立在端点提供的功能基础之上，并以可伸缩的方式进行扩展。当服务具有大量（\u0026gt;100）网络端点， 它们将被分成多个较小的端点切片资源，而不是单个大型端点资源。\n启用端点切片 . feature-state for_k8s_version=\u0026quot;v1.16\u0026rdquo; state=\u0026quot;alpha\u0026rdquo; \u0026gt;}}\n. note \u0026gt;}}\n尽管端点切片最终可能会取代端点，但许多 Kubernetes 组件仍然依赖于端点。目前，启用端点切片应该被视为集群中端点的补充，而不是它们的替代。\n. /note \u0026gt;}}\n作为 Alpha 功能，默认情况下，Kubernetes 中未启用端点切片。启用端点切片需要对 Kubernetes 集群进行多达 3 项配置修改。\n要启用包括端点切片的 Discovery API 组，请使用运行时配置标志（--runtime-config=discovery.k8s.io/v1alpha1=true）。\n该逻辑负责监视服务，pod 和节点以及创建或更新与之关联，在端点切片控制器内的端点切片。 默认情况下，此功能处于禁用状态，但可以通过启用在 kube-controller-manager 控制器的标志（--controllers=endpointslice）来开启。\n对于像 kube-proxy 这样的 Kubernetes 组件真正开始使用端点切片，需要开启端点切片功能标志（--feature-gates=EndpointSlice=true）。\n使用端点切片 在集群中完全启用端点切片的情况下，您应该看到对应的每个端点资源的端点切片资源。除了兼容现有的端点功能，端点切片应包括拓扑等新的信息。它们将使集群中网络端点具有更强的可伸缩性，可扩展性。\n"
},
{
	"uri": "https://lijun.in/tasks/administer-cluster/namespaces-walkthrough/",
	"title": "命名空间演练",
	"tags": [],
	"description": "",
	"content": "Kubernetes . glossary_tooltip text=\u0026quot;命名空间\u0026rdquo; term_id=\u0026quot;namespace\u0026rdquo; \u0026gt;}} 有助于不同的项目、团队或客户去共享 Kubernetes 集群。\n名字空间通过以下方式实现这点：\n 为名字设置作用域. 为集群中的部分资源关联鉴权和策略的机制。  使用多个命名空间是可选的。\n此示例演示了如何使用 Kubernetes 命名空间细分群集。\n. heading \u0026ldquo;prerequisites\u0026rdquo; %}} . include \u0026ldquo;task-tutorial-prereqs.md\u0026rdquo; \u0026gt;}} . version-check \u0026gt;}}\n环境准备 此示例作如下假设：\n 您已拥有一个 配置好的 Kubernetes 集群。 您已对 Kubernetes 的 Pods, Services 和 Deployments 有基本理解。   理解默认命名空间  默认情况下，Kubernetes 集群会在配置集群时实例化一个默认命名空间，用以存放集群所使用的默认 Pods、Services 和 Deployments 集合。\n假设您有一个新的集群，您可以通过执行以下操作来检查可用的命名空间：\nkubectl get namespaces NAME STATUS AGE default Active 13m 创建新的命名空间 在本练习中，我们将创建两个额外的 Kubernetes 命名空间来保存我们的内容。\n我们假设一个场景，某组织正在使用共享的 Kubernetes 集群来支持开发和生产：\n开发团队希望在集群中维护一个空间，以便他们可以查看用于构建和运行其应用程序的 Pods、Services 和 Deployments 列表。在这个空间里，Kubernetes 资源被自由地加入或移除，对谁能够或不能修改资源的限制被放宽，以实现敏捷开发。\n运维团队希望在集群中维护一个空间，以便他们可以强制实施一些严格的规程，对谁可以或谁不可以操作运行生产站点的 Pods、Services 和 Deployments 集合进行控制。\n该组织可以遵循的一种模式是将 Kubernetes 集群划分为两个命名空间：development 和 production。\n让我们创建两个新的命名空间来保存我们的工作。\n文件 namespace-dev.json 描述了 development 命名空间:\n. codenew language=\u0026quot;json\u0026rdquo; file=\u0026quot;admin/namespace-dev.json\u0026rdquo; \u0026gt;}}\n使用 kubectl 创建 development 命名空间。\nkubectl create -f https://k8s.io/examples/admin/namespace-dev.json 将下列的内容保存到文件 namespace-prod.json 中，这些内容是对 production 命名空间的描述：\n. codenew language=\u0026quot;json\u0026rdquo; file=\u0026quot;admin/namespace-prod.json\u0026rdquo; \u0026gt;}}\n让我们使用 kubectl 创建 production 命名空间。\nkubectl create -f https://k8s.io/examples/admin/namespace-prod.json 为了确保一切正常，我们列出集群中的所有命名空间。\nkubectl get namespaces --show-labels NAME STATUS AGE LABELS default Active 32m \u0026lt;none\u0026gt; development Active 29s name=development production Active 23s name=production 在每个命名空间中创建 pod Kubernetes 命名空间为集群中的 Pods、Services 和 Deployments 提供了作用域。\n与一个命名空间交互的用户不会看到另一个命名空间中的内容。\n为了演示这一点，让我们在 development 命名空间中启动一个简单的 Deployment 和 Pod。\n我们首先检查一下当前的上下文：\nkubectl config view apiVersion: v1 clusters: - cluster: certificate-authority-data: REDACTED server: https://130.211.122.180 name: lithe-cocoa-92103_kubernetes contexts: - context: cluster: lithe-cocoa-92103_kubernetes user: lithe-cocoa-92103_kubernetes name: lithe-cocoa-92103_kubernetes current-context: lithe-cocoa-92103_kubernetes kind: Config preferences: {} users: - name: lithe-cocoa-92103_kubernetes user: client-certificate-data: REDACTED client-key-data: REDACTED token: 65rZW78y8HbwXXtSXuUw9DbP4FLjHi4b - name: lithe-cocoa-92103_kubernetes-basic-auth user: password: h5M0FtUUIflBSdI7 username: admin kubectl config current-context lithe-cocoa-92103_kubernetes 下一步是为 kubectl 客户端定义一个上下文，以便在每个命名空间中工作。\u0026ldquo;cluster\u0026rdquo; 和 \u0026ldquo;user\u0026rdquo; 字段的值将从当前上下文中复制。\nkubectl config set-context dev --namespace=development \\  --cluster=lithe-cocoa-92103_kubernetes \\  --user=lithe-cocoa-92103_kubernetes kubectl config set-context prod --namespace=production \\  --cluster=lithe-cocoa-92103_kubernetes \\  --user=lithe-cocoa-92103_kubernetes 默认地，上述命令会添加两个上下文到 .kube/config 文件中。 您现在可以查看上下文并根据您希望使用的命名空间并在这两个新的请求上下文之间切换。\n查看新的上下文：\nkubectl config view apiVersion: v1 clusters: - cluster: certificate-authority-data: REDACTED server: https://130.211.122.180 name: lithe-cocoa-92103_kubernetes contexts: - context: cluster: lithe-cocoa-92103_kubernetes user: lithe-cocoa-92103_kubernetes name: lithe-cocoa-92103_kubernetes - context: cluster: lithe-cocoa-92103_kubernetes namespace: development user: lithe-cocoa-92103_kubernetes name: dev - context: cluster: lithe-cocoa-92103_kubernetes namespace: production user: lithe-cocoa-92103_kubernetes name: prod current-context: lithe-cocoa-92103_kubernetes kind: Config preferences: {} users: - name: lithe-cocoa-92103_kubernetes user: client-certificate-data: REDACTED client-key-data: REDACTED token: 65rZW78y8HbwXXtSXuUw9DbP4FLjHi4b - name: lithe-cocoa-92103_kubernetes-basic-auth user: password: h5M0FtUUIflBSdI7 username: admin 让我们切换到 development 命名空间进行操作。\nkubectl config use-context dev 您可以使用下列命令验证当前上下文：\nkubectl config current-context dev 此时，我们从命令行向 Kubernetes 集群发出的所有请求都限定在 development 命名空间中。\n让我们创建一些内容。\nkubectl run snowflake --image=k8s.gcr.io/serve_hostname --replicas=2 我们刚刚创建了一个副本大小为 2 的 deployment，该 deployment 运行名为 snowflake 的 pod，其中包含一个仅提供主机名服务的基本容器。请注意，kubectl run 仅在 Kubernetes 集群版本 \u0026gt;= v1.2 时创建 deployment。如果您运行在旧版本上，则会创建 replication controller。如果期望执行旧版本的行为，请使用 --generator=run/v1 创建 replication controller。 参见 kubectl run 获取更多细节。\nkubectl get deployment NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE snowflake 2 2 2 2 2m kubectl get pods -l run=snowflake NAME READY STATUS RESTARTS AGE snowflake-3968820950-9dgr8 1/1 Running 0 2m snowflake-3968820950-vgc4n 1/1 Running 0 2m 这很棒，开发人员可以做他们想要的事情，而不必担心影响 production 命名空间中的内容。\n让我们切换到 production 命名空间，展示一个命名空间中的资源如何对另一个命名空间不可见。\nkubectl config use-context prod production 命名空间应该是空的，下列命令应该返回的内容为空。\nkubectl get deployment kubectl get pods 生产环境需要运行 cattle，让我们创建一些名为 cattle 的 pods。\nkubectl run cattle --image=k8s.gcr.io/serve_hostname --replicas=5 kubectl get deployment NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE cattle 5 5 5 5 10s kubectl get pods -l run=cattle NAME READY STATUS RESTARTS AGE cattle-2263376956-41xy6 1/1 Running 0 34s cattle-2263376956-kw466 1/1 Running 0 34s cattle-2263376956-n4v97 1/1 Running 0 34s cattle-2263376956-p5p3i 1/1 Running 0 34s cattle-2263376956-sxpth 1/1 Running 0 34s 此时，应该很清楚的展示了用户在一个命名空间中创建的资源对另一个命名空间是不可见的。\n随着 Kubernetes 中的策略支持的发展，我们将扩展此场景，以展示如何为每个命名空间提供不同的授权规则。\n"
},
{
	"uri": "https://lijun.in/setup/production-environment/turnkey/aws/",
	"title": "在 AWS EC2 上运行 Kubernetes",
	"tags": [],
	"description": "",
	"content": "本页面介绍了如何在 AWS 上安装 Kubernetes 集群。\n. heading \u0026ldquo;prerequisites\u0026rdquo; %}} 在 AWS 上创建 Kubernetes 集群，您将需要 AWS 的 Access Key ID 和 Secret Access Key。\n支持的生产级别工具  conjure-up 是 Kubernetes 的开源安装程序，可在 Ubuntu 上创建与原生 AWS 集成的 Kubernetes 集群。   Kubernetes Operations - 生产级 K8s 的安装、升级和管理。支持在 AWS 运行 Debian、Ubuntu、CentOS 和 RHEL。   CoreOS Tectonic 包括开源的 Tectonic 安装程序，它用于在 AWS 上创建带有 Container Linux 节点的 Kubernetes 集群。   起源于 CoreOS，Kubernetes Incubator 维护的 CLI 工具， kube-aws ，该工具使用 Container Linux 节点创建和管理 Kubernetes 集群，它使用了 AWS 工具：EC2、CloudFormation 和 Autoscaling。   KubeOne 是一个开源集群生命周期管理工具，它可用于创建，升级和管理高可用 Kubernetes 集群。  集群入门 命令行管理工具：kubectl 集群启动脚本将在您的工作站上为您提供一个 kubernetes 目录。 或者，您可以从此页面下载最新的 Kubernetes 版本。\n接下来，将适当的二进制文件夹添加到您的 PATH 以访问 kubectl：\n# macOS export PATH=\u0026lt;path/to/kubernetes-directory\u0026gt;/platforms/darwin/amd64:$PATH # Linux export PATH=\u0026lt;path/to/kubernetes-directory\u0026gt;/platforms/linux/amd64:$PATH 此工具的最新文档页面位于此处：kubectl 手册\n默认情况下，kubectl 将使用在集群启动期间生成的 kubeconfig 文件对 API 进行身份验证。 有关更多信息，请阅读 kubeconfig 文件。\n示例 请参阅一个简单的 nginx 示例试用您的新集群。\n“Guestbook” 应用程序是另一个入门 Kubernetes 的流行示例：[guestbook 示例](https://github.com/kubernetes/examples/tree/. param \u0026ldquo;githubbranch\u0026rdquo; \u0026gt;}}/guestbook/)。\n有关更完整的应用程序，请查看[示例目录](https://github.com/kubernetes/examples/tree/. param \u0026ldquo;githubbranch\u0026rdquo; \u0026gt;}}/)。\n集群伸缩 不支持通过 kubectl 添加和删除节点。您仍然可以通过调整在安装过程中创建的 Auto Scaling Group 中的 “Desired” 和 “Max” 属性来手动伸缩节点数量。\n集群拆除 确保您用于配置集群的环境变量已被导出，然后在运行如下在 Kubernetes 目录的脚本：\ncluster/kube-down.sh 支持等级    IaaS 提供商 配置管理 操作系统 网络 文档 符合率 支持等级     AWS kops Debian k8s (VPC) docs  Community (@justinsb)   AWS CoreOS CoreOS flannel docs  Community   AWS Juju Ubuntu flannel, calico, canal docs 100% Commercial, Community   AWS KubeOne Ubuntu, CoreOS, CentOS canal, weavenet docs 100% Commercial, Community    进一步阅读 请参阅 Kubernetes 文档了解有关管理和使用 Kubernetes 集群的更多详细信息。\n"
},
{
	"uri": "https://lijun.in/setup/production-environment/turnkey/azure/",
	"title": "在 Azure 上运行 Kubernetes",
	"tags": [],
	"description": "",
	"content": "Azure Kubernetes 服务 (AKS) Azure Kubernetes 服务提供了简单的 Kubernetes 集群部署方式。\n有关通过 Azure Kubernetes 服务将 Kubernetes 集群部署到 Azure 的示例：\n微软 Azure Kubernetes 服务\n定制部署：AKS 引擎 Azure Kubernetes 服务的核心是开源，并且可以在 GitHub 上让社区使用和参与贡献：AKS 引擎。旧版 ACS 引擎 代码库已被弃用，以支持AKS-engine。\n如果您需要在 Azure Kubernetes 服务正式支持的范围之外对部署进行自定义，则 AKS 引擎是一个不错的选择。这些自定义包括部署到现有虚拟网络中，利用多个代理程序池等。一些社区对 AKS 引擎的贡献甚至可能成为 Azure Kubernetes 服务的特性。\nAKS 引擎的输入是一个描述 Kubernetes 集群的 apimodel JSON 文件。它和用于直接通过 Azure Kubernetes 服务部署集群的 Azure 资源管理器（ARM）模板语法相似。产生的输出是一个 ARM 模板，可以将其签入源代码管理，并使用它将 Kubernetes 集群部署到 Azure。\n您可以按照 **AKS 引擎 Kubernetes 教程**开始使用。\n适用于 Azure 的 CoreOS Tectonic 适用于 Azure 的 CoreOS Tectonic Installer 是开源的，它可以让社区在 GitHub 上使用和参与贡献：Tectonic Installer。\n当您需要进行自定义集群时，Tectonic Installer是一个不错的选择，因为它是基于 Hashicorp 的 Terraform，Azure资源管理器（ARM）提供程序构建的。这使用户可以使用熟悉的 Terraform 工具进行自定义或集成。\n您可以开始使用 在 Azure 上安装 Tectonic 指南。\n"
},
{
	"uri": "https://lijun.in/tasks/administer-cluster/nodelocaldns/",
	"title": "在 Kubernetes 集群中使用 NodeLocal DNSCache",
	"tags": [],
	"description": "",
	"content": "本页概述了 Kubernetes 中的 NodeLocal DNSCache 功能。\n. heading \u0026ldquo;prerequisites\u0026rdquo; %}} . include \u0026ldquo;task-tutorial-prereqs.md\u0026rdquo; \u0026gt;}} . version-check \u0026gt;}}\n引言 NodeLocal DNSCache 通过在集群节点上作为 DaemonSet 运行 dns 缓存代理来提高集群 DNS 性能。 在当今的体系结构中，处于 ClusterFirst DNS 模式的 Pod 可以连接到 kube-dns serviceIP 进行 DNS 查询。 通过 kube-proxy 添加的 iptables 规则将其转换为 kube-dns/CoreDNS 端点。 借助这种新架构，Pods 将可以访问在同一节点上运行的 dns 缓存代理，从而避免了 iptables DNAT 规则和连接跟踪。 本地缓存代理将查询 kube-dns 服务以获取集群主机名的缓存缺失（默认为 cluster.local 后缀）。\n动机  使用当前的 DNS 体系结构，如果没有本地 kube-dns/CoreDNS 实例，则具有最高 DNS QPS 的 Pod 可能必须延伸到另一个节点。 在这种脚本下，拥有本地缓存将有助于改善延迟。   跳过 iptables DNAT 和连接跟踪将有助于减少 conntrack 竞争并避免 UDP DNS 条目填满 conntrack 表。   从本地缓存代理到 kube-dns 服务的连接可以升级到 TCP 。 TCP conntrack 条目将在连接关闭时被删除，相反 UDP 条目必须超时(默认 nf_conntrack_udp_timeout 是 30 秒)   将 DNS 查询从 UDP 升级到 TCP 将减少归因于丢弃的 UDP 数据包和 DNS 超时的尾部等待时间，通常长达 30 秒（3 次重试+ 10 秒超时）。   在节点级别对 dns 请求的度量和可见性。   可以重新启用负缓存，从而减少对 kube-dns 服务的查询数量。  架构图 启用 NodeLocal DNSCache 之后，这是 DNS 查询所遵循的路径：\n. figure src=\u0026rdquo;/images/docs/nodelocaldns.svg\u0026rdquo; alt=\u0026quot;NodeLocal DNSCache 流\u0026rdquo; title=\u0026quot;Nodelocal DNSCache 流\u0026rdquo; caption=\u0026quot;此图显示了 NodeLocal DNSCache 如何处理 DNS 查询。\u0026rdquo; \u0026gt;}}\n配置 可以使用以下命令启用此功能：\nKUBE_ENABLE_NODELOCAL_DNS=true go run hack/e2e.go -v --up\n这适用于在 GCE 上创建 e2e 集群。 在所有其他环境上，以下步骤将设置 NodeLocal DNSCache ：\n 可以使用 kubectl create -f 命令应用类似于这个的 Yaml 。   需要修改 kubelet 的 \u0026ndash;cluster-dns 标志以使用 NodeLocal DNSCache 正在侦听的 LOCAL_DNS IP（默认为 169.254.20.10 ）  启用后，node-local-dns Pods 将在每个集群节点上的 kube-system 名称空间中运行。 此 Pod 在缓存模式下运行 CoreDNS ，因此每个节点都可以使用不同插件公开的所有 CoreDNS 指标。\n功能可用性 可以在任何 K8s 版本中使用上面指定的 yaml 应用该插件。 功能支持如下所述：\n   k8s 版本 功能支持     1.15 Beta(默认情况下未启用)   1.13 Alpha(默认情况下未启用)    "
},
{
	"uri": "https://lijun.in/tasks/administer-cluster/sysctl-cluster/",
	"title": "在 Kubernetes 集群中使用 sysctl",
	"tags": [],
	"description": "",
	"content": ". feature-state for_k8s_version=\u0026quot;v1.11\u0026rdquo; state=\u0026quot;beta\u0026rdquo; \u0026gt;}}\n本文档介绍如何通过 sysctl 接口在 Kubernetes 集群中配置和使用内核参数。\n. heading \u0026ldquo;prerequisites\u0026rdquo; %}} . include \u0026ldquo;task-tutorial-prereqs.md\u0026rdquo; \u0026gt;}} . version-check \u0026gt;}}\n获取 Sysctl 的参数列表 在 Linux 中，管理员可以通过 sysctl 接口修改内核运行时的参数。在 /proc/sys/ 虚拟文件系统下存放许多内核参数。这些参数涉及了多个内核子系统，如：\n 内核子系统 (通常前缀为: kernel.) 网络子系统 (通常前缀为: net.) 虚拟内存子系统 (通常前缀为: vm.) MDADM 子系统 (通常前缀为: dev.) 更多子系统请参见 内核文档。  若要获取完整的参数列表，请执行以下命令\n$ sudo sysctl -a 启用非安全的 Sysctl 参数 sysctl 参数分为 安全 和 非安全的。安全 sysctl 参数除了需要设置恰当的命名空间外，在同一 node 上的不同 Pod 之间也必须是 相互隔离的。这意味着在 Pod 上设置 安全 sysctl 参数\n 必须不能影响到节点上的其他 Pod 必须不能损害节点的健康 必须不允许使用超出 Pod 的资源限制的 CPU 或内存资源。  至今为止，大多数 有命名空间的 sysctl 参数不一定被认为是 安全 的。以下几种 sysctl 参数是 安全的：\n kernel.shm_rmid_forced, net.ipv4.ip_local_port_range, net.ipv4.tcp_syncookies.  . note \u0026gt;}}\n注意: 示例中的 net.ipv4.tcp_syncookies 在Linux 内核 4.4 或更低的版本中是无命名空间的。 . /note \u0026gt;}}\n在未来的 Kubernetes 版本中，若kubelet 支持更好的隔离机制，则上述列表中将会列出更多 安全的 sysctl 参数。\n所有 安全的 sysctl 参数都默认启用。\n所有 非安全的 sysctl 参数都默认禁用，且必须由集群管理员在每个节点上手动开启。那些设置了不安全 sysctl 参数的 Pod 仍会被调度，但无法正常启动。\n参考上述警告，集群管理员只有在一些非常特殊的情况下（如：高可用或实时应用调整），才可以启用特定的 非安全的 sysctl 参数。如需启用 非安全的 sysctl 参数，请您在每个节点上分别设置 kubelet 命令行参数，例如：\n$ kubelet --allowed-unsafe-sysctls \\  \u0026#39;kernel.msg*,net.core.somaxconn\u0026#39; ... 如果您使用 minikube，可以通过 extra-config 参数来配置：\n$ minikube start --extra-config=\u0026#34;kubelet.AllowedUnsafeSysctls=kernel.msg*,net.core.somaxconn\u0026#34;... 只有 有命名空间的 sysctl 参数可以通过该方式启用。\n设置 Pod 的 Sysctl 参数 目前，在 Linux 内核中，有许多的 sysctl 参数都是 有命名空间的 。 这就意味着可以为节点上的每个 Pod 分别去设置它们的 sysctl 参数。 在 Kubernetes 中，只有那些有命名空间的 sysctl 参数可以通过 Pod 的 securityContext 对其进行配置。\n以下列出有命名空间的 sysctl 参数，在未来的 Linux 内核版本中，此列表可能会发生变化。\n kernel.shm*, kernel.msg*, kernel.sem, fs.mqueue.*, net.*（内核中网络配置项相关参数），如果它可以在容器命名空间里被更改。然而，也有一些特例 (例如，net.netfilter.nf_conntrack_max 和 net.netfilter.nf_conntrack_expect_max 可以在容器命名空间里被更改，但它们是非命名空间的)。  没有命名空间的 sysctl 参数称为 节点级别的 sysctl 参数。 如果需要对其进行设置，则必须在每个节点的操作系统上手动地去配置它们，或者通过在 DaemonSet 中运行特权模式容器来配置。\n可使用 pod 的 securityContext 来配置有命名空间的 sysctl 参数，securityContext 应用于同一个 pod 中的所有容器。\n此示例中，使用 Pod SecurityContext 来对一个安全的 sysctl 参数 kernel.shm_rmid_forced 以及两个非安全的 sysctl 参数 net.core.somaxconn和 kernel.msgmax 进行设置。在 Pod 规格中对 安全的 和 非安全的 sysctl 参数不做区分。\n. warning \u0026gt;}}\n为了避免破坏操作系统的稳定性，请您在了解变更后果之后再修改 sysctl 参数。 . /warning \u0026gt;}}\napiVersion: v1 kind: Pod metadata: name: sysctl-example spec: securityContext: sysctls: - name: kernel.shm_rmid_forced value: \u0026#34;0\u0026#34; - name: net.core.somaxconn value: \u0026#34;1024\u0026#34; - name: kernel.msgmax value: \u0026#34;65536\u0026#34; ... . warning \u0026gt;}}\n警告：由于 非安全的 sysctl 参数其本身具有不稳定性，在使用 非安全的 sysctl 参数时可能会导致一些严重问题，如容器的错误行为、机器资源不足或节点被完全破坏，用户需自行承担风险。 . /warning \u0026gt;}}\n最佳实践方案是将集群中具有特殊 sysctl 设置的节点视为 受感染的，并且只调度需要使用到特殊 sysctl 设置的 Pod 到这些节点上。 建议使用 Kubernetes 的 taints 和 toleration 特性 来实现它。\n设置了 非安全的 sysctl 参数的 pod，在禁用了以下两种 非安全的 sysctl 参数配置的节点上启动都会失败。与 节点级别的 sysctl 一样，建议开启 taints 和 toleration 特性 或 taints on nodes 以便将 Pod 调度到正确的节点之上。\nPodSecurityPolicy 您可以通过在 PodSecurityPolicy 的 forbiddenSysctls 和/或 allowedUnsafeSysctls 字段中，指定 sysctl 或填写 sysctl 匹配模式来进一步为 Pod 设置 sysctl 参数。sysctl 参数匹配模式以 * 字符结尾，如 kernel.*。 单独的 * 字符匹配所有 sysctl 参数。\n所有 安全的 sysctl 参数都默认启用。\nforbiddenSysctls 和 allowedUnsafeSysctls 的值都是字符串列表类型，可以添加 sysctl 参数名称，也可以添加 sysctl 参数匹配模式（以*结尾）。 只填写 * 则匹配所有的 sysctl 参数。\nforbiddenSysctls 字段用于禁用特定的 sysctl 参数。 您可以在列表中禁用安全和非安全的 sysctl 参数的组合。 要禁用所有的 sysctl 参数，请设置为 *。\n如果要在 allowedUnsafeSysctls 字段中指定一个非安全的 sysctl 参数，并且它在forbiddenSysctls 字段中未被禁用，则可以在 Pod 中通过 PodSecurityPolicy 启用该 sysctl 参数。 若要在 PodSecurityPolicy 中开启所有非安全的 sysctl 参数，请设 allowedUnsafeSysctls 字段值为 *。\nallowedUnsafeSysctls 与 forbiddenSysctls 两字段的配置不能重叠，否则这就意味着存在某个 sysctl 参数既被启用又被禁用。\n. warning \u0026gt;}}\n警告：如果您通过 PodSecurityPolicy 中的 allowedUnsafeSysctls 字段将非安全的 sysctl 参数列入白名单，但该 sysctl 参数未通过 kubelet 命令行参数 --allowed-unsafe-sysctls 在节点上将其列入白名单，则设置了这个 sysctl 参数的 Pod 将会启动失败。 . /warning \u0026gt;}}\n以下示例设置启用了以 kernel.msg 为前缀的非安全的 sysctl 参数，以及禁用了 sysctl 参数 kernel.shm_rmid_forced。\napiVersion: policy/v1beta1 kind: PodSecurityPolicy metadata: name: sysctl-psp spec: allowedUnsafeSysctls: - kernel.msg* forbiddenSysctls: - kernel.shm_rmid_forced ... "
},
{
	"uri": "https://lijun.in/tasks/administer-cluster/reconfigure-kubelet/",
	"title": "在实时集群上重新配置节点的 Kubelet",
	"tags": [],
	"description": "",
	"content": ". feature-state for_k8s_version=\u0026quot;v1.11\u0026rdquo; state=\u0026quot;beta\u0026rdquo; \u0026gt;}}\n动态Kubelet配置 引导你在一个运行的 Kubernetes 集群上更改每一个 Kubelet 的配置，通过部署 ConfigMap 并配置每个节点来使用它。\n. warning \u0026gt;}}\n警告：所有Kubelet配置参数都可以动态地更改，但这对某些参数来说是不安全的。在决定动态更改参数之前，你需要深刻理解这种变化将如何影响你的集群的行为。 在把一组节点推广到集群范围之前，都要仔细地测试这些节点上的配置变化。与配置相关的建议可以在具体的文件下找到，内联 KubeletConfiguration 类型文档。 . /warning \u0026gt;}}\n. heading \u0026ldquo;prerequisites\u0026rdquo; %}}  Kubernetes v1.11 或者更高版本配置在主节点和节点上 kubectl v1.11 或者更高版本和集群配置通信 The Kubelet --dynamic-config-dir flag 必须设置在节点的可写目录上  在你集群中的一个实时节点上配置Kubelet 基本工作流程概述 在实时集群中配置 Kubelet 的基本工作流程如下：\n 编写一个 YAML 或 JSON 的配置文件包含 Kubelet 的配置。 将此文件包装在 ConfigMap 中并将其保存到 Kubernetes 控制平面。 更新 Kubelet 的相应节点对象以使用此 ConfigMap。  每个 Kubelet 都会在其各自的节点对象上查看配置引用。当此引用更改时，Kubelet 将下载新配置， 更新本地引用以引用该文件，然后退出。 要想使该功能正常地工作，您必须运行操作系统级别的服务管理器（如systemd），如果退出，将重启Kubelet。 当Kubelet重新启动时，它将开始使用新配置。\n这个新配置完全地覆盖 --config 所提供的配置，并被命令行标志覆盖。新配置中未指定的值将收到适合配置版本的默认值 (e.g. kubelet.config.k8s.io/v1beta1)，除非被标志覆盖。\n这个节点的 Kubelet 配置状态通过命令 Node.Spec.Status.Config 获取。一旦您已经改变了一个节点去使用新的 ConfigMap ， 您就可以观察此状态以确认这个节点正在使用的预期配置。\n这个文档描述编辑节点信息用命令 kubectl edit，还有一些其他的方式去修改节点的规范，包括命令 kubectl patch， 例如，哪一个更利于脚本化的工作流程。\n这个文档仅仅讲述了在单节点上使用每一个 ConfigMap。请注意对于多个节点使用相同的 ConfigMap 也是有效的。\n. warning \u0026gt;}}\n警告：通过更新本地的 ConfigMap 有可能 会改变配置信息，这样会导致所有 Kubelets 所配置的 ConfigMap 同步更新。 它是更安全的去处理 ConfigMap 按照惯例不变，借助于 kubectl's --append-hash 选项，并逐步把更新推广到 Node.Spec.ConfigSource。 . /warning \u0026gt;}}\n节点授权器的自动RBAC规则 以前，您需要手动创建RBAC规则允许节点访问其分配的ConfigMaps。 节点授权器现在 自动地配置这些规则。\n生成包含当前配置的文件 动态 Kubelet 配置特性允许您提供覆盖对于整个配置对象，而不是每个字段的叠加。 这是一个 更简单的模型，可以更轻松地跟踪配置值的来源和调试问题。 然而，妥协是你必须从现有配置的认识开始， 以确保您只更改您打算修改的字段。\n理想情况下，Kubelet 将被引导从磁盘上的一个文件，并且你可以编辑这个文件（也可以是版本控制的）， 去创建第一个 Kubelet ConfigMap (参考文档 通过配置文件设置Kubelet参数)， 目前，Kubelet 使用 文件和命令行标志的组合 进行引导来覆盖文件中的配置。 作为解决方法，您可以生成一个配置文件包含节点当前的组态，通过 kubectl 代理访问Kubelet服务器的 configz 端点。 该端点在其当前实现中，旨在被用来作为一个调试辅助工具。在生产环境下，不要依赖此端点的特性。 下面的例子使用 jq 命令来简化使用 JSON。按照所写的步骤，您需要安装 jq ， 但如果您喜欢手动提取 kubeletconfig 子对象，您也可以完成这个任务。\n生成配置文件   选择一个节点去重新配置，在此示例中，此节点的名称为 NODE_NAME。\n  使用以下命令在后台启动 kubectl 代理：\nkubectl proxy --port=8001 \u0026amp;   运行以下命令从 configz 端点中下载并解压配置。这个命令很长，因此在复制和黏贴时要小心。 如果您使用 zsh，请注意常见的 zsh 配置添加反斜杠转义 URL 中变量名称周围的大括号的开始和结束。 例如：在粘贴期间，$ {NODE_NAME} 将被重写为 $\\{NODE_NAME\\}。 您必须在运行命令之前删除反斜杠，否则命令将失败。\n  NODE_NAME=\u0026#34;the-name-of-the-node-you-are-reconfiguring\u0026#34;; curl -sSL \u0026#34;http://localhost:8001/api/v1/nodes/${NODE_NAME}/proxy/configz\u0026#34; | jq \u0026#39;.kubeletconfig|.kind=\u0026#34;KubeletConfiguration\u0026#34;|.apiVersion=\u0026#34;kubelet.config.k8s.io/v1beta1\u0026#34;\u0026#39; \u0026gt; kubelet_configz_${NODE_NAME} . note \u0026gt;}}\n注意：You need to manually add the kind and apiVersion to the downloaded object，because they are not reported by the configz endpoint。 您需要手动将 kind 和 apiVersion 添加到下载对象中，因为它们不是由 configz 端点报出的。\n. /note \u0026gt;}}\n修改配置文件 使用文本编辑器，在这个文件里，改变之前的程序生成的一个参数。例如，你或许会修改 QPS 参数 eventRecordQPS。\n把配置文件推送到控制平面 用以下命令把配置文件推送到控制平面：\nkubectl -n kube-system create configmap my-node-config --from-file=kubelet=kubelet_configz_${NODE_NAME} --append-hash -o yaml 这是一个有效响应的例子：\napiVersion: v1 kind: ConfigMap metadata: creationTimestamp: 2017-09-14T20:23:33Z name: my-node-config-gkt4c2m4b2 namespace: kube-system resourceVersion: \u0026quot;119980\u0026quot; selfLink: /api/v1/namespaces/kube-system/configmaps/my-node-config-gkt4c2m4b2 uid: 946d785e-998a-11e7-a8dd-42010a800006 data: kubelet: | {...} ConfigMap 是在 kube-system 命名空间中创建的，因为 ConfigMap 配置了 Kubelet，它是 Kubernetes 的系统组件。\n--append-hash 选项给 ConfigMap 内容附加了一个简短校验和。这对于编辑然后推送工作流程很方便，因为它 自动并确定地为新 ConfigMaps 生成新的名称。在以下示例中，包含生成的哈希名称为 CONFIG_MAP_NAME。\n用新配置创建新的节点 用下面的命令行编辑节点的参数来指向新的 ConfigMap：\nkubectl edit node ${NODE_NAME} 在您的文本编辑器中，在 spec 下增添以下 YAML：\nconfigSource: configMap: name: CONFIG_MAP_NAME namespace: kube-system kubeletConfigKey: kubelet 您必须指定这三个参数中的每一个name，namespace和kubeletConfigKey。 kubeletConfigKey这个参数显示出 Kubelet 上的哪个 key 包含了 ConfigMap 的配置。\n使用新配置监察节点 用 kubectl get node ${NODE_NAME} -o yaml 命令回收节点并用命令 Node.Status.Config 检查节点状态配置。 在这个状态报告的配置里，对应这些配置源active，assigned和 lastKnownGood。\n active 是 Kubelet 当前运行的版本。 assigned 参数是 Kubelet 基于 Node.Spec.ConfigSource 的最新版本。 lastKnownGood 参数是 Kubelet 的回退版本，如果在 Node.Spec.ConfigSource 中分配了无效的配置。  如果用本地的配置部署节点，使其设置成默认值，这个lastKnownGood配置可能不存在。 在 Kubelet 配置好后，将更新 lastKnownGood 去匹配一个有效的 assigned 配置。 判断如何配置 Kubelet 的细节是使lastKnownGood不受 API 限制，但目前实施为 10 分钟的宽限期。\n您可以使用以下命令（using jq）过滤到配置状态：\nkubectl get no ${NODE_NAME} -o json | jq \u0026#39;.status.config\u0026#39; 以下是一个响应示例：\n{ \u0026#34;active\u0026#34;: { \u0026#34;configMap\u0026#34;: { \u0026#34;kubeletConfigKey\u0026#34;: \u0026#34;kubelet\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;my-node-config-9mbkccg2cc\u0026#34;, \u0026#34;namespace\u0026#34;: \u0026#34;kube-system\u0026#34;, \u0026#34;resourceVersion\u0026#34;: \u0026#34;1326\u0026#34;, \u0026#34;uid\u0026#34;: \u0026#34;705ab4f5-6393-11e8-b7cc-42010a800002\u0026#34; } }, \u0026#34;assigned\u0026#34;: { \u0026#34;configMap\u0026#34;: { \u0026#34;kubeletConfigKey\u0026#34;: \u0026#34;kubelet\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;my-node-config-9mbkccg2cc\u0026#34;, \u0026#34;namespace\u0026#34;: \u0026#34;kube-system\u0026#34;, \u0026#34;resourceVersion\u0026#34;: \u0026#34;1326\u0026#34;, \u0026#34;uid\u0026#34;: \u0026#34;705ab4f5-6393-11e8-b7cc-42010a800002\u0026#34; } }, \u0026#34;lastKnownGood\u0026#34;: { \u0026#34;configMap\u0026#34;: { \u0026#34;kubeletConfigKey\u0026#34;: \u0026#34;kubelet\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;my-node-config-9mbkccg2cc\u0026#34;, \u0026#34;namespace\u0026#34;: \u0026#34;kube-system\u0026#34;, \u0026#34;resourceVersion\u0026#34;: \u0026#34;1326\u0026#34;, \u0026#34;uid\u0026#34;: \u0026#34;705ab4f5-6393-11e8-b7cc-42010a800002\u0026#34; } } } 如果发生错误，Kubelet 会在 Node.Status.Config.Error 中显示出它的结构体。可能的错误列在 了解节点配置错误信息。 您可以在 Kubelet 日志中搜索相同的文本以获取更多详细信息和有关错误的上下文。\n做出更多的改变 按照下面的工作流程做出更多的改变并再次推送它们。你每次推送一个 ConfigMap 的新内容时，\u0026ndash;append-hash kubectl 选项都会给 ConfigMap 创建一个新的名称。 最安全的推出策略是首先创建一个新的 ConfigMap，然后更新 节点 以使用新的 ConfigMap。\n重置节点以使用其本地默认配置 重置节点去使用已经配好的的配置，用 kubectl edit node $ {NODE_NAME} 命令编辑节点，并删除 Node.Spec.ConfigSource 字段。\n监察正在使用本地默认配置的节点 在删除此字段后，Node.Status.Config 最终变成空，所有配置源都已重置为 nil，这表示 本地默认配置是assigned，active 和 lastKnownGood这三个参数，没有报告错误。\nKubectl 补丁示例 您可以使用几种不同的机制来更改节点的 configSource。 这个例子使用kubectl patch：\nkubectl patch node ${NODE_NAME} -p \u0026#34;{\\\u0026#34;spec\\\u0026#34;:{\\\u0026#34;configSource\\\u0026#34;:{\\\u0026#34;configMap\\\u0026#34;:{\\\u0026#34;name\\\u0026#34;:\\\u0026#34;${CONFIG_MAP_NAME}\\\u0026#34;,\\\u0026#34;namespace\\\u0026#34;:\\\u0026#34;kube-system\\\u0026#34;,\\\u0026#34;kubeletConfigKey\\\u0026#34;:\\\u0026#34;kubelet\\\u0026#34;}}}}\u0026#34; 了解 Kubelet 检查点的配置方式 当为节点分配新配置时，Kubelet 会在本地磁盘上，下载并解压配置负载为一组文件。Kubelet 还记录元数据 在本地跟踪已分配和最后已知良好的配置源，以便 Kubelet 知道在重新启动时使用哪个配置，即使 API 服务器变为不可用。在检查点配置和相关元数据之后，如果检测到已分配的配置改变了，则 Kubelet 退出。当 Kubelet 被 OS 级服务管理器（例如systemd）重新启动时，它会读取新的元数据并使用新配置。\n当记录的元数据已被完全解析时，意味着它包含的所有必需的信息去选择一个指定的 配置版本通常是UID和ResourceVersion。与Node.Spec.ConfigSource形成对比， 通过幂等namespace/name预期声明配置来标识目标 ConfigMap；Kubelet 尝试使用此 ConfigMap 的最新版本。\n当您在节点上调试问题时，可以检查 Kubelet 的配置元数据和检查点。Kubelet 的检查点目录结构是：\n- --dynamic-config-dir (root for managing dynamic config) | - meta | - assigned (encoded kubeletconfig/v1beta1.SerializedNodeConfigSource object, indicating the assigned config) | - last-known-good (encoded kubeletconfig/v1beta1.SerializedNodeConfigSource object, indicating the last-known-good config) | - checkpoints | - uid1 (dir for versions of object identified by uid1) | - resourceVersion1 (dir for unpacked files from resourceVersion1 of object with uid1) | - ... | - ... 了解节点配置错误信息 下表描述了使用 Dynamic Kubelet 配置时可能发生的错误消息。您可以在 Kubelet 日志中搜索相同的文本 来获取有关错误的其他详细信息和上下文。\n"
},
{
	"uri": "https://lijun.in/tasks/debug-application-cluster/local-debugging/",
	"title": "在本地开发和调试服务",
	"tags": [],
	"description": "",
	"content": "Kubernetes 应用程序通常由多个独立的服务组成，每个服务都在自己的容器中运行。 在远端的 Kubernetes 集群上开发和调试这些服务可能很麻烦，需要在运行的容器上打开 shell，然后在远端 shell 中运行您所需的工具。\ntelepresence 是一种工具，用于在本地轻松开发和调试服务，同时将服务代理到远程 Kubernetes 集群。 使用 telepresence 可以为本地服务使用自定义工具（如调试器和 IDE），并提供对 Configmap、Secrets 和远程集群上运行的服务的完全访问。\n Kubernetes 集群安装完毕 配置好 kubectl 与集群交互 Telepresence 安装完毕  打开终端，不带参数运行 telepresence，以打开 telepresence shell。这个 shell 在本地运行，使您可以完全访问本地文件系统。\ntelepresence shell 的使用方式多种多样。 例如，在你的笔记本电脑上写一个 shell 脚本，然后直接在 shell 中实时运行它。 您也可以在远端 shell 上执行此操作，但这样可能无法使用首选的代码编辑器，并且在容器终止时脚本将被删除。\n开发和调试现有的服务 在 Kubernetes 上开发应用程序时，通常对单个服务进行编程或调试。 服务可能需要访问其他服务以进行测试和调试。 一种选择是使用连续部署管道，但即使最快的部署管道也会在程序或调试周期中引入延迟。\n使用 --swap-deployment 选项将现有部署与 Telepresence 代理交换。交换允许您在本地运行服务并能够连接到远端的 Kubernetes 集群。远端的集群中的服务现在就可以访问本地运行的实例。\n到运行 telepresence 并带有 --swap-deployment 选项，请输入：\ntelepresence --swap-deployment $DEPLOYMENT_NAME\n这里的 $DEPLOYMENT_NAME 是您现有的部署名称。\n运行此命令将生成 shell。在 shell 中，启动您的服务。 然后，您就可以在本地对源代码进行编辑、保存并能看到更改立即生效。您还可以在调试器或任何其他本地开发工具中运行服务。\n. heading \u0026ldquo;whatsnext\u0026rdquo; %}} 如果您对实践教程感兴趣，请查看本教程，其中介绍了在 Google Kubernetes Engine 上本地开发 Guestbook 应用程序。\nTelepresence 有多种代理选项，以满足您的各种情况。\n要了解更多信息，请访问 Telepresence 网站。\n"
},
{
	"uri": "https://lijun.in/tasks/federation/set-up-placement-policies-federation/",
	"title": "在联邦中设置放置策略",
	"tags": [],
	"description": "",
	"content": ". deprecationfilewarning \u0026gt;}} . include \u0026ldquo;federation-deprecation-warning-note.md\u0026rdquo; \u0026gt;}} . /deprecationfilewarning \u0026gt;}}\n此页面显示如何使用外部策略引擎对联邦资源强制执行基于策略的放置决策。\n. heading \u0026ldquo;prerequisites\u0026rdquo; %}} 您需要一个正在运行的 Kubernetes 集群(它被引用为主机集群)。有关您的平台的安装说明，请参阅入门指南。\nDeploying 联邦并配置外部策略引擎 可以使用 kubefed init 部署联邦控制平面。\nDeploying 联邦控制平面之后，必须在联邦 API 服务器中配置一个准入控制器，该控制器强制执行从外部策略引擎接收到的放置决策。\nkubectl create -f scheduling-policy-admission.yaml  下图是准入控制器的 ConfigMap 示例：\n. codenew file=\u0026quot;federation/scheduling-policy-admission.yaml\u0026rdquo; \u0026gt;}}\nConfigMap 包含三个文件：\n config.yml 指定 调度策略 准入控制器配置文件的位置。 scheduling-policy-config.yml 指定与外部策略引擎联系所需的 kubeconfig 文件的位置。 该文件还可以包含一个 retryBackoff 值，该值以毫秒为单位控制初始重试 backoff 延迟。 opa-kubeconfig 是一个标准的 kubeconfig，包含联系外部策略引擎所需的 URL 和凭证。  编辑联邦 API 服务器部署以启用 SchedulingPolicy 准入控制器。\nkubectl -n federation-system edit deployment federation-apiserver  更新 Federation API 服务器命令行参数以启用准入控制器， 并将 ConfigMap 挂载到容器中。如果存在现有的 -enable-admissionplugins 参数，则追加 SchedulingPolicy 而不是添加另一行。\n--enable-admission-plugins=SchedulingPolicy --admission-control-config-file=/etc/kubernetes/admission/config.yml  将以下卷添加到联邦 API 服务器 pod：\n- name: admission-config configMap: name: admission  添加以下卷挂载联邦 API 服务器的 apiserver 容器：\nvolumeMounts: - name: admission-config mountPath: /etc/kubernetes/admission  Deploying 外部策略引擎 Open Policy Agent (OPA) 是一个开源的通用策略引擎， 您可以使用它在联邦控制平面中执行基于策略的放置决策。\n在主机群集中创建服务以联系外部策略引擎：\nkubectl create -f policy-engine-service.yaml  下面显示的是 OPA 的示例服务。\n. codenew file=\u0026quot;federation/policy-engine-service.yaml\u0026rdquo; \u0026gt;}}\n使用联邦控制平面在主机群集中创建部署：\nkubectl create -f policy-engine-deployment.yaml  下面显示的是 OPA 的部署示例。\n. codenew file=\u0026quot;federation/policy-engine-deployment.yaml\u0026rdquo; \u0026gt;}}\n通过 ConfigMaps 配置放置策略 外部策略引擎将发现在 Federation API 服务器的 kube-federation-scheduling-policy 命名空间中创建的放置策略。\n如果命名空间尚不存在，请创建它：\nkubectl --context=federation create namespace kube-federation-scheduling-policy  配置一个示例策略来测试外部策略引擎:\n# OPA supports a high-level declarative language named Rego for authoring and # enforcing policies. For more information on Rego, visit # http://openpolicyagent.org. # Rego policies are namespaced by the \u0026quot;package\u0026quot; directive. package kubernetes.placement # Imports provide aliases for data inside the policy engine. In this case, the # policy simply refers to \u0026quot;clusters\u0026quot; below. import data.kubernetes.clusters # The \u0026quot;annotations\u0026quot; rule generates a JSON object containing the key # \u0026quot;federation.kubernetes.io/replica-set-preferences\u0026quot; mapped to \u0026lt;preferences\u0026gt;. # The preferences values is generated dynamically by OPA when it evaluates the # rule. # # The SchedulingPolicy Admission Controller running inside the Federation API # server will merge these annotations into incoming Federated resources. By # setting replica-set-preferences, we can control the placement of Federated # ReplicaSets. # # Rules are defined to generate JSON values (booleans, strings, objects, etc.) # When OPA evaluates a rule, it generates a value IF all of the expressions in # the body evaluate successfully. All rules can be understood intuitively as # \u0026lt;head\u0026gt; if \u0026lt;body\u0026gt; where \u0026lt;body\u0026gt; is true if \u0026lt;expr-1\u0026gt; AND \u0026lt;expr-2\u0026gt; AND ... # \u0026lt;expr-N\u0026gt; is true (for some set of data.) annotations[\u0026quot;federation.kubernetes.io/replica-set-preferences\u0026quot;] = preferences { input.kind = \u0026quot;ReplicaSet\u0026quot; value = {\u0026quot;clusters\u0026quot;: cluster_map, \u0026quot;rebalance\u0026quot;: true} json.marshal(value, preferences) } # This \u0026quot;annotations\u0026quot; rule generates a value for the \u0026quot;federation.alpha.kubernetes.io/cluster-selector\u0026quot; # annotation. # # In English, the policy asserts that resources in the \u0026quot;production\u0026quot; namespace # that are not annotated with \u0026quot;criticality=low\u0026quot; MUST be placed on clusters # labelled with \u0026quot;on-premises=true\u0026quot;. annotations[\u0026quot;federation.alpha.kubernetes.io/cluster-selector\u0026quot;] = selector { input.metadata.namespace = \u0026quot;production\u0026quot; not input.metadata.annotations.criticality = \u0026quot;low\u0026quot; json.marshal([{ \u0026quot;operator\u0026quot;: \u0026quot;=\u0026quot;, \u0026quot;key\u0026quot;: \u0026quot;on-premises\u0026quot;, \u0026quot;values\u0026quot;: \u0026quot;[true]\u0026quot;, }], selector) } # Generates a set of cluster names that satisfy the incoming Federated # ReplicaSet's requirements. In this case, just PCI compliance. replica_set_clusters[cluster_name] { clusters[cluster_name] not insufficient_pci[cluster_name] } # Generates a set of clusters that must not be used for Federated ReplicaSets # that request PCI compliance. insufficient_pci[cluster_name] { clusters[cluster_name] input.metadata.annotations[\u0026quot;requires-pci\u0026quot;] = \u0026quot;true\u0026quot; not pci_clusters[cluster_name] } # Generates a set of clusters that are PCI certified. In this case, we assume # clusters are annotated to indicate if they have passed PCI compliance audits. pci_clusters[cluster_name] { clusters[cluster_name].metadata.annotations[\u0026quot;pci-certified\u0026quot;] = \u0026quot;true\u0026quot; } # Helper rule to generate a mapping of desired clusters to weights. In this # case, weights are static. cluster_map[cluster_name] = {\u0026quot;weight\u0026quot;: 1} { replica_set_clusters[cluster_name] } 下面显示的是创建示例策略的命令：\nkubectl --context=federation -n kube-federation-scheduling-policy create configmap scheduling-policy --from-file=policy.rego  这个示例策略说明了一些关键思想：\n 位置策略可以引用联邦资源中的任何字段。 放置策略可以利用外部上下文(例如，集群元数据)来做出决策。 管理策略可以集中管理。 策略可以定义简单的接口(例如 requirements -pci 注解)，以避免在清单中重复逻辑。  测试放置政策 注释其中一个集群以表明它是经过 PCI 认证的。\nkubectl --context=federation annotate clusters cluster-name-1 pci-certified=true  部署联邦副本来测试放置策略。\n. codenew file=\u0026quot;federation/replicaset-example-policy.yaml\u0026rdquo; \u0026gt;}}\n下面显示的命令用于部署与策略匹配的副本集。\nkubectl --context=federation create -f replicaset-example-policy.yaml  检查副本集以确认已应用适当的注解：\nkubectl --context=federation get rs nginx-pci -o jsonpath='{.metadata.annotations}'  "
},
{
	"uri": "https://lijun.in/setup/production-environment/turnkey/tencent/",
	"title": "在腾讯云容器服务上运行 Kubernetes",
	"tags": [],
	"description": "",
	"content": "腾讯云容器服务 腾讯云容器服务（TKE）提供本地 Kubernetes 容器管理服务。您只需几个步骤即可使用 TKE 部署和管理 Kubernetes 集群。有关详细说明，请参阅部署腾讯云容器服务。\nTKE 是认证的 Kubernetes 产品。它与原生 Kubernetes API 完全兼容。\n定制部署 腾讯 Kubernetes Engine 的核心是开源的，并且可以在 GitHub 上使用。\n使用 TKE 创建 Kubernetes 集群时，可以选择托管模式或独立部署模式。另外，您可以根据需要自定义部署。例如，您可以选择现有的 Cloud Virtual Machine 实例来创建集群，也可以在 IPVS 模式下启用 Kube-proxy。\n下一步 要了解更多信息，请参阅 TKE 文档。\n"
},
{
	"uri": "https://lijun.in/setup/production-environment/turnkey/alibaba-cloud/",
	"title": "在阿里云上运行 Kubernetes",
	"tags": [],
	"description": "",
	"content": "阿里云容器服务 阿里云容器服务使您可以在阿里云 ECS 实例集群上运行和管理 Docker 应用程序。它支持流行的开源容器编排引擎：Docker Swarm 和 Kubernetes。\n为了简化集群的部署和管理，请使用 容器服务 Kubernetes 版。您可以按照 Kubernetes 演练快速入门，其中有一些使用中文书写的容器服务 Kubernetes 版教程。\n要使用自定义二进制文件或开源版本的 Kubernetes，请按照以下说明进行操作。\n自定义部署 阿里云 Kubernetes Cloud Provider 实现 的源代码是开源的，可在 GitHub 上获得。\n有关更多信息，请参阅中文版本快速部署 Kubernetes - 阿里云上的VPC环境和英文版本。\n"
},
{
	"uri": "https://lijun.in/tasks/administer-cluster/declare-network-policy/",
	"title": "声明网络策略",
	"tags": [],
	"description": "",
	"content": "本文可以帮助您开始使用 Kubernetes 的 NetworkPolicy API 声明网络策略去管理 Pod 之间的通信\n. heading \u0026ldquo;prerequisites\u0026rdquo; %}} 您首先需要有一个支持网络策略的 Kubernetes 集群。已经有许多支持 NetworkPolicy 的网络提供商，包括：\n Calico Romana Weave 网络  注意：以上列表是根据产品名称按字母顺序排序，而不是按推荐或偏好排序。下面示例对于使用了上面任何提供商的 Kubernetes 集群都是有效的\n创建一个nginx deployment 并且通过服务将其暴露 为了查看 Kubernetes 网络策略是怎样工作的，可以从创建一个nginx deployment 并且通过服务将其暴露开始\n$ kubectl run nginx --image=nginx --replicas=2 deployment \u0026quot;nginx\u0026quot; created $ kubectl expose deployment nginx --port=80 service \u0026quot;nginx\u0026quot; exposed 在 default 命名空间下运行了两个 nginx pod，而且通过一个名字为 nginx 的服务进行了暴露\n$ kubectl get svc,pod NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE svc/kubernetes 10.100.0.1 \u0026lt;none\u0026gt; 443/TCP 46m svc/nginx 10.100.0.16 \u0026lt;none\u0026gt; 80/TCP 33s NAME READY STATUS RESTARTS AGE po/nginx-701339712-e0qfq 1/1 Running 0 35s po/nginx-701339712-o00ef 1/1 Running 0 35s 测试服务能够被其它的 pod 访问 您应该可以从其它的 pod 访问这个新的 nginx 服务。为了验证它，从 default 命名空间下的其它 pod 来访问该服务。请您确保在该命名空间下没有执行孤立动作。\n启动一个 busybox 容器，然后在容器中使用 wget 命令去访问 nginx 服务：\n$ kubectl run busybox --rm -ti --image=busybox /bin/sh Waiting for pod default/busybox-472357175-y0m47 to be running, status is Pending, pod ready: false Hit enter for command prompt / # wget --spider --timeout=1 nginx Connecting to nginx (10.100.0.16:80) / # 限制访问 nginx 服务 如果说您想限制 nginx 服务，只让那些拥有标签 access: true 的 pod 访问它，那么您可以创建一个只允许从那些 pod 连接的 NetworkPolicy：\nkind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: name: access-nginx spec: podSelector: matchLabels: run: nginx ingress: - from: - podSelector: matchLabels: access: \u0026#34;true\u0026#34; 为服务指定策略 使用 kubectl 工具根据上面的 nginx-policy.yaml 文件创建一个 NetworkPolicy：\n$ kubectl create -f nginx-policy.yaml networkpolicy \u0026quot;access-nginx\u0026quot; created 当访问标签没有定义时测试访问服务 如果您尝试从没有设定正确标签的 pod 中去访问 nginx 服务，请求将会超时：\n$ kubectl run busybox --rm -ti --image=busybox /bin/sh Waiting for pod default/busybox-472357175-y0m47 to be running, status is Pending, pod ready: false Hit enter for command prompt / # wget --spider --timeout=1 nginx Connecting to nginx (10.100.0.16:80) wget: download timed out / # 定义访问标签后再次测试 创建一个拥有正确标签的 pod，您将看到请求是被允许的：\n$ kubectl run busybox --rm -ti --labels=\u0026quot;access=true\u0026quot; --image=busybox /bin/sh Waiting for pod default/busybox-472357175-y0m47 to be running, status is Pending, pod ready: false Hit enter for command prompt / # wget --spider --timeout=1 nginx Connecting to nginx (10.100.0.16:80) / # "
},
{
	"uri": "https://lijun.in/concepts/cluster-administration/addons/",
	"title": "安装扩展（Addons）",
	"tags": [],
	"description": "",
	"content": "Add-ons 扩展了 Kubernetes 的功能。\n本文列举了一些可用的 add-ons 以及到它们各自安装说明的链接。\n每个 add-ons 按字母顺序排序 - 顺序不代表任何优先地位。\n网络和网络策略  ACI 通过 Cisco ACI 提供集成的容器网络和安全网络。 Calico 是一个安全的 L3 网络和网络策略提供者。 Canal 结合 Flannel 和 Calico，提供网络和网络策略。 Cilium 是一个 L3 网络和网络策略插件，能够透明的实施 HTTP/API/L7 策略。同时支持路由（routing）和叠加/封装（overlay/encapsulation）模式。 CNI-Genie 使 Kubernetes 无缝连接到一种 CNI 插件，例如：Flannel、Calico、Canal、Romana 或者 Weave。 Contiv 为多种用例提供可配置网络（使用 BGP 的原生 L3，使用 vxlan 的 overlay，经典 L2 和 Cisco-SDN/ACI）和丰富的策略框架。Contiv 项目完全开源。安装工具同时提供基于和不基于 kubeadm 的安装选项。 基于 Tungsten Fabric 的 Contrail，是一个开源的多云网络虚拟化和策略管理平台，Contrail 和 Tungsten Fabric 与业务流程系统（例如 Kubernetes、OpenShift、OpenStack和Mesos）集成在一起，并为虚拟机、容器或 Pod 以及裸机工作负载提供了隔离模式。 Flannel 是一个可以用于 Kubernetes 的 overlay 网络提供者。 Knitter 是为 kubernetes 提供复合网络解决方案的网络组件。 Multus 是一个多插件，可在 Kubernetes 中提供多种网络支持，以支持所有 CNI 插件（例如 Calico，Cilium，Contiv，Flannel），而且包含了在 Kubernetes 中基于 SRIOV、DPDK、OVS-DPDK 和 VPP 的工作负载。 NSX-T 容器插件（ NCP ）提供了 VMware NSX-T 与容器协调器（例如 Kubernetes）之间的集成，以及 NSX-T 与基于容器的 CaaS / PaaS 平台（例如关键容器服务（PKS）和 OpenShift）之间的集成。 Nuage 是一个 SDN 平台，可在 Kubernetes Pods 和非 Kubernetes 环境之间提供基于策略的联网，并具有可视化和安全监控。 Romana 是一个 pod 网络的层 3 解决方案，并且支持 NetworkPolicy API。Kubeadm add-on 安装细节可以在这里找到。 Weave Net 提供了在网络分组两端参与工作的网络和网络策略，并且不需要额外的数据库。  服务发现  CoreDNS 是一种灵活的，可扩展的 DNS 服务器，可以安装为集群内的 Pod 提供 DNS 服务。  可视化管理  Dashboard 是一个 Kubernetes 的 web 控制台界面。 Weave Scope 是一个图形化工具，用于查看你的 containers、 pods、services 等。 请和一个 Weave Cloud account 一起使用，或者自己运行 UI。  基础设施  KubeVirt 是可以让 Kubernetes 运行虚拟机的 add-ons。通常运行在裸机群集上。  遗留 Add-ons 还有一些其它 add-ons 归档在已废弃的 cluster/addons 路径中。\n维护完善的 add-ons 应该被链接到这里。欢迎提出 PRs！\n"
},
{
	"uri": "https://lijun.in/concepts/containers/container-environment-variables/",
	"title": "容器环境变量",
	"tags": [],
	"description": "",
	"content": "本文介绍容器环境中对容器可用的资源。\n容器环境 Kubernetes 容器环境为容器提供了几类重要的资源：\n 一个文件系统，其中包含一个镜像和一个或多个卷。 容器本身相关的信息。 集群中其他对象相关的信息。  容器信息 容器的 hostname 是容器所在的 Pod 名称。 可以通过 hostname 命令或调用 libc 中的 gethostname 函数来获取。\nPod 名称和名字空间可以通过 downward API 以环境变量方式访问。\n与 Docker 镜像中静态指定的环境变量一样，Pod 中用户定义的环境变量也可用于容器。\n集群信息 容器创建时运行的所有服务的列表都会作为环境变量提供给容器。 这些环境变量与 Docker 链接语法相匹配。\n对一个名为 foo ，映射到名为 bar 的容器端口的服务， 会定义如下变量：\nFOO_SERVICE_HOST=\u0026lt;服务所在的主机地址\u0026gt; FOO_SERVICE_PORT=\u0026lt;服务所启用的端口\u0026gt; 服务具有专用 IP 地址，如果启用了 [DNS 插件](http://releases.k8s.io/ param \u0026ldquo;githubbranch\u0026rdquo; \u0026gt;}}/cluster/addons/dns/)，还可以在容器中通过 DNS 进行访问。\n  查看容器生命周期挂钩（hooks）了解更多。 获取为容器生命周期事件附加处理程序的实践经验。  "
},
{
	"uri": "https://lijun.in/tasks/manage-daemon/rollback-daemon-set/",
	"title": "对 DaemonSet 执行回滚",
	"tags": [],
	"description": "",
	"content": "本文展示了如何对 DaemonSet 执行回滚。\n. heading \u0026ldquo;prerequisites\u0026rdquo; %}}  DaemonSet 滚动升级历史和 DaemonSet 回滚特性仅在 Kubernetes 1.7 及以后版本的 kubectl 中支持。 确保您了解如何 对 DaemonSet 执行滚动升级。  对 DaemonSet 执行回滚 步骤 1： 找到想要 DaemonSet 回滚到的历史版本（revision） 如果只想回滚到最后一个版本，可以跳过这一步。\n列出 DaemonSet 的所有版本：\nkubectl rollout history daemonset \u0026lt;daemonset-name\u0026gt; 该命令返回 DaemonSet 版本列表：\ndaemonsets \u0026#34;\u0026lt;daemonset-name\u0026gt;\u0026#34; REVISION CHANGE-CAUSE 1 ... 2 ... ...  在创建时，DaemonSet 的变化原因从 kubernetes.io/change-cause 注解（annotation）复制到其版本中。 用户可以在 kubectl 中指定 --record=true ，将执行的命令记录在变化原因注解中。  执行以下命令，来查看指定版本的详细信息：\nkubectl rollout history daemonset \u0026lt;daemonset-name\u0026gt; --revision=1 该命令返回相应版本的详细信息：\ndaemonsets \u0026#34;\u0026lt;daemonset-name\u0026gt;\u0026#34; with revision #1 Pod Template: Labels: foo=bar Containers: app: Image: ... Port: ... Environment: ... Mounts: ... Volumes: ... 步骤 2： 回滚到指定版本 # 在 --to-revision 中指定您从步骤 1 中获取的版本序号 kubectl rollout undo daemonset \u0026lt;daemonset-name\u0026gt; --to-revision=\u0026lt;revision\u0026gt; 如果成功，命令会返回：\ndaemonset \u0026#34;\u0026lt;daemonset-name\u0026gt;\u0026#34; rolled back 如果 --to-revision 参数未指定，将选中最近的版本。\n步骤 3： 观察 DaemonSet 回滚进度 kubectl rollout undo daemonset 向服务器表明启动 DaemonSet 回滚。 真正的回滚是在服务器端异步完成的。\n执行以下命令，来观察 DaemonSet 回滚进度：\nkubectl rollout status ds/\u0026lt;daemonset-name\u0026gt; 回滚完成时，输出形如：\ndaemonset \u0026#34;\u0026lt;daemonset-name\u0026gt;\u0026#34; successfully rolled out 理解 DaemonSet 版本 在前面的 kubectl rollout history 步骤中，您获得了一个版本列表，每个版本都存储在名为 ControllerRevision 的资源中。 ControllerRevision 仅在 Kubernetes 1.7 及以后的版本中可用。\n查找原始的版本资源，来查看每个版本中存储了什么内容：\nkubectl get controllerrevision -l \u0026lt;daemonset-selector-key\u0026gt;=\u0026lt;daemonset-selector-value\u0026gt; 该命令返回 ControllerRevisions 列表：\nNAME CONTROLLER REVISION AGE \u0026lt;daemonset-name\u0026gt;-\u0026lt;revision-hash\u0026gt; DaemonSet/\u0026lt;daemonset-name\u0026gt; 1 1h \u0026lt;daemonset-name\u0026gt;-\u0026lt;revision-hash\u0026gt; DaemonSet/\u0026lt;daemonset-name\u0026gt; 2 1h 每个 ControllerRevision 中存储了相应 DaemonSet 版本的注解和模板。\nkubectl rollout undo 采用特定 ControllerRevision ，并用 ControllerRevision 中存储的模板代替 DaemonSet 的模板。 kubectl rollout undo 相当于通过其他命令（如 kubectl edit 或 kubectl apply）将 DaemonSet 模板更新至先前的版本。\n注意 DaemonSet 版本只会向前滚动。 也就是说，回滚完成后，所回滚到的 ControllerRevision 版本号 (.revision 字段) 会增加。 例如，如果用户在系统中有版本 1 和版本 2，并从版本 2 回滚到版本 1 ，带有 .revision: 1 的ControllerRevision 将变为 .revision: 3。\n故障排除  查看 DaemonSet 滚动升级故障排除。  "
},
{
	"uri": "https://lijun.in/tasks/manage-daemon/update-daemon-set/",
	"title": "对 DaemonSet 执行滚动更新",
	"tags": [],
	"description": "",
	"content": "本文介绍了如何对 DaemonSet 执行滚动更新。\n. heading \u0026ldquo;prerequisites\u0026rdquo; %}}  Kubernetes 1.6 或者更高版本中才支持 DaemonSet 滚动更新功能。  DaemonSet 更新策略 DaemonSet 有两种更新策略：\n OnDelete: 使用 OnDelete 更新策略时，在更新 DaemonSet 模板后，只有当您手动删除老的 DaemonSet pods 之后，新的 DaemonSet pods 才会被自动创建。跟 Kubernetes 1.6 以前的版本类似。 RollingUpdate: 这是默认的更新策略。使用 RollingUpdate 更新策略时，在更新 DaemonSet 模板后，老的 DaemonSet pods 将被终止，并且将以受控方式自动创建新的 DaemonSet pods。  执行滚动更新 要启用 DaemonSet 的滚动更新功能，必须设置 .spec.updateStrategy.type 为 RollingUpdate。\n您可能想设置.spec.updateStrategy.rollingUpdate.maxUnavailable (默认为 1) 和.spec.minReadySeconds (默认为 0)。\n步骤 1: 检查 DaemonSet 的滚动更新策略 首先，检查 DaemonSet 的更新策略，确保已经将其设置为 RollingUpdate:\nkubectl get ds/\u0026lt;daemonset-name\u0026gt; -o go-template=\u0026#39;{{.spec.updateStrategy.type}}{{\u0026#34;\\n\u0026#34;}}\u0026#39; 如果还没在系统中创建 DaemonSet，请使用以下命令检查 DaemonSet 的清单：\nkubectl create -f ds.yaml --dry-run -o go-template=\u0026#39;{{.spec.updateStrategy.type}}{{\u0026#34;\\n\u0026#34;}}\u0026#39; 两个命令的输出都应该为：\nRollingUpdate 如果输出不是 RollingUpdate，请返回并相应地修改 DaemonSet 对象或者清单。\n步骤 2：使用 RollingUpdate 更新策略创建 DaemonSet 如果已经创建了 DaemonSet，则可以跳过该步骤并跳转到步骤 3。\n验证 DaemonSet 清单的更新策略后，创建 DaemonSet：\nkubectl create -f ds.yaml 或者，您打算使用 kubectl apply 更新 DaemonSet，请使用 kubectl apply 创建相同的 DaemonSet。\nkubectl apply -f ds.yaml 步骤 3：更新 DaemonSet 模板 对 RollingUpdate DaemonSet .spec.template 的任何更新都将触发滚动更新。这可以通过几个不同的 kubectl 命令来完成。\n声明式命令 如果您使用配置文件来更新 DaemonSets，请使用 kubectl apply:\nkubectl apply -f ds-v2.yaml 命令式命令 如果您使用命令式命令来更新 DaemonSets，请使用kubectl edit 或者 kubectl patch:\nkubectl edit ds/\u0026lt;daemonset-name\u0026gt; kubectl patch ds/\u0026lt;daemonset-name\u0026gt; -p=\u0026lt;strategic-merge-patch\u0026gt; 只更新容器镜像 如果您只需要更新 DaemonSet 模板里的容器镜像，比如，.spec.template.spec.containers[*].image, 请使用 kubectl set image:\nkubectl set image ds/\u0026lt;daemonset-name\u0026gt; \u0026lt;container-name\u0026gt;=\u0026lt;container-new-image\u0026gt; 步骤 4：查看滚动更新状态 最后，观察 DaemonSet 最新滚动更新的进度：\nkubectl rollout status ds/\u0026lt;daemonset-name\u0026gt; 当滚动更新完成时，输出结果如下：\ndaemonset \u0026#34;\u0026lt;daemonset-name\u0026gt;\u0026#34; successfully rolled out 故障排查 DaemonSet 滚动更新卡住 有时，DaemonSet 滚动更新可能会卡住。可能原因如下：\n一些节点资源用尽 由于新 DaemonSet pods 无法调度到至少一个节点时，滚动更新就会卡住。这可能是由于节点已经资源用尽。\n发生这种情况时，通过对 kubectl get nodes 和下面命令行的输出作比较，找出没有调度部署 DaemonSet pods 的节点：\nkubectl get pods -l \u0026lt;daemonset-selector-key\u0026gt;=\u0026lt;daemonset-selector-value\u0026gt; -o wide 一旦找到这些节点，从节点上删除一些非 DaemonSet pods，为新的 DaemonSet pods 腾出空间。\n. note \u0026gt;}} 当所删除的 pods 不受任何控制器管理，也不是多副本的 pods，上述操作将导致服务中断。 同时，上述操作也不会考虑 PodDisruptionBudget 所施加的约束。 . /note \u0026gt;}}\n滚动更新中断 如果最近的 DaemonSet 模板更新被破坏了，比如，容器处于崩溃循环状态或者容器镜像不存在(通常由于拼写错误)，就会发生 DaemonSet 滚动更新中断。\n要解决此问题，只需再次更新 DaemonSet 模板即可。以前不健康的滚动更新不会阻止新的滚动更新。\n时钟偏差 如果在 DaemonSet 中指定了 .spec.minReadySeconds，主节点和工作节点之间的时钟偏差会使 DaemonSet 无法检测到正确的滚动更新进度。\n. heading \u0026ldquo;whatsnext\u0026rdquo; %}}  查看任务: 在 DaemonSet 上执行回滚 查看概念: 创建 DaemonSet 以适应现有的 DaemonSet pods  "
},
{
	"uri": "https://lijun.in/reference/tools/",
	"title": "工具",
	"tags": [],
	"description": "",
	"content": "Kubernetes 包含一些内置工具，可以帮助用户更好的使用 Kubernetes 系统。\nKubectl kubectl 是 Kubernetes 命令行工具，可以用来操控 Kubernetes 集群。\nKubeadm kubeadm 是一个命令行工具，可以用来在物理机、云服务器或虚拟机（目前处于 alpha 阶段）上轻松部署一个安全可靠的 Kubernetes 集群。\nKubefed kubefed 是一个命令行工具，可以用来帮助用户管理联邦集群。\nMinikube minikube 是一个可以方便用户在其工作站点本地部署一个单节点 Kubernetes 集群的工具，用于开发和测试。\nDashboard Dashboard, 是 Kubernetes 基于 Web 的用户管理界面，允许用户部署容器化应用到 Kubernetes 集群，进行故障排查以及管理集群和集群资源。\nHelm Kubernetes Helm 是一个管理预先配置 Kubernetes 资源包的工具，这里的资源在 Helm 中也被称作 Kubernetes charts。\n使用 Helm：\n 查找并使用已经打包为 Kubernetes charts 的流行软件 分享您自己的应用作为 Kubernetes charts 为 Kubernetes 应用创建可重复执行的构建 为您的 Kubernetes 清单文件提供更智能化的管理 管理 Helm 软件包的发布  Kompose Kompose 一个转换工具，用来帮助 Docker Compose 用户迁移至 Kubernetes。\n使用 Kompose:\n 将一个 Docker Compose 文件解释成 Kubernetes 对象 将本地 Docker 开发 转变成通过 Kubernetes 来管理 转换 v1 或 v2 Docker Compose yaml 文件 或 分布式应用程序包  "
},
{
	"uri": "https://lijun.in/tasks/debug-application-cluster/debug-application/",
	"title": "应用故障排查",
	"tags": [],
	"description": "",
	"content": "本指南帮助用户来调试kubernetes上那些没有正常运行的应用。 本指南不能调试集群。如果想调试集群的话，请参阅这里。\n. toc \u0026gt;}}\n诊断问题 故障排查的第一步是先给问题分下类。这个问题是什么？Pods，Replication Controller或者Service？\n Debugging Pods Debugging Replication Controllers Debugging Services  Debugging Pods 调试pod的第一步是看一下这个pod的信息，用如下命令查看一下pod的当前状态和最近的事件：\n$ kubectl describe pods ${POD_NAME} 查看一下pod中的容器所处的状态。这些容器的状态都是Running吗？最近有没有重启过？\n后面的调试都是要依靠pods的状态的。\npod停留在pending状态 如果一个pod卡在Pending状态，则表示这个pod没有被调度到一个节点上。通常这是因为资源不足引起的。 敲一下kubectl describe ...这个命令，输出的信息里面应该有显示为什么没被调度的原因。 常见原因如下：\n  资源不足: 你可能耗尽了集群上所有的CPU和内存，此时，你需要删除pods，调整资源请求，或者增加节点。 更多信息请参阅Compute Resources document\n  使用了hostPort: 如果绑定一个pod到hostPort，那么能创建的pod个数就有限了。 多数情况下，hostPort是非必要的，而应该采用服务来暴露pod。 如果确实需要使用hostPort，那么能创建的pod的数量就是节点的个数。\n  pod停留在waiting状态 如果一个pod卡在Waiting状态，则表示这个pod已经调试到节点上，但是没有运行起来。 再次敲一下kubectl describe ...这个命令来查看相关信息。 最常见的原因是拉取镜像失败。可以通过以下三种方式来检查：\n 使用的镜像名字正确吗？ 镜像仓库里有没有这个镜像？ 用docker pull \u0026lt;image\u0026gt;命令手动拉下镜像试试。  pod处于crashing状态或者unhealthy 首先，看一下容器的log:\n$ kubectl logs ${POD_NAME} ${CONTAINER_NAME} 如果容器是crashed的，用如下命令可以看到crash的log:\n$ kubectl logs --previous ${POD_NAME} ${CONTAINER_NAME} 或者，用exec在容器内运行一些命令：\n$ kubectl exec ${POD_NAME} -c ${CONTAINER_NAME} -- ${CMD} ${ARG1} ${ARG2} ... ${ARGN} 注意：当一个pod内只有一个容器时，可以不带参数-c ${CONTAINER_NAME}。\n例如，名为Cassandra的pod，处于running态，要查看它的log，可运行如下命令：\n$ kubectl exec cassandra -- cat /var/log/cassandra/system.log 如果以上方法都不起作用，找到这个pod所在的节点并用SSH登录进去做进一步的分析。 通常情况下，是不需要在Kubernetes API中再给出另外的工具的。 因此，如果你发现需要ssh进一个主机来分析问题时，请在GitHub上提一个特性请求，描述一个你的场景并说明为什么已经提供的工具不能满足需求。\npod处于running态，但是没有正常工作 如果创建的pod不符合预期，那么创建pod的描述文件应该是存在某种错误的，并且这个错误在创建pod时被忽略掉。 通常pod的定义中，章节被错误的嵌套，或者一个字段名字被写错，都可能会引起被忽略掉。 例如，希望在pod中用命令行执行某个命令，但是将command写成commnd，pod虽然可以创建，但命令并没有执行。\n如何查出来哪里出错？ 首先，删掉这个pod再重新创建一个，重创时，像下面这样带着--validate这个参数： kubectl create --validate -f mypod.yaml，command写成commnd的拼写错误就会打印出来了。\nI0805 10:43:25.129850 46757 schema.go:126] unknown field: commnd I0805 10:43:25.129973 46757 schema.go:129] this may be a false alarm, see https://github.com/kubernetes/kubernetes/issues/6842 pods/mypod 如果上面方法没有看到相关异常的信息，那么接下来就要验证从apiserver获取到的pod是否与期望的一致，比如创建Pod的yaml文件是mypod.yaml。\n运行如下命令来获取apiserver创建的pod信息并保存成一个文件： kubectl get pods/mypod -o yaml \u0026gt; mypod-on-apiserver.yaml。\n然后手动对这两个文件进行比较: apiserver获得的yaml文件中的一些行，不在创建pod的yaml文件内，这是正常的。 如果创建Pod的yaml文件内的一些行，在piserver获得的yaml文件中不存在，可以说明创建pod的yaml中的定义有问题。\nDebugging Replication Controllers RC相当简单。他们要么能创建pod，要么不能。如果不能创建pod，请参阅上述Debugging Pods。\n也可以使用kubectl describe rc ${CONTROLLER_NAME}命令来监视RC相关的事件。\nDebugging Services 服务提供了多个Pod之间的负载均衡功能。 有一些常见的问题可以造成服务无法正常工作。以下说明将有助于调试服务的问题。\n首先，验证服务是否有端点。对于每一个Service对像，apiserver使endpoints资源可用。\n通过如下命令可以查看endpoints资源：\n$ kubectl get endpoints ${SERVICE_NAME} 确保endpoints与服务内容器个数一致。 例如，如果你创建了一个nginx服务，它有3个副本，那么你就会在这个服务的endpoints中看到3个不同的IP地址。\n服务缺少endpoints 如果缺少endpoints，请尝试使用服务的labels列出所有的pod。 假如有一个服务，有如下的label：\n... spec: - selector: name: nginx type: frontend 你可以使用如下命令列出与selector相匹配的pod，并验证这些pod是否归属于创建的服务：\n$ kubectl get pods --selector=name=nginx,type=frontend 如果pod列表附合预期，但是endpoints仍然为空，那么可能没有暴露出正确的端口。 如果服务指定了containerPort，但是列表中的Pod没有列出该端口，则不会将其添加到端口列表。\n验证该pod的containerPort与服务的containerPort是否匹配。\n网络业务不工作 如果可以连接到服务上，但是连接立即被断开了，并且在endpoints列表中有endpoints，可能是代理和pods之间不通。\n确认以下3件事情：\n Pods工作是否正常？ 看一下重启计数，并参阅Debugging Pods； 可以直接连接到pod上吗？获取pod的IP地址，然后尝试直接连接到该IP上； 应用是否在配置的端口上进行服务？Kubernetes不进行端口重映射，所以如果应用在8080端口上服务，那么containerPort字段就需要设定为8080。  更多信息 如果上述都不能解决你的问题，请按照Debugging Service document中的介绍来确保你的Service处于running态，有Endpoints，Pods真正的在服务；你有DNS在工作，安装了iptables规则，kube-proxy也没有异常行为。\n你也可以访问troubleshooting document来获取更多信息。\n"
},
{
	"uri": "https://lijun.in/tasks/debug-application-cluster/debug-application-introspection/",
	"title": "应用自测与调试",
	"tags": [],
	"description": "",
	"content": "运行应用时，不可避免的需要定位问题。 前面我们介绍了如何使用 kubectl get pods 来查询 pod 的简单信息。 除此之外，还有一系列的方法来获取应用的更详细信息。\n使用 kubectl describe pod 命令获取 pod 详情 与之前的例子类似，我们使用一个 Deployment 来创建两个 pod。\n. codenew file=\u0026quot;application/nginx-with-request.yaml\u0026rdquo; \u0026gt;}}\n使用如下命令创建 deployment：\nkubectl apply -f https://k8s.io/examples/application/nginx-with-request.yaml deployment.apps/nginx-deployment created 使用如下命令查看 pod 状态：\nkubectl get pods NAME READY STATUS RESTARTS AGE nginx-deployment-1006230814-6winp 1/1 Running 0 11s nginx-deployment-1006230814-fmgu3 1/1 Running 0 11s 我们可以使用 kubectl describe pod 命令来查询每个 pod 的更多信息，比如：\nkubectl describe pod nginx-deployment-1006230814-6winp Name:\tnginx-deployment-1006230814-6winp Namespace:\tdefault Node:\tkubernetes-node-wul5/10.240.0.9 Start Time:\tThu, 24 Mar 2016 01:39:49 +0000 Labels:\tapp=nginx,pod-template-hash=1006230814 Annotations: kubernetes.io/created-by={\u0026quot;kind\u0026quot;:\u0026quot;SerializedReference\u0026quot;,\u0026quot;apiVersion\u0026quot;:\u0026quot;v1\u0026quot;,\u0026quot;reference\u0026quot;:{\u0026quot;kind\u0026quot;:\u0026quot;ReplicaSet\u0026quot;,\u0026quot;namespace\u0026quot;:\u0026quot;default\u0026quot;,\u0026quot;name\u0026quot;:\u0026quot;nginx-deployment-1956810328\u0026quot;,\u0026quot;uid\u0026quot;:\u0026quot;14e607e7-8ba1-11e7-b5cb-fa16\u0026quot; ... Status:\tRunning IP:\t10.244.0.6 Controllers:\tReplicaSet/nginx-deployment-1006230814 Containers: nginx: Container ID:\tdocker://90315cc9f513c724e9957a4788d3e625a078de84750f244a40f97ae355eb1149 Image:\tnginx Image ID:\tdocker://6f62f48c4e55d700cf3eb1b5e33fa051802986b77b874cc351cce539e5163707 Port:\t80/TCP QoS Tier: cpu:\tGuaranteed memory:\tGuaranteed Limits: cpu:\t500m memory:\t128Mi Requests: memory:\t128Mi cpu:\t500m State:\tRunning Started:\tThu, 24 Mar 2016 01:39:51 +0000 Ready:\tTrue Restart Count:\t0 Environment: \u0026lt;none\u0026gt; Mounts: /var/run/secrets/kubernetes.io/serviceaccount from default-token-5kdvl (ro) Conditions: Type Status Initialized True Ready True PodScheduled True Volumes: default-token-4bcbi: Type:\tSecret (a volume populated by a Secret) SecretName:\tdefault-token-4bcbi Optional: false QoS Class: Guaranteed Node-Selectors: \u0026lt;none\u0026gt; Tolerations: \u0026lt;none\u0026gt; Events: FirstSeen\tLastSeen\tCount\tFrom\tSubobjectPath\tType\tReason\tMessage ---------\t--------\t-----\t----\t-------------\t--------\t------\t------- 54s\t54s\t1\t{default-scheduler }\tNormal\tScheduled\tSuccessfully assigned nginx-deployment-1006230814-6winp to kubernetes-node-wul5 54s\t54s\t1\t{kubelet kubernetes-node-wul5}\tspec.containers{nginx}\tNormal\tPulling\tpulling image \u0026quot;nginx\u0026quot; 53s\t53s\t1\t{kubelet kubernetes-node-wul5}\tspec.containers{nginx}\tNormal\tPulled\tSuccessfully pulled image \u0026quot;nginx\u0026quot; 53s\t53s\t1\t{kubelet kubernetes-node-wul5}\tspec.containers{nginx}\tNormal\tCreated\tCreated container with docker id 90315cc9f513 53s\t53s\t1\t{kubelet kubernetes-node-wul5}\tspec.containers{nginx}\tNormal\tStarted\tStarted container with docker id 90315cc9f513 这里可以看到容器和 pod 的 label、资源需求等配置信息，还可以看到状态、就绪状态、重启次数、事件等状态信息。\n容器状态包括 Waiting、Running 和 Terminated。 根据状态的不同，将提供额外的信息——在这里您可以看到，对于处于运行状态的容器，系统会告诉您容器的启动时间。\nReady 指示是否通过了最后一个就绪探测。 (在本例中，容器没有配置就绪探测；如果没有配置就绪探测，则假定容器已经就绪。)\nRestart Count 告诉您容器已重启的次数； 这些信息对于定位配置了“always”重启策略的容器持续崩溃问题非常有用。\n目前，唯一与 Pod 有关的状态是就绪状态，这表明 Pod 能够为请求提供服务，并且应该添加到相应服务的负载平衡池中。\n最后，您将看到与 Pod 相关的近期事件。 系统通过指示第一次和最后一次看到它以及看到它的次数来压缩多个相同的事件。 “From”表示记录事件的组件， “SubobjectPath”告诉您引用了哪个对象(例如pod中的容器)， “Reason”和“Message”告诉您发生了什么。\n例子: 调试 Pending Pods 一个常见的场景是，当你创建了一个 Pod，但是他没有被调度到任何一个 node，可以使用 event 来调试。 比如说，这个 Pod 需求的资源比较多，没有任何一个 node 能够满足，或者它指定了一个标签，没有 node 被匹配到。 假如，我们创建之前的 Deployment 时指定副本数是5（不再是2），并且需求 600 millicore（不再是 500）， 对于一个4个节点的集群并且每个节点只有1个CPU。 这个情况下，至少有一个 Pod 不能被调度。 （需要注意的是，其他集群组件，比如 fluentd、skydns等等会在每个 node 上运行， 如果我们需求 1000 millicore，那么将不会有 Pod 会被调度。）\nkubectl get pods NAME READY STATUS RESTARTS AGE nginx-deployment-1006230814-6winp 1/1 Running 0 7m nginx-deployment-1006230814-fmgu3 1/1 Running 0 7m nginx-deployment-1370807587-6ekbw 1/1 Running 0 1m nginx-deployment-1370807587-fg172 0/1 Pending 0 1m nginx-deployment-1370807587-fz9sd 0/1 Pending 0 1m 为了查找 Pod nginx-deployment-1370807587-fz9sd 没有运行的原因，我们可以使用 kubectl describe pod 命令查询处理 pending 状态的 Pod：\nkubectl describe pod nginx-deployment-1370807587-fz9sd Name:\tnginx-deployment-1370807587-fz9sd Namespace:\tdefault Node:\t/ Labels:\tapp=nginx,pod-template-hash=1370807587 Status:\tPending IP: Controllers:\tReplicaSet/nginx-deployment-1370807587 Containers: nginx: Image:\tnginx Port:\t80/TCP QoS Tier: memory:\tGuaranteed cpu:\tGuaranteed Limits: cpu:\t1 memory:\t128Mi Requests: cpu:\t1 memory:\t128Mi Environment Variables: Volumes: default-token-4bcbi: Type:\tSecret (a volume populated by a Secret) SecretName:\tdefault-token-4bcbi Events: FirstSeen\tLastSeen\tCount\tFrom\tSubobjectPath\tType\tReason\tMessage ---------\t--------\t-----\t----\t-------------\t--------\t------\t------- 1m\t48s\t7\t{default-scheduler }\tWarning\tFailedScheduling\tpod (nginx-deployment-1370807587-fz9sd) failed to fit in any node fit failure on node (kubernetes-node-6ta5): Node didn't have enough resource: CPU, requested: 1000, used: 1420, capacity: 2000 fit failure on node (kubernetes-node-wul5): Node didn't have enough resource: CPU, requested: 1000, used: 1100, capacity: 2000 这里你可以看到由调度器记录的事件，它表明了 Pod 不能被调度的原因是 FailedScheduling（也可能是其他值）。 这个信息表明，没有任何 node 拥有足够多的资源。\n要纠正这种情况，可以使用“kubectl scale”更新部署，以指定四个或更少的副本。 (或者你可以让 Pod 继续保持这个状态，这是无害的。)\n与您在“kubectl describe pod”结尾处看到的一样，这些事件都将保存在 etcd 中，并提供关于集群中正在发生的事情的高级信息。 如果需要列出所有事件，可使用命令：\nkubectl get events 但是，需要注意的是，事件是按照 namespace 分组的。 如果你对些些 namespace 下的对象感兴趣（比如查看 namespace my-namespace 下的 Pod 事件）, 你需要显式的在命令行中指定 namespace：\nkubectl get events --namespace=my-namespace 查看所有 namespace 的事件，可使用 --all-namespaces 参数：\n除了 kubectl describe pod 以外，另一种获取 Pod 额外信息（超越 kubectl get pod）的方法是给 kubectl get pod 增加 -o yaml 输出格式参数。 这将以YAML格式为您提供比“kubectl describe pod”更多的信息——实际上是系统拥有的关于pod的所有信息。 在这里，您将看到注释(没有标签限制的键值元数据，由Kubernetes系统组件在内部使用)、重启策略、端口和卷。\nkubectl get pod nginx-deployment-1006230814-6winp -o yaml apiVersion: v1 kind: Pod metadata: annotations: kubernetes.io/created-by: | {\u0026#34;kind\u0026#34;:\u0026#34;SerializedReference\u0026#34;,\u0026#34;apiVersion\u0026#34;:\u0026#34;v1\u0026#34;,\u0026#34;reference\u0026#34;:{\u0026#34;kind\u0026#34;:\u0026#34;ReplicaSet\u0026#34;,\u0026#34;namespace\u0026#34;:\u0026#34;default\u0026#34;,\u0026#34;name\u0026#34;:\u0026#34;nginx-deployment-1006230814\u0026#34;,\u0026#34;uid\u0026#34;:\u0026#34;4c84c175-f161-11e5-9a78-42010af00005\u0026#34;,\u0026#34;apiVersion\u0026#34;:\u0026#34;extensions\u0026#34;,\u0026#34;resourceVersion\u0026#34;:\u0026#34;133434\u0026#34;}} creationTimestamp: 2016-03-24T01:39:50Z generateName: nginx-deployment-1006230814- labels: app: nginx pod-template-hash: \u0026#34;1006230814\u0026#34; name: nginx-deployment-1006230814-6winp namespace: default resourceVersion: \u0026#34;133447\u0026#34; uid: 4c879808-f161-11e5-9a78-42010af00005 spec: containers: - image: nginx imagePullPolicy: Always name: nginx ports: - containerPort: 80 protocol: TCP resources: limits: cpu: 500m memory: 128Mi requests: cpu: 500m memory: 128Mi terminationMessagePath: /dev/termination-log volumeMounts: - mountPath: /var/run/secrets/kubernetes.io/serviceaccount name: default-token-4bcbi readOnly: true dnsPolicy: ClusterFirst nodeName: kubernetes-node-wul5 restartPolicy: Always securityContext: {} serviceAccount: default serviceAccountName: default terminationGracePeriodSeconds: 30 volumes: - name: default-token-4bcbi secret: secretName: default-token-4bcbi status: conditions: - lastProbeTime: null lastTransitionTime: 2016-03-24T01:39:51Z status: \u0026#34;True\u0026#34; type: Ready containerStatuses: - containerID: docker://90315cc9f513c724e9957a4788d3e625a078de84750f244a40f97ae355eb1149 image: nginx imageID: docker://6f62f48c4e55d700cf3eb1b5e33fa051802986b77b874cc351cce539e5163707 lastState: {} name: nginx ready: true restartCount: 0 state: running: startedAt: 2016-03-24T01:39:51Z hostIP: 10.240.0.9 phase: Running podIP: 10.244.0.6 startTime: 2016-03-24T01:39:49Z 例子: 调试 down/unreachable node 有时候，在调试时，查看节点的状态是很有用的——例如，因为您已经注意到节点上运行的 Pod 的奇怪行为， 或者想了解为什么 Pod 不会调度到节点上。 与Pods一样，您可以使用 kubectl describe node 和 kubectl get node -o yaml 来查询节点的详细信息。 例如，如果某个节点关闭(与网络断开连接，或者 kubelet 挂掉，无法重新启动，等等)，您将看到以下情况。 请注意显示节点未就绪的事件，也请注意 pod 不再运行(它们在5分钟未就绪状态后被驱逐)。\nkubectl get nodes NAME STATUS ROLES AGE VERSION kubernetes-node-861h NotReady \u0026lt;none\u0026gt; 1h v1.13.0 kubernetes-node-bols Ready \u0026lt;none\u0026gt; 1h v1.13.0 kubernetes-node-st6x Ready \u0026lt;none\u0026gt; 1h v1.13.0 kubernetes-node-unaj Ready \u0026lt;none\u0026gt; 1h v1.13.0 kubectl describe node kubernetes-node-861h Name:\tkubernetes-node-861h Role Labels:\tkubernetes.io/arch=amd64 kubernetes.io/os=linux kubernetes.io/hostname=kubernetes-node-861h Annotations: node.alpha.kubernetes.io/ttl=0 volumes.kubernetes.io/controller-managed-attach-detach=true Taints: \u0026lt;none\u0026gt; CreationTimestamp:\tMon, 04 Sep 2017 17:13:23 +0800 Phase: Conditions: Type\tStatus\tLastHeartbeatTime\tLastTransitionTime\tReason\tMessage ---- ------ ----------------- ------------------ ------ ------- OutOfDisk Unknown Fri, 08 Sep 2017 16:04:28 +0800 Fri, 08 Sep 2017 16:20:58 +0800 NodeStatusUnknown Kubelet stopped posting node status. MemoryPressure Unknown Fri, 08 Sep 2017 16:04:28 +0800 Fri, 08 Sep 2017 16:20:58 +0800 NodeStatusUnknown Kubelet stopped posting node status. DiskPressure Unknown Fri, 08 Sep 2017 16:04:28 +0800 Fri, 08 Sep 2017 16:20:58 +0800 NodeStatusUnknown Kubelet stopped posting node status. Ready Unknown Fri, 08 Sep 2017 16:04:28 +0800 Fri, 08 Sep 2017 16:20:58 +0800 NodeStatusUnknown Kubelet stopped posting node status. Addresses:\t10.240.115.55,104.197.0.26 Capacity: cpu: 2 hugePages: 0 memory: 4046788Ki pods: 110 Allocatable: cpu: 1500m hugePages: 0 memory: 1479263Ki pods: 110 System Info: Machine ID: 8e025a21a4254e11b028584d9d8b12c4 System UUID: 349075D1-D169-4F25-9F2A-E886850C47E3 Boot ID: 5cd18b37-c5bd-4658-94e0-e436d3f110e0 Kernel Version: 4.4.0-31-generic OS Image: Debian GNU/Linux 8 (jessie) Operating System: linux Architecture: amd64 Container Runtime Version: docker://1.12.5 Kubelet Version: v1.6.9+a3d1dfa6f4335 Kube-Proxy Version: v1.6.9+a3d1dfa6f4335 ExternalID: 15233045891481496305 Non-terminated Pods: (9 in total) Namespace Name CPU Requests CPU Limits Memory Requests Memory Limits --------- ---- ------------ ---------- --------------- ------------- ...... Allocated resources: (Total limits may be over 100 percent, i.e., overcommitted.) CPU Requests CPU Limits Memory Requests Memory Limits ------------ ---------- --------------- ------------- 900m (60%) 2200m (146%) 1009286400 (66%) 5681286400 (375%) Events: \u0026lt;none\u0026gt; kubectl get node kubernetes-node-861h -o yaml apiVersion: v1 kind: Node metadata: creationTimestamp: 2015-07-10T21:32:29Z labels: kubernetes.io/hostname: kubernetes-node-861h name: kubernetes-node-861h resourceVersion: \u0026#34;757\u0026#34; selfLink: /api/v1/nodes/kubernetes-node-861h uid: 2a69374e-274b-11e5-a234-42010af0d969 spec: externalID: \u0026#34;15233045891481496305\u0026#34; podCIDR: 10.244.0.0/24 providerID: gce://striped-torus-760/us-central1-b/kubernetes-node-861h status: addresses: - address: 10.240.115.55 type: InternalIP - address: 104.197.0.26 type: ExternalIP capacity: cpu: \u0026#34;1\u0026#34; memory: 3800808Ki pods: \u0026#34;100\u0026#34; conditions: - lastHeartbeatTime: 2015-07-10T21:34:32Z lastTransitionTime: 2015-07-10T21:35:15Z reason: Kubelet stopped posting node status. status: Unknown type: Ready nodeInfo: bootID: 4e316776-b40d-4f78-a4ea-ab0d73390897 containerRuntimeVersion: docker://Unknown kernelVersion: 3.16.0-0.bpo.4-amd64 kubeProxyVersion: v0.21.1-185-gffc5a86098dc01 kubeletVersion: v0.21.1-185-gffc5a86098dc01 machineID: \u0026#34;\u0026#34; osImage: Debian GNU/Linux 7 (wheezy) systemUUID: ABE5F6B4-D44B-108B-C46A-24CCE16C8B6E . heading \u0026ldquo;whatsnext\u0026rdquo; %}} 了解更多的调试工具：\n 日志 监控 使用 exec 进入容器 使用代理连接容器 使用端口转发连接容器 使用 crictl 检查节点  "
},
{
	"uri": "https://lijun.in/tasks/administer-cluster/developing-cloud-controller-manager/",
	"title": "开发云控制器管理器",
	"tags": [],
	"description": "",
	"content": ". feature-state for_k8s_version=\u0026quot;v1.11\u0026rdquo; state=\u0026quot;beta\u0026rdquo; \u0026gt;}}\n在即将发布的版本中，云控制器管理器将是把 Kubernetes 与任何云集成的首选方式。 这将确保驱动可以独立于核心 Kubernetes 发布周期开发其功能。\n. feature-state for_k8s_version=\u0026quot;1.8\u0026rdquo; state=\u0026quot;alpha\u0026rdquo; \u0026gt;}}\n在讨论如何构建自己的云控制器管理器之前，了解有关它如何工作的一些背景知识是有帮助的。云控制器管理器是来自 kube-controller-manager 的代码，利用 Go 接口允许插入任何云的实现。大多数框架和通用控制器的实现在 core，但只要满足 云提供者接口，它就会始终执行它所提供的云接口。\n为了深入了解实施细节，所有云控制器管理器都将从 Kubernetes 核心导入依赖包，唯一的区别是每个项目都会通过调用 cloudprovider.RegisterCloudProvider 来注册自己的驱动，更新可用驱动的全局变量。\n开发 Out of Tree 要为您的云构建一个 out-of-tree 云控制器管理器，请按照下列步骤操作：\n 使用满足 cloudprovider.Interface 的实现创建一个 go 包。 使用来自 Kubernetes 核心包的 cloud-controller-manager 中的 main.go 作为 main.go 的模板。如上所述，唯一的区别应该是将导入的云包。 在 main.go 中导入你的云包，确保你的包有一个 init 块来运行 cloudprovider.RegisterCloudProvider。  用现有的 out-of-tree 云驱动作为例子可能会有所帮助。你可以在这里找到 清单。\nIn Tree 对于 in-tree 驱动，您可以将 in-tree 云控制器管理器作为群集中的 Daemonset 运行。有关详细信息，请参阅 运行的云控制器管理器文档。\n"
},
{
	"uri": "https://lijun.in/tasks/run-application/scale-stateful-set/",
	"title": "弹缩StatefulSet",
	"tags": [],
	"description": "",
	"content": "本文介绍如何弹缩StatefulSet.\n. heading \u0026ldquo;prerequisites\u0026rdquo; %}}  StatefulSets仅适用于Kubernetes1.5及以上版本. 不是所有Stateful应用都适合弹缩. 在弹缩前您的应用前. 您必须充分了解您的应用, 不适当的弹缩StatefulSet或许会造成应用自身功能的不稳定. 仅当您确定该Stateful应用的集群是完全健康才可执行弹缩操作.  使用 kubectl 弹缩StatefulSets 弹缩请确认 kubectl 已经升级到Kubernetes1.5及以上版本. 如果不确定, 执行 kubectl version 命令并检查使用的 Client Version.\nkubectl 弹缩 首先, 找到您想要弹缩的StatefulSet. 记住, 您需先清楚是否能弹缩该应用.\nkubectl get statefulsets \u0026lt;stateful-set-name\u0026gt; 改变StatefulSet副本数量:\nkubectl scale statefulsets \u0026lt;stateful-set-name\u0026gt; --replicas=\u0026lt;new-replicas\u0026gt; 可使用其他命令: kubectl apply / kubectl edit / kubectl patch 另外, 您可以 in-place updates StatefulSets.\n如果您的StatefulSet开始由 kubectl apply 或 kubectl create --save-config 创建,更新StatefulSet manifests中的 .spec.replicas, 然后执行命令 kubectl apply:\nkubectl apply -f \u0026lt;stateful-set-file-updated\u0026gt; 除此之外, 可以通过命令 kubectl edit 编辑该字段:\nkubectl edit statefulsets \u0026lt;stateful-set-name\u0026gt; 或使用 kubectl patch:\nkubectl patch statefulsets \u0026lt;stateful-set-name\u0026gt; -p \u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;replicas\u0026#34;:\u0026lt;new-replicas\u0026gt;}}\u0026#39; 排查故障 缩容工作不正常 当Stateful管理下的任何一个Pod不健康时您不能缩容该StatefulSet. 仅当Stateful下的所有Pods都处于运行和ready状态后才可缩容.\n当一个StatefulSet的size \u0026gt; 1, 如果有一个Pod不健康, 没有办法让Kubernetes知道是否是由于永久性故障还是瞬态(升级/维护/节点重启)导致. 如果该Pod不健康是由于永久性 故障导致, 则在不纠正该故障的情况下进行缩容可能会导致一种状态， 即StatefulSet下的Pod数量低于应正常运行的副本数. 这也许会导致StatefulSet不可用.\n如果由于瞬态故障而导致Pod不健康,并且Pod可能再次可用，那么瞬态错误可能会干扰您对 StatefulSet的扩容/缩容操作. 一些分布式数据库在节点加入和同时离开时存在问题. 在 这些情况下，最好是在应用级别进行弹缩操作, 并且只有在您确保Stateful应用的集群是完全健康时才执行弹缩.\n. heading \u0026ldquo;whatsnext\u0026rdquo; %}} 了解更多 deleting a StatefulSet.\n"
},
{
	"uri": "https://lijun.in/tasks/debug-application-cluster/troubleshooting/",
	"title": "排错",
	"tags": [],
	"description": "",
	"content": "有时候事情会出错。本指南旨在正确解决这些问题。它包含两个部分：\n 应用排错 - 用于部署代码到 Kubernetes 并想知道代码为什么不能正常运行的用户。 集群排错 - 用于集群管理员以及 Kubernetes 集群表现异常的用户。  您也应该查看所用版本的已知问题。\n获取帮助 如果您的问题在上述指南中没有得到答案，您还有另外几种方式从 Kubernetes 团队获得帮助。\n提问 网站上的文档针对回答各类问题进行了结构化组织和分类。 概念部分解释了 Kubernetes 体系结构以及每个组件的工作方式，安装部分提供了入门的实用说明。 任务部分展示了如何完成常用任务，入门部分则是对现实世界、特定行业或端到端开发场景的更全面的演练。 参考部分提供了详细的 [Kubernetes API](/docs/reference/generated/kubernetes-api/. param \u0026ldquo;version\u0026rdquo; \u0026gt;}}/) 文档和命令行 (CLI) 接口，例如kubectl。\n您还可以找到堆栈溢出相关的主题：\n Kubernetes Google Kubernetes Engine  求救！我的问题还没有解决！我需要立即得到帮助！ 堆栈溢出 社区中的其他人可能已经问过和您类似的问题，这或许能帮助解决您的问题。 Kubernetes 团队还会监视带有 Kubernetes 标签的帖子。 如果现有的问题对您没有帮助，请问一个新问题!\nSlack Kubernetes 团队在 Slack 中建有 #kubernetes-users 频道。 您可以在这里参加与 Kubernetes 团队的讨论。 Slack 需要注册，但 Kubernetes 团队公开邀请任何人在这里注册。 欢迎您随时来问任何问题。\n一旦注册完成，您就可以浏览各种感兴趣的频道列表。 例如，Kubernetes 新人可能还想加入 #kubernetes-novice 频道。作为另一个例子，开发人员应该加入 #kubernetes-dev 频道。\n还有许多国家/地区语言频道。请随时加入这些频道以获得本地化支持和信息：\n 中国： #cn-users, #cn-events 法国： #fr-users, #fr-events 德国： #de-users, #de-events 印度： #in-users, #in-events 意大利： #it-users, #it-events 日本： #jp-users, #jp-events 韩国： #kr-users 荷兰： #nl-users 挪威： #norw-users 波兰： #pl-users 俄罗斯： #ru-users 西班牙： #es-users 土耳其： #tr-users, #tr-events  论坛 Kubernetes 官方论坛 discuss.kubernetes.io\n如果你发现一个看起来像 bug 的东西，或者你想提出一个功能请求，请使用Github 问题跟踪系统。\n在提交问题之前，请搜索现有问题以查看是否已涵盖您的问题。\n如果提交 bug，请提供如何重现问题的详细信息，例如：\n Kubernetes 版本：获取版本的命令为 kubectl version 云提供商，OS 发行版、网络配置和 Docker 版本 重现问题的步骤  "
},
{
	"uri": "https://lijun.in/tasks/administer-cluster/cpu-management-policies/",
	"title": "控制节点上的 CPU 管理策略",
	"tags": [],
	"description": "",
	"content": ". feature-state for_k8s_version=\u0026quot;v1.12\u0026rdquo; state=\u0026quot;beta\u0026rdquo; \u0026gt;}}\n按照设计，Kubernetes 对 pod 执行相关的很多方面进行了抽象，使得用户不必关心。然 而，为了正常运行，有些工作负载要求在延迟和/或性能方面有更强的保证。 为此，kubelet 提供方法来实现更复杂的负载放置策略，同时保持抽象，避免显式的放置指令。\n. heading \u0026ldquo;prerequisites\u0026rdquo; %}} . include \u0026ldquo;task-tutorial-prereqs.md\u0026rdquo; \u0026gt;}} . version-check \u0026gt;}}\nCPU 管理策略 默认情况下，kubelet 使用 CFS 配额 来执行 pod 的 CPU 约束。当节点上运行了很多 CPU 密集的 pod 时，工作负载可能会迁移到不同的 CPU 核，这取决于调度时 pod 是否被扼制，以及哪些 CPU 核是可用的。许多工作负载对这种迁移不敏感，因此无需任何干预即可正常工作。\n然而，有些工作负载的性能明显地受到 CPU 缓存亲和性以及调度延迟的影响，对此，kubelet 提供了可选的 CPU 管理策略，来确定节点上的一些分配偏好。\n配置 CPU 管理器（CPU Manager）作为 alpha 特性引入 Kubernetes 1.8 版本。从 1.10 版本开始，作为 beta 特性默认开启。\nCPU 管理策略通过 kubelet 参数 --cpu-manager-policy 来指定。支持两种策略：\n none: 默认策略，表示现有的调度行为。 static: 允许为节点上具有某些资源特征的 pod 赋予增强的 CPU 亲和性和独占性。  CPU 管理器定期通过 CRI 写入资源更新，以保证内存中 CPU 分配与 cgroupfs 一致。同步频率通过新增的 Kubelet 配置参数 --cpu-manager-reconcile-period 来设置。 如果不指定，默认与 --node-status-update-frequency 的周期相同。\nNone 策略 none 策略显式地启用现有的默认 CPU 亲和方案，不提供操作系统调度器默认行为之外的亲和性策略。 通过 CFS 配额来实现 Guaranteed pods 的 CPU 使用限制。\nStatic 策略 static 策略针对具有整数型 CPU requests 的 Guaranteed pod ，它允许该类 pod 中的容器访问节点上的独占 CPU 资源。这种独占性是使用 cpuset cgroup 控制器 来实现的。\n. note \u0026gt;}} 诸如容器运行时和 kubelet 本身的系统服务可以继续在这些独占 CPU 上运行。独占性仅针对其他 pod。 . /note \u0026gt;}}\n. note \u0026gt;}} 该策略的 alpha 版本不保证 Kubelet 重启前后的静态独占性分配。 . /note \u0026gt;}}\n. note \u0026gt;}} CPU 管理器不支持运行时下线和上线 CPUs。此外，如果节点上的在线 CPUs 集合发生变化，则必须驱逐节点上的 pods，并通过删除 kubelet 根目录中的状态文件 cpu_manager_state 来手动重置 CPU 管理器。 . /note \u0026gt;}}\n该策略管理一个共享 CPU 资源池，最初，该资源池包含节点上所有的 CPU 资源。可用 的独占性 CPU 资源数量等于节点的 CPU 总量减去通过 --kube-reserved 或 --system-reserved 参数保留的 CPU 。从1.17版本开始，CPU保留列表可以通过 kublet 的 \u0026lsquo;\u0026ndash;reserved-cpus\u0026rsquo; 参数显式地设置。 通过 \u0026lsquo;\u0026ndash;reserved-cpus\u0026rsquo; 指定的显式CPU列表优先于使用 \u0026lsquo;\u0026ndash;kube-reserved\u0026rsquo; 和 \u0026lsquo;\u0026ndash;system-reserved\u0026rsquo; 参数指定的保留CPU。 通过这些参数预留的 CPU 是以整数方式，按物理内 核 ID 升序从初始共享池获取的。 共享池是 BestEffort 和 Burstable pod 运行 的 CPU 集合。Guaranteed pod 中的容器，如果声明了非整数值的 CPU requests ，也将运行在共享池的 CPU 上。只有 Guaranteed pod 中，指定了整数型 CPU requests 的容器，才会被分配独占 CPU 资源。\n. note \u0026gt;}} 当启用 static 策略时，要求使用 --kube-reserved 和/或 --system-reserved 或 --reserved-cpus 来保证预留的 CPU 值大于零。 这是因为零预留 CPU 值可能使得共享池变空。 . /note \u0026gt;}}\n当 Guaranteed pod 调度到节点上时，如果其容器符合静态分配要求，相应的 CPU 会被从共享池中移除，并放置到容器的 cpuset 中。因为这些容器所使用的 CPU 受到调度域本身的限制，所以不需要使用 CFS 配额来进行 CPU 的绑定。换言之，容器 cpuset 中的 CPU 数量与 pod 规格中指定的整数型 CPU limit 相等。这种静态分配增强了 CPU 亲和性，减少了 CPU 密集的工作负载在节流时引起的上下文切换。\n考虑以下 Pod 规格的容器：\nspec: containers: - name: nginx image: nginx 该 pod 属于 BestEffort QoS 类型，因为其未指定 requests 或 limits 值。 所以该容器运行在共享 CPU 池中。\nspec: containers: - name: nginx image: nginx resources: limits: memory: \u0026#34;200Mi\u0026#34; requests: memory: \u0026#34;100Mi\u0026#34; 该 pod 属于 Burstable QoS 类型，因为其资源 requests 不等于 limits， 且未指定 cpu 数量。所以该容器运行在共享 CPU 池中。\nspec: containers: - name: nginx image: nginx resources: limits: memory: \u0026#34;200Mi\u0026#34; cpu: \u0026#34;2\u0026#34; requests: memory: \u0026#34;100Mi\u0026#34; cpu: \u0026#34;1\u0026#34; 该 pod 属于 Burstable QoS 类型，因为其资源 requests 不等于 limits。所以该容器运行在共享 CPU 池中。\nspec: containers: - name: nginx image: nginx resources: limits: memory: \u0026#34;200Mi\u0026#34; cpu: \u0026#34;2\u0026#34; requests: memory: \u0026#34;200Mi\u0026#34; cpu: \u0026#34;2\u0026#34; 该 pod 属于 Guaranteed QoS 类型，因为其 requests 值与 limits相等。同时，容器对 CPU 资源的限制值是一个大于或等于 1 的整数值。所以，该 nginx 容器被赋予 2 个独占 CPU。\nspec: containers: - name: nginx image: nginx resources: limits: memory: \u0026#34;200Mi\u0026#34; cpu: \u0026#34;1.5\u0026#34; requests: memory: \u0026#34;200Mi\u0026#34; cpu: \u0026#34;1.5\u0026#34; 该 pod 属于 Guaranteed QoS 类型，因为其 requests 值与 limits相等。但是容器对 CPU 资源的限制值是一个小数。所以该容器运行在共享 CPU 池中。\nspec: containers: - name: nginx image: nginx resources: limits: memory: \u0026#34;200Mi\u0026#34; cpu: \u0026#34;2\u0026#34; 该 pod 属于 Guaranteed QoS 类型，因其指定了 limits 值，同时当未显式指定时，requests 值被设置为与 limits 值相等。同时，容器对 CPU 资源的限制值是一个大于或等于 1 的整数值。所以，该 nginx 容器被赋予 2 个独占 CPU。\n"
},
{
	"uri": "https://lijun.in/tasks/administer-cluster/topology-manager/",
	"title": "控制节点上的拓扑管理策略",
	"tags": [],
	"description": "",
	"content": ". feature-state state=\u0026quot;alpha\u0026rdquo; \u0026gt;}}\n越来越多的系统利用 CPU 和硬件加速器的组合来支持对延迟要求较高的任务和高吞吐量的并行计算。 这类负载包括电信、科学计算、机器学习、金融服务和数据分析等。 此类混合系统即用于构造这些高性能环境。\n为了获得最佳性能，需要进行与 CPU 隔离、内存和设备局部性有关的优化。 但是，在 Kubernetes 中，这些优化由各自独立的组件集合来处理。\n拓扑管理器（Topology Manager） 是一个 Kubelet 的一部分，旨在协调负责这些优化的一组组件。\n. heading \u0026ldquo;prerequisites\u0026rdquo; %}} . include \u0026ldquo;task-tutorial-prereqs.md\u0026rdquo; \u0026gt;}} . version-check \u0026gt;}}\n拓扑管理器如何工作 在引入拓扑管理器之前， Kubernetes 中的 CPU 和设备管理器相互独立地做出资源分配决策。 这可能会导致在多处理系统上出现并非期望的资源分配；由于这些与期望相左的分配，对性能或延迟敏感的应用将受到影响。 这里的不符合期望意指，例如， CPU 和设备是从不同的 NUMA 节点分配的，因此会导致额外的延迟。\n拓扑管理器是一个 Kubelet 组件，扮演信息源的角色，以便其他 Kubelet 组件可以做出与拓扑结构相对应的资源分配决定。\n拓扑管理器为组件提供了一个称为 建议供应者（Hint Providers） 的接口，以发送和接收拓扑信息。 拓扑管理器具有一组节点级策略，具体说明如下。\n拓扑管理器从 建议提供者 接收拓扑信息，作为表示可用的 NUMA 节点和首选分配指示的位掩码。 拓扑管理器策略对所提供的建议执行一组操作，并根据策略对提示进行约减以得到最优解；如果存储了与预期不符的建议，则该建议的优选字段将被设置为 false。 在当前策略中，首选的是最窄的优选掩码。 所选建议将被存储为拓扑管理器的一部分。 取决于所配置的策略，所选建议可用来决定节点接受或拒绝 Pod 。 之后，建议会被存储在拓扑管理器中，供 建议提供者 进行资源分配决策时使用。\n拓扑管理器策略 当前拓扑管理器：\n 在启用了 static CPU 管理器策略的节点上起作用。 请参阅控制 CPU 管理策略 适用于通过扩展资源发出 CPU 请求或设备请求的 Pod  如果满足这些条件，则拓扑管理器将调整请求的资源。\n拓扑管理器支持四种分配策略。 您可以通过 Kubelet 标志 --topology-manager-policy 设置策略。 所支持的策略有四种：\n none (默认) best-effort restricted single-numa-node  none 策略 这是默认策略，不执行任何拓扑对齐。\nbest-effort 策略 对于 Guaranteed 类的 Pod 中的每个容器，具有 best-effort 拓扑管理策略的 kubelet 将调用每个建议提供者以确定资源可用性。 使用此信息，拓扑管理器存储该容器的首选 NUMA 节点亲和性。 如果亲和性不是首选，则拓扑管理器将存储该亲和性，并且无论如何都将 pod 接纳到该节点。\n之后 建议提供者 可以在进行资源分配决策时使用这个信息。\nrestricted 策略 对于 Guaranteed 类 Pod 中的每个容器， 配置了 restricted 拓扑管理策略的 kubelet 调用每个建议提供者以确定其资源可用性。。 使用此信息，拓扑管理器存储该容器的首选 NUMA 节点亲和性。 如果亲和性不是首选，则拓扑管理器将从节点中拒绝此 Pod 。 这将导致 Pod 处于 Terminated 状态，且 Pod 无法被节点接纳。\n一旦 Pod 处于 Terminated 状态，Kubernetes 调度器将不会尝试重新调度该 Pod。建议使用 ReplicaSet 或者 Deployment 来重新部署 Pod。 还可以通过实现外部控制环，以启动对具有 Topology Affinity 错误的 Pod 的重新部署。\n如果 Pod 被允许运行在该节点，则 建议提供者 可以在做出资源分配决定时使用此信息。\nsingle-numa-node 策略 对于 Guaranteed 类 Pod 中的每个容器， 配置了 single-numa-nodde 拓扑管理策略的 kubelet 调用每个建议提供者以确定其资源可用性。 使用此信息，拓扑管理器确定单 NUMA 节点亲和性是否可能。 如果是这样，则拓扑管理器将存储此信息，然后 建议提供者 可以在做出资源分配决定时使用此信息。 如果不可能，则拓扑管理器将拒绝 Pod 运行于该节点。 这将导致 Pod 处于 Terminated 状态，且 Pod 无法被节点接受。\n一旦 Pod 处于 Terminated 状态，Kubernetes 调度器将不会尝试重新调度该 Pod。建议使用 ReplicaSet 或者 Deployment 来重新部署 Pod。 还可以通过实现外部控制环，以触发具有 Topology Affinity 错误的 Pod 的重新部署。\nPod 与拓扑管理器策略的交互 考虑以下 pod 规范中的容器：\nspec: containers: - name: nginx image: nginx 该 Pod 在 BestEffort QoS 类中运行，因为没有指定资源 requests 或 limits 。\nspec: containers: - name: nginx image: nginx resources: limits: memory: \u0026#34;200Mi\u0026#34; requests: memory: \u0026#34;100Mi\u0026#34; 由于 requests 数少于 limits，因此该 Pod 以 Burstable QoS 类运行。\n如果选择的策略是 none 以外的任何其他策略，拓扑管理器不会考虑这些 Pod 中的任何一个规范。\nspec: containers: - name: nginx image: nginx resources: limits: memory: \u0026#34;200Mi\u0026#34; cpu: \u0026#34;2\u0026#34; example.com/device: \u0026#34;1\u0026#34; requests: memory: \u0026#34;200Mi\u0026#34; cpu: \u0026#34;2\u0026#34; example.com/device: \u0026#34;1\u0026#34; 此 Pod 在 Guaranteed QoS 类中运行，因为其 requests 值等于 limits 值。\nspec: containers: - name: nginx image: nginx resources: limits: example.com/deviceA: \u0026#34;1\u0026#34; example.com/deviceB: \u0026#34;1\u0026#34; requests: example.com/deviceA: \u0026#34;1\u0026#34; example.com/deviceB: \u0026#34;1\u0026#34; 由于没有 CPU 和内存请求，因此该 Pod 在 BestEffort QoS 类中运行。\n拓扑管理器将考虑以上两个 Pod。拓扑管理器将咨询 CPU 和设备管理器，以获取 Pod 的拓扑提示。 对于 Guaranteed Pod，static CPU 管理器策略将返回与 CPU 请求有关的提示，而设备管理器将返回有关所请求设备的提示。\n对于 BestEffort Pod，由于没有 CPU 请求，CPU 管理器将发送默认提示，而设备管理器将为每个请求的设备发送提示。\n使用此信息，拓扑管理器将为 Pod 计算最佳提示并存储该信息，并且供提示提供程序在进行资源分配时使用。\n已知的局限性  从 K8s 1.16 开始，当前只能在保证 Pod 规范中的 单个 容器需要相匹配的资源时，拓扑管理器才能正常工作。这是由于生成的提示信息是基于当前资源分配的，并且 pod 中的所有容器都会在进行任何资源分配之前生成提示信息。这样会导致除 Pod 中的第一个容器以外的所有容器生成不可靠的提示信息。   由于此限制，如果 kubelet 快速连续考虑多个 Pod/容器，它们可能不遵守拓扑管理器策略。  拓扑管理器允许的最大 NUMA 节点数为 8，并且在尝试枚举可能的 NUMA 关联并生成其提示信息时，将出现状态问题。  调度器不支持拓扑功能，因此可能会由于拓扑管理器的原因而在节点上进行调度，然后在该节点上调度失败。  "
},
{
	"uri": "https://lijun.in/concepts/overview/working-with-objects/common-labels/",
	"title": "推荐使用的标签",
	"tags": [],
	"description": "",
	"content": "除了 kubectl 和 dashboard 之外，您可以使用其他工具来可视化和管理 Kubernetes 对象。一组通用的标签可以让多个工具之间相互操作，用所有工具都能理解的通用方式描述对象。\n除了支持工具外，推荐的标签还以一种可以查询的方式描述了应用程序。\n元数据围绕 应用（application） 的概念进行组织。Kubernetes 不是 平台即服务（PaaS），没有或强制执行正式的应用程序概念。 相反，应用程序是非正式的，并使用元数据进行描述。应用程序包含的定义是松散的。\n这些是推荐的标签。它们使管理应用程序变得更容易但不是任何核心工具所必需的。\n共享标签和注解都使用同一个前缀：app.kubernetes.io。没有前缀的标签是用户私有的。共享前缀可以确保共享标签不会干扰用户自定义的标签。\n标签 为了充分利用这些标签，应该在每个资源对象上都使用它们。\n   键 描述 示例 类型     app.kubernetes.io/name 应用程序的名称 mysql 字符串   app.kubernetes.io/instance 用于唯一确定应用实例的名称 wordpress-abcxzy 字符串   app.kubernetes.io/version 应用程序的当前版本（例如，语义版本，修订版哈希等） 5.7.21 字符串   app.kubernetes.io/component 架构中的组件 database 字符串   app.kubernetes.io/part-of 此级别的更高级别应用程序的名称 wordpress 字符串   app.kubernetes.io/managed-by 用于管理应用程序的工具 helm 字符串    为说明这些标签的实际使用情况，请看下面的 StatefulSet 对象：\napiVersion: apps/v1 kind: StatefulSet metadata: labels: app.kubernetes.io/name: mysql app.kubernetes.io/instance: wordpress-abcxzy app.kubernetes.io/version: \u0026#34;5.7.21\u0026#34; app.kubernetes.io/component: database app.kubernetes.io/part-of: wordpress app.kubernetes.io/managed-by: helm 应用和应用实例 应用可以在 Kubernetes 集群中安装一次或多次。在某些情况下，可以安装在同一命名空间中。例如，可以不止一次地为不同的站点安装不同的 wordpress。\n应用的名称和实例的名称是分别记录的。例如，某 WordPress 实例的 app.kubernetes.io/name 为 wordpress，而其实例名称表现为 app.kubernetes.io/instance 的属性值 wordpress-abcxzy。这使应用程序和应用程序的实例成为可能是可识别的。应用程序的每个实例都必须具有唯一的名称。\n示例 为了说明使用这些标签的不同方式，以下示例具有不同的复杂性。\n一个简单的无状态服务 考虑使用 Deployment 和 Service 对象部署的简单无状态服务的情况。以下两个代码段表示如何以最简单的形式使用标签。\n下面的 Deployment 用于监督运行应用本身的 pods。\napiVersion: apps/v1 kind: Deployment metadata: labels: app.kubernetes.io/name: myservice app.kubernetes.io/instance: myservice-abcxzy ... 下面的 Service 用于暴露应用。\napiVersion: v1 kind: Service metadata: labels: app.kubernetes.io/name: myservice app.kubernetes.io/instance: myservice-abcxzy ... 带有一个数据库的 Web 应用程序 考虑一个稍微复杂的应用：一个使用 Helm 安装的 Web 应用（WordPress），其中 使用了数据库（MySQL）。以下代码片段说明用于部署此应用程序的对象的开始。\n以下 Deployment 的开头用于 WordPress：\napiVersion: apps/v1 kind: Deployment metadata: labels: app.kubernetes.io/name: wordpress app.kubernetes.io/instance: wordpress-abcxzy app.kubernetes.io/version: \u0026#34;4.9.4\u0026#34; app.kubernetes.io/managed-by: helm app.kubernetes.io/component: server app.kubernetes.io/part-of: wordpress ... 这个 Service 用于暴露 WordPress：\napiVersion: v1 kind: Service metadata: labels: app.kubernetes.io/name: wordpress app.kubernetes.io/instance: wordpress-abcxzy app.kubernetes.io/version: \u0026#34;4.9.4\u0026#34; app.kubernetes.io/managed-by: helm app.kubernetes.io/component: server app.kubernetes.io/part-of: wordpress ... MySQL 作为一个 StatefulSet 暴露，包含它和它所属的较大应用程序的元数据：\napiVersion: apps/v1 kind: StatefulSet metadata: labels: app.kubernetes.io/name: mysql app.kubernetes.io/instance: mysql-abcxzy app.kubernetes.io/version: \u0026#34;5.7.21\u0026#34; app.kubernetes.io/managed-by: helm app.kubernetes.io/component: database app.kubernetes.io/part-of: wordpress ... Service 用于将 MySQL 作为 WordPress 的一部分暴露：\napiVersion: v1 kind: Service metadata: labels: app.kubernetes.io/name: mysql app.kubernetes.io/instance: mysql-abcxzy app.kubernetes.io/version: \u0026#34;5.7.21\u0026#34; app.kubernetes.io/managed-by: helm app.kubernetes.io/component: database app.kubernetes.io/part-of: wordpress ... 使用 MySQL StatefulSet 和 Service，您会注意到有关 MySQL 和 Wordpress 的信息，包括更广泛的应用程序。\n"
},
{
	"uri": "https://lijun.in/tasks/administer-cluster/highly-available-master/",
	"title": "搭建高可用的 Kubernetes Masters",
	"tags": [],
	"description": "",
	"content": ". feature-state for_k8s_version=\u0026quot;1.5\u0026rdquo; state=\u0026quot;alpha\u0026rdquo; \u0026gt;}}\n您可以在谷歌计算引擎（GCE）的 kubeup 或 kube-down 脚本中复制 Kubernetes Master。 本文描述了如何使用 kube-up/down 脚本来管理高可用（HA）的 Master，以及如何使用 GCE 实现高可用 Master。\n. heading \u0026ldquo;prerequisites\u0026rdquo; %}} . include \u0026ldquo;task-tutorial-prereqs.md\u0026rdquo; \u0026gt;}} . version-check \u0026gt;}}\n启动一个兼容高可用的集群 要创建一个新的兼容高可用的集群，您必须在 kubeup 脚本中设置以下标志:\n  MULTIZONE=true - 为了防止从不同于 Master 默认区域的区域中删除 kubelets 副本。如果您希望在不同的区域运行 Master 副本，那么这一项是必需并且推荐的。\n  ENABLE_ETCD_QUORUM_READ=true - 确保从所有 API 服务器读取数据时将返回最新的数据。如果为 true，读操作将被定向到 leader etcd 副本。可以选择将这个值设置为 true，那么读取将更可靠，但也会更慢。\n  您还可以指定一个 GCE 区域，在这里创建第一个 Master 副本。设置以下标志:\n KUBE_GCE_ZONE=zone - 将运行第一个 Master 副本的区域。  下面的命令演示在 GCE europe-west1-b 区域中设置一个兼容高可用的集群:\nMULTIZONE=true KUBE_GCE_ZONE=europe-west1-b ENABLE_ETCD_QUORUM_READS=true ./cluster/kube-up.sh 注意，上面的命令创建一个只有单一 Master 的集群; 但是，您可以使用后续命令将新的 Master 副本添加到集群中。\n增加一个新的 Master 副本 在创建了兼容高可用的集群之后，可以向其中添加 Master 副本。 您可以使用带有如下标记的 kubeup 脚本添加 Master 副本:\n  KUBE_REPLICATE_EXISTING_MASTER=true - 创建一个已经存在的 Master 的副本。\n  KUBE_GCE_ZONE=zone - Master 副本将运行的区域。必须与其他副本位于同一区域。\n  您无需设置 MULTIZONE 或 ENABLE_ETCD_QUORUM_READS 标志，因为他们可以从兼容高可用的集群中继承。\n使用下面的命令可以复制现有兼容高可用的集群上的 Master:\nKUBE_GCE_ZONE=europe-west1-c KUBE_REPLICATE_EXISTING_MASTER=true ./cluster/kube-up.sh 删除一个 Master 副本 你可以使用一个 kube-down 脚本从高可用集群中删除一个 Master 副本，并可以使用以下标记:\n  KUBE_DELETE_NODES=false - 限制删除 kubelets。\n  KUBE_GCE_ZONE=zone - 将移除 Master 副本的区域。\n  KUBE_REPLICA_NAME=replica_name - （可选）要删除的 Master 副本的名称。 如果为空：将删除给定区域中的所有副本。\n  使用下面的命令可以从一个现有的高可用集群中删除一个 Master副本:\nKUBE_DELETE_NODES=false KUBE_GCE_ZONE=europe-west1-c ./cluster/kube-down.sh 处理 Master 副本失败 如果高可用集群中的一个 Master 副本失败，最佳实践是从集群中删除副本，并在相同的区域中添加一个新副本。 下面的命令演示了这个过程:\n 删除失败的副本:  KUBE_DELETE_NODES=false KUBE_GCE_ZONE=replica_zone KUBE_REPLICA_NAME=replica_name ./cluster/kube-down.sh KUBE_GCE_ZONE=replica-zone KUBE_REPLICATE_EXISTING_MASTER=true ./cluster/kube-up.sh 高可用集群复制 Master 的最佳实践   尝试将 Master 副本放置在不同的区域。在某区域故障时，放置在该区域内的所有主机都将失败。 为了在区域故障中幸免，请同样将工作节点放置在多区域中（详情请见多区域）。\n  不要使用具有两个 Master 副本的集群。在双副本集群上达成一致需要在更改持久状态时两个副本都处于运行状态。因此，两个副本都是需要的，任一副本的失败都会将集群带入多数失败状态。因此，就高可用而言，双副本集群不如单个副本集群。\n  添加 Master 副本时，集群状态（etcd）会被复制到一个新实例。如果集群很大，可能需要很长时间才能复制它的状态。 这个操作可以通过迁移 etcd 数据存储来加速, 详情参见 这里 （我们正在考虑在未来添加对迁移 etcd 数据存储的支持）。\n  实施注意事项 概述 每个 Master 副本将以以下模式运行以下组件:\n  etcd 实例： 所有实例将会以共识方式组建集群；\n  API 服务器： 每个服务器将与本地 etcd 通信——集群中的所有 API 服务器都可用;\n  控制器、调度器和集群自动扩缩器：将使用租约机制 —— 每个集群中只有一个实例是可用的；\n  add-on manager：每个管理器将独立工作，试图保持插件同步。\n  此外，在 API 服务器前面将有一个负载均衡器，用于将外部和内部通信路由到他们。\n负载均衡器 启动第二个 Master 副本时，将创建一个包含两个副本的负载均衡器，并将第一个副本的 IP 地址提升为负载均衡器的 IP 地址。 类似地，在删除倒数第二个 Master 副本之后，将删除负载均衡器，并将其 IP 地址分配给最后一个剩余的副本。 请注意，创建和删除负载均衡器是复杂的操作，可能需要一些时间（约20分钟）来同步。\nMaster 服务 \u0026amp; kubelets Kubernetes 并不试图在其服务中保持 apiserver 的列表为最新，相反，它将将所有访问请求指向外部 IP：\n  在拥有一个 Master 的集群中，IP 指向单一的 Master，\n  在拥有多个 Master 的集群中，IP 指向 Master 前面的负载均衡器。\n  类似地，kubelets 将使用外部 IP 与 Master 通信。\nMaster 证书 Kubernetes 为每个副本的外部公共 IP 和本地 IP 生成 Master TLS 证书。 副本的临时公共 IP 没有证书； 要通过其临时公共 IP 访问副本，必须跳过TLS验证。\netcd 集群 为了允许 etcd 组建集群，需开放 etcd 实例之间通信所需的端口（用于集群内部通信）。 为了使这种部署安全，etcd 实例之间的通信使用 SSL 进行鉴权。\n拓展阅读 自动化高可用集群部署 - 设计文档\n"
},
{
	"uri": "https://lijun.in/tasks/administer-cluster/change-default-storage-class/",
	"title": "改变默认 StorageClass",
	"tags": [],
	"description": "",
	"content": "本文展示了如何改变默认的 Storage Class，它用于为没有特殊需求的 PersistentVolumeClaims 配置 volumes。\n. heading \u0026ldquo;prerequisites\u0026rdquo; %}} . include \u0026ldquo;task-tutorial-prereqs.md\u0026rdquo; \u0026gt;}} . version-check \u0026gt;}}\n为什么要改变默认 storage class？ 取决于安装模式，您的 Kubernetes 集群可能和一个被标记为默认的已有 StorageClass 一起部署。这个默认的 StorageClass 以后将被用于动态的为没有特定 storage class 需求的 PersistentVolumeClaims 配置存储。更多细节请查看 PersistentVolumeClaim 文档。\n预先安装的默认 StorageClass 可能不能很好的适应您期望的工作负载；例如，它配置的存储可能太过昂贵。如果是这样的话，您可以改变默认 StorageClass，或者完全禁用它以防止动态配置存储。\n简单的删除默认 StorageClass 可能行不通，因为它可能会被您集群中的扩展管理器自动重建。请查阅您的安装文档中关于扩展管理器的细节，以及如何禁用单个扩展。\n改变默认 StorageClass   列出您集群中的 StorageClasses：\nkubectl get storageclass 输出类似这样：\nNAME PROVISIONER AGE standard (default) kubernetes.io/gce-pd 1d gold kubernetes.io/gce-pd 1d 默认 StorageClass 以 (default) 标记。\n  标记默认 StorageClass 非默认：\n默认 StorageClass 的注解 storageclass.kubernetes.io/is-default-class 设置为 true。注解的其它任意值或者缺省值将被解释为 false。\n要标记一个 StorageClass 为非默认的，您需要改变它的值为 false：\nkubectl patch storageclass \u0026lt;your-class-name\u0026gt; -p \u0026#39;{\u0026#34;metadata\u0026#34;: {\u0026#34;annotations\u0026#34;:{\u0026#34;storageclass.kubernetes.io/is-default-class\u0026#34;:\u0026#34;false\u0026#34;}}}\u0026#39; 这里的 \u0026lt;your-class-name\u0026gt; 是您选择的 StorageClass 的名字。\n  标记一个 StorageClass 为默认的：\n和前面的步骤类似，您需要添加/设置注解 storageclass.kubernetes.io/is-default-class=true。\nkubectl patch storageclass \u0026lt;your-class-name\u0026gt; -p \u0026#39;{\u0026#34;metadata\u0026#34;: {\u0026#34;annotations\u0026#34;:{\u0026#34;storageclass.kubernetes.io/is-default-class\u0026#34;:\u0026#34;true\u0026#34;}}}\u0026#39; 请注意，最多只能有一个 StorageClass 能够被标记为默认。如果它们中有两个或多个被标记为默认，Kubernetes 将忽略这个注解，也就是它将表现为没有默认 StorageClass。\n  验证您选用的 StorageClass 为默认的：\nkubectl get storageclass 输出类似这样：\nNAME PROVISIONER AGE standard kubernetes.io/gce-pd 1d gold (default) kubernetes.io/gce-pd 1d   . heading \u0026ldquo;whatsnext\u0026rdquo; %}}  了解更多关于 StorageClasses。  "
},
{
	"uri": "https://lijun.in/tasks/administer-cluster/change-pv-reclaim-policy/",
	"title": "更改 PersistentVolume 的回收策略",
	"tags": [],
	"description": "",
	"content": "本文展示了如何更改 Kubernetes PersistentVolume 的回收策略。\n. heading \u0026ldquo;prerequisites\u0026rdquo; %}} . include \u0026ldquo;task-tutorial-prereqs.md\u0026rdquo; \u0026gt;}} . version-check \u0026gt;}}\n为什么要更改 PersistentVolume 的回收策略 PersistentVolumes 可以有多种回收策略，包括 \u0026ldquo;Retain\u0026rdquo;、\u0026ldquo;Recycle\u0026rdquo; 和 \u0026ldquo;Delete\u0026rdquo;。对于动态配置的 PersistentVolumes 来说，默认回收策略为 \u0026ldquo;Delete\u0026rdquo;。这表示当用户删除对应的 PersistentVolumeClaim 时，动态配置的 volume 将被自动删除。如果 volume 包含重要数据时，这种自动行为可能是不合适的。那种情况下，更适合使用 \u0026ldquo;Retain\u0026rdquo; 策略。使用 \u0026ldquo;Retain\u0026rdquo; 时，如果用户删除 PersistentVolumeClaim，对应的 PersistentVolume 不会被删除。相反，它将变为 Released 状态，表示所有的数据可以被手动恢复。\n更改 PersistentVolume 的回收策略   列出你集群中的 PersistentVolumes\nkubectl get pv  输出类似于这样：\n NAME CAPACITY ACCESSMODES RECLAIMPOLICY STATUS CLAIM REASON AGE pvc-b6efd8da-b7b5-11e6-9d58-0ed433a7dd94 4Gi RWO Delete Bound default/claim1 10s pvc-b95650f8-b7b5-11e6-9d58-0ed433a7dd94 4Gi RWO Delete Bound default/claim2 6s pvc-bb3ca71d-b7b5-11e6-9d58-0ed433a7dd94 4Gi RWO Delete Bound default/claim3 3s  这个列表同样包含了绑定到每个 volume 的 claims 名称，以便更容易的识别动态配置的 volumes。\n  选择你的 PersistentVolumes 中的一个并更改它的回收策略：\nkubectl patch pv \u0026lt;your-pv-name\u0026gt; -p \u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;persistentVolumeReclaimPolicy\u0026#34;:\u0026#34;Retain\u0026#34;}}\u0026#39; 这里的 \u0026lt;your-pv-name\u0026gt; 是你选择的 PersistentVolume 的名字。\n  验证你选择的 PersistentVolume 拥有正确的策略：\nkubectl get pv 输出类似于这样：\n NAME CAPACITY ACCESSMODES RECLAIMPOLICY STATUS CLAIM REASON AGE pvc-b6efd8da-b7b5-11e6-9d58-0ed433a7dd94 4Gi RWO Delete Bound default/claim1 40s pvc-b95650f8-b7b5-11e6-9d58-0ed433a7dd94 4Gi RWO Delete Bound default/claim2 36s pvc-bb3ca71d-b7b5-11e6-9d58-0ed433a7dd94 4Gi RWO Retain Bound default/claim3 33s  在前面的输出中，你可以看到绑定到 claim default/claim3 的 volume 拥有的回收策略为 Retain。当用户删除 claim default/claim3 时，它不会被自动删除。\n  . heading \u0026ldquo;whatsnext\u0026rdquo; %}}  了解更多关于 PersistentVolumes的信息。 了解更多关于 PersistentVolumeClaims 的信息。  参考   [PersistentVolume](/docs/api-reference/. param \u0026ldquo;version\u0026rdquo; \u0026gt;}}/#persistentvolume-v1-core)\n  [PersistentVolumeClaim](/docs/api-reference/. param \u0026ldquo;version\u0026rdquo; \u0026gt;}}/#persistentvolumeclaim-v1-core)\n  查阅 [PersistentVolumeSpec](/docs/api-reference/. param \u0026ldquo;version\u0026rdquo; \u0026gt;}}/#persistentvolumeclaim-v1-core) 的 persistentVolumeReclaimPolicy 字段。\n  "
},
{
	"uri": "https://lijun.in/concepts/storage/storage-limits/",
	"title": "特定于节点的卷数限制",
	"tags": [],
	"description": "",
	"content": "此页面描述了各个云供应商可关联至一个节点的最大卷数。\n谷歌、亚马逊和微软等云供应商通常对可以关联到节点的卷数量进行限制。 Kubernetes 需要尊重这些限制。 否则，在节点上调度的 Pod 可能会卡住去等待卷的关联。\nKubernetes 的默认限制 The Kubernetes 调度器对关联于一个节点的卷数有默认限制：\n自定义限制 您可以通过设置 KUBE_MAX_PD_VOLS 环境变量的值来设置这些限制，然后再启动调度器。 CSI 驱动程序可能具有不同的过程，关于如何自定义其限制请参阅相关文档。\n如果设置的限制高于默认限制，请谨慎使用。请参阅云提供商的文档以确保节点可支持您设置的限制。\n此限制应用于整个集群，所以它会影响所有节点。\n动态卷限制 feature-state state=\u0026quot;stable\u0026rdquo; for_k8s_version=\u0026quot;v1.17\u0026rdquo; \u0026gt;}}\n以下卷类型支持动态卷限制。\n Amazon EBS Google Persistent Disk Azure Disk CSI  对于由内建插件管理的卷，Kubernetes 会自动确定节点类型并确保节点上可关联的卷数目合规。 例如：\n  在 Google Compute Engine环境中, 根据节点类型最多可以将127个卷关联到节点。\n  对于 M5、C5、R5、T3 和 Z1D 类型实例的 Amazon EBS 磁盘，Kubernetes 仅允许 25 个卷关联到节点。 对于 ec2 上的其他实例类型 Amazon Elastic Compute Cloud (EC2), Kubernetes 允许 39 个卷关联至节点。\n  在 Azure 环境中, 根据节点类型，最多 64 个磁盘可以关联至一个节点。 更多详细信息，请参阅Azure 虚拟机的数量大小。\n  如果 CSI 存储驱动程序（使用 NodeGetInfo ）为节点通告卷数上限，则kube-scheduler 将遵守该限制值。 参考 CSI 规范 获取更多详细信息。\n  对于由已迁移到 CSI 驱动程序的树内插件管理的卷，最大卷数将是 CSI 驱动程序报告的卷数。\n  "
},
{
	"uri": "https://lijun.in/tasks/debug-application-cluster/determine-reason-pod-failure/",
	"title": "确定 Pod 失败的原因",
	"tags": [],
	"description": "",
	"content": "本文介绍如何编写和读取容器的终止消息。\n终止消息为容器提供了一种方法，可以将有关致命事件的信息写入某个位置，在该位置可以通过仪表板和监控软件等工具轻松检索和显示致命事件。 在大多数情况下，您放入终止消息中的信息也应该写入常规 Kubernetes 日志。\n. heading \u0026ldquo;prerequisites\u0026rdquo; %}} . include \u0026ldquo;task-tutorial-prereqs.md\u0026rdquo; \u0026gt;}} . version-check \u0026gt;}}\n读写终止消息 在本练习中，您将创建运行一个容器的 Pod。 配置文件指定在容器启动时要运行的命令。\n. codenew file=\u0026quot;debug/termination.yaml\u0026rdquo; \u0026gt;}}\n   kubectl create -f https://k8s.io/examples/debug/termination.yaml  YAML 文件中，在 cmd 和 args 字段，你可以看到容器休眠 10 秒然后将 \u0026ldquo;Sleep expired\u0026rdquo; 写入 /dev/termination-log 文件。 容器写完 \u0026ldquo;Sleep expired\u0026rdquo; 消息后，它就终止了。\n   kubectl get pod termination-demo  重复前面的命令直到 Pod 不再运行。\n   kubectl get pod --output=yaml   apiVersion: v1 kind: Pod ... lastState: terminated: containerID: ... exitCode: 0 finishedAt: ... message: | Sleep expired ...     kubectl get pod termination-demo -o go-template=\u0026quot;{{range .status.containerStatuses}}{{.lastState.terminated.message}}{{end}}\u0026quot;    定制终止消息 Kubernetes 从容器的 terminationMessagePath 字段中指定的终止消息文件中检索终止消息，默认值为 /dev/termination-log。 通过定制这个字段，您可以告诉 Kubernetes 使用不同的文件。 Kubernetes 使用指定文件中的内容在成功和失败时填充容器的状态消息。\n在下例中，容器将终止消息写入 /tmp/my-log 给 Kubernetes 来接收：\napiVersion: v1 kind: Pod metadata: name: msg-path-demo spec: containers: - name: msg-path-demo-container image: debian terminationMessagePath: \u0026#34;/tmp/my-log\u0026#34; 此外，用户可以设置容器的 terminationMessagePolicy 字段，以便进一步自定义。 此字段默认为 \u0026ldquo;File\u0026rdquo;，这意味着仅从终止消息文件中检索终止消息。 通过将 terminationMessagePolicy 设置为 \u0026ldquo;FallbackToLogsOnError\u0026rdquo;，你就可以告诉 Kubernetes，在容器因错误退出时，如果终止消息文件为空，则使用容器日志输出的最后一块作为终止消息。 日志输出限制为 2048 字节或 80 行，以较小者为准。\n. heading \u0026ldquo;whatsnext\u0026rdquo; %}}  参考[容器](/docs/reference/generated/kubernetes-api/. param \u0026ldquo;version\u0026rdquo; \u0026gt;}}/#container-v1-core)的 terminationMessagePath 字段。 了解接收日志。 了解 Go 模版。  "
},
{
	"uri": "https://lijun.in/tasks/example-task-template/",
	"title": "示例任务的模板",
	"tags": [],
	"description": "",
	"content": ". note \u0026gt;}} 还要确保为新文档在目录中创建一个条目。 . /note \u0026gt;}}\n这个页面展示了如何\u0026hellip;\n. heading \u0026ldquo;prerequisites\u0026rdquo; %}}  . include \u0026ldquo;task-tutorial-prereqs.md\u0026rdquo; \u0026gt;}} . version-check \u0026gt;}} 做这个。 也做这个。  做\u0026hellip;  做这个。 接下来做这个。可能需要阅读一下这个相关解释.  理解 \u0026hellip; [可选部分]\n关于你刚才所做的过程，有一点是需要知道的。\n. heading \u0026ldquo;whatsnext\u0026rdquo; %}} [可选部分]\n 了解更多关于撰写新主题. 查看使用页面模板-任务模板 for how to use this template.  "
},
{
	"uri": "https://lijun.in/tutorials/stateful-application/cassandra/",
	"title": "示例：使用 Stateful Sets 部署 Cassandra",
	"tags": [],
	"description": "",
	"content": "目录  准备工作 Cassandra docker 镜像 快速入门 步骤1：创建 Cassandra Headless Service 步骤2：使用 StatefulSet 创建 Cassandra Ring 环 步骤3：验证并修改 Cassandra StatefulSet 步骤4：删除 Cassandra StatefulSet 步骤5：使用 Replication Controller 创建 Cassandra 节点 pods 步骤6：Cassandra 集群扩容 步骤7：删除 Replication Controller 步骤8：使用 DaemonSet 替换 Replication Controller 步骤9：资源清理 Seed Provider Source  下文描述了在 Kubernetes 上部署一个_云原生_ Cassandra 的过程。当我们说_云原生_时，指的是一个应用能够理解它运行在一个集群管理器内部，并且使用这个集群的管理基础设施来帮助实现这个应用。特别的，本例使用了一个自定义的 Cassandra SeedProvider 帮助 Cassandra 发现新加入集群 Cassandra 节点。\n本示例也使用了Kubernetes的一些核心组件：\n Pods Services Replication Controllers Stateful Sets Daemon Sets  准备工作 本示例假设你已经安装运行了一个 Kubernetes集群（版本 \u0026gt;=1.2），并且还在某个路径下安装了 kubectl 命令行工具。请查看 getting started guides 获取关于你的平台的安装说明。\n本示例还需要一些代码和配置文件。为了避免手动输入，你可以 git clone Kubernetes 源到你本地。\nCassandra Docker 镜像 Pod 使用来自 Google 容器仓库 的 gcr.io/google-samples/cassandra:v12 镜像。这个 docker 镜像基于 debian:jessie 并包含 OpenJDK 8。该镜像包含一个从 Apache Debian 源中安装的标准 Cassandra。你可以通过使用环境变量改变插入到 cassandra.yaml 文件中的参数值。\n   ENV VAR DEFAULT VALUE     CASSANDRA_CLUSTER_NAME \u0026lsquo;Test Cluster\u0026rsquo;   CASSANDRA_NUM_TOKENS 32   CASSANDRA_RPC_ADDRESS 0.0.0.0    快速入门 . codenew file=\u0026quot;application/cassandra/cassandra-service.yaml\u0026rdquo; \u0026gt;}}\n如果你希望直接跳到我们使用的命令，以下是全部步骤：\nkubectl apply -f https://k8s.io/examples/application/cassandra/cassandra-service.yaml . codenew file=\u0026quot;application/cassandra/cassandra-statefulset.yaml\u0026rdquo; \u0026gt;}}\n# 创建 statefulset kubectl apply -f https://k8s.io/examples/application/cassandra/cassandra-statefulset.yaml # 验证 Cassandra 集群。替换一个 pod 的名称。 kubectl exec -ti cassandra-0 -- nodetool status # 清理 grace=$(kubectl get po cassandra-0 -o=jsonpath='{.spec.terminationGracePeriodSeconds}') \\ \u0026amp;\u0026amp; kubectl delete statefulset,po -l app=cassandra \\ \u0026amp;\u0026amp; echo \u0026quot;Sleeping $grace\u0026quot; \\ \u0026amp;\u0026amp; sleep $grace \\ \u0026amp;\u0026amp; kubectl delete pvc -l app=cassandra # # 资源控制器示例 # # 创建一个副本控制器来复制 cassandra 节点 kubectl create -f cassandra/cassandra-controller.yaml # 验证 Cassandra 集群。替换一个 pod 的名称。 kubectl exec -ti cassandra-xxxxx -- nodetool status # 扩大 Cassandra 集群 kubectl scale rc cassandra --replicas=4 # 删除副本控制器 kubectl delete rc cassandra # # 创建一个 DaemonSet，在每个 kubernetes 节点上放置一个 cassandra 节点 # kubectl create -f cassandra/cassandra-daemonset.yaml --validate=false # 资源清理 kubectl delete service -l app=cassandra kubectl delete daemonset cassandra 步骤 1：创建 Cassandra Headless Service Kubernetes Service 描述一组执行同样任务的 Pod。在 Kubernetes 中，一个应用的原子调度单位是一个 Pod：一个或多个_必须_调度到相同主机上的容器。\n这个 Service 用于在 Kubernetes 集群内部进行 Cassandra 客户端和 Cassandra Pod 之间的 DNS 查找。\n以下为这个 service 的描述：\napiVersion: v1 kind: Service metadata: labels: app: cassandra name: cassandra spec: clusterIP: None ports: - port: 9042 selector: app: cassandra Download cassandra-service.yaml and cassandra-statefulset.yaml\n为 StatefulSet 创建 service\nkubectl apply -f https://k8s.io/examples/application/cassandra/cassandra-service.yaml 以下命令显示了 service 是否被成功创建。\n$ kubectl get svc cassandra 命令的响应应该像这样：\nNAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE cassandra None \u0026lt;none\u0026gt; 9042/TCP 45s 如果返回错误则表示 service 创建失败。\n步骤 2：使用 StatefulSet 创建 Cassandra Ring环 StatefulSets（以前叫做 PetSets）特性在 Kubernetes 1.5 中升级为一个 Beta组件。在集群环境中部署类似于 Cassandra 的有状态分布式应用是一项具有挑战性的工作。我们实现了 StatefulSet，极大的简化了这个过程。本示例使用了 StatefulSet 的多个特性，但其本身超出了本文的范围。请参考 StatefulSet 文档。\n以下是 StatefulSet 的清单文件，用于创建一个由三个 pod 组成的 Cassandra ring 环。\n本示例使用了 GCE Storage Class，请根据你运行的云平台做适当的修改。\napiVersion: \u0026#34;apps/v1beta1\u0026#34; kind: StatefulSet metadata: name: cassandra spec: serviceName: cassandra replicas: 3 template: metadata: labels: app: cassandra spec: containers: - name: cassandra image: gcr.io/google-samples/cassandra:v12 imagePullPolicy: Always ports: - containerPort: 7000 name: intra-node - containerPort: 7001 name: tls-intra-node - containerPort: 7199 name: jmx - containerPort: 9042 name: cql resources: limits: cpu: \u0026#34;500m\u0026#34; memory: 1Gi requests: cpu: \u0026#34;500m\u0026#34; memory: 1Gi securityContext: capabilities: add: - IPC_LOCK lifecycle: preStop: exec: command: [\u0026#34;/bin/sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;PID=$(pidof java) \u0026amp;\u0026amp; kill $PID \u0026amp;\u0026amp; while ps -p $PID \u0026gt; /dev/null; do sleep 1; done\u0026#34;] env: - name: MAX_HEAP_SIZE value: 512M - name: HEAP_NEWSIZE value: 100M - name: CASSANDRA_SEEDS value: \u0026#34;cassandra-0.cassandra.default.svc.cluster.local\u0026#34; - name: CASSANDRA_CLUSTER_NAME value: \u0026#34;K8Demo\u0026#34; - name: CASSANDRA_DC value: \u0026#34;DC1-K8Demo\u0026#34; - name: CASSANDRA_RACK value: \u0026#34;Rack1-K8Demo\u0026#34; - name: CASSANDRA_AUTO_BOOTSTRAP value: \u0026#34;false\u0026#34; - name: POD_IP valueFrom: fieldRef: fieldPath: status.podIP readinessProbe: exec: command: - /bin/bash - -c - /ready-probe.sh initialDelaySeconds: 15 timeoutSeconds: 5 # These volume mounts are persistent. They are like inline claims, # but not exactly because the names need to match exactly one of # the stateful pod volumes. volumeMounts: - name: cassandra-data mountPath: /cassandra_data # These are converted to volume claims by the controller # and mounted at the paths mentioned above. # do not use these in production until ssd GCEPersistentDisk or other ssd pd volumeClaimTemplates: - metadata: name: cassandra-data annotations: volume.beta.kubernetes.io/storage-class: fast spec: accessModes: [ \u0026#34;ReadWriteOnce\u0026#34; ] resources: requests: storage: 1Gi --- kind: StorageClass apiVersion: storage.k8s.io/v1beta1 metadata: name: fast provisioner: kubernetes.io/gce-pd parameters: type: pd-ssd 创建 Cassandra StatefulSet 如下：\nkubectl apply -f https://k8s.io/examples/application/cassandra/cassandra-statefulset.yaml 步骤 3：验证和修改 Cassandra StatefulSet 这个 StatefulSet 的部署展示了 StatefulSets 提供的两个新特性：\n Pod 的名称已知 Pod 以递增顺序部署  首先，运行下面的 kubectl 命令，验证 StatefulSet 已经被成功部署。\n$ kubectl get statefulset cassandra 这个命令的响应应该像这样：\nNAME DESIRED CURRENT AGE cassandra 3 3 13s 接下来观察 Cassandra pod 以一个接一个的形式部署。StatefulSet 资源按照数字序号的模式部署 pod：1, 2, 3 等。如果在 pod 部署前执行下面的命令，你就能够看到这种顺序的创建过程。\n$ kubectl get pods -l=\u0026quot;app=cassandra\u0026quot; NAME READY STATUS RESTARTS AGE cassandra-0 1/1 Running 0 1m cassandra-1 0/1 ContainerCreating 0 8s 上面的示例显示了三个 Cassandra StatefulSet pod 中的两个已经部署。一旦所有的 pod 都部署成功，相同的命令会显示一个完整的 StatefulSet。\n$ kubectl get pods -l=\u0026quot;app=cassandra\u0026quot; NAME READY STATUS RESTARTS AGE cassandra-0 1/1 Running 0 10m cassandra-1 1/1 Running 0 9m cassandra-2 1/1 Running 0 8m 运行 Cassandra 工具 nodetool 将显示 ring 环的状态。\n$ kubectl exec cassandra-0 -- nodetool status Datacenter: DC1-K8Demo ====================== Status=Up/Down |/ State=Normal/Leaving/Joining/Moving -- Address Load Tokens Owns (effective) Host ID Rack UN 10.4.2.4 65.26 KiB 32 63.7% a9d27f81-6783-461d-8583-87de2589133e Rack1-K8Demo UN 10.4.0.4 102.04 KiB 32 66.7% 5559a58c-8b03-47ad-bc32-c621708dc2e4 Rack1-K8Demo UN 10.4.1.4 83.06 KiB 32 69.6% 9dce943c-581d-4c0e-9543-f519969cc805 Rack1-K8Demo 你也可以运行 cqlsh 来显示集群的 keyspaces。\n$ kubectl exec cassandra-0 -- cqlsh -e 'desc keyspaces' system_traces system_schema system_auth system system_distributed 你需要使用 kubectl edit 来增加或减小 Cassandra StatefulSet 的大小。你可以在文档 中找到更多关于 edit 命令的信息。\n使用以下命令编辑 StatefulSet。\n$ kubectl edit statefulset cassandra 这会在你的命令行中创建一个编辑器。你需要修改的行是 replicas。这个例子没有包含终端窗口的所有内容，下面示例中的最后一行就是你希望改变的 replicas 行。\n# Please edit the object below. Lines beginning with a '#' will be ignored, # and an empty file will abort the edit. If an error occurs while saving this file will be # reopened with the relevant failures. # apiVersion: apps/v1beta1 kind: StatefulSet metadata: creationTimestamp: 2016-08-13T18:40:58Z generation: 1 labels: app: cassandra name: cassandra namespace: default resourceVersion: \u0026quot;323\u0026quot; uid: 7a219483-6185-11e6-a910-42010a8a0fc0 spec: replicas: 3 按下面的示例修改清单文件并保存。\nspec: replicas: 4 这个 StatefulSet 现在将包含四个 pod。\n$ kubectl get statefulset cassandra 这个command的响应应该像这样：\nNAME DESIRED CURRENT AGE cassandra 4 4 36m 对于 Kubernetes 1.5 发布版，beta StatefulSet 资源没有像 Deployment, ReplicaSet, Replication Controller 或者 Job 一样，包含 kubectl scale 功能，\n步骤 4：删除 Cassandra StatefulSet 删除或者缩容 StatefulSet 时不会删除与之关联的 volumes。这样做是为了优先保证安全。你的数据比其它会被自动清除的 StatefulSet 关联资源更宝贵。删除 Persistent Volume Claims 可能会导致关联的 volumes 被删除，这种行为依赖 storage class 和 reclaim policy。永远不要期望能在 claim 删除后访问一个 volume。\n使用如下命令删除 StatefulSet。\n$ grace=$(kubectl get po cassandra-0 -o=jsonpath='{.spec.terminationGracePeriodSeconds}') \\ \u0026amp;\u0026amp; kubectl delete statefulset -l app=cassandra \\ \u0026amp;\u0026amp; echo \u0026quot;Sleeping $grace\u0026quot; \\ \u0026amp;\u0026amp; sleep $grace \\ \u0026amp;\u0026amp; kubectl delete pvc -l app=cassandra 步骤 5：使用 Replication Controller 创建 Cassandra 节点 pod Kubernetes Replication Controller 负责复制一个完全相同的 pod 集合。像 Service 一样，它具有一个 selector query，用来识别它的集合成员。和 Service 不一样的是，它还具有一个期望的副本数，并且会通过创建或删除 Pod 来保证 Pod 的数量满足它期望的状态。\n和我们刚才定义的 Service 一起，Replication Controller 能够让我们轻松的构建一个复制的、可扩展的 Cassandra 集群。\n让我们创建一个具有两个初始副本的 replication controller。\napiVersion: v1 kind: ReplicationController metadata: name: cassandra # The labels will be applied automatically # from the labels in the pod template, if not set # labels: # app: cassandra spec: replicas: 2 # The selector will be applied automatically # from the labels in the pod template, if not set. # selector: # app: cassandra template: metadata: labels: app: cassandra spec: containers: - command: - /run.sh resources: limits: cpu: 0.5 env: - name: MAX_HEAP_SIZE value: 512M - name: HEAP_NEWSIZE value: 100M - name: CASSANDRA_SEED_PROVIDER value: \u0026#34;io.k8s.cassandra.KubernetesSeedProvider\u0026#34; - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace - name: POD_IP valueFrom: fieldRef: fieldPath: status.podIP image: gcr.io/google-samples/cassandra:v12 name: cassandra ports: - containerPort: 7000 name: intra-node - containerPort: 7001 name: tls-intra-node - containerPort: 7199 name: jmx - containerPort: 9042 name: cql volumeMounts: - mountPath: /cassandra_data name: data volumes: - name: data emptyDir: {} 下载示例\n在这个描述中需要注意几件事情。\nselector 属性包含了控制器的 selector query。它能够被显式指定，或者在没有设置时，像此处一样从 pod 模板中的 labels 中自动应用。\nPod 模板的标签 app:cassandra 匹配步骤1中的 Service selector。这就是 Service 如何选择 replication controller 创建的 pod 的原理。\nreplicas 属性指明了期望的副本数量，在本例中最开始为 2。我们很快将要扩容更多数量。\n创建 Replication Controller：\n$ kubectl create -f cassandra/cassandra-controller.yaml 你可以列出新建的 controller：\n$ kubectl get rc -o wide NAME DESIRED CURRENT AGE CONTAINER(S) IMAGE(S) SELECTOR cassandra 2 2 11s cassandra gcr.io/google-samples/cassandra:v12 app=cassandra 现在，如果你列出集群中的 pod，并且使用 app=cassandra 标签过滤，你应该能够看到两个 Cassandra pod。（wide 参数使你能够看到 pod 被调度到了哪个 Kubernetes 节点上）\n$ kubectl get pods -l=\u0026quot;app=cassandra\u0026quot; -o wide NAME READY STATUS RESTARTS AGE NODE cassandra-21qyy 1/1 Running 0 1m kubernetes-minion-b286 cassandra-q6sz7 1/1 Running 0 1m kubernetes-minion-9ye5 因为这些 pod 拥有 app=cassandra 标签，它们被映射给了我们在步骤 1 中创建的 service。\n你可以使用下面的 service endpoint 查询命令来检查 Pod 是否对 Service 可用。\n$ kubectl get endpoints cassandra -o yaml apiVersion: v1 kind: Endpoints metadata: creationTimestamp: 2015-06-21T22:34:12Z labels: app: cassandra name: cassandra namespace: default resourceVersion: \u0026quot;944373\u0026quot; uid: a3d6c25f-1865-11e5-a34e-42010af01bcc subsets: - addresses: - ip: 10.244.3.15 targetRef: kind: Pod name: cassandra namespace: default resourceVersion: \u0026quot;944372\u0026quot; uid: 9ef9895d-1865-11e5-a34e-42010af01bcc ports: - port: 9042 protocol: TCP 为了显示 SeedProvider 逻辑是按设想在运行，你可以使用 nodetool 命令来检查 Cassandra 集群的状态。为此，请使用 kubectl exec 命令，这样你就能在一个 Cassandra pod 上运行 nodetool。同样的，请替换 cassandra-xxxxx 为任意一个 pods的真实名字。\n$ kubectl exec -ti cassandra-xxxxx -- nodetool status Datacenter: datacenter1 ======================= Status=Up/Down |/ State=Normal/Leaving/Joining/Moving -- Address Load Tokens Owns (effective) Host ID Rack UN 10.244.0.5 74.09 KB 256 100.0% 86feda0f-f070-4a5b-bda1-2eeb0ad08b77 rack1 UN 10.244.3.3 51.28 KB 256 100.0% dafe3154-1d67-42e1-ac1d-78e7e80dce2b rack1 步骤 6：Cassandra 集群扩容 现在，让我们把 Cassandra 集群扩展到 4 个 pod。我们通过告诉 Replication Controller 现在我们需要 4 个副本来完成。\n$ kubectl scale rc cassandra --replicas=4 你可以看到列出了新的 pod：\n$ kubectl get pods -l=\u0026quot;app=cassandra\u0026quot; -o wide NAME READY STATUS RESTARTS AGE NODE cassandra-21qyy 1/1 Running 0 6m kubernetes-minion-b286 cassandra-81m2l 1/1 Running 0 47s kubernetes-minion-b286 cassandra-8qoyp 1/1 Running 0 47s kubernetes-minion-9ye5 cassandra-q6sz7 1/1 Running 0 6m kubernetes-minion-9ye5 一会儿你就能再次检查 Cassandra 集群的状态，你可以看到新的 pod 已经被自定义的 SeedProvider 检测到：\n$ kubectl exec -ti cassandra-xxxxx -- nodetool status Datacenter: datacenter1 ======================= Status=Up/Down |/ State=Normal/Leaving/Joining/Moving -- Address Load Tokens Owns (effective) Host ID Rack UN 10.244.0.6 51.67 KB 256 48.9% d07b23a5-56a1-4b0b-952d-68ab95869163 rack1 UN 10.244.1.5 84.71 KB 256 50.7% e060df1f-faa2-470c-923d-ca049b0f3f38 rack1 UN 10.244.1.6 84.71 KB 256 47.0% 83ca1580-4f3c-4ec5-9b38-75036b7a297f rack1 UN 10.244.0.5 68.2 KB 256 53.4% 72ca27e2-c72c-402a-9313-1e4b61c2f839 rack1 步骤 7：删除 Replication Controller 在你开始步骤 5 之前， __删除__你在上面创建的 replication controller。\n$ kubectl delete rc cassandra 步骤 8：使用 DaemonSet 替换 Replication Controller 在 Kubernetes中，DaemonSet 能够将 pod 一对一的分布到 Kubernetes 节点上。和 ReplicationController 相同的是它也有一个用于识别它的集合成员的 selector query。但和 ReplicationController 不同的是，它拥有一个节点 selector，用于限制基于模板的 pod 可以调度的节点。并且 pod 的复制不是基于一个设置的数量，而是为每一个节点分配一个 pod。\n示范用例：当部署到云平台时，预期情况是实例是短暂的并且随时可能终止。Cassandra 被搭建成为在各个节点间复制数据以便于实现数据冗余。这样的话，即使一个实例终止了，存储在它上面的数据却没有，并且集群会通过重新复制数据到其它运行节点来作为响应。\nDaemonSet 设计为在 Kubernetes 集群中的每个节点上放置一个 pod。那样就会给我们带来数据冗余度。让我们创建一个 DaemonSet 来启动我们的存储集群：\napiVersion: extensions/v1beta1 kind: DaemonSet metadata: labels: name: cassandra name: cassandra spec: template: metadata: labels: app: cassandra spec: # Filter to specific nodes: # nodeSelector: # app: cassandra containers: - command: - /run.sh env: - name: MAX_HEAP_SIZE value: 512M - name: HEAP_NEWSIZE value: 100M - name: CASSANDRA_SEED_PROVIDER value: \u0026#34;io.k8s.cassandra.KubernetesSeedProvider\u0026#34; - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace - name: POD_IP valueFrom: fieldRef: fieldPath: status.podIP image: gcr.io/google-samples/cassandra:v12 name: cassandra ports: - containerPort: 7000 name: intra-node - containerPort: 7001 name: tls-intra-node - containerPort: 7199 name: jmx - containerPort: 9042 name: cql # If you need it, it will go away in C* 4.0. #- containerPort: 9160 # name: thrift resources: requests: cpu: 0.5 volumeMounts: - mountPath: /cassandra_data name: data volumes: - name: data emptyDir: {} 下载示例\n这个 DaemonSet 绝大部分的定义和上面的 ReplicationController 完全相同；它只是简单的给 daemonset 一个创建新的 Cassandra pod 的方法，并且以集群中所有的 Cassandra 节点为目标。\n不同之处在于 nodeSelector 属性，它允许 DaemonSet 以全部节点的一个子集为目标（你可以向其他资源一样标记节点），并且没有 replicas 属性，因为它使用1对1的 node-pod 关系。\n创建这个 DaemonSet：\n$ kubectl create -f cassandra/cassandra-daemonset.yaml 你可能需要禁用配置文件检查，像这样：\n$ kubectl create -f cassandra/cassandra-daemonset.yaml --validate=false 你可以看到 DaemonSet 已经在运行：\n$ kubectl get daemonset NAME DESIRED CURRENT NODE-SELECTOR cassandra 3 3 \u0026lt;none\u0026gt; 现在，如果你列出集群中的 pods，并且使用 app=cassandra 标签过滤，你应该能够看到你的网络中的每一个节点上都有一个（且只有一个）新的 cassandra pod。\n$ kubectl get pods -l=\u0026quot;app=cassandra\u0026quot; -o wide NAME READY STATUS RESTARTS AGE NODE cassandra-ico4r 1/1 Running 0 4s kubernetes-minion-rpo1 cassandra-kitfh 1/1 Running 0 1s kubernetes-minion-9ye5 cassandra-tzw89 1/1 Running 0 2s kubernetes-minion-b286 为了证明这是按设想的在工作，你可以再次使用 nodetool 命令来检查集群的状态。为此，请使用 kubectl exec 命令在任何一个新建的 cassandra pod 上运行 nodetool。\n$ kubectl exec -ti cassandra-xxxxx -- nodetool status Datacenter: datacenter1 ======================= Status=Up/Down |/ State=Normal/Leaving/Joining/Moving -- Address Load Tokens Owns (effective) Host ID Rack UN 10.244.0.5 74.09 KB 256 100.0% 86feda0f-f070-4a5b-bda1-2eeb0ad08b77 rack1 UN 10.244.4.2 32.45 KB 256 100.0% 0b1be71a-6ffb-4895-ac3e-b9791299c141 rack1 UN 10.244.3.3 51.28 KB 256 100.0% dafe3154-1d67-42e1-ac1d-78e7e80dce2b rack1 注意：这个示例让你在创建 DaemonSet 前删除了 cassandra 的 Replication Controller。这是因为为了保持示例的简单，RC 和 DaemonSet 使用了相同的 app=cassandra 标签（如此它们的 pod 映射到了我们创建的 service，这样 SeedProvider 就能识别它们）。\n如果我们没有预先删除 RC，这两个资源在需要运行多少 pod 上将会发生冲突。如果希望的话，我们可以使用额外的标签和 selectors 来支持同时运行它们。\n步骤 9：资源清理 当你准备删除你的资源时，按以下执行：\n$ kubectl delete service -l app=cassandra $ kubectl delete daemonset cassandra Seed Provider Source 我们使用了一个自定义的 SeedProvider 来在 Kubernetes 之上运行 Cassandra。仅当你通过 replication control 或者 daemonset 部署 Cassandra 时才需要使用自定义的 seed provider。在 Cassandra 中，SeedProvider 引导 Cassandra 使用 gossip 协议来查找其它 Cassandra 节点。Seed 地址是被视为连接端点的主机。Cassandra 实例使用 seed 列表来查找彼此并学习 ring 环拓扑。KubernetesSeedProvider 通过 Kubernetes API 发现 Cassandra seeds IP 地址，那些 Cassandra 实例在 Cassandra Service 中定义。\n请查阅自定义 seed provider 的 README 文档，获取 KubernetesSeedProvider 进阶配置。对于本示例来说，你应该不需要自定义 Seed Provider 的配置。\n查看本示例的 image 目录，了解如何构建容器的 docker 镜像及其内容。\n你可能还注意到我们设置了一些 Cassandra 参数（MAX_HEAP_SIZE和HEAP_NEWSIZE），并且增加了关于 namespace 的信息。我们还告诉 Kubernetes 容器暴露了 CQL 和 Thrift API 端口。最后，我们告诉集群管理器我们需要 0.1 cpu（0.1 核）。\n!Analytics]()\n"
},
{
	"uri": "https://lijun.in/reference/access-authn-authz/service-accounts-admin/",
	"title": "管理 Service Accounts",
	"tags": [],
	"description": "",
	"content": "这是一篇针对service accounts（服务账户）的集群管理员指南。 它呈现了 User Guide to Service Accounts中的信息。\n对授权和用户账户的支持已在规划中，当前并不完备，为了更好地描述 service accounts，有时这些不完善的特性也会被提及。\n用户账户与服务账户 Kubernetes 区分用户账户和服务账户的概念主要基于以下原因：\n 用户账户是针对人而言的。 服务账户是针对运行在 pod 中的进程而言的。 用户账户是全局性的。 其名称在集群各 namespace 中都是全局唯一的，未来的用户资源不会做 namespace 隔离， 服务账户是 namespace 隔离的。 通常情况下，集群的用户账户可能会从企业数据库进行同步，其创建需要特殊权限，并且涉及到复杂的业务流程。 服务账户创建的目的是为了更轻量，允许集群用户为了具体的任务创建服务账户 ( 即权限最小化原则 )。 对人员和服务账户审计所考虑的因素可能不同。 针对复杂系统的配置可能包含系统组件相关的各种服务账户的定义。 因为服务账户可以定制化地创建，并且有 namespace 级别的名称，这种配置是很轻量的。  服务账户的自动化 三个独立组件协作完成服务账户相关的自动化 :\n 服务账户准入控制器（Service account admission controller） Token 控制器（Token controller） 服务账户控制器（Service account controller）  服务账户准入控制器 对 pod 的改动通过一个被称为 Admission Controller 的插件来实现。它是 apiserver 的一部分。 当 pod 被创建或更新时，它会同步地修改 pod。 当该插件处于激活状态 ( 在大多数发行版中都是默认的 )，当 pod 被创建或更新时它会进行以下动作：\n 如果该 pod 没有 ServiceAccount 设置，将其 ServiceAccount 设为 default。 保证 pod 所关联的 ServiceAccount 存在，否则拒绝该 pod。 如果 pod 不包含 ImagePullSecrets 设置，那么 将 ServiceAccount 中的 ImagePullSecrets 信息添加到 pod 中。 将一个包含用于 API 访问的 token 的 volume 添加到 pod 中。 将挂载于 /var/run/secrets/kubernetes.io/serviceaccount 的 volumeSource 添加到 pod 下的每个容器中。  Token 管理器 Token 管理器是 controller-manager 的一部分。 以异步的形式工作：\n 检测服务账户的创建，并且创建相应的 Secret 以支持 API 访问。 检测服务账户的删除，并且删除所有相应的服务账户 Token Secret。 检测 Secret 的增加，保证相应的服务账户存在，如有需要，为 Secret 增加 token。 检测 Secret 的删除，如有需要，从相应的服务账户中移除引用。  你需要通过 --service-account-private-key-file 参数项传入一个服务账户私钥文件至 Token 管理器。 私钥用于为生成的服务账户 token 签名。 同样地，你需要通过 --service-account-key-file 参数将对应的公钥传入 kube-apiserver。 公钥用于认证过程中的 token 校验。\n创建额外的 API tokens 控制器中有专门的循环来保证每个服务账户中都存在 API token 对应的 Secret。 当需要为服务账户创建额外的 API token 时，创建一个类型为 ServiceAccountToken 的 Secret，并在 annotation 中引用服务账户，控制器会生成 token 并更新 :\nsecret.json:\n{ \u0026#34;kind\u0026#34;: \u0026#34;Secret\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;v1\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;mysecretname\u0026#34;, \u0026#34;annotations\u0026#34;: { \u0026#34;kubernetes.io/service-account.name\u0026#34;: \u0026#34;myserviceaccount\u0026#34; } }, \u0026#34;type\u0026#34;: \u0026#34;kubernetes.io/service-account-token\u0026#34; } kubectl create -f ./secret.json kubectl describe secret mysecretname 删除 / 失效 服务账户 token kubectl delete secret mysecretname 服务账户管理器 服务账户管理器管理各命名空间下的服务账户，并且保证每个活跃的命名空间下存在一个名为 \u0026ldquo;default\u0026rdquo; 的服务账户\n"
},
{
	"uri": "https://lijun.in/tasks/tls/managing-tls-in-a-cluster/",
	"title": "管理集群中的 TLS 认证",
	"tags": [],
	"description": "",
	"content": "Kubernetes提供一个 certificates.k8s.io API，可让您配置 由您控制的证书颁发机构（CA）签名的TLS证书。 您的工作负载可以使用这些CA和证书来建立信任。\ncertificates.k8s.io API使用的协议类似于ACME 草稿。\n. note \u0026gt;}}\n使用certificates.k8s.io API创建的证书由 指定 CA 颁发。 将集群配置为使用集群根目录 CA 可以达到这个目的，但是您永远不要依赖它。不要以为 这些证书将针对群根目录 CA 进行验证。 . /note \u0026gt;}}\n. heading \u0026ldquo;prerequisites\u0026rdquo; %}} . include \u0026ldquo;task-tutorial-prereqs.md\u0026rdquo; \u0026gt;}} . version-check \u0026gt;}}\n集群中的 TLS 信任 让 Pod 中运行的应用程序信任集群根 CA 通常需要一些额外的应用程序配置。您将需要将 CA 证书包添加到 TLS 客户端或服务器信任的 CA 证书列表中。例如，您可以使用 golang TLS 配置通过解析证书链并将解析的证书添加到 tls.Config 结构中的 RootCAs 字段中。\nCA 证书捆绑包将使用默认服务账户自动加载到 pod 中，路径为 /var/run/secrets/kubernetes.io/serviceaccount/ca.crt。如果您没有使用默认服务账户，请请求集群管理员构建包含您有权访问使用的证书包的 configmap。\n请求认证 以下部分演示如何为通过 DNS 访问的 Kubernetes 服务创建 TLS 证书。\n. note \u0026gt;}} 本教程使用 CFSSL：Cloudflare\u0026rsquo;s PKI 和 TLS 工具包点击此处了解更多信息。 . /note \u0026gt;}}\n下载并安装 CFSSL 本例中使用的 cfssl 工具可以在 https://pkg.cfssl.org/ 下载。\n创建证书签名请求 通过运行以下命令生成私钥和证书签名请求（或 CSR）:\ncat \u0026lt;\u0026lt;EOF | cfssl genkey - | cfssljson -bare server { \u0026#34;hosts\u0026#34;: [ \u0026#34;my-svc.my-namespace.svc.cluster.local\u0026#34;, \u0026#34;my-pod.my-namespace.pod.cluster.local\u0026#34;, \u0026#34;172.168.0.24\u0026#34;, \u0026#34;10.0.34.2\u0026#34; ], \u0026#34;CN\u0026#34;: \u0026#34;my-pod.my-namespace.pod.cluster.local\u0026#34;, \u0026#34;key\u0026#34;: { \u0026#34;algo\u0026#34;: \u0026#34;ecdsa\u0026#34;, \u0026#34;size\u0026#34;: 256 } } EOF 其中 172.168.0.24 是服务的集群 IP，my-svc.my-namespace.svc.cluster.local 是服务的 DNS 名称，10.0.34.2 是 pod 的 IP 和 my-pod.my-namespace.pod.cluster.local 是 pod 的 DNS 名称。您能看到以下的输出：\n2017/03/21 06:48:17 [INFO] generate received request 2017/03/21 06:48:17 [INFO] received CSR 2017/03/21 06:48:17 [INFO] generating key: ecdsa-256 2017/03/21 06:48:17 [INFO] encoded CSR 该命令生成两个文件；它生成包含 PEM 编码 pkcs#10 认证请求的 server.csr，以及包含证书的 PEM 编码密钥的 server-key.pem 还有待生成。\n创建证书签名请求对象发送到 Kubernetes API 使用以下命令创建 CSR yaml 文件，并发送到 API server：\ncat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: certificates.k8s.io/v1beta1 kind: CertificateSigningRequest metadata: name: my-svc.my-namespace spec: groups: - system:authenticated request: $(cat server.csr | base64 | tr -d \u0026#39;\\n\u0026#39;) usages: - digital signature - key encipherment - server auth EOF 请注意，在步骤1中创建的 server.csr 文件是 base64 编码并存储在 .spec.request 字段中的，我们还要求提供 “数字签名”，“密钥加密” 和 “服务器身份验证” 密钥用途的证书。我们这里支持列出的所有关键用途和扩展的关键用途，以便您可以使用相同的 API 请求客户端证书和其他证书。\n在 API server 中可以看到这些 CSR 处于 pending 状态。执行下面的命令您将可以看到：\nkubectl describe csr my-svc.my-namespace Name: my-svc.my-namespace Labels: \u0026lt;none\u0026gt; Annotations: \u0026lt;none\u0026gt; CreationTimestamp: Tue, 21 Mar 2017 07:03:51 -0700 Requesting User: yourname@example.com Status: Pending Subject: Common Name: my-svc.my-namespace.svc.cluster.local Serial Number: Subject Alternative Names: DNS Names: my-svc.my-namespace.svc.cluster.local IP Addresses: 172.168.0.24 10.0.34.2 Events: \u0026lt;none\u0026gt; 获取批准的证书签名请求 批准证书签名请求是通过自动批准过程完成的，或由集群管理员一次性完成。有关这方面涉及的更多信息，请参见下文。\n下载证书并使用它 CSR 被签署并获得批准后，您应该看到以下内容：\nkubectl get csr NAME AGE REQUESTOR CONDITION my-svc.my-namespace 10m yourname@example.com Approved,Issued 您可以通过运行以下命令下载颁发的证书并将其保存到 server.crt 文件中：\nkubectl get csr my-svc.my-namespace -o jsonpath=\u0026#39;{.status.certificate}\u0026#39; \\  | base64 --decode \u0026gt; server.crt 现在您可以将 server.crt 和 server-key.pem 作为键值对来启动 HTTPS 服务器。\n批准证书签名请求 Kubernetes 管理员（具有适当权限）可以使用 kubectl certificate approve 和 kubectl certificate deny 命令手动批准（或拒绝）证书签名请求。但是，如果您打算大量使用此 API，则可以考虑编写自动化的证书控制器。\n无论上述机器或人使用 kubectl，批准者的作用是验证 CSR 满足如下两个要求：\n CSR 的主体控制用于签署 CSR 的私钥。这解决了伪装成授权主体的第三方的威胁。在上述示例中，此步骤将验证该 pod 控制了用于生成 CSR 的私钥。 CSR 的主体被授权在请求的上下文中执行。这解决了我们加入群集的我们不期望的主体的威胁。在上述示例中，此步骤将是验证该 pod 是否被允许加入到所请求的服务中。  当且仅当满足这两个要求时，审批者应该批准 CSR，否则拒绝 CSR。\n关于批准许可的警告 批准 CSR 的能力决定谁信任群集中的谁。这包括 Kubernetes API 信任的人。批准 CSR 的能力不能过于广泛和轻率。在给予本许可之前，应充分了解上一节中提到的挑战和发布特定证书的后果。有关证书与认证交互的信息，请参阅此处。\n给集群管理员的一个建议 本教程假设将签名者设置为服务证书 API。Kubernetes controller manager 提供了一个签名者的默认实现。 要启用它，请将--cluster-signing-cert-file 和 --cluster-signing-key-file 参数传递给 controller manager，并配置具有证书颁发机构的密钥对的路径。\n"
},
{
	"uri": "https://lijun.in/concepts/cluster-administration/federation/",
	"title": "联邦",
	"tags": [],
	"description": "",
	"content": "本页面阐明了为何以及如何使用联邦创建Kubernetes集群。\n为何使用联邦 联邦可以使多个集群的管理简单化。它提供了两个主要构件模块：\n 跨集群同步资源：联邦能够让资源在多个集群中同步。例如，你可以确保在多个集群中存在同样的部署。 跨集群发现：联邦能够在所有集群的后端自动配置DNS服务和负载均衡。例如，通过多个集群的后端，你可以确保全局的VIP或DNS记录可用。  联邦技术的其他应用场景：\n 高可用性：通过跨集群分摊负载，自动配置DNS服务和负载均衡，联邦将集群失败所带来的影响降到最低。 避免供应商锁定：跨集群使迁移应用程序变得更容易，联邦服务避免了供应商锁定。  只有在多个集群的场景下联邦服务才是有帮助的。这里列出了一些你会使用多个集群的原因：\n 降低延迟：在多个区域含有集群，可使用离用户最近的集群来服务用户，从而最大限度降低延迟。 故障隔离：对于故障隔离，也许有多个小的集群比有一个大的集群要更好一些（例如：一个云供应商的不同可用域里有多个集群）。详细信息请参阅多集群指南。 可伸缩性：对于单个kubernetes集群是有伸缩性限制的（但对于大多数用户来说并非如此。更多细节参考Kubernetes扩展和性能目标）。 混合云：可以有多个集群，它们分别拥有不同的云供应商或者本地数据中心。  注意事项 虽然联邦有很多吸引人的场景，但这里还是有一些需要关注的事项：\n 增加网络的带宽和损耗：联邦控制面会监控所有的集群，来确保集群的当前状态与预期一致。那么当这些集群运行在一个或者多个云提供者的不同区域中，则会带来重大的网络损耗。 降低集群的隔离：当联邦控制面中存在一个故障时，会影响所有的集群。把联邦控制面的逻辑降到最小可以缓解这个问题。 无论何时，它都是kubernetes集群里控制面的代表。设计和实现也使其变得更安全,避免多集群运行中断。 完整性：联邦项目相对较新，还不是很成熟。不是所有资源都可用，且很多资源才刚刚开始。Issue 38893 列举了一些团队正忙于解决的系统已知问题。  混合云的能力 Kubernetes集群里的联邦包括运行在不同云供应商上的集群（例如，谷歌云、亚马逊），和本地部署的集群（例如，OpenStack）。只需在适当的云供应商和/或位置创建所需的所有集群，并将每个集群的API endpoint和凭据注册到您的联邦API服务中（详情参考联邦管理指南）。\n在此之后，您的API资源就可以跨越不同的集群和云供应商。\n建立联邦 若要能联合多个集群，首先需要建立一个联邦控制面。参照安装指南 建立联邦控制面。\nAPI资源 控制面建立完成后，就可以开始创建联邦API资源了。 以下指南详细介绍了一些资源：\n Cluster ConfigMap DaemonSets Deployment Events Ingress Namespaces ReplicaSets Secrets Services  API参考文档列举了联邦API服务支持的所有资源。\n级联删除 Kubernetes1.6版本支持联邦资源级联删除。使用级联删除，即当删除联邦控制面的一个资源时，也删除了所有底层集群中的相应资源。\n当使用REST API时，级联删除功能不是默认开启的。若使用REST API从联邦控制面删除一个资源时，要开启级联删除功能，即需配置选项 DeleteOptions.orphanDependents=false。使用kubectl delete使级联删除功能默认开启。使用kubectl delete --cascade=false禁用级联删除功能。\n注意：Kubernetes1.5版本开始支持联邦资源子集的级联删除。\n单个集群的范围 对于IaaS供应商如谷歌计算引擎或亚马逊网络服务，一个虚拟机存在于一个域或可用域中。 我们建议一个Kubernetes集群里的所有虚机应该在相同的可用域里，因为：\n 与单一的全局Kubernetes集群对比，该方式有较少的单点故障。 与跨可用域的集群对比，该方式更容易推断单区域集群的可用性属性。 当Kubernetes开发者设计一个系统(例如，对延迟、带宽或相关故障进行假设)，他们也会假设所有的机器都在一个单一的数据中心，或者以其他方式紧密相连。  每个可用区域里包含多个集群当然是可以的，但是总的来说我们认为集群数越少越好。 偏爱较少集群数的原因是：\n 在某些情况下，在一个集群里有更多的节点，可以改进Pods的装箱问题（更少的资源碎片）。 减少操作开销（尽管随着OPS工具和流程的成熟而降低了这块的优势）。 为每个集群的固定资源花费降低开销，例如，使用apiserver的虚拟机（但是在全体集群开销中，中小型集群的开销占比要小的多）。  多集群的原因包括：\n 严格的安全性策略要求隔离一类工作与另一类工作（但是，请参见下面的集群分割）。 测试集群或其他集群软件直至最优的新Kubernetes版本发布。  选择合适的集群数 Kubernetes集群数量选择也许是一个相对静止的选择，因为对其重新审核的情况很少。相比之下，一个集群中的节点数和一个服务中的pods数可能会根据负载和增长频繁变化。\n选择集群的数量，首先，需要决定哪些区域对于将要运行在Kubernetes上的服务，可以有足够的时间到达所有的终端用户（如果使用内容分发网络，则不需要考虑CDN-hosted内容的延迟需求）。法律问题也可能影响这一点。例如，拥有全球客户群的公司可能会对于在美国、欧盟、亚太和南非地区拥有集群起到决定权。使用R代表区域的数量。\n其次，决定有多少集群在同一时间不可用，而一些仍然可用。使用U代表不可用的数量。如果不确定，最好选择1。\n如果允许负载均衡在集群故障发生时将通信引导到任何区域，那么至少需要较大的R或U + 1集群。若非如此（例如，若要在集群故障发生时确保所有用户的低延迟），则需要R * (U + 1)集群(在每一个R区域里都有U + 1)。在任何情况下，尝试将每个集群放在不同的区域中。\n最后，如果你的集群需求超过一个Kubernetes集群推荐的最大节点数，那么你可能需要更多的集群。Kubernetes1.3版本支持多达1000个节点的集群规模。\n  进一步学习[联邦提案](https://github.com/kubernetes/community/blob/ param \u0026ldquo;githubbranch\u0026rdquo; \u0026gt;}}/contributors/design-proposals/multicluster/federation.md)。 集群联邦参考该配置指导。 查看Kubecon2016浅谈联邦  "
},
{
	"uri": "https://lijun.in/tasks/federation/administer-federation/configmap/",
	"title": "联邦 ConfigMap",
	"tags": [],
	"description": "",
	"content": ". deprecationfilewarning \u0026gt;}} . include \u0026ldquo;federation-deprecation-warning-note.md\u0026rdquo; \u0026gt;}} . /deprecationfilewarning \u0026gt;}}\n本指南介绍如何在联邦控制平面中使用 ConfigMap。\n联邦 ConfigMap 与传统 Kubernetes ConfigMap 非常相似且提供相同的功能。 在联邦控制平面中创建它们可以确保它们在联邦的所有集群中同步。\n. heading \u0026ldquo;prerequisites\u0026rdquo; %}}  . include \u0026ldquo;federated-task-tutorial-prereqs.md\u0026rdquo; \u0026gt;}}   通常我们还期望您拥有基本的 Kubernetes 应用知识， 特别是 ConfigMap 相关的应用知识。  创建联邦 ConfigMap 联邦 ConfigMap 的 API 100% 兼容传统 Kubernetes ConfigMap 的 API。您可以通过向联邦 apiserver 发送请求来创建 ConfigMap。 您可以通过使用 kubectl 运行下面的指令来创建联邦 ConfigMap：\nkubectl --context=federation-cluster create -f myconfigmap.yaml --context=federation-cluster 参数告诉 kubectl 将请求提交到联邦 apiserver 而不是发送给某一个 Kubernetes 集群。\n一旦联邦 ConfigMap 被创建，联邦控制平面就会在所有底层 Kubernetes 集群中创建匹配的 ConfigMap。 您可以通过检查底层每个集群来对其进行验证，例如：\nkubectl --context=gce-asia-east1a get configmap myconfigmap 上面的命令假定您在客户端中配置了一个叫做 ‘gce-asia-east1a’ 的上下文。\n这些底层集群中的 ConfigMap 将与 联邦 ConfigMap 相匹配。\n更新联邦 ConfigMap 您可以像更新 Kubernetes ConfigMap 一样更新联邦 ConfigMap。 但是对于联邦 ConfigMap，您必须发送请求到联邦 apiserver 而不是某个特定的 Kubernetes 集群。 联邦控制平面会确保每当联邦 ConfigMap 更新时，它会更新所有底层集群中的 ConfigMap 来和更新后的内容保持一致。\n删除联邦 ConfigMap 您可以像删除 Kubernetes ConfigMap 一样删除联邦 ConfigMap。 但是，对于联邦 ConfigMap，您必须发送请求到联邦 apiserver 而不是某个特定的 Kubernetes 集群。 例如，您可以使用 kubectl 运行下面的命令来删除联邦 ConfigMap：\nkubectl --context=federation-cluster delete configmap . note \u0026gt;}}\n要注意的是这时删除联邦 ConfigMap 并不会删除底层集群中对应的 ConfigMap。您必须自己手动删除底层集群中的 ConfigMap。 . /note \u0026gt;}}\n"
},
{
	"uri": "https://lijun.in/tasks/federation/administer-federation/daemonset/",
	"title": "联邦 DaemonSet",
	"tags": [],
	"description": "",
	"content": ". deprecationfilewarning \u0026gt;}} . include \u0026ldquo;federation-deprecation-warning-note.md\u0026rdquo; \u0026gt;}} . /deprecationfilewarning \u0026gt;}}\n本指南说明了如何在联邦控制平面中使用 DaemonSet。\n联邦控制平面中的 DaemonSet（在本指南中称为 “联邦 DaemonSet”）与传统的 Kubernetes DaemonSet 非常类似，并提供相同的功能。在联邦控制平面中创建联邦 DaemonSet 可以确保它们同步到联邦的所有集群中。\n. heading \u0026ldquo;prerequisites\u0026rdquo; %}}  . include \u0026ldquo;federated-task-tutorial-prereqs.md\u0026rdquo; \u0026gt;}}   你还应该具备基本的 Kubernetes 应用知识，特别是 DaemonSet 相关的应用知识。  创建联邦 Daemonset 联邦 Daemonset 的 API 和传统的 Kubernetes Daemonset API 是 100% 兼容的。您可以通过向联邦 apiserver 发送请求来创建一个 DaemonSet。\n您可以通过使用 kubectl 运行下面的指令来创建联邦 Daemonset：\nkubectl --context=federation-cluster create -f mydaemonset.yaml --context=federation-cluster 参数告诉 kubectl 发送请求到联邦 apiserver 而不是某个 Kubernetes 集群。\n一旦联邦 Daemonset 被创建，联邦控制平面就会在所有底层 Kubernetes 集群中创建匹配的 Daemonset。您可以通过检查底层每个集群来对其进行验证，例如：\nkubectl --context=gce-asia-east1a get daemonset mydaemonset 上面的命令假定您在客户端中配置了一个叫做 ‘gce-asia-east1a’ 的上下文。\n更新联邦 Daemonset 您可以像更新 Kubernetes Daemonset 一样更新联邦 Daemonset。但是，对于联邦 Daemonset，您必须发送请求到联邦 apiserver 而不是某个特定的 Kubernetes 集群。联邦控制平面会确保每当联邦 Daemonset 更新时，它会更新所有底层集群中的 Daemonset 来和更新后的内容保持一致。\n删除联邦 Daemonset 您可以像删除 Kubernetes Daemonset 一样删除联邦 Daemonset。但是，对于联邦 Daemonset，您必须发送请求到联邦 apiserver 而不是某个特定的 Kubernetes 集群。\n例如，您可以使用 kubectl 运行下面的命令来删除联邦 Daemonset：\nkubectl --context=federation-cluster delete daemonset mydaemonset "
},
{
	"uri": "https://lijun.in/tasks/federation/administer-federation/deployment/",
	"title": "联邦 Deployment",
	"tags": [],
	"description": "",
	"content": ". deprecationfilewarning \u0026gt;}} . include \u0026ldquo;federation-deprecation-warning-note.md\u0026rdquo; \u0026gt;}} . /deprecationfilewarning \u0026gt;}}\n本指南说明了如何在联邦控制平面中使用 Deployment。\n联邦控制平面中的 Deployment（在本指南中称为 “联邦 Deployment”）与传统的 Kubernetes Deployment 非常类似，并提供相同的功能。在联邦控制平面中创建联邦 Deployment 确保所需的副本数存在于注册的群集中。\n. feature-state for_k8s_version=\u0026quot;1.5\u0026rdquo; state=\u0026quot;alpha\u0026rdquo; \u0026gt;}}\n一些特性（例如完整的 rollout 兼容性）仍在开发中。\n. heading \u0026ldquo;prerequisites\u0026rdquo; %}}  . include \u0026ldquo;federated-task-tutorial-prereqs.md\u0026rdquo; \u0026gt;}}   您还应当拥有基本的 Kubernetes 应用知识，特别是在 Deployments 方面。  创建联邦 Deployment 联邦 Deployment 的 API 和传统的 Kubernetes Deployment API 是兼容的。 您可以通过向联邦 apiserver 发送请求来创建一个 Deployment。\n您可以通过使用 kubectl 运行下面的指令：\nkubectl --context=federation-cluster create -f mydeployment.yaml --context=federation-cluster 参数告诉 kubectl 发送请求到联邦 apiserver 而不是某个 Kubernetes 集群。\n一旦联邦 Deployment 被创建，联邦控制平面会在所有底层 Kubernetes 集群中创建一个 Deployment。 您可以通过检查底层每个集群来对其进行验证，例如：\nkubectl --context=gce-asia-east1a get deployment mydep 上面的命令假定您在客户端中配置了一个叫做 ‘gce-asia-east1a’ 的上下文，\n底层集群中的这些 Deployment 会匹配联邦 Deployment 中副本数和修订版本相关注解_之外_的信息。 联邦控制平面确保所有集群中的副本总数与联邦 Deployment 中请求的副本数量匹配。\n在底层集群中分布副本 默认情况下，副本会被平均分布到所有的底层集群中。例如：如果您有 3 个注册的集群并且创建了一个副本数为 9(spec.replicas = 9) 的联邦 Deployment，那么这 3 个集群中的每个 Deployment 都将有 3 个副本 (spec.replicas=3)。 为修改每个集群中的副本数，您可以在联邦 Deployment 中以注解的形式指定 [FederatedReplicaSetPreference](https://github.com/kubernetes/federation/blob/. param \u0026ldquo;githubbranch\u0026rdquo; \u0026gt;}}/apis/federation/types.go)，其中注解的键为 federation.kubernetes.io/deployment-preferences。\n更新联邦 Deployment 您可以像更新 Kubernetes Deployment 一样更新联邦 Deployment。但是，对于联邦 Deployment，您必须发送请求到联邦 apiserver 而不是某个特定的 Kubernetes 集群。联邦控制平面会确保每当联邦 Deployment 更新时，它会更新所有底层集群中相应的 Deployment 来和更新后的内容保持一致。 所以如果（在联邦 Deployment 中）选择了滚动更新，那么底层集群会独立地进行滚动更新，并且联邦 Deployment 中的 maxSurge 和 maxUnavailable 只会应用于独立的集群中。将来这种行为可能会改变。\n如果您的更新包括副本数量的变化，联邦控制平面会改变底层集群中的副本数量，以确保它们的总数等于联邦 Deployment 中请求的数量。\n删除联邦 Deployment 您可以像删除 Kubernetes Deployment 一样删除联邦 Deployment。但是，对于联邦 Deployment，您必须发送请求到联邦 apiserver 而不是某个特定的 Kubernetes 集群。\n例如，您可以使用 kubectl 运行下面的命令来删除联邦 Deployment：\nkubectl --context=federation-cluster delete deployment mydep "
},
{
	"uri": "https://lijun.in/tasks/federation/administer-federation/job/",
	"title": "联邦 Job",
	"tags": [],
	"description": "",
	"content": ". deprecationfilewarning \u0026gt;}} . include \u0026ldquo;federation-deprecation-warning-note.md\u0026rdquo; \u0026gt;}} . /deprecationfilewarning \u0026gt;}}\n本指南解释了如何在联邦控制平面中使用 job。\n联邦控制平面中的一次性任务（在本指南中称为“联邦一次性任务”）类似于传统的 Kubernetes 一次性任务，并且提供相同的功能。 在联邦控制平面中创建 job 可以确保在已注册的集群中存在所需的并行性和完成数。\n. heading \u0026ldquo;prerequisites\u0026rdquo; %}}  . include \u0026ldquo;federated-task-tutorial-prereqs.md\u0026rdquo; \u0026gt;}} 你需要具备基本的 Kubernetes 的工作知识，特别是 job。  创建一个联邦 job 用于联邦 job 的 API 与用于传统 Kubernetes job 的 API 完全兼容。您可以通过向联邦 apiserver 发送请求来创建 job。\n你可以使用 kubectl 来运行：\nkubectl --context=federation-cluster create -f myjob.yaml --context=federation-cluster 参数告诉 kubectl 将请求提交到联邦 API 服务器，而不是发送到 Kubernetes 集群。\n一旦创建了联邦 job，联邦控制平面将在所有底层 Kubernetes 集群中创建一个 job。 你可以通过检查每个集群底层来验证这一点，例如：\nkubectl --context=gce-asia-east1a get job myjob 前面的示例假设你的客户端中为该区域中的集群配置了一个名为 gce-asia-east1a 的上下文。\n集群底层中的 job 与联邦 job 匹配，但并行性和完成数不匹配。 联邦控制平面确保每个集群中的并行性和完成数之和与联合作业中所需的并行度和完成数匹配。\n将 job 任务分散到集群底层中 默认情况下，并行性和完成数在所有底层集群中平均分布。例如： 如果你有 3 个已注册的集群，并且创建了一个联邦 job spec.parallelism = 9 和 spec.completions = 18，那么 3 个集群中的每个 job 都有 spec.parallelism = 3 和 spec.completions = 6。 要修改每个集群中的并行性和完成数，可以指定 [ReplicaAllocationPreferences](https://github.com/kubernetes/federation/blob/. param \u0026ldquo;githubbranch\u0026rdquo; \u0026gt;}}/apis/federation/types.go) 作为 federation.kubernetes.io/job-preferences 联邦 job 上的 key 的注释。\n更新联邦 job 可以像更新 Kubernetes job 一样更新联邦 job；但是，对于联邦 job，必须将请求发送到联邦 API 服务器，不是发送到指定的 Kubernetes 集群。 联邦控制平面确保无论何时更新联邦 job，它都会更新所有集群底层中的相应 job 以匹配它。\n如果您的更新包含并行性和完成数的更改，则联邦控制平面将更改集群底层中的并行性和完成数， 确保它们的总和仍然等于联邦 job 中所需的并行性和完成数。\n删除联邦 job 可以删除联邦 job，就像删除 Kubernetes job 一样;但是，对于联邦 job，必须将请求发送到联邦 API 服务器，不是发送到指定的 Kubernetes 集群。\n例如，使用 kubectl：\nkubectl --context=federation-cluster delete job myjob . note \u0026gt;}}\n删除联邦作业不会从基础集群中删除相应的 job。 您必须手动删除基础 job。\n. /note \u0026gt;}}\n"
},
{
	"uri": "https://lijun.in/tasks/federation/administer-federation/replicaset/",
	"title": "联邦 ReplicaSet",
	"tags": [],
	"description": "",
	"content": ". deprecationfilewarning \u0026gt;}} . include \u0026ldquo;federation-deprecation-warning-note.md\u0026rdquo; \u0026gt;}} . /deprecationfilewarning \u0026gt;}}\n本指南阐述了如何在联邦控制平面中使用 ReplicaSet。 在联邦控制平面中的 ReplicaSet (在本指南中称为”联邦 ReplicaSet”) 和传统的 Kubernetes ReplicaSet 很相似，提供了一样的功能。在联邦控制平面中创建联邦 ReplicaSet 可以确保在联邦的所有集群中都有预期数量的副本。\n. heading \u0026ldquo;prerequisites\u0026rdquo; %}}  . include \u0026ldquo;federated-task-tutorial-prereqs.md\u0026rdquo; \u0026gt;}}   你还应该具备基本的 Kubernetes 应用知识，特别是 ReplicaSets 相关的应用知识。  创建联邦 ReplicaSet 联邦 ReplicaSet 的 API 和传统的 Kubernetes ReplicaSet API 是 100% 兼容的。您可以通过请求联邦 apiserver 来创建联邦 ReplicaSet。\n您可以通过使用 kubectl 运行下面的指令来创建联邦 ReplicaSet：\nkubectl --context=federation-cluster create -f myrs.yaml --context=federation-cluster 参数告诉 kubectl 发送请求到联邦 apiserver 而不是某个 Kubernetes 集群。\n一旦联邦 ReplicaSet 被创建了，联邦控制平面就会在所有底层 Kubernetes 集群中创建一个 ReplicaSet。您可以通过检查底层每个集群来对其进行验证，例如：\nkubectl --context=gce-asia-east1a get rs myrs 上面的命令假定您在客户端中配置了一个叫做 ‘gce-asia-east1a’ 的上下文。\n底层集群中的 ReplicaSet 的副本数将会和联邦 ReplicaSet 的副本数保持一致。联邦控制平面将确保联邦的所有集群都和联邦 ReplicaSet 有同样的副本数。\n底层集群中副本的分布 默认情况下，副本在所有底层集群中是均匀分布的。例如：如果您有 3 个注册的集群并且用 spec.replicas = 9 参数创建了一个联邦 ReplicaSet，然后在这 3 个集群中每个 ReplicaSet 的副本数会是 spec.replicas=3。 如果要修改每个集群中的副本数，您可以在联邦 ReplicaSet 中使用 federation.kubernetes.io/replica-set-preferences 作为注解键值来修改联合副本集。 注解的键值是序列化的 JSON，其中包含以下示例中显示的字段：\n{ \u0026quot;rebalance\u0026quot;: true, \u0026quot;clusters\u0026quot;: { \u0026quot;foo\u0026quot;: { \u0026quot;minReplicas\u0026quot;: 10, \u0026quot;maxReplicas\u0026quot;: 50, \u0026quot;weight\u0026quot;: 100 }, \u0026quot;bar\u0026quot;: { \u0026quot;minReplicas\u0026quot;: 10, \u0026quot;maxReplicas\u0026quot;: 100, \u0026quot;weight\u0026quot;: 200 } } } rebalance 布尔字段指定是否可以移动已调度和正在运行的副本，以便将当前状态与指定的首选项相匹配。 clusters 对象字段包含一个映射，用户可以在其中指定跨集群的副本放置的约束（示例中为 foo 和 bar）。 对于每个集群，您可以指定应分配给它的最小副本数（默认值为零），集群可以接受的最大副本数（默认为无限制）以及表示要添加该群集的副本的首选项的相对权重的数字。\n更新联邦 ReplicaSet 您可以像更新 Kubernetes ReplicaSet 一样更新联邦 ReplicaSet。但是对于联邦 ReplicaSet，您必须发送请求到联邦 apiserver 而不是某个特定的 Kubernetes 集群。联邦控制平面会确保任何时候联邦 ReplicaSet 更新后，它会将对应的 ReplicaSet 更新到所有的底层集群中来和它保持一致。\n如果您做了包含副本数量的更改，联邦控制平面将会更改底层集群中的副本数以确保它们的总数和联邦 ReplicaSet 期望的副本数保持一致。\n删除联邦 ReplicaSet 您可以像删除 Kubernetes ReplicaSet 一样删除联邦 ReplicaSet。但是对于联邦 ReplicaSet ，您必须发送请求到联邦 apiserver 而不是某个特定的 Kubernetes 集群。\n例如，您可以使用 kubectl 运行下面的命令来删除联邦 ReplicaSet：\nkubectl --context=federation-cluster delete rs myrs . note \u0026gt;}}\n要注意的是这时删除联邦 ReplicaSet 并不会删除底层集群中对应的 ReplicaSet。您必须自己手动删除底层集群中的 ReplicaSet。我们打算在将来修复这个问题。 . /note \u0026gt;}}\n"
},
{
	"uri": "https://lijun.in/tasks/federation/administer-federation/secret/",
	"title": "联邦 Secret",
	"tags": [],
	"description": "",
	"content": ". deprecationfilewarning \u0026gt;}} . include \u0026ldquo;federation-deprecation-warning-note.md\u0026rdquo; \u0026gt;}} . /deprecationfilewarning \u0026gt;}}\n本指南解释了如何在联邦控制平面中使用 secret。\n联邦控制平面中的 Secret（在本指南中称为“联邦 secret”）与提供相同功能的传统 Kubernetes Secret 非常相似。 在联邦控制平面中创建它们可以确保它们跨联邦中的所有集群同步。\n先决条件 本指南假设你有一个正在运行的 Kubernetes 集群联邦安装。 如果没有，请访问联邦管理指南，了解如何启动联邦集群（或者让集群管理员为你做这件事）。 其他教程，例如这里 Kelsey Hightower，也可以帮助您。\n你还应该具有一个基本的 Kubernetes 工作知识， 特别是 Secret。\n创建联邦 Secret 用于联邦 Secret 的 API 与用于传统的 Kubernetes Secret 的 API 100% 兼容。 您可以通过向联邦 apiserver 发送请求来创建一个 Secret。\n你可以使用 kubectl 来运行：\nkubectl --context=federation-cluster create -f mysecret.yaml --context=federation-cluster 参数通知 kubectl 将请求提交给联邦 apiserver，而不是将其发送到 Kubernetes 集群。\n创建联邦命名空间后，联邦控制平面将在所有基础 Kubernetes 集群中创建匹配的命名空间。您可以通过检查每个基础集群来验证这一点，例如：\nkubectl --context=gce-asia-east1a get secret mysecret 以上假设您在客户端中为该区域中的集群配置了名为 “gce-asia-east1a” 的上下文。 集群底层中的这些 secret 将与联邦 secret 匹配。\n更新联邦 Secret 您可以像更新 Kubernetes secret 一样更新联邦 secret，但是，对于联邦 secret 必须将请求发送到联邦 apiserver， 而不是将其发送到指定的 Kubernetes 集群。联邦控制平面将确保每当更新联邦 secret 时，它都会更新所有基础集群中的相应 secret 以与其匹配。\n删除联邦 Secret 你可以删除一个联邦 secret，就像删除一个 Kubernetes secret 一样；但是， 对于联邦 secret，必须将请求发送到联邦 apiserver，而不是发送到指定的 Kubernetes 集群。\n例如，您可以通过运行以下命令使用 kubectl 执行此操作：\nkubectl --context=federation-cluster delete secret mysecret . note \u0026gt;}}\n此时，删除联邦 secret 不会从集群底层中删除相应的 secret。你必须手动删除底层 secret。我们打算将来解决这个问题。\n. /note \u0026gt;}}\n"
},
{
	"uri": "https://lijun.in/tasks/federation/administer-federation/events/",
	"title": "联邦事件",
	"tags": [],
	"description": "",
	"content": ". deprecationfilewarning \u0026gt;}} . include \u0026ldquo;federation-deprecation-warning-note.md\u0026rdquo; \u0026gt;}} . /deprecationfilewarning \u0026gt;}}\n本指南介绍如何在联邦控制平面中使用事件来帮助调试。\n先决条件 本指南假定您正在运行 Kubernetes 集群联邦安装。 如果没有，请转到联邦管理员指南，了解如何启动集群联邦（或让集群管理员为您执行此操作）。 其他教程，例如这个由 Kelsey Hightower，也可为您提供帮助。\n你还应该具备 kubernetes 基本工作知识。\n查看联邦事件 联邦控制平面中的事件（本指南中称为“联邦事件”）与提供相同功能的传统 Kubernetes 事件非常相似。 联邦事件仅存储在联邦控制平面中，不会传递给基础 Kubernetes 集群。\n联邦控制器在处理 API 资源时创建事件，以便向用户显示它们所处的状态。您可以通过运行以下命令从联邦 apiserver 获取所有事件：\nkubectl --context=federation-cluster get events 标准的 kubectl get，update，delete 命令都可以正常工作。\n"
},
{
	"uri": "https://lijun.in/tasks/federation/administer-federation/namespaces/",
	"title": "联邦命名空间",
	"tags": [],
	"description": "",
	"content": ". deprecationfilewarning \u0026gt;}} . include \u0026ldquo;federation-deprecation-warning-note.md\u0026rdquo; \u0026gt;}} . /deprecationfilewarning \u0026gt;}}\n本指南介绍如何在联邦控制平面中使用命名空间。\n联邦控制平面中的命名空间（本指南中称为“联邦命名空间”）与提供相同功能的传统 Kubernetes 命名空间非常相似。 在联邦控制平面中创建它们可确保它们在联邦中的所有集群之间同步\n. heading \u0026ldquo;prerequisites\u0026rdquo; %}}  . include \u0026ldquo;federated-task-tutorial-prereqs.md\u0026rdquo; \u0026gt;}} 您还需要具备基本的 Kubernetes 工作知识， 特别是命名空间。  创建联邦命名空间 联邦命名空间的 API 与传统 Kubernetes 命名空间的 API 100％ 兼容。您可以通过向联邦身份验证程序发送请求来创建命名空间。\n您可以通过运行以下命令使用 kubectl 执行此操作：\nkubectl --context=federation-cluster create -f myns.yaml --context=federation-cluster 参数通知 kubectl 将请求提交给联邦 apiserver，而不是将其发送到 Kubernetes 集群。\n创建联邦命名空间后，联邦控制平面将在所有基础 Kubernetes 集群中创建匹配的命名空间。您可以通过检查每个基础集群来验证这一点，例如：\nkubectl --context=gce-asia-east1a get namespaces myns 以上假设您在客户端中为该区域中的集群配置了名为 “gce-asia-east1a” 的上下文。 基础命名空间的名称和规范将与您在上面创建的联邦命名空间的名称和规范相匹配。\n更新联邦命名空间 您可以像更新 Kubernetes 命名空间一样更新联邦命名空间，只需将请求发送到联邦身份验证程序，而不是将其发送到指定的 Kubernetes 集群。 联邦控制平面将确保每当更新联邦命名空间时，它都会更新所有基础集群中的相应命名空间以与其匹配。\n删除联邦命名空间 你可以删除联邦命名空间，就像删除 Kubernetes 命名空间一样，只需将请求发送到联邦身份验证器，而不是发送到指定的 Kubernetes 群集。\n例如，您可以通过运行以下命令使用 kubectl 执行此操作：\nkubectl --context=federation-cluster delete ns myns 与在 Kubernetes 中一样，删除联邦命名空间将从联邦控制平面中删除该命名空间中的所有资源。\n. note \u0026gt;}}\n此时，删除联邦命名空间，不会从底层集群中删除相应的命名空间或这些命名空间中的资源。用户必须手动删除它们。我们打算将来解决这个问题。\n. /note \u0026gt;}}\n"
},
{
	"uri": "https://lijun.in/tasks/administer-cluster/dns-custom-nameservers/",
	"title": "自定义 DNS 服务",
	"tags": [],
	"description": "",
	"content": "本页说明如何配置 DNS Pod 和自定义 DNS 解析过程。 在 Kubernetes 1.11 和更高版本中，CoreDNS 位于 GA 并且默认情况下与 kubeadm 一起安装。 请参见CoreDNS 的 ConfigMap 选项 and 使用 CoreDNS 进行服务发现。\n. heading \u0026ldquo;prerequisites\u0026rdquo; %}}  . include \u0026ldquo;task-tutorial-prereqs.md\u0026rdquo; \u0026gt;}} . version-check \u0026gt;}} Kubernetes 版本 1.6 或更新。如果与 CoreDNS 匹配，版本 1.9 或更新。 合适的 add-on 插件: kube-dns 或 CoreDNS. 使用 kubeadm 安装，请参见 kubeadm 帮助文档.  介绍 DNS 是使用插件管理器[集群 add-on](http://releases.k8s.io/. param \u0026ldquo;githubbranch\u0026rdquo; \u0026gt;}}/cluster/addons/README.md)自动启动的内置的 Kubernetes 服务。\n从 Kubernetes v1.12 开始，CoreDNS 是推荐的 DNS 服务器，取代了kube-dns。 但是，默认情况下，某些 Kubernetes 安装程序工具仍可能安装 kube-dns。 请参阅安装程序提供的文档，以了解默认情况下安装了哪个 DNS 服务器。\nCoreDNS 的部署，作为一个 Kubernetes 服务，通过静态 IP 的方式暴露。 CoreDNS 和 kube-dns 服务在 metadata.name 字段中均被命名为 kube-dns。 这样做是为了与依靠传统 kube-dns 服务名称来解析集群内部地址的工作负载具有更大的互操作性。它抽象出哪个 DNS 提供程序在该公共端点后面运行的实现细节。 kubelet 使用 --cluster-dns = \u0026lt;dns-service-ip\u0026gt; 标志将 DNS 传递到每个容器。\nDNS 名称也需要域。 您可在 kubelet 中使用 --cluster-domain = \u0026lt;default-local-domain\u0026gt; 标志配置本地域。\nDNS 服务器支持正向查找（A 记录），端口发现（SRV 记录），反向 IP 地址发现（PTR 记录）等。 更多信息，请参见[Pod 和 服务的 DNS] (/docs/concepts/services-networking/dns-pod-service/)。\n如果 Pod 的 dnsPolicy 设置为 \u0026ldquo;default\u0026rdquo;，则它将从 Pod 运行所在节点上的配置中继承名称解析配置。 Pod 的 DNS 解析应该与节点相同。 但请参阅已知问题。\n如果您不想这样做，或者想要为 Pod 使用其他 DNS 配置，则可以 使用 kubelet 的 --resolv-conf 标志。 将此标志设置为 \u0026quot;\u0026rdquo; 以避免 Pod 继承 DNS。 将其设置为有效的文件路径以指定除以下以外的文件 /etc/resolv.conf，用于 DNS 继承。\nCoreDNS CoreDNS是通用的权威DNS服务器，可以用作集群DNS，符合[dns 规范] (https://github.com/kubernetes/dns/blob/master/docs/specification.md)。\nCoreDNS ConfigMap 选项 CoreDNS 是模块化且可插拔的 DNS 服务器，每个插件都为 CoreDNS 添加了新功能。 可以通过维护Corefile，即CoreDNS 配置文件。 集群管理员可以修改 CoreDNS Corefile 的 ConfigMap，以更改服务发现的工作方式。\n在 Kubernetes 中，已经使用以下默认 Corefile 配置安装了 CoreDNS。\napiVersion: v1 kind: ConfigMap metadata: name: coredns namespace: kube-system data: Corefile: | .:53 { errors health kubernetes cluster.local in-addr.arpa ip6.arpa { pods insecure upstream fallthrough in-addr.arpa ip6.arpa } prometheus :9153 proxy ./etc/resolv.conf cache 30 loop reload loadbalance } Corefile 配置包括以下 CoreDNS 的 插件：\n 错误：错误记录到 stdout。 健康：CoreDNS 的健康报告给 http://localhost:8080/health。 kubernetes：CoreDNS 将基于 Kubernetes 的服务和 Pod 的 IP 答复 DNS 查询。 您可以在 此处.   提供 pods insecure 选项是为了与 kube-dns 向前兼容。 您可以使用 pods verified 选项，该选项仅在相同名称空间中存在具有匹配 IP 的 pod 时才返回 A 记录。 如果您不使用 Pod 记录，则可以使用 pods disabled 选项。\n \u0026lsquo;Upstream\u0026rsquo; 用来解析指向外部主机的服务（外部服务）。\n prometheus：CoreDNS的度量标准以Prometheus格式在 http://localhost:9153/metrics 上提供。 proxy: 不在 Kubernetes 集群域内的任何查询都将转发到预定义的解析器 (/etc/resolv.conf). cache：这将启用前端缓存。 loop：检测到简单的转发循环，如果发现死循环，则中止 CoreDNS 进程。 reload：允许自动重新加载已更改的 Corefile。 编辑 ConfigMap 配置后，请等待两分钟，以使更改生效。 loadbalance：这是一个轮询 DNS 负载均衡器，它在应答中随机分配 A，AAAA 和 MX 记录的顺序。  您可以通过修改 ConfigMap 来修改默认的 CoreDNS 行为。\n使用 CoreDN 配置存根域和上游域名服务器 CoreDNS 能够使用 proxy plugin. 配置存根域和上游域名服务器。\n示例 如果集群操作员的 Consul 域服务器位于 10.150.0.1，并且所有 Consul 名称都带有后缀.consul.local。 要在 CoreDNS 中对其进行配置，集群管理员可以在 CoreDNS 的 ConfigMap 中创建加入以下字段。\nconsul.local:53 { errors cache 30 proxy . 10.150.0.1 } 要显式强制所有非集群 DNS 查找通过特定的域名服务器（位于172.16.0.1），请将 proxy 和 forward 指向域名服务器，而不是 /etc/resolv.conf。\nproxy . 172.16.0.1 最终的 ConfigMap 以及默认的 Corefile 配置如下所示：\napiVersion: v1 kind: ConfigMap metadata: name: coredns namespace: kube-system data: Corefile: | .:53 { errors health kubernetes cluster.local in-addr.arpa ip6.arpa { pods insecure upstream 172.16.0.1 fallthrough in-addr.arpa ip6.arpa } prometheus :9153 proxy . 172.16.0.1 cache 30 loop reload loadbalance } consul.local:53 { errors cache 30 proxy . 10.150.0.1 } 在 Kubernetes 1.10 和更高版本中，kubeadm 支持将 kube-dns ConfigMap 自动转换为 CoreDNS ConfigMap。 注意：尽管kube-dns接受 stubdomain 和 nameserver 的 FQDN（例如：ns.foo.com），但 CoreDNS 不支持此功能。 转换期间，CoreDNS 配置中将省略所有 FQDN 域名服务器。\nKube-dns 由于 CoreDNS 现在是默认设置，因此 Kube-dns 现在可以用作可选的 DNS 服务器。 正在运行的DNS Pod包含3个容器：\n \u0026ldquo;kubedns\u0026rdquo;：监测 Kubernetes 主节点的服务和 Endpoints 的更改，并维护内存中的查找结构以服务 DNS 请求。 \u0026ldquo;dnsmasq\u0026rdquo;：添加 DNS 缓存以提高性能。 \u0026ldquo;sidecar\u0026rdquo;：提供单个运行状况检查端点，对 dnsmasq 和 Kubedns 进行健康检查。  配置存根域和上游 DNS 服务器 集群管理员可以指定自定义存根域和上游域名服务器通过为 kube-dns (kube-system:kube-dns) 提供 ConfigMap。\n例如，以下 ConfigMap 使用单个存根域和两个上游域名服务器设置 DNS 配置：\napiVersion: v1 kind: ConfigMap metadata: name: kube-dns namespace: kube-system data: stubDomains: | {\u0026#34;acme.local\u0026#34;: [\u0026#34;1.2.3.4\u0026#34;]} upstreamNameservers: | [\u0026#34;8.8.8.8\u0026#34;, \u0026#34;8.8.4.4\u0026#34;] 带 “.acme.local” 后缀的 DNS 请求 被转发到侦听 1.2.3.4 的 DNS。 通过 Google 公共 DNS 进行向上查询。\n下表描述了具有特定域名的查询如何映射到 其目标DNS服务器：\n   域名 服务器回答查询     kubernetes.default.svc.cluster.local kube-dns   foo.acme.local 自定义 DNS（1.2.3.4）   widget.com 上游 DNS（8.8.8.8、8.8.4.4中的一个）    请参见 ConfigMap options 有关配置选项格式的详细信息。\n对 Pod 的影响 自定义上游域名服务器和存根域不影响 dnsPolicy 设置为 \u0026ldquo;Default\u0026rdquo; 或 \u0026ldquo;None\u0026rdquo; 的 Pod。\n如果 Pod 的 dnsPolicy 设置为 \u0026ldquo;ClusterFirst\u0026rdquo;，则根据是否配置了存根域和上游 DNS 服务器来不同地处理其名称解析。\n不使用自定义配置：任何与配置不匹配的查询 集群域后缀（例如 \u0026ldquo;www.kubernetes.io\u0026rdquo;）将转发到从节点继承的上游域名服务器。\n使用自定义配置：如果存根域和上游 DNS 服务器 配置完成后，DNS 查询将按照以下流程进行路由：：\n1.首先将查询发送到 kube-dns 中的 DNS 缓存层。\n1.在以下情况下，从缓存层检查请求的后缀，然后将其转发到适当的 DNS：：\n * 带集群后缀的名称，例如 \u0026ldquo;.cluster.local\u0026rdquo;： 该请求被发送到 kube-dns。\n * 带存根域名后缀的名称，例如 \u0026ldquo;.acme.local\u0026rdquo;： 该请求将发送到已配置的自定义 DNS 解析器，例如在 1.2.3.4 处进行侦听。\n * 名称没有匹配的后缀，例如 \u0026ldquo;widget.com\u0026rdquo;： 该请求被转发到上游 DNS， 例如位于 8.8.8.8 和 8.8.4.4 的 Google 公共 DNS 服务器。\nConfigMap 选项 kube-dns kube-system:kube-dns 的 ConfigMap 选项：\n   领域 格式 描述     stubDomains（可选） 使用 DNS 后缀键（例如“ acme.local”）和由 DNS IP 的 JSON 数组组成的值的 JSON 映射。 目标域名服务器本身可以是 Kubernetes 服务。 例如，您可以运行自己的 dnsmasq 副本，以将自定义 DNS 名称导出到 ClusterDNS 命名空间中。   upstreamNameservers（可选） DNS IP的 JSON 数组。 如果指定，则这些值替换默认情况下从节点的 /etc/resolv.conf 中获取的域名服务器。 限制：最多可以指定三个上游域名服务器。    例子 示例：存根域 在此示例中，用户具有他们想与 kube-dns 集成的 Consul DNS 服务发现系统。 consul 域服务器位于 10.150.0.1，所有领事名称均带有后缀 .consul.local。 要配置 Kubernetes，集群管理员将创建以下 ConfigMap：\napiVersion: v1 kind: ConfigMap metadata: name: kube-dns namespace: kube-system data: stubDomains: | {\u0026#34;consul.local\u0026#34;: [\u0026#34;10.150.0.1\u0026#34;]} ​``` \u0026lt;!-- Note that the cluster administrator does not want to override the node’s upstream nameservers, so they did not specify the optional `upstreamNameservers` field. ##### Example: Upstream nameserver In this example the cluster administrator wants to explicitly force all non-cluster DNS lookups to go through their own nameserver at 172.16.0.1. In this case, they create a ConfigMap with the `upstreamNameservers` field specifying the desired nameserver: --\u0026gt; 需要注意的是集群管理员不希望覆盖节点的上游域名服务器，所以他们没有指定可选的 `upstreamNameservers` 字段。 ##### 示例： 上游域名服务器 在此示例中，集群管理员希望显式强制所有非集群 DNS 查找通过其自己的域名服务器172.16.0.1）。 在这种情况下，他们使用指定所需域名服务器的 `upstreamNameservers` 字段创建一个 ConfigMap： ```yaml apiVersion: v1 kind: ConfigMap metadata: name: kube-dns namespace: kube-system data: upstreamNameservers: | [\u0026#34;172.16.0.1\u0026#34;] CoreDNS 配置等同于 kube-dns CoreDNS 不仅仅提供 kube-dns 的功能。 为 kube-dns 创建的 ConfigMap 支持 StubDomains 和 upstreamNameservers 转换为 CoreDNS 中的 proxy 插件。 同样，kube-dns 中的 Federations 插件会转换为 CoreDNS 中的 federation 插件。\n示例 用于 kubedns 的此示例 ConfigMap 描述了 federations, stubdomains and upstreamnameservers：\napiVersion: v1 data: federations: | {\u0026#34;foo\u0026#34; : \u0026#34;foo.feddomain.com\u0026#34;} stubDomains: | {\u0026#34;abc.com\u0026#34; : [\u0026#34;1.2.3.4\u0026#34;], \u0026#34;my.cluster.local\u0026#34; : [\u0026#34;2.3.4.5\u0026#34;]} upstreamNameservers: | [\u0026#34;8.8.8.8\u0026#34;, \u0026#34;8.8.4.4\u0026#34;] kind: ConfigMap CoreDNS 中的等效配置将创建一个 Corefile：\n For federations:  federation cluster.local { foo foo.feddomain.com }  For stubDomains:  abc.com:53 { errors cache 30 proxy . 1.2.3.4 } my.cluster.local:53 { errors cache 30 proxy . 2.3.4.5 } 带有默认插件的完整 Corefile：\n.:53 { errors health kubernetes cluster.local in-addr.arpa ip6.arpa { upstream 8.8.8.8 8.8.4.4 pods insecure fallthrough in-addr.arpa ip6.arpa } federation cluster.local { foo foo.feddomain.com } prometheus :9153 proxy . 8.8.8.8 8.8.4.4 cache 30 } abc.com:53 { errors cache 30 proxy . 1.2.3.4 } my.cluster.local:53 { errors cache 30 proxy . 2.3.4.5 } 迁移到 CoreDNS 要将 kube-dns 迁移到 CoreDNS，可使用 详细博客 来帮助用户在迁移自 kube-dns。 集群管理员还可以使用部署脚本 进行迁移。\n＃＃ 下一步是什么\n 调试 DNS 解析。  "
},
{
	"uri": "https://lijun.in/tasks/debug-application-cluster/monitor-node-health/",
	"title": "节点健康监测",
	"tags": [],
	"description": "",
	"content": "节点问题探测器 是一个 DaemonSet 用来监控节点健康。它从各种守护进程收集节点问题，并以NodeCondition 和 [Event](/docs/reference/generated/kubernetes-api/. param \u0026ldquo;version\u0026rdquo; \u0026gt;}}/#event-v1-core) 的形式报告给 apiserver 。\n它现在支持一些已知的内核问题检测，并将随着时间的推移，检测更多节点问题。\n目前，Kubernetes 不会对节点问题检测器监测到的节点状态和事件采取任何操作。将来可能会引入一个补救系统来处理这些节点问题。\n更多信息请参阅 这里。\n. heading \u0026ldquo;prerequisites\u0026rdquo; %}} . include \u0026ldquo;task-tutorial-prereqs.md\u0026rdquo; \u0026gt;}} . version-check \u0026gt;}}\n局限性  节点问题检测器的内核问题检测现在只支持基于文件类型的内核日志。 它不支持像 journald 这样的命令行日志工具。   节点问题检测器的内核问题检测对内核日志格式有一定要求，现在它只适用于 Ubuntu 和 Debian。但是，将其扩展为 支持其它日志格式 也很容易。  在 GCE 集群中启用/禁用 节点问题检测器在 gce 集群中以集群插件的形式默认启用。\n您可以在运行 kube-up.sh 之前，以设置环境变量 KUBE_ENABLE_NODE_PROBLEM_DETECTOR 的形式启用/禁用它。\n在其它环境中使用 要在 GCE 之外的其他环境中启用节点问题检测器，您可以使用 kubectl 或插件 pod。\nKubectl 这是在 GCE 之外启动节点问题检测器的推荐方法。它的管理更加灵活，例如覆盖默认配置以使其适合您的环境或检测自定义节点问题。\n 步骤 1: node-problem-detector.yaml:  . codenew file=\u0026quot;debug/node-problem-detector.yaml\u0026rdquo; \u0026gt;}}\n请注意保证您的系统日志路径与您的 OS 发行版相对应。\n 步骤 2: 执行 kubectl 来启动节点问题检测器：  kubectl create -f https://k8s.io/examples/debug/node-problem-detector.yaml 插件 Pod 这适用于拥有自己的集群引导程序解决方案的用户，并且不需要覆盖默认配置。 他们可以利用插件 Pod 进一步自动化部署。\n只需创建 node-problem-detector.yaml，并将其放在主节点上的插件 pod 目录 /etc/kubernetes/addons/node-problem-detector 下。\n覆盖配置文件 构建节点问题检测器的 docker 镜像时，会嵌入默认配置。\n不过，您可以像下面这样使用 ConfigMap 将其覆盖：\n 步骤 1: 在 config/ 中更改配置文件。 步骤 2: 使用 kubectl create configmap node-problem-detector-config --from-file=config/ 创建 node-problem-detector-config 。 步骤 3: 更改 node-problem-detector.yaml 以使用 ConfigMap:  . codenew file=\u0026quot;debug/node-problem-detector-configmap.yaml\u0026rdquo; \u0026gt;}}\n 步骤 4: 使用新的 yaml 文件重新创建节点问题检测器：  kubectl delete -f https://k8s.io/examples/debug/node-problem-detector.yaml # If you have a node-problem-detector running kubectl create -f https://k8s.io/examples/debug/node-problem-detector-configmap.yaml 请注意，此方法仅适用于通过 kubectl 启动的节点问题检测器。\n由于插件管理器不支持ConfigMap，因此现在不支持对于作为集群插件运行的节点问题检测器的配置进行覆盖。\n内核监视器 内核监视器 是节点问题检测器中的问题守护进程。它监视内核日志并按照预定义规则检测已知内核问题。\n内核监视器根据 config/kernel-monitor.json 中的一组预定义规则列表匹配内核问题。 规则列表是可扩展的，您始终可以通过覆盖配置来扩展它。\n添加新的 NodeCondition 您可以使用新的状态描述来扩展 config/kernel-monitor.json 中的 conditions 字段以支持新的节点状态。\n{ \u0026#34;type\u0026#34;: \u0026#34;NodeConditionType\u0026#34;, \u0026#34;reason\u0026#34;: \u0026#34;CamelCaseDefaultNodeConditionReason\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;arbitrary default node condition message\u0026#34; } 检测新的问题 您可以使用新的规则描述来扩展 config/kernel-monitor.json 中的 rules 字段以检测新问题。\n{ \u0026#34;type\u0026#34;: \u0026#34;temporary/permanent\u0026#34;, \u0026#34;condition\u0026#34;: \u0026#34;NodeConditionOfPermanentIssue\u0026#34;, \u0026#34;reason\u0026#34;: \u0026#34;CamelCaseShortReason\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;regexp matching the issue in the kernel log\u0026#34; } 更改日志路径 不同操作系统发行版的内核日志的可能不同。 config/kernel-monitor.json 中的 log 字段是容器内的日志路径。您始终可以修改配置使其与您的 OS 发行版匹配。\n支持其它日志格式 内核监视器使用 [Translator] 插件将内核日志转换为内部数据结构。我们可以很容易为新的日志格式实现新的翻译器。\n注意事项 我们建议在集群中运行节点问题检测器来监视节点运行状况。但是，您应该知道这将在每个节点上引入额外的资源开销。一般情况下没有影响，因为：\n 内核日志生成相对较慢。 节点问题检测器有资源限制。 即使在高负载下，资源使用也是可以接受的。 (参阅 基准测试结果)  "
},
{
	"uri": "https://lijun.in/tasks/debug-application-cluster/get-shell-running-container/",
	"title": "获取正在运行容器的 Shell",
	"tags": [],
	"description": "",
	"content": "本文介绍怎样使用 kubectl exec 命令获取正在运行容器的 Shell。\n.% heading \u0026ldquo;prerequisites\u0026rdquo; %}} . include \u0026ldquo;task-tutorial-prereqs.md\u0026rdquo; \u0026gt;}} . version-check \u0026gt;}}\n获取容器的 Shell 在本练习中，你将创建包含一个容器的 Pod。容器运行 nginx 镜像。下面是 Pod 的配置文件：\n. codenew file=\u0026quot;application/shell-demo.yaml\u0026rdquo; \u0026gt;}}\n创建 Pod：\nkubectl create -f https://k8s.io/examples/application/shell-demo.yaml 检查容器是否运行正常：\nkubectl get pod shell-demo 获取正在运行容器的 Shell：\nkubectl exec -it shell-demo -- /bin/bash . note \u0026gt;}}\n双破折号 \u0026ldquo;\u0026ndash;\u0026rdquo; 用于将要传递给命令的参数与 kubectl 的参数分开。 note \u0026gt;}}\n在 shell 中，打印根目录：\nroot@shell-demo:/# ls / 在 shell 中，实验其他命令。下面是一些示例：\nroot@shell-demo:/# ls / root@shell-demo:/# cat /proc/mounts root@shell-demo:/# cat /proc/1/maps root@shell-demo:/# apt-get update root@shell-demo:/# apt-get install -y tcpdump root@shell-demo:/# tcpdump root@shell-demo:/# apt-get install -y lsof root@shell-demo:/# lsof root@shell-demo:/# apt-get install -y procps root@shell-demo:/# ps aux root@shell-demo:/# ps aux | grep nginx 编写 nginx 的 根页面 在看一下 Pod 的配置文件。该 Pod 有个 emptyDir 卷，容器将该卷挂载到了 /usr/share/nginx/html。\n在 shell 中，在 /usr/share/nginx/html 目录创建一个 `index.html 文件：\nroot@shell-demo:/# echo Hello shell demo \u0026gt; /usr/share/nginx/html/index.html 在 shell 中，向 nginx 服务器发送 GET 请求：\nroot@shell-demo:/# apt-get update root@shell-demo:/# apt-get install curl root@shell-demo:/# curl localhost 输出结果显示了你在 index.html 中写入的文本。\nHello shell demo 当用完 shell 后，输入 exit 退出。\n在容器中运行单个命令 在普通的命令窗口（而不是 shell）中，打印环境运行容器中的变量：\nkubectl exec shell-demo env 实验运行其他命令。下面是一些示例：\nkubectl exec shell-demo ps aux kubectl exec shell-demo ls / kubectl exec shell-demo cat /proc/1/mounts 当 Pod 包含多个容器时打开 shell 如果 Pod 有多个容器，--container 或者 -c 可以在 kubectl exec 命令中指定容器。 例如，您有个名为 my-pod 的容器，该 Pod 有两个容器分别为 main-app 和 healper-app。 下面的命令将会打开一个 shell 访问 main-app 容器。\nkubectl exec -it my-pod --container main-app -- /bin/bash .% heading \u0026ldquo;whatsnext\u0026rdquo; %}}  kubectl exec  "
},
{
	"uri": "https://lijun.in/tasks/administer-cluster/access-cluster-services/",
	"title": "访问集群上运行的服务",
	"tags": [],
	"description": "",
	"content": "本文展示了如何连接 Kubernetes 集群上运行的服务。\n. heading \u0026ldquo;prerequisites\u0026rdquo; %}} . include \u0026ldquo;task-tutorial-prereqs.md\u0026rdquo; \u0026gt;}} . version-check \u0026gt;}}\n访问集群上运行的服务 在 Kubernetes 里， nodes、pods 和 services 都有它们自己的 IP。许多情况下，集群上的 node IP、pod IP 和某些 service IP 路由不可达，所以不能从一个集群之外的节点访问它们，例如从你自己的台式机。\n连接方式 你有多种从集群外连接 nodes、pods 和 services 的选项：\n 通过公共 IP 访问 services。  使用具有 NodePort 或 LoadBalancer 类型的 service，可以从外部访问它们。请查阅 services 和 kubectl expose 文档。 取决于你的集群环境，你可以仅把 service 暴露在你的企业网络环境中，也可以将其暴露在因特网上。需要考虑暴露的 service 是否安全，它是否有自己的用户认证？ 将 pods 放置于 services 背后。如果要访问一个副本集合中特定的 pod，例如用于调试目的时，请给 pod 指定一个独特的标签并创建一个新 service 选择这个标签。 大部分情况下，都不需要应用开发者通过节点 IP 直接访问 nodes。   通过 Proxy Verb 访问 services、nodes 或者 pods。  在访问 Apiserver 远程服务之前是否经过认证和授权？如果你的服务暴露到因特网中不够安全，或者需要获取 node IP 之上的端口，又或者处于调试目的时，请使用这个特性。 Proxies 可能给某些应用带来麻烦。 仅适用于 HTTP/HTTPS。 在这里描述   从集群中的 node 或者 pod 访问。  运行一个 pod，然后使用 kubectl exec 连接到它的一个shell。从那个 shell 连接其他的 nodes、pods 和 services。 某些集群可能允许你 ssh 到集群中的节点。你可能可以从那儿访问集群服务。这是一个非标准的方式，可能在一些集群上能工作，但在另一些上却不能。浏览器和其他工具可能安装或可能不会安装。集群 DNS 可能不会正常工作。    发现内置服务 典型情况下，kube-system 会启动集群中的几个服务。使用 kubectl cluster-info 命令获取它们的列表：\n$ kubectl cluster-info Kubernetes master is running at https://104.197.5.247 elasticsearch-logging is running at https://104.197.5.247/api/v1/namespaces/kube-system/services/elasticsearch-logging/proxy kibana-logging is running at https://104.197.5.247/api/v1/namespaces/kube-system/services/kibana-logging/proxy kube-dns is running at https://104.197.5.247/api/v1/namespaces/kube-system/services/kube-dns/proxy grafana is running at https://104.197.5.247/api/v1/namespaces/kube-system/services/monitoring-grafana/proxy heapster is running at https://104.197.5.247/api/v1/namespaces/kube-system/services/monitoring-heapster/proxy 这显示了用于访问每个服务的 proxy-verb URL。例如，这个集群启用了（使用 Elasticsearch）集群层面的日志，如果提供合适的凭据可以通过 https://104.197.5.247/api/v1/namespaces/kube-system/services/elasticsearch-logging/proxy/ 访问，或通过一个 kubectl 代理地址访问，如：http://localhost:8080/api/v1/namespaces/kube-system/services/elasticsearch-logging/proxy/。（请查看 上文 关于如何传递凭据或者使用 kubectl 代理的说明。）\n手动构建 apiserver 代理 URLs 如同上面所提到的，你可以使用 kubectl cluster-info 命令取得 service 的代理 URL。为了创建包含 service endpoints、suffixes 和 parameters 的代理 URLs，你可以简单的在 service 的代理 URL中 添加： http://kubernetes_master_address/api/v1/namespaces/namespace_name/services/service_name[:port_name]/proxy\n如果还没有为你的端口指定名称，你可以不用在 URL 中指定 port_name。\n示例  你可以通过 http://104.197.5.247/api/v1/namespaces/kube-system/services/elasticsearch-logging/proxy/_search?q=user:kimchy 访问 Elasticsearch service endpoint _search?q=user:kimchy。 你可以通过 https://104.197.5.247/api/v1/namespaces/kube-system/services/elasticsearch-logging/proxy/_cluster/health?pretty=true 访问 Elasticsearch 集群健康信息 endpoint _cluster/health?pretty=true。  { \u0026#34;cluster_name\u0026#34; : \u0026#34;kubernetes_logging\u0026#34;, \u0026#34;status\u0026#34; : \u0026#34;yellow\u0026#34;, \u0026#34;timed_out\u0026#34; : false, \u0026#34;number_of_nodes\u0026#34; : 1, \u0026#34;number_of_data_nodes\u0026#34; : 1, \u0026#34;active_primary_shards\u0026#34; : 5, \u0026#34;active_shards\u0026#34; : 5, \u0026#34;relocating_shards\u0026#34; : 0, \u0026#34;initializing_shards\u0026#34; : 0, \u0026#34;unassigned_shards\u0026#34; : 5 } 通过 web 浏览器访问集群中运行的服务 你或许能够将 apiserver 代理的 url 放入浏览器的地址栏，然而：\n Web 服务器不总是能够传递令牌，所以你可能需要使用基本（密码）认证。 Apiserver 可以配置为接受基本认证，但你的集群可能并没有这样配置。 某些 web 应用可能不能工作，特别是那些使用客户端侧 javascript 的应用，它们构造 url 的方式可能不能理解代理路径前缀。  "
},
{
	"uri": "https://lijun.in/tasks/tls/certificate-rotation/",
	"title": "证书轮换",
	"tags": [],
	"description": "",
	"content": "本文展示如何在 kubelet 中启用并配置证书轮换。\n. heading \u0026ldquo;prerequisites\u0026rdquo; %}}   要求 Kubernetes 1.8.0 或更高的版本\n  Kubelet 证书轮换在 1.8.0 版本中处于 beta 阶段, 这意味着该特性可能在没有通知的情况下发生变化。\n  概述 Kubelet 使用证书进行 Kubernetes API 的认证。 默认情况下，这些证书的签发期限为一年，所以不需要太频繁地进行更新。\nKubernetes 1.8 版本中包含 beta 特性 kubelet 证书轮换， 在当前证书即将过期时， 将自动生成新的秘钥，并从 Kubernetes API 申请新的证书。 一旦新的证书可用，它将被用于与 Kubernetes API 间的连接认证。\n启用客户端证书轮换 kubelet 进程接收 --rotate-certificates 参数，该参数决定 kubelet 在当前使用的证书即将到期时， 是否会自动申请新的证书。 由于证书轮换是 beta 特性，必须通过参数 --feature-gates=RotateKubeletClientCertificate=true 进行启用。\nkube-controller-manager 进程接收 --experimental-cluster-signing-duration 参数，该参数控制证书签发的有效期限。\n理解证书轮换配置 当 kubelet 启动时，如被配置为自举（使用--bootstrap-kubeconfig 参数），kubelet 会使用其初始证书连接到 Kubernetes API ，并发送证书签名的请求。 可以通过以下方式查看证书签名请求的状态：\nkubectl get csr 最初，来自节点上 kubelet 的证书签名请求处于 Pending 状态。 如果证书签名请求满足特定条件， 控制器管理器会自动批准，此时请求会处于 Approved 状态。 接下来，控制器管理器会签署证书， 证书的有效期限由 --experimental-cluster-signing-duration 参数指定，签署的证书会被附加到证书签名请求中。\nKubelet 会从 Kubernetes API 取回签署的证书，并将其写入磁盘，存储位置通过 --cert-dir 参数指定。 然后 kubelet 会使用新的证书连接到 Kubernetes API。\n当签署的证书即将到期时，kubelet 会使用 Kubernetes API，发起新的证书签名请求。 同样地，控制器管理器会自动批准证书请求，并将签署的证书附加到证书签名请求中。 Kubelet 会从 Kubernetes API 取回签署的证书，并将其写入磁盘。 然后它会更新与 Kubernetes API 的连接，使用新的证书重新连接到 Kubernetes API。\n"
},
{
	"uri": "https://lijun.in/tasks/debug-application-cluster/debug-init-containers/",
	"title": "调试 Init 容器",
	"tags": [],
	"description": "",
	"content": "此页显示如何核查与 init 容器执行相关的问题。 下面的示例命令行将 Pod 称为 \u0026lt;pod-name\u0026gt;，而 init 容器称为 \u0026lt;init-container-1\u0026gt; 和 \u0026lt;init-container-2\u0026gt;。\n. heading \u0026ldquo;prerequisites\u0026rdquo; %}} . include \u0026ldquo;task-tutorial-prereqs.md\u0026rdquo; \u0026gt;}} . version-check \u0026gt;}}\n 您应该熟悉 Init 容器的基础知识。 您应该已经配置好一个 Init 容器。  检查 Init 容器的状态 显示你的 Pod 的状态：\nkubectl get pod \u0026lt;pod-name\u0026gt; 例如，状态 Init:1/2 表明两个 Init 容器中的一个已经成功完成：\nNAME READY STATUS RESTARTS AGE \u0026lt;pod-name\u0026gt; 0/1 Init:1/2 0 7s 更多状态值及其含义请参考了解 Pod 的状态。\n获取 Init 容器详情 查看 Init 容器运行的更多详情：\nkubectl describe pod \u0026lt;pod-name\u0026gt; 例如，对于包含两个 Init 容器的 Pod 应该显示如下信息：\nInit Containers: \u0026lt;init-container-1\u0026gt;: Container ID: ... ... State: Terminated Reason: Completed Exit Code: 0 Started: ... Finished: ... Ready: True Restart Count: 0 ... \u0026lt;init-container-2\u0026gt;: Container ID: ... ... State: Waiting Reason: CrashLoopBackOff Last State: Terminated Reason: Error Exit Code: 1 Started: ... Finished: ... Ready: False Restart Count: 3 ... 您还可以通过读取 Pod Spec 上的 status.initContainerStatuses 字段以编程方式了解 Init 容器的状态：\nkubectl get pod nginx --template \u0026#39;{{.status.initContainerStatuses}}\u0026#39; 此命令将返回与原始 JSON 中相同的信息.\n通过 Init 容器访问日志 一起传递 Init 容器名称与 Pod 名称来访问它的日志。\nkubectl logs \u0026lt;pod-name\u0026gt; -c \u0026lt;init-container-2\u0026gt; 运行 shell 脚本打印命令的init容器,执行 shell 脚本。 例如，您可以在 Bash 中通过在脚本的开头运行 set -x 来实现。\n了解 Pod 的状态 以 Init: 开头的 Pod 状态汇总了 Init 容器执行的状态。 下表介绍调试 Init 容器时可能看到的一些状态值示例。\n   状态 含义     Init:N/M Pod 包含 M 个 Init 容器，其中 N 个已经运行完成。   Init:Error Init 容器已执行失败。   Init:CrashLoopBackOff Init 容器反复执行失败。   Pending Pod 还没有开始执行 Init 容器。   PodInitializing or Running Pod 已经完成执行 Init 容器。    "
},
{
	"uri": "https://lijun.in/tasks/debug-application-cluster/debug-pod-replication-controller/",
	"title": "调试 Pods 和 Replication Controllers",
	"tags": [],
	"description": "",
	"content": "此页面告诉您如何调试 Pod 和 ReplicationController。\n. heading \u0026ldquo;prerequisites\u0026rdquo; %}} . include \u0026ldquo;task-tutorial-prereqs.md\u0026rdquo; \u0026gt;}} . version-check \u0026gt;}}\n 您应该先熟悉 Pods 和 Pod Lifecycle 的基础概念。  调试 Pod 调试一个 pod 的第一步是观察它。使用下面的命令检查这个 pod 的当前状态和最近事件：\nkubectl describe pods ${POD_NAME} 看看 pod 中的容器的状态。他们都是 Running 吗？有最近重启了吗？\n根据 pod 的状态继续调试。\n我的 Pod 卡在 Pending 如果一个 pod 被卡在 Pending 状态，就意味着它不能调度在某个节点上。一般来说，这是因为某种类型的资源不足而 阻止调度。 看看上面的命令 kubectl describe ... 的输出。调度器的消息中应该会包含无法调度 Pod 的原因。 原因包括：\n资源不足 您可能已经耗尽了集群中供应的 CPU 或内存。在这个情况下你可以尝试几件事情：\n  添加更多节点 到集群。\n  终止不需要的 pod 为 pending 中的 pod 提供空间。\n  检查该 pod 是否不大于您的节点。例如，如果全部节点具有 cpu:1 容量，那么具有 cpu: 1.1 请求的 pod 永远不会被调度。\n您可以使用 kubectl get nodes -o \u0026lt;format\u0026gt; 命令来检查节点容量。 下面是一些能够提取必要信息的命令示例：\nkubectl get nodes -o yaml | egrep \u0026#39;\\sname:|cpu:|memory:\u0026#39; kubectl get nodes -o json | jq \u0026#39;.items[] | {name: .metadata.name, cap: .status.capacity}\u0026#39;   可以考虑配置 资源配额 来限制可耗用的资源总量。如果与命名空间一起使用，它可以防止一个团队吞噬所有的资源。\n使用hostPort 当你将一个 pod 绑定到一个 hostPort 时，这个 pod 能被调度的位置数量有限。 在大多数情况下，hostPort 是不必要的; 尝试使用服务对象来暴露您的 pod。 如果你需要 hostPort，那么你可以调度的 Pod 数量不能超过集群的节点个数。\n我的 Pod 一直在 Waiting 如果一个 pod 被卡在 Waiting 状态，那么它已被调度在某个工作节点，但它不能在该机器上运行。 再次，来自 kubectl describe ... 的内容应该是可以提供信息的。 最常见的原因 Waiting 的 pod 是无法拉取镜像。有三件事要检查：\n 确保您的镜像的名称正确。 您是否将镜像推送到存储库？ 在您的机器上手动运行 docker pull \u0026lt;image\u0026gt;，看看是否可以拉取镜像。  我的 Pod 一直 Crashing 或者有别的不健康状态 首先，查看当前容器的日志：\n$ kubectl logs ${POD_NAME} ${CONTAINER_NAME}  如果您的容器先前已崩溃，则可以访问上一个容器的崩溃日志：\n$ kubectl logs --previous ${POD_NAME} ${CONTAINER_NAME}  或者，您可以使用 exec 在该容器内运行命令：\n$ kubectl exec ${POD_NAME} -c ${CONTAINER_NAME} -- ${CMD} ${ARG1} ${ARG2} ... ${ARGN}  . note \u0026gt;}} -c ${CONTAINER_NAME} 是可选的，对于只包含一个容器的 pod 可以省略。 . /note \u0026gt;}}\n例如，要查看正在运行的Cassandra pod的日志，可以运行：\nkubectl exec cassandra -- cat /var/log/cassandra/system.log 如果这些方法都不起作用，您可以找到该运行 pod 所在的主机并 SSH 到该主机。\n调试 Replication Controller Replication Controller 相当简单。他们只会能或不能创建 pod。如果他们无法创建 pod，那么请参考 上面的说明 来调试你的pod。\n您也可以使用kubectl describe rc ${CONTROLLER_NAME}来检查和Replication Controllers有关的事件。\n"
},
{
	"uri": "https://lijun.in/tasks/debug-application-cluster/debug-service/",
	"title": "调试 Service",
	"tags": [],
	"description": "",
	"content": "对于新安装的 Kubernetes，经常出现的一个问题是 Service 没有正常工作。如果您已经运行了 Deployment 并创建了一个 Service，但是当您尝试访问它时没有得到响应，希望这份文档能帮助您找出问题所在。\n约定 在整个文档中，您将看到各种可以运行的命令。有些命令需要在 Pod 中运行，有些命令需要在 Kubernetes Node 上运行，还有一些命令可以在您拥有 kubectl 和集群凭证的任何地方运行。为了明确预期的效果，本文档将使用以下约定。\n如果命令 \u0026ldquo;COMMAND\u0026rdquo; 期望在 Pod 中运行，并且产生 \u0026ldquo;OUTPUT\u0026rdquo;：\nu@pod$ COMMAND OUTPUT 如果命令 \u0026ldquo;COMMAND\u0026rdquo; 期望在 Node 上运行，并且产生 \u0026ldquo;OUTPUT\u0026rdquo;：\nu@node$ COMMAND OUTPUT 如果命令是 \u0026ldquo;kubectl ARGS\u0026rdquo;：\n$ kubectl ARGS OUTPUT 在 pod 中运行命令 对于这里的许多步骤，您可能希望知道运行在集群中的 Pod 看起来是什么样的。最简单的方法是运行一个交互式的 busybox Pod：\n$ kubectl run -it --rm --restart=Never busybox --image=busybox sh 如果你没有看到命令提示符，请尝试按 Enter 键。 / # 如果您已经有了您喜欢使用的正在运行的 Pod，则可以运行一下命令去使用：\n$ kubectl exec \u0026lt;POD-NAME\u0026gt; -c \u0026lt;CONTAINER-NAME\u0026gt; -- \u0026lt;COMMAND\u0026gt; 设置 为了完成本次演练的目的，我们先运行几个 Pod。因为可能正在调试您自己的 Service，所以，您可以使用自己的详细信息进行替换，或者，您也可以跟随并开始下面的步骤来获得第二个数据点。\n$ kubectl run hostnames --image=k8s.gcr.io/serve_hostname \\  --labels=app=hostnames \\  --port=9376 \\  --replicas=3 deployment.apps/hostnames created kubectl 命令将打印创建或变更的资源的类型和名称，它们可以在后续命令中使用。 . note \u0026gt;}} 这与您使用以下 YAML 启动 Deployment 相同：\napiVersion: apps/v1 kind: Deployment metadata: name: hostnames spec: selector: matchLabels: app: hostnames replicas: 3 template: metadata: labels: app: hostnames spec: containers: - name: hostnames image: k8s.gcr.io/serve_hostname ports: - containerPort: 9376 protocol: TCP . /note \u0026gt;}}\n确认您的 Pods 是运行状态:\n$ kubectl get pods -l app=hostnames NAME READY STATUS RESTARTS AGE hostnames-632524106-bbpiw 1/1 Running 0 2m hostnames-632524106-ly40y 1/1 Running 0 2m hostnames-632524106-tlaok 1/1 Running 0 2m Service 存在吗？ 细心的读者会注意到我们还没有真正创建一个 Service - 其实这是我们有意的。这是一个有时会被遗忘的步骤，也是第一件要检查的事情。\n那么，如果我试图访问一个不存在的 Service，会发生什么呢？假设您有另一个 Pod，想通过名称使用这个 Service，您将得到如下内容：\nu@pod$ wget -O- hostnames Resolving hostnames (hostnames)... failed: Name or service not known. wget: unable to resolve host address \u0026#39;hostnames\u0026#39; 因此，首先要检查的是 Service 是否确实存在：\n$ kubectl get svc hostnames No resources found. Error from server (NotFound): services \u0026#34;hostnames\u0026#34; not found 我们已经有一个罪魁祸首了，让我们来创建 Service。就像前面一样，这里的内容仅仅是为了步骤的执行 - 在这里您可以使用自己的 Service 细节。\n$ kubectl expose deployment hostnames --port=80 --target-port=9376 service/hostnames exposed 再查询一遍，确定一下：\n$ kubectl get svc hostnames NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE hostnames ClusterIP 10.0.1.175 \u0026lt;none\u0026gt; 80/TCP 5s 与前面相同，这与您使用 YAML 启动的 Service 一样：\napiVersion: v1 kind: Service metadata: name: hostnames spec: selector: app: hostnames ports: - name: default protocol: TCP port: 80 targetPort: 9376 现在您可以确认 Service 存在。\nService 是否通过 DNS 工作？ 从相同 Namespace 下的 Pod 中运行：\nu@pod$ nslookup hostnames Address 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.local Name: hostnames Address 1: 10.0.1.175 hostnames.default.svc.cluster.local 如果失败，那么您的 Pod 和 Service 可能位于不同的 Namespace 中，请尝试使用限定命名空间的名称：\nu@pod$ nslookup hostnames.default Address 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.local Name: hostnames.default Address 1: 10.0.1.175 hostnames.default.svc.cluster.local 如果成功，那么需要调整您的应用，使用跨命名空间的名称去访问服务，或者，在相同的 Namespace 中运行应用和 Service。如果仍然失败，请尝试一个完全限定的名称：\nu@pod$ nslookup hostnames.default.svc.cluster.local Address 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.local Name: hostnames.default.svc.cluster.local Address 1: 10.0.1.175 hostnames.default.svc.cluster.local 注意这里的后缀：\u0026ldquo;default.svc.cluster.local\u0026rdquo;。\u0026ldquo;default\u0026rdquo; 是我们正在操作的 Namespace。\u0026ldquo;svc\u0026rdquo; 表示这是一个 Service。\u0026ldquo;cluster.local\u0026rdquo; 是您的集群域，在您自己的集群中可能会有所不同。\n您也可以在集群中的 Node 上尝试此操作：\n. note \u0026gt;}} 10.0.0.10 是我的 DNS Service，您的可能不同）. . /note \u0026gt;}}\nu@node$ nslookup hostnames.default.svc.cluster.local 10.0.0.10 Server: 10.0.0.10 Address: 10.0.0.10#53 Name: hostnames.default.svc.cluster.local Address: 10.0.1.175 如果您能够使用完全限定的名称查找，但不能使用相对名称，则需要检查 /etc/resolv.conf 文件是否正确。\nu@pod$ cat /etc/resolv.conf nameserver 10.0.0.10 search default.svc.cluster.local svc.cluster.local cluster.local example.com options ndots:5 nameserver 行必须指示您的集群的 DNS Service，它通过 --cluster-dns 标志传递到 kubelet。\nsearch 行必须包含一个适当的后缀，以便查找 Service 名称。在本例中，它在本地 Namespace（default.svc.cluster.local）、所有 Namespace 中的 Service（svc.cluster.local）以及集群（cluster.local）中查找服务。 根据您自己的安装情况，可能会有额外的记录（最多 6 条）。集群后缀通过 --cluster-domain 标志传递给 kubelet。 本文档中，我们假定它是 “cluster.local”，但是您的可能不同，这种情况下，您应该在上面的所有命令中更改它。\noptions 行必须设置足够高的 ndots，以便 DNS 客户端库考虑搜索路径。在默认情况下，Kubernetes 将这个值设置为 5，这个值足够高，足以覆盖它生成的所有 DNS 名称。\nDNS 中是否存在任何服务？ 如果上面仍然失败 - DNS 查找不到您需要的 Service - 我们可以后退一步，看看还有什么不起作用。Kubernetes 主 Service 应该一直是工作的：\nu@pod$ nslookup kubernetes.default Server: 10.0.0.10 Address 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.local Name: kubernetes.default Address 1: 10.0.0.1 kubernetes.default.svc.cluster.local 如果失败，您可能需要转到这个文档的 kube-proxy 部分，或者甚至回到文档的顶部重新开始，但不是调试您自己的 Service，而是调试 DNS。\nService 能够通过 IP 访问么？ 假设我们可以确认 DNS 工作正常，那么接下来要测试的是您的 Service 是否工作正常。从集群中的一个节点，访问 Service 的 IP（从上面的 kubectl get 命令获取）。\nu@node$ curl 10.0.1.175:80 hostnames-0uton u@node$ curl 10.0.1.175:80 hostnames-yp2kp u@node$ curl 10.0.1.175:80 hostnames-bvc05 如果 Service 是正常的，您应该得到正确的响应。如果没有，有很多可能出错的地方，请继续。\nService 是对的吗？ 这听起来可能很愚蠢，但您应该加倍甚至三倍检查您的 Service 是否正确，并且与您的 Pod 匹配。查看您的 Service 并验证它：\n$ kubectl get service hostnames -o json { \u0026#34;kind\u0026#34;: \u0026#34;Service\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;v1\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;hostnames\u0026#34;, \u0026#34;namespace\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;selfLink\u0026#34;: \u0026#34;/api/v1/namespaces/default/services/hostnames\u0026#34;, \u0026#34;uid\u0026#34;: \u0026#34;428c8b6c-24bc-11e5-936d-42010af0a9bc\u0026#34;, \u0026#34;resourceVersion\u0026#34;: \u0026#34;347189\u0026#34;, \u0026#34;creationTimestamp\u0026#34;: \u0026#34;2015-07-07T15:24:29Z\u0026#34;, \u0026#34;labels\u0026#34;: { \u0026#34;app\u0026#34;: \u0026#34;hostnames\u0026#34; } }, \u0026#34;spec\u0026#34;: { \u0026#34;ports\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;protocol\u0026#34;: \u0026#34;TCP\u0026#34;, \u0026#34;port\u0026#34;: 80, \u0026#34;targetPort\u0026#34;: 9376, \u0026#34;nodePort\u0026#34;: 0 } ], \u0026#34;selector\u0026#34;: { \u0026#34;app\u0026#34;: \u0026#34;hostnames\u0026#34; }, \u0026#34;clusterIP\u0026#34;: \u0026#34;10.0.1.175\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;ClusterIP\u0026#34;, \u0026#34;sessionAffinity\u0026#34;: \u0026#34;None\u0026#34; }, \u0026#34;status\u0026#34;: { \u0026#34;loadBalancer\u0026#34;: {} } } spec.ports[] 中描述的是您想要尝试访问的端口吗？targetPort 对您的 Pod 来说正确吗（许多 Pod 选择使用与 Service 不同的端口）？如果您想把它变成一个数字端口，那么它是一个数字（9376）还是字符串 “9376”？如果您想把它当作一个指定的端口，那么您的 Pod 是否公开了一个同名端口？端口的 protocol 和 Pod 的一样吗？\nService 有端点吗？ 如果您已经走到了这一步，我们假设您已经确认您的 Service 存在，并能通过 DNS 解析。现在，让我们检查一下，您运行的 Pod 确实是由 Service 选择的。\n早些时候，我们已经看到 Pod 是运行状态。我们可以再检查一下：\n$ kubectl get pods -l app=hostnames NAME READY STATUS RESTARTS AGE hostnames-0uton 1/1 Running 0 1h hostnames-bvc05 1/1 Running 0 1h hostnames-yp2kp 1/1 Running 0 1h \u0026ldquo;AGE\u0026rdquo; 列表明这些 Pod 已经启动一个小时了，这意味着它们运行良好，而不是崩溃。\n-l app=hostnames 参数是一个标签选择器 - 就像我们的 Service 一样。在 Kubernetes 系统中有一个控制循环，它评估每个 Service 的选择器，并将结果保存到 Endpoints 对象中。\n$ kubectl get endpoints hostnames NAME ENDPOINTS hostnames 10.244.0.5:9376,10.244.0.6:9376,10.244.0.7:9376 这证实 endpoints 控制器已经为您的 Service 找到了正确的 Pods。如果 hostnames 行为空，则应检查 Service 的 spec.selector 字段，以及您实际想选择的 Pods 的 metadata.labels 的值。常见的错误是输入错误或其他错误，例如 Service 想选择 run=hostnames，但是 Deployment 指定的是 app=hostnames。\nPod 正常工作吗？ 到了这步，我们知道您的 Service 存在并选择了您的 Pods。让我们检查一下 Pod 是否真的在工作 - 我们可以绕过 Service 机制，直接进入 Pod。\n. note \u0026gt;}} 这些命令使用的是 Pod 端口（9376），而不是 Service 端口（80）。 . /note \u0026gt;}}\nu@pod$ wget -qO- 10.244.0.5:9376 hostnames-0uton pod $ wget -qO- 10.244.0.6:9376 hostnames-bvc05 u@pod$ wget -qO- 10.244.0.7:9376 hostnames-yp2kp 我们期望的是 Endpoints 列表中的每个 Pod 返回自己的主机名。如果这没有发生（或者您自己的 Pod 的正确行为没有发生），您应该调查发生了什么。您会发现 kubectl logs 这个时候非常有用，或者使用 kubectl exec 直接进入到您的 Pod，并从那里检查服务。\n另一件要检查的事情是，您的 Pod 没有崩溃或正在重新启动。频繁的重新启动可能会导致断断续续的连接问题。\n$ kubectl get pods -l app=hostnames NAME READY STATUS RESTARTS AGE hostnames-632524106-bbpiw 1/1 Running 0 2m hostnames-632524106-ly40y 1/1 Running 0 2m hostnames-632524106-tlaok 1/1 Running 0 2m 如果重新启动计数很高，请查阅有关如何调试 pods 获取更多信息。\nkube-proxy 正常工作吗？ 如果您到了这里，那么您的 Service 正在运行，也有 Endpoints，而您的 Pod 实际上也正在服务。在这一点上，整个 Service 代理机制是否正常就是可疑的了。我们来确认一下，一部分一部分来。\nkube-proxy 在运行吗？ 确认 kube-proxy 正在您的 Nodes 上运行。您应该得到如下内容：\nu@node$ ps auxw | grep kube-proxy root 4194 0.4 0.1 101864 17696 ? Sl Jul04 25:43 /usr/local/bin/kube-proxy --master=https://kubernetes-master --kubeconfig=/var/lib/kube-proxy/kubeconfig --v=2 下一步，确认它并没有出现明显的失败，比如连接主节点失败。要做到这一点，您必须查看日志。访问日志取决于您的 Node 操作系统。在某些操作系统是一个文件，如 /var/log/messages kube-proxy.log，而其他操作系统使用 journalctl 访问日志。您应该看到类似的东西：\nI1027 22:14:53.995134 5063 server.go:200] Running in resource-only container \u0026quot;/kube-proxy\u0026quot; I1027 22:14:53.998163 5063 server.go:247] Using iptables Proxier. I1027 22:14:53.999055 5063 server.go:255] Tearing down userspace rules. Errors here are acceptable. I1027 22:14:54.038140 5063 proxier.go:352] Setting endpoints for \u0026quot;kube-system/kube-dns:dns-tcp\u0026quot; to [10.244.1.3:53] I1027 22:14:54.038164 5063 proxier.go:352] Setting endpoints for \u0026quot;kube-system/kube-dns:dns\u0026quot; to [10.244.1.3:53] I1027 22:14:54.038209 5063 proxier.go:352] Setting endpoints for \u0026quot;default/kubernetes:https\u0026quot; to [10.240.0.2:443] I1027 22:14:54.038238 5063 proxier.go:429] Not syncing iptables until Services and Endpoints have been received from master I1027 22:14:54.040048 5063 proxier.go:294] Adding new service \u0026quot;default/kubernetes:https\u0026quot; at 10.0.0.1:443/TCP I1027 22:14:54.040154 5063 proxier.go:294] Adding new service \u0026quot;kube-system/kube-dns:dns\u0026quot; at 10.0.0.10:53/UDP I1027 22:14:54.040223 5063 proxier.go:294] Adding new service \u0026quot;kube-system/kube-dns:dns-tcp\u0026quot; at 10.0.0.10:53/TCP 如果您看到有关无法连接主节点的错误消息，则应再次检查节点配置和安装步骤。\nkube-proxy 无法正确运行的可能原因之一是找不到所需的 conntrack 二进制文件。在一些 Linux 系统上，这也是可能发生的，这取决于您如何安装集群，例如，您正在从头开始安装 Kubernetes。如果是这样的话，您需要手动安装 conntrack 包（例如，在 Ubuntu 上使用 sudo apt install conntrack），然后重试。\nkube-proxy 是否在写 iptables 规则？ kube-proxy 的主要职责之一是写实现 Services 的 iptables 规则。让我们检查一下这些规则是否已经被写好了。\nkube-proxy 可以在 \u0026ldquo;userspace\u0026rdquo; 模式、 \u0026ldquo;iptables\u0026rdquo; 模式或者 \u0026ldquo;ipvs\u0026rdquo; 模式下运行。 希望您正在使用 \u0026ldquo;iptables\u0026rdquo; 模式或者 \u0026ldquo;ipvs\u0026rdquo; 模式。您应该看到以下情况之一。\nUserpace u@node$ iptables-save | grep hostnames -A KUBE-PORTALS-CONTAINER -d 10.0.1.175/32 -p tcp -m comment --comment \u0026#34;default/hostnames:default\u0026#34; -m tcp --dport 80 -j REDIRECT --to-ports 48577 -A KUBE-PORTALS-HOST -d 10.0.1.175/32 -p tcp -m comment --comment \u0026#34;default/hostnames:default\u0026#34; -m tcp --dport 80 -j DNAT --to-destination 10.240.115.247:48577 您的 Service 上的每个端口应该有两个规则（本例中只有一个）- \u0026ldquo;KUBE-PORTALS-CONTAINER\u0026rdquo; 和 \u0026ldquo;KUBE-PORTALS-HOST\u0026rdquo;。如果您没有看到这些，请尝试将 -V 标志设置为 4 之后重新启动 kube-proxy，然后再次查看日志。\n几乎没有人应该再使用 \u0026ldquo;userspace\u0026rdquo; 模式了，所以我们不会在这里花费更多的时间。\nIptables u@node$ iptables-save | grep hostnames -A KUBE-SEP-57KPRZ3JQVENLNBR -s 10.244.3.6/32 -m comment --comment \u0026#34;default/hostnames:\u0026#34; -j MARK --set-xmark 0x00004000/0x00004000 -A KUBE-SEP-57KPRZ3JQVENLNBR -p tcp -m comment --comment \u0026#34;default/hostnames:\u0026#34; -m tcp -j DNAT --to-destination 10.244.3.6:9376 -A KUBE-SEP-WNBA2IHDGP2BOBGZ -s 10.244.1.7/32 -m comment --comment \u0026#34;default/hostnames:\u0026#34; -j MARK --set-xmark 0x00004000/0x00004000 -A KUBE-SEP-WNBA2IHDGP2BOBGZ -p tcp -m comment --comment \u0026#34;default/hostnames:\u0026#34; -m tcp -j DNAT --to-destination 10.244.1.7:9376 -A KUBE-SEP-X3P2623AGDH6CDF3 -s 10.244.2.3/32 -m comment --comment \u0026#34;default/hostnames:\u0026#34; -j MARK --set-xmark 0x00004000/0x00004000 -A KUBE-SEP-X3P2623AGDH6CDF3 -p tcp -m comment --comment \u0026#34;default/hostnames:\u0026#34; -m tcp -j DNAT --to-destination 10.244.2.3:9376 -A KUBE-SERVICES -d 10.0.1.175/32 -p tcp -m comment --comment \u0026#34;default/hostnames: cluster IP\u0026#34; -m tcp --dport 80 -j KUBE-SVC-NWV5X2332I4OT4T3 -A KUBE-SVC-NWV5X2332I4OT4T3 -m comment --comment \u0026#34;default/hostnames:\u0026#34; -m statistic --mode random --probability 0.33332999982 -j KUBE-SEP-WNBA2IHDGP2BOBGZ -A KUBE-SVC-NWV5X2332I4OT4T3 -m comment --comment \u0026#34;default/hostnames:\u0026#34; -m statistic --mode random --probability 0.50000000000 -j KUBE-SEP-X3P2623AGDH6CDF3 -A KUBE-SVC-NWV5X2332I4OT4T3 -m comment --comment \u0026#34;default/hostnames:\u0026#34; -j KUBE-SEP-57KPRZ3JQVENLNBR KUBE-SERVICES 中应该有 1 条规则，KUBE-SVC-(hash) 中每个端点有 1 或 2 条规则（取决于 SessionAffinity），每个端点中应有 1 条 KUBE-SEP-(hash) 链。准确的规则将根据您的确切配置（包括节点、端口组合以及负载均衡器设置）而有所不同。\nIPVS u@node$ ipvsadm -ln Prot LocalAddress:Port Scheduler Flags -\u0026gt; RemoteAddress:Port Forward Weight ActiveConn InActConn ... TCP 10.0.1.175:80 rr -\u0026gt; 10.244.0.5:9376 Masq 1 0 0 -\u0026gt; 10.244.0.6:9376 Masq 1 0 0 -\u0026gt; 10.244.0.7:9376 Masq 1 0 0 ... IPVS 代理将为每个服务器地址（例如集群 IP、外部 IP、节点端口 IP、负载均衡 IP等）创建虚拟服务器，并为服务的端点创建一些相应的真实服务器（如果有）。在这个例子中，服务器主机名（10.0.1.175:80）有 3 个端点(10.244.0.5:9376, 10.244.0.6:9376, 10.244.0.7:9376)，你会得到类似上面的结果。\nkube-proxy 在执行代理操作么？ 假设您确实看到了上述规则，请再次尝试通过 IP 访问您的 Service：\nu@node$ curl 10.0.1.175:80 hostnames-0uton 如果失败了，并且您正在使用 userspace 代理，您可以尝试直接访问代理。如果您使用的是 iptables 代理，请跳过本节。\n回顾上面的 iptables-save 输出，并提取 kube-proxy 用于您的 Service 的端口号。在上面的例子中，它是 “48577”。现在连接到它：\nu@node$ curl localhost:48577 hostnames-yp2kp 如果仍然失败，请查看 kube-proxy 日志中的特定行，如：\nSetting endpoints for default/hostnames:default to [10.244.0.5:9376 10.244.0.6:9376 10.244.0.7:9376] 如果您没有看到这些，请尝试将 -V 标志设置为 4 并重新启动 kube-proxy，然后再查看日志。\nPod 无法通过 Service IP 访问自己 如果网络没有为“发夹模式”流量生成正确配置，通常当 kube-proxy 以 iptables 模式运行，并且 Pod 与桥接网络连接时，就会发生这种情况。Kubelet 公开了一个 hairpin-mode 标志，如果 pod 试图访问它们自己的 Service VIP，就可以让 Service 的端点重新负载到他们自己身上。hairpin-mode 标志必须设置为 hairpin-veth 或者 promiscuous-bridge。\n解决这一问题的常见步骤如下：\n 确认 hairpin-mode 被设置为 hairpin-veth 或者 promiscuous-bridge。您应该看到下面这样的内容。在下面的示例中，hairpin-mode 被设置为 promiscuous-bridge。  u@node$ ps auxw|grep kubelet root 3392 1.1 0.8 186804 65208 ? Sl 00:51 11:11 /usr/local/bin/kubelet --enable-debugging-handlers=true --config=/etc/kubernetes/manifests --allow-privileged=True --v=4 --cluster-dns=10.0.0.10 --cluster-domain=cluster.local --configure-cbr0=true --cgroup-root=/ --system-cgroups=/system --hairpin-mode=promiscuous-bridge --runtime-cgroups=/docker-daemon --kubelet-cgroups=/kubelet --babysit-daemons=true --max-pods=110 --serialize-image-pulls=false --outofdisk-transition-frequency=0  确认有效的 hairpin-mode。要做到这一点，您必须查看 kubelet 日志。访问日志取决于节点的操作系统。在一些操作系统上，它是一个文件，如 /var/log/kubelet.log，而其他操作系统则使用 journalctl 访问日志。请注意，由于兼容性，有效的 hairpin-mode 可能不匹配 --hairpin-mode 标志。在 kubelet.log 中检查是否有带有关键字 hairpin 的日志行。应该有日志行指示有效的 hairpin-mode，比如下面的内容。  I0629 00:51:43.648698 3252 kubelet.go:380] Hairpin mode set to \u0026#34;promiscuous-bridge\u0026#34;  如果有效的发夹模式是 hairpin-veth，请确保 Kubelet 具有在节点上的 /sys 中操作的权限。如果一切正常工作，您应该看到如下内容：  for intf in /sys/devices/virtual/net/cbr0/brif/*; do cat $intf/hairpin_mode; done 1 1 1 1  如果有效的发夹模式是 promiscuous-bridge，则请确保 Kubelet 拥有在节点上操纵 Linux 网桥的权限。如果正确使用和配置了 cbr0 网桥，您应该看到：  u@node$ ifconfig cbr0 |grep PROMISC UP BROADCAST RUNNING PROMISC MULTICAST MTU:1460 Metric:1  如果上述任何一项都没有效果，请寻求帮助。  寻求帮助 如果您走到这一步，那么就真的是奇怪的事情发生了。您的 Service 正在运行，有 Endpoints，您的 Pods 也确实在服务中。您的 DNS 正常，iptables 规则已经安装，kube-proxy 看起来也正常。然而 Service 不起作用。这种情况下，您应该让我们知道，这样我们可以帮助调查！\n使用 Slack 或者 Forum 或者 GitHub 联系我们。\n. heading \u0026ldquo;whatsnext\u0026rdquo; %}} 访问故障排查文档获取更多信息。\n"
},
{
	"uri": "https://lijun.in/tasks/debug-application-cluster/debug-stateful-set/",
	"title": "调试StatefulSet",
	"tags": [],
	"description": "",
	"content": "此任务展示如何调试StatefulSet。\n. heading \u0026ldquo;prerequisites\u0026rdquo; %}}  你需要有一个Kubernetes集群，通过必要的配置使kubectl命令行工具与您的集群进行通信。 你应该有一个运行中的StatefulSet，以便用于调试。  调试StatefulSet 由于StatefulSet在创建时设置了app=myapp标签，列出仅属于该StatefulSet的所有pod时，可以使用以下命令：\nkubectl get pods -l app=myapp 如果您发现列出的任何Pods长时间处于Unknown 或Terminating状态，关于如何处理它们的说明任务,请参阅删除 StatefulSet Pods。您可以参考调试 Pods指南来调试StatefulSet中的各个Pod。\nStatefulSets提供调试机制，可以使用注解来暂停所有控制器在Pod上的操作。在任何StatefulSet Pod上设置pod.alpha.kubernetes.io/initialized注解为\u0026quot;false\u0026quot;将暂停 StatefulSet的所有操作。暂停时，StatefulSet将不执行任何伸缩操作。一旦调试钩子设置完成后，就可以在StatefulSet pod的容器内执行命令，而不会造成伸缩操作的干扰。您可以通过执行以下命令将注解设置为\u0026quot;false\u0026quot;：\nkubectl annotate pods \u0026lt;pod-name\u0026gt; pod.alpha.kubernetes.io/initialized=\u0026#34;false\u0026#34; --overwrite 当注解设置为\u0026quot;false\u0026quot;时，StatefulSet在其Pods变得不健康或不可用时将不会响应。StatefulSet不会创建副本Pod直到每个Pod上删除注解或将注解设置为\u0026quot;true\u0026quot;。\n逐步初始化 创建StatefulSet之前，您可以通过使用和上文相同的注解，即将yaml文件中.spec.template.metadata.annotations里的pod.alpha.kubernetes.io/initialized字段设置为\u0026quot;false\u0026quot;，对竞态条件的StatefulSet进行调试。\napiVersion: apps/v1beta1 kind: StatefulSet metadata: name: my-app spec: serviceName: \u0026#34;my-app\u0026#34; replicas: 3 template: metadata: labels: app: my-app annotations: pod.alpha.kubernetes.io/initialized: \u0026#34;false\u0026#34; ... ... ... 设置注解后，如果创建了StatefulSet，您可以等待每个Pod来验证它是否正确初始化。StatefulSet将不会创建任何后续的Pods，直到在已经创建的每个Pod上将调试注解设置为\u0026quot;true\u0026quot; (或删除)。 您可以通过执行以下命令将注解设置为\u0026quot;true\u0026quot;：\nkubectl annotate pods \u0026lt;pod-name\u0026gt; pod.alpha.kubernetes.io/initialized=\u0026#34;true\u0026#34; --overwrite . heading \u0026ldquo;whatsnext\u0026rdquo; %}} 点击链接调试init-container，了解更多信息。\n"
},
{
	"uri": "https://lijun.in/tasks/debug-application-cluster/resource-metrics-pipeline/",
	"title": "资源指标管道",
	"tags": [],
	"description": "",
	"content": "从 Kubernetes 1.8开始，资源使用指标，例如容器 CPU 和内存使用率，可通过 Metrics API 在 Kubernetes 中获得。这些指标可以直接被用户访问，比如使用kubectl top命令行，或者这些指标由集群中的控制器使用，例如，Horizontal Pod Autoscaler，使用这些指标来做决策。\nMetrics API 通过 Metrics API，您可以获得指定节点或 pod 当前使用的资源量。此 API 不存储指标值，因此想要获取某个指定节点10分钟前的资源使用量是不可能的。\n此 API 与其他 API 没有区别：\n 此 API 和其它 Kubernetes API 一起位于同一端点（endpoint）之下，是可发现的，路径为/apis/metrics.k8s.io/ 它提供相同的安全性、可扩展性和可靠性保证  Metrics API 在k8s.io/metrics 仓库中定义。您可以在那里找到有关 Metrics API 的更多信息。\n. note \u0026gt;}} Metrics API 需要在集群中部署 Metrics Server。否则它将不可用。 . /note \u0026gt;}}\nMetrics Server Metrics Server是集群范围资源使用数据的聚合器。 从 Kubernetes 1.8开始，它作为 Deployment 对象，被默认部署在由kube-up.sh脚本创建的集群中。 如果您使用不同的 Kubernetes 安装方法，则可以使用提供的deployment yamls来部署。它在 Kubernetes 1.7+中得到支持（详见下文）。\nMetric server 从每个节点上的 Kubelet 公开的 Summary API 中采集指标信息。\n通过在主 API server 中注册的 Metrics Server Kubernetes 聚合器 来采集指标信息， 这是在 Kubernetes 1.7 中引入的。\n在设计文档中可以了解到有关 Metrics Server 的更多信息。\n"
},
{
	"uri": "https://lijun.in/tasks/debug-application-cluster/resource-usage-monitoring/",
	"title": "资源监控工具",
	"tags": [],
	"description": "",
	"content": "要扩展应用程序并提供可靠的服务，您需要了解应用程序在部署时的行为。 您可以通过检测容器检查 Kubernetes 集群中的应用程序性能，pods, 服务和整个集群的特征。 Kubernetes 在每个级别上提供有关应用程序资源使用情况的详细信息。 此信息使您可以评估应用程序的性能，以及在何处可以消除瓶颈以提高整体性能。\n在 Kubernetes 中，应用程序监控不依赖单个监控解决方案。 在新集群上，您可以使用资源度量或完整度量管道来收集监视统计信息。\n资源度量管道 资源指标管道提供了一组与集群组件，例如[Horizontal Pod Autoscaler]控制器(/docs/tasks/run-application/horizontal-pod-autoscale)，以及 kubectl top 实用程序相关的有限度量。 这些指标是由轻量级的、短期内存度量服务器收集的，通过 metrics.k8s.io 公开。\n度量服务器发现集群中的所有节点，并且查询每个节点的kubelet以获取 CPU 和内存使用情况。 Kubelet 充当 Kubernetes 主节点与节点之间的桥梁，管理机器上运行的 Pod 和容器。 kubelet 将每个 pod 转换为其组成的容器，并在容器运行时通过容器运行时界面获取各个容器使用情况统计信息。 kubelet 从集成的 cAdvisor 获取此信息，以进行旧式 Docker 集成。 然后，它通过 metrics-server Resource Metrics API 公开聚合的 pod 资源使用情况统计信息。 该 API 在 kubelet 的经过身份验证和只读的端口上的/metrics/resource/v1beta1中提供。\n完整度量管道 一个完整度量管道可以让您访问更丰富的度量。 Kubernetes 还可以根据集群的当前状态，使用 Pod 水平自动扩缩器等机制，通过自动调用扩展或调整集群来响应这些度量。 监控管道从 kubelet 获取度量，然后通过适配器将它们公开给 Kubernetes，方法是实现 custom.metrics.k8s.io 或 external.metrics.k8s.io API。\nPrometheus，一个 CNCF 项目，可以原生监控 Kubernetes、节点和 Prometheus 本身。 完整度量管道项目不属于 CNCF 的一部分，不在 Kubernetes 文档的范围之内。\n"
},
{
	"uri": "https://lijun.in/tutorials/stateful-application/zookeeper/",
	"title": "运行 ZooKeeper， 一个 CP 分布式系统",
	"tags": [],
	"description": "",
	"content": "本教程展示了在 Kubernetes 上使用 PodDisruptionBudgets 和 PodAntiAffinity 特性运行 Apache Zookeeper。\n. heading \u0026ldquo;prerequisites\u0026rdquo; %}} 在开始本教程前，你应该熟悉以下 Kubernetes 概念。\n Pods Cluster DNS Headless Services PersistentVolumes [PersistentVolume Provisioning](http://releases.k8s.io/. param \u0026ldquo;githubbranch\u0026rdquo; \u0026gt;}}/examples/persistent-volume-provisioning/) ConfigMaps StatefulSets PodDisruptionBudgets PodAntiAffinity kubectl CLI  你需要一个至少包含四个节点的集群，每个节点至少 2 CPUs 和 4 GiB 内存。在本教程中你将会 cordon 和 drain 集群的节点。这意味着集群节点上所有的 Pods 将会被终止并移除。这些节点也会暂时变为不可调度。在本教程中你应该使用一个独占的集群，或者保证你造成的干扰不会影响其它租户。\n本教程假设你的集群配置为动态的提供 PersistentVolumes。如果你的集群没有配置成这样，在开始本教程前，你需要手动准备三个 20 GiB 的卷。\n. heading \u0026ldquo;objectives\u0026rdquo; %}} 在学习本教程后，你将熟悉下列内容。\n 如何使用 StatefulSet 部署一个 ZooKeeper ensemble。 如何使用 ConfigMaps 一致性配置 ensemble。 如何在 ensemble 中 分布 ZooKeeper 服务的部署。 如何在计划维护中使用 PodDisruptionBudgets 确保服务可用性。  ZooKeeper 基础 Apache ZooKeeper 是一个分布式的开源协调服务，用于分布式系统。ZooKeeper 允许你读取、写入数据和发现数据更新。数据按层次结构组织在文件系统中，并复制到 ensemble（一个 ZooKeeper 服务的集合） 中所有的 ZooKeeper 服务。对数据的所有操作都是原子的和顺序一致的。ZooKeeper 通过 Zab 一致性协议在 ensemble 的所有服务之间复制一个状态机来确保这个特性。\nensemble 使用 Zab 协议选举一个 leader，在选举出 leader 前不能写入数据。一旦选举出了 leader，ensemble 使用 Zab 保证所有写入被复制到一个 quorum，然后这些写入操作才会被确认并对客户端可用。如果没有遵照加权 quorums，一个 quorum 表示包含当前 leader 的 ensemble 的多数成员。例如，如果 ensemble 有3个服务，一个包含 leader 的成员和另一个服务就组成了一个 quorum。如果 ensemble 不能达成一个 quorum，数据将不能被写入。\nZooKeeper 在内存中保存它们的整个状态机，但是每个改变都被写入一个在存储介质上的持久 WAL（Write Ahead Log）。当一个服务故障时，它能够通过回放 WAL 恢复之前的状态。为了防止 WAL 无限制的增长，ZooKeeper 服务会定期的将内存状态快照保存到存储介质。这些快照能够直接加载到内存中，所有在这个快照之前的 WAL 条目都可以被安全的丢弃。\n创建一个 ZooKeeper Ensemble 下面的清单包含一个 Headless Service， 一个 Service， 一个 PodDisruptionBudget， 和一个 StatefulSet。\n. codenew file=\u0026quot;application/zookeeper/zookeeper.yaml\u0026rdquo; \u0026gt;}}\n打开一个命令行终端，使用 kubectl apply 创建这个清单。\nkubectl apply -f https://k8s.io/examples/application/zookeeper/zookeeper.yaml 这个操作创建了 zk-hs Headless Service、zk-cs Service、zk-pdb PodDisruptionBudget 和 zk StatefulSet。\nservice/zk-hs created service/zk-cs created poddisruptionbudget.policy/zk-pdb created statefulset.apps/zk created 使用 [kubectl get](/zh/docs/user-guide/kubectl/. param \u0026ldquo;version\u0026rdquo; \u0026gt;}}/#get) 查看 StatefulSet 控制器创建的 Pods。\nkubectl get pods -w -l app=zk 一旦 zk-2 Pod 变成 Running 和 Ready 状态，使用 CRTL-C 结束 kubectl。\nNAME READY STATUS RESTARTS AGE zk-0 0/1 Pending 0 0s zk-0 0/1 Pending 0 0s zk-0 0/1 ContainerCreating 0 0s zk-0 0/1 Running 0 19s zk-0 1/1 Running 0 40s zk-1 0/1 Pending 0 0s zk-1 0/1 Pending 0 0s zk-1 0/1 ContainerCreating 0 0s zk-1 0/1 Running 0 18s zk-1 1/1 Running 0 40s zk-2 0/1 Pending 0 0s zk-2 0/1 Pending 0 0s zk-2 0/1 ContainerCreating 0 0s zk-2 0/1 Running 0 19s zk-2 1/1 Running 0 40s StatefulSet 控制器创建了3个 Pods，每个 Pod 包含一个 ZooKeeper 3.4.9 服务。\n促成 Leader 选举 由于在匿名网络中没有用于选举 leader 的终止算法，Zab 要求显式的进行成员关系配置，以执行 leader 选举。Ensemble 中的每个服务都需要具有一个独一无二的标识符，所有的服务均需要知道标识符的全集，并且每个标志都需要和一个网络地址相关联。\n使用 [kubectl exec](/zh/docs/user-guide/kubectl/. param \u0026ldquo;version\u0026rdquo; \u0026gt;}}/#exec) 获取 zk StatefulSet 中 Pods 的主机名。\nfor i in 0 1 2; do kubectl exec zk-$i -- hostname; done StatefulSet 控制器基于每个 Pod 的序号索引为它们各自提供一个唯一的主机名。主机名采用 \u0026lt;statefulset name\u0026gt;-\u0026lt;ordinal index\u0026gt; 的形式。由于 zk StatefulSet 的 replicas 字段设置为3，这个 Set 的控制器将创建3个 Pods，主机名为：zk-0、zk-1 和 zk-2。\nzk-0 zk-1 zk-2 ZooKeeper ensemble 中的服务使用自然数作为唯一标识符，每个服务的标识符都保存在服务的数据目录中一个名为 myid 的文件里。\n检查每个服务的 myid 文件的内容。\nfor i in 0 1 2; do echo \u0026#34;myid zk-$i\u0026#34;;kubectl exec zk-$i -- cat /var/lib/zookeeper/data/myid; done 由于标识符为自然数并且序号索引是非负整数，你可以在序号上加 1 来生成一个标识符。\nmyid zk-0 1 myid zk-1 2 myid zk-2 3 获取 zk StatefulSet 中每个 Pod 的 FQDN (Fully Qualified Domain Name，正式域名)。\nfor i in 0 1 2; do kubectl exec zk-$i -- hostname -f; done zk-headless Service 为所有 Pods 创建了一个 domain：zk-headless.default.svc.cluster.local。\nzk-0.zk-headless.default.svc.cluster.local zk-1.zk-headless.default.svc.cluster.local zk-2.zk-headless.default.svc.cluster.local Kubernetes DNS 中的 A 记录将 FQDNs 解析成为 Pods 的 IP 地址。如果 Pods 被调度，这个 A 记录将会使用 Pods 的新 IP 地址更新，但 A 记录的名称不会改变。\nZooKeeper 在一个名为 zoo.cfg 的文件中保存它的应用配置。使用 kubectl exec 在 zk-0 Pod 中查看 zoo.cfg 文件的内容。\nkubectl exec zk-0 -- cat /opt/zookeeper/conf/zoo.cfg 文件底部为 server.1、server.2 和 server.3，其中的 1、2和3分别对应 ZooKeeper 服务的 myid 文件中的标识符。它们被设置为 zk StatefulSet 中的 Pods 的 FQDNs。\nclientPort=2181 dataDir=/var/lib/zookeeper/data dataLogDir=/var/lib/zookeeper/log tickTime=2000 initLimit=10 syncLimit=2000 maxClientCnxns=60 minSessionTimeout= 4000 maxSessionTimeout= 40000 autopurge.snapRetainCount=3 autopurge.purgeInterval=0 server.1=zk-0.zk-headless.default.svc.cluster.local:2888:3888 server.2=zk-1.zk-headless.default.svc.cluster.local:2888:3888 server.3=zk-2.zk-headless.default.svc.cluster.local:2888:3888 达成一致 一致性协议要求每个参与者的标识符唯一。在 Zab 协议里任何两个参与者都不应该声明相同的唯一标识符。对于让系统中的进程协商哪些进程已经提交了哪些数据而言，这是必须的。如果有两个 Pods 使用相同的序号启动，这两个 ZooKeeper 服务会将自己识别为相同的服务。\n当你创建 zk StatefulSet 时，StatefulSet 控制器按照 Pods 的序号索引顺序的创建每个 Pod。在创建下一个 Pod 前会等待每个 Pod 变成 Running 和 Ready 状态。\nkubectl get pods -w -l app=zk NAME READY STATUS RESTARTS AGE zk-0 0/1 Pending 0 0s zk-0 0/1 Pending 0 0s zk-0 0/1 ContainerCreating 0 0s zk-0 0/1 Running 0 19s zk-0 1/1 Running 0 40s zk-1 0/1 Pending 0 0s zk-1 0/1 Pending 0 0s zk-1 0/1 ContainerCreating 0 0s zk-1 0/1 Running 0 18s zk-1 1/1 Running 0 40s zk-2 0/1 Pending 0 0s zk-2 0/1 Pending 0 0s zk-2 0/1 ContainerCreating 0 0s zk-2 0/1 Running 0 19s zk-2 1/1 Running 0 40s 每个 Pod 的 A 记录仅在 Pod 变成 Ready状态时被录入。因此，ZooKeeper 服务的 FQDNs 只会解析到一个 endpoint，而那个 endpoint 将会是一个唯一的 ZooKeeper 服务，这个服务声明了配置在它的 myid 文件中的标识符。\nzk-0.zk-headless.default.svc.cluster.local zk-1.zk-headless.default.svc.cluster.local zk-2.zk-headless.default.svc.cluster.local 这保证了 ZooKeepers 的 zoo.cfg 文件中的 servers 属性代表了一个正确配置的 ensemble。\nserver.1=zk-0.zk-headless.default.svc.cluster.local:2888:3888 server.2=zk-1.zk-headless.default.svc.cluster.local:2888:3888 server.3=zk-2.zk-headless.default.svc.cluster.local:2888:3888 当服务使用 Zab 协议尝试提交一个值的时候，它们会达成一致并成功提交这个值（如果 leader 选举成功并且至少有两个 Pods 处于 Running 和 Ready状态），或者将会失败（如果没有满足上述条件中的任意一条）。当一个服务承认另一个服务的代写时不会有状态产生。\nEnsemble 健康检查 最基本的健康检查是向一个 ZooKeeper 服务写入一些数据，然后从另一个服务读取这些数据。\n使用 zkCli.sh 脚本在 zk-0 Pod 上写入 world 到路径 /hello。\nkubectl exec zk-0 zkCli.sh create /hello world 这将会把 world 写入 ensemble 的 /hello 路径。\nWATCHER:: WatchedEvent state:SyncConnected type:None path:null Created /hello 从 zk-1 Pod 获取数据。\nkubectl exec zk-1 zkCli.sh get /hello 你在 zk-0 创建的数据在 ensemble 中所有的服务上都是可用的。\nWATCHER:: WatchedEvent state:SyncConnected type:None path:null world cZxid = 0x100000002 ctime = Thu Dec 08 15:13:30 UTC 2016 mZxid = 0x100000002 mtime = Thu Dec 08 15:13:30 UTC 2016 pZxid = 0x100000002 cversion = 0 dataVersion = 0 aclVersion = 0 ephemeralOwner = 0x0 dataLength = 5 numChildren = 0 准备持久存储 如同在 ZooKeeper 基础 一节所提到的，ZooKeeper 提交所有的条目到一个持久 WAL，并周期性的将内存快照写入存储介质。对于使用一致性协议实现一个复制状态机的应用来说，使用 WALs 提供持久化是一种常用的技术，对于普通的存储应用也是如此。\n使用 [kubectl delete](/zh/docs/user-guide/kubectl/. param \u0026ldquo;version\u0026rdquo; \u0026gt;}}/#delete) 删除 zk StatefulSet。\nkubectl delete statefulset zk statefulset \u0026#34;zk\u0026#34; deleted 观察 StatefulSet 中的 Pods 变为终止状态。\nget pods -w -l app=zk 当 zk-0 完全终止时，使用 CRTL-C 结束 kubectl。\nzk-2 1/1 Terminating 0 9m zk-0 1/1 Terminating 0 11m zk-1 1/1 Terminating 0 10m zk-2 0/1 Terminating 0 9m zk-2 0/1 Terminating 0 9m zk-2 0/1 Terminating 0 9m zk-1 0/1 Terminating 0 10m zk-1 0/1 Terminating 0 10m zk-1 0/1 Terminating 0 10m zk-0 0/1 Terminating 0 11m zk-0 0/1 Terminating 0 11m zk-0 0/1 Terminating 0 11m 重新应用 zookeeper.yaml 中的代码清单。\nkubectl apply -f https://k8s.io/docs/tutorials/stateful-application/zookeeper.yaml zk StatefulSet 将会被创建。由于清单中的其他 API 对象已经存在，所以它们不会被修改。\nstatefulset \u0026#34;zk\u0026#34; created Error from server (AlreadyExists): error when creating \u0026#34;zookeeper.yaml\u0026#34;: services \u0026#34;zk-headless\u0026#34; already exists Error from server (AlreadyExists): error when creating \u0026#34;zookeeper.yaml\u0026#34;: configmaps \u0026#34;zk-config\u0026#34; already exists Error from server (AlreadyExists): error when creating \u0026#34;zookeeper.yaml\u0026#34;: poddisruptionbudgets.policy \u0026#34;zk-budget\u0026#34; already exists 观察 StatefulSet 控制器重建 StatefulSet 的 Pods。\nkubectl get pods -w -l app=zk 一旦 zk-2 Pod 处于 Running 和 Ready 状态，使用 CRTL-C 停止 kubectl命令。\nNAME READY STATUS RESTARTS AGE zk-0 0/1 Pending 0 0s zk-0 0/1 Pending 0 0s zk-0 0/1 ContainerCreating 0 0s zk-0 0/1 Running 0 19s zk-0 1/1 Running 0 40s zk-1 0/1 Pending 0 0s zk-1 0/1 Pending 0 0s zk-1 0/1 ContainerCreating 0 0s zk-1 0/1 Running 0 18s zk-1 1/1 Running 0 40s zk-2 0/1 Pending 0 0s zk-2 0/1 Pending 0 0s zk-2 0/1 ContainerCreating 0 0s zk-2 0/1 Running 0 19s zk-2 1/1 Running 0 40s 从 zk-2 Pod 中获取你在健康检查中输入的值。\nkubectl exec zk-2 zkCli.sh get /hello 尽管 zk StatefulSet 中所有的 Pods 都已经被终止并重建过，ensemble 仍然使用原来的数值提供服务。\nWATCHER:: WatchedEvent state:SyncConnected type:None path:null world cZxid = 0x100000002 ctime = Thu Dec 08 15:13:30 UTC 2016 mZxid = 0x100000002 mtime = Thu Dec 08 15:13:30 UTC 2016 pZxid = 0x100000002 cversion = 0 dataVersion = 0 aclVersion = 0 ephemeralOwner = 0x0 dataLength = 5 numChildren = 0 zk StatefulSet 的 spec 中的 volumeClaimTemplates 字段标识了将要为每个 Pod 准备的 PersistentVolume。\nvolumeClaimTemplates: - metadata: name: datadir annotations: volume.alpha.kubernetes.io/storage-class: anything spec: accessModes: [ \u0026#34;ReadWriteOnce\u0026#34; ] resources: requests: storage: 20Gi StatefulSet 控制器为 StatefulSet 中的每个 Pod 生成一个 PersistentVolumeClaim。\n获取 StatefulSet 的 PersistentVolumeClaims。\nkubectl get pvc -l app=zk 当 StatefulSet 重新创建它的 Pods时，Pods 的 PersistentVolumes 会被重新挂载。\nNAME STATUS VOLUME CAPACITY ACCESSMODES AGE datadir-zk-0 Bound pvc-bed742cd-bcb1-11e6-994f-42010a800002 20Gi RWO 1h datadir-zk-1 Bound pvc-bedd27d2-bcb1-11e6-994f-42010a800002 20Gi RWO 1h datadir-zk-2 Bound pvc-bee0817e-bcb1-11e6-994f-42010a800002 20Gi RWO 1h StatefulSet 的容器 template 中的 volumeMounts 一节使得 PersistentVolumes 被挂载到 ZooKeeper 服务的数据目录。\nvolumeMounts: - name: datadir mountPath: /var/lib/zookeeper 当 zk StatefulSet 中的一个 Pod 被（重新）调度时，它总是拥有相同的 PersistentVolume，挂载到 ZooKeeper 服务的数据目录。即使在 Pods 被重新调度时，所有对 ZooKeeper 服务的 WALs 的写入和它们的全部快照都仍然是持久的。\n确保一致性配置 如同在 促成 leader 选举 和 达成一致 小节中提到的，ZooKeeper ensemble 中的服务需要一致性的配置来选举一个 leader 并形成一个 quorum。它们还需要 Zab 协议的一致性配置来保证这个协议在网络中正确的工作。你可以使用 ConfigMaps 达到目的。\n获取 zk-config 的 ConfigMap。\nkubectl get cm zk-config -o yaml apiVersion: v1 data: client.cnxns: \u0026#34;60\u0026#34; ensemble: zk-0;zk-1;zk-2 init: \u0026#34;10\u0026#34; jvm.heap: 2G purge.interval: \u0026#34;0\u0026#34; snap.retain: \u0026#34;3\u0026#34; sync: \u0026#34;5\u0026#34; tick: \u0026#34;2000\u0026#34; zk StatefulSet 的 template 中的 env 字段读取 ConfigMap 到环境变量中。这些变量将被注入到容器的运行环境里。\nenv: - name : ZK_ENSEMBLE valueFrom: configMapKeyRef: name: zk-config key: ensemble - name : ZK_HEAP_SIZE valueFrom: configMapKeyRef: name: zk-config key: jvm.heap - name : ZK_TICK_TIME valueFrom: configMapKeyRef: name: zk-config key: tick - name : ZK_INIT_LIMIT valueFrom: configMapKeyRef: name: zk-config key: init - name : ZK_SYNC_LIMIT valueFrom: configMapKeyRef: name: zk-config key: tick - name : ZK_MAX_CLIENT_CNXNS valueFrom: configMapKeyRef: name: zk-config key: client.cnxns - name: ZK_SNAP_RETAIN_COUNT valueFrom: configMapKeyRef: name: zk-config key: snap.retain - name: ZK_PURGE_INTERVAL valueFrom: configMapKeyRef: name: zk-config key: purge.interval 在启动 ZooKeeper 服务进程前，容器的入口点调用了一个 bash 脚本：zkGenConfig.sh。这个 bash 脚本从提供的环境变量中生成了 ZooKeeper 的配置文件。\ncommand: - sh - -c - zkGenConfig.sh \u0026amp;\u0026amp; zkServer.sh start-foreground 检查 zk StatefulSet 中所有 Pods 的环境变量。\nfor i in 0 1 2; do kubectl exec zk-$i env | grep ZK_*;echo\u0026#34;\u0026#34;; done 所有从 zk-config 取得的参数都包含完全相同的值。这将允许 zkGenConfig.sh 脚本为 ensemble 中所有的 ZooKeeper 服务创建一致性的配置。\nZK_ENSEMBLE=zk-0;zk-1;zk-2 ZK_HEAP_SIZE=2G ZK_TICK_TIME=2000 ZK_INIT_LIMIT=10 ZK_SYNC_LIMIT=2000 ZK_MAX_CLIENT_CNXNS=60 ZK_SNAP_RETAIN_COUNT=3 ZK_PURGE_INTERVAL=0 ZK_CLIENT_PORT=2181 ZK_SERVER_PORT=2888 ZK_ELECTION_PORT=3888 ZK_USER=zookeeper ZK_DATA_DIR=/var/lib/zookeeper/data ZK_DATA_LOG_DIR=/var/lib/zookeeper/log ZK_LOG_DIR=/var/log/zookeeper ZK_ENSEMBLE=zk-0;zk-1;zk-2 ZK_HEAP_SIZE=2G ZK_TICK_TIME=2000 ZK_INIT_LIMIT=10 ZK_SYNC_LIMIT=2000 ZK_MAX_CLIENT_CNXNS=60 ZK_SNAP_RETAIN_COUNT=3 ZK_PURGE_INTERVAL=0 ZK_CLIENT_PORT=2181 ZK_SERVER_PORT=2888 ZK_ELECTION_PORT=3888 ZK_USER=zookeeper ZK_DATA_DIR=/var/lib/zookeeper/data ZK_DATA_LOG_DIR=/var/lib/zookeeper/log ZK_LOG_DIR=/var/log/zookeeper ZK_ENSEMBLE=zk-0;zk-1;zk-2 ZK_HEAP_SIZE=2G ZK_TICK_TIME=2000 ZK_INIT_LIMIT=10 ZK_SYNC_LIMIT=2000 ZK_MAX_CLIENT_CNXNS=60 ZK_SNAP_RETAIN_COUNT=3 ZK_PURGE_INTERVAL=0 ZK_CLIENT_PORT=2181 ZK_SERVER_PORT=2888 ZK_ELECTION_PORT=3888 ZK_USER=zookeeper ZK_DATA_DIR=/var/lib/zookeeper/data ZK_DATA_LOG_DIR=/var/lib/zookeeper/log ZK_LOG_DIR=/var/log/zookeeper 配置日志 zkGenConfig.sh 脚本产生的一个文件控制了 ZooKeeper 的日志行为。ZooKeeper 使用了 Log4j 并默认使用基于文件大小和时间的滚动文件追加器作为日志配置。 从 zk StatefulSet 的一个 Pods 中获取日志配置。\nkubectl exec zk-0 cat /usr/etc/zookeeper/log4j.properties 下面的日志配置会使 ZooKeeper 进程将其所有的日志写入标志输出文件流中。\nzookeeper.root.logger=CONSOLE zookeeper.console.threshold=INFO log4j.rootLogger=${zookeeper.root.logger} log4j.appender.CONSOLE=org.apache.log4j.ConsoleAppender log4j.appender.CONSOLE.Threshold=${zookeeper.console.threshold} log4j.appender.CONSOLE.layout=org.apache.log4j.PatternLayout log4j.appender.CONSOLE.layout.ConversionPattern=%d{ISO8601} [myid:%X{myid}] - %-5p [%t:%C{1}@%L] - %m%n 这是在容器里安全记录日志的最简单的方法。由于应用的日志被写入标准输出，Kubernetes 将会为你处理日志轮转。Kubernetes 还实现了一个智能保存策略，保证写入标准输出和标准错误流的应用日志不会耗尽本地存储媒介。\n使用 [kubectl logs](/zh/docs/user-guide/kubectl/. param \u0026ldquo;version\u0026rdquo; \u0026gt;}}/#logs) 从一个 Pod 中取回最后几行日志。\nkubectl logs zk-0 --tail 20 使用 kubectl logs 或者从 Kubernetes Dashboard 可以查看写入到标准输出和标准错误流中的应用日志。\n2016-12-06 19:34:16,236 [myid:1] - INFO [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@827] - Processing ruok command from /127.0.0.1:52740 2016-12-06 19:34:16,237 [myid:1] - INFO [Thread-1136:NIOServerCnxn@1008] - Closed socket connection for client /127.0.0.1:52740 (no session established for client) 2016-12-06 19:34:26,155 [myid:1] - INFO [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxnFactory@192] - Accepted socket connection from /127.0.0.1:52749 2016-12-06 19:34:26,155 [myid:1] - INFO [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@827] - Processing ruok command from /127.0.0.1:52749 2016-12-06 19:34:26,156 [myid:1] - INFO [Thread-1137:NIOServerCnxn@1008] - Closed socket connection for client /127.0.0.1:52749 (no session established for client) 2016-12-06 19:34:26,222 [myid:1] - INFO [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxnFactory@192] - Accepted socket connection from /127.0.0.1:52750 2016-12-06 19:34:26,222 [myid:1] - INFO [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@827] - Processing ruok command from /127.0.0.1:52750 2016-12-06 19:34:26,226 [myid:1] - INFO [Thread-1138:NIOServerCnxn@1008] - Closed socket connection for client /127.0.0.1:52750 (no session established for client) 2016-12-06 19:34:36,151 [myid:1] - INFO [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxnFactory@192] - Accepted socket connection from /127.0.0.1:52760 2016-12-06 19:34:36,152 [myid:1] - INFO [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@827] - Processing ruok command from /127.0.0.1:52760 2016-12-06 19:34:36,152 [myid:1] - INFO [Thread-1139:NIOServerCnxn@1008] - Closed socket connection for client /127.0.0.1:52760 (no session established for client) 2016-12-06 19:34:36,230 [myid:1] - INFO [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxnFactory@192] - Accepted socket connection from /127.0.0.1:52761 2016-12-06 19:34:36,231 [myid:1] - INFO [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@827] - Processing ruok command from /127.0.0.1:52761 2016-12-06 19:34:36,231 [myid:1] - INFO [Thread-1140:NIOServerCnxn@1008] - Closed socket connection for client /127.0.0.1:52761 (no session established for client) 2016-12-06 19:34:46,149 [myid:1] - INFO [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxnFactory@192] - Accepted socket connection from /127.0.0.1:52767 2016-12-06 19:34:46,149 [myid:1] - INFO [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@827] - Processing ruok command from /127.0.0.1:52767 2016-12-06 19:34:46,149 [myid:1] - INFO [Thread-1141:NIOServerCnxn@1008] - Closed socket connection for client /127.0.0.1:52767 (no session established for client) 2016-12-06 19:34:46,230 [myid:1] - INFO [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxnFactory@192] - Accepted socket connection from /127.0.0.1:52768 2016-12-06 19:34:46,230 [myid:1] - INFO [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@827] - Processing ruok command from /127.0.0.1:52768 2016-12-06 19:34:46,230 [myid:1] - INFO [Thread-1142:NIOServerCnxn@1008] - Closed socket connection for client /127.0.0.1:52768 (no session established for client) 配置非特权用户 在容器中允许应用以特权用户运行这条最佳实践是值得商讨的。如果你的组织要求应用以非特权用户运行，你可以使用 SecurityContext 控制运行容器入口点的用户。\nzk StatefulSet 的 Pod 的 template 包含了一个 SecurityContext。\nsecurityContext: runAsUser: 1000 fsGroup: 1000 在 Pods 容器内部，UID 1000 对应用户 zookeeper，GID 1000对应用户组 zookeeper。\n从 zk-0 Pod 获取 ZooKeeper 进程信息。\nkubectl exec zk-0 -- ps -elf 由于 securityContext 对象的 runAsUser 字段被设置为1000而不是 root，ZooKeeper进程将以 zookeeper 用户运行。\nF S UID PID PPID C PRI NI ADDR SZ WCHAN STIME TTY TIME CMD 4 S zookeep+ 1 0 0 80 0 - 1127 - 20:46 ? 00:00:00 sh -c zkGenConfig.sh \u0026amp;\u0026amp; zkServer.sh start-foreground 0 S zookeep+ 27 1 0 80 0 - 1155556 - 20:46 ? 00:00:19 /usr/lib/jvm/java-8-openjdk-amd64/bin/java -Dzookeeper.log.dir=/var/log/zookeeper -Dzookeeper.root.logger=INFO,CONSOLE -cp /usr/bin/../build/classes:/usr/bin/../build/lib/*.jar:/usr/bin/../share/zookeeper/zookeeper-3.4.9.jar:/usr/bin/../share/zookeeper/slf4j-log4j12-1.6.1.jar:/usr/bin/../share/zookeeper/slf4j-api-1.6.1.jar:/usr/bin/../share/zookeeper/netty-3.10.5.Final.jar:/usr/bin/../share/zookeeper/log4j-1.2.16.jar:/usr/bin/../share/zookeeper/jline-0.9.94.jar:/usr/bin/../src/java/lib/*.jar:/usr/bin/../etc/zookeeper: -Xmx2G -Xms2G -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.local.only=false org.apache.zookeeper.server.quorum.QuorumPeerMain /usr/bin/../etc/zookeeper/zoo.cfg 默认情况下，当 Pod 的 PersistentVolume 被挂载到 ZooKeeper 服务的数据目录时，它只能被 root 用户访问。这个配置将阻止 ZooKeeper 进程写入它的 WAL 及保存快照。\n在 zk-0 Pod 上获取 ZooKeeper 数据目录的文件权限。\nkubectl exec -ti zk-0 -- ls -ld /var/lib/zookeeper/data 由于 securityContext 对象的 fsGroup 字段设置为1000，Pods 的 PersistentVolumes 的所有权属于 zookeeper 用户组，因而 ZooKeeper 进程能够成功的读写数据。\ndrwxr-sr-x 3 zookeeper zookeeper 4096 Dec 5 20:45 /var/lib/zookeeper/data 管理 ZooKeeper 进程 ZooKeeper documentation 文档指出“你将需要一个监管程序用于管理每个 ZooKeeper 服务进程（JVM）”。在分布式系统中，使用一个看门狗（监管程序）来重启故障进程是一种常用的模式。\n处理进程故障 Restart Policies 控制 Kubernetes 如何处理一个 Pod 中容器入口点的进程故障。对于 StatefulSet 中的 Pods 来说，Always 是唯一合适的 RestartPolicy，这也是默认值。你应该绝不覆盖 stateful 应用的默认策略。\n检查 zk-0 Pod 中运行的 ZooKeeper 服务的进程树。\nkubectl exec zk-0 -- ps -ef 作为容器入口点的命令的 PID 为 1，Zookeeper 进程是入口点的子进程，PID 为23。\nUID PID PPID C STIME TTY TIME CMD zookeep+ 1 0 0 15:03 ? 00:00:00 sh -c zkGenConfig.sh \u0026amp;\u0026amp; zkServer.sh start-foreground zookeep+ 27 1 0 15:03 ? 00:00:03 /usr/lib/jvm/java-8-openjdk-amd64/bin/java -Dzookeeper.log.dir=/var/log/zookeeper -Dzookeeper.root.logger=INFO,CONSOLE -cp /usr/bin/../build/classes:/usr/bin/../build/lib/*.jar:/usr/bin/../share/zookeeper/zookeeper-3.4.9.jar:/usr/bin/../share/zookeeper/slf4j-log4j12-1.6.1.jar:/usr/bin/../share/zookeeper/slf4j-api-1.6.1.jar:/usr/bin/../share/zookeeper/netty-3.10.5.Final.jar:/usr/bin/../share/zookeeper/log4j-1.2.16.jar:/usr/bin/../share/zookeeper/jline-0.9.94.jar:/usr/bin/../src/java/lib/*.jar:/usr/bin/../etc/zookeeper: -Xmx2G -Xms2G -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.local.only=false org.apache.zookeeper.server.quorum.QuorumPeerMain /usr/bin/../etc/zookeeper/zoo.cfg 在一个终端观察 zk StatefulSet 中的 Pods。\nkubectl get pod -w -l app=zk 在另一个终端杀掉 Pod zk-0 中的 ZooKeeper 进程。\nkubectl exec zk-0 -- pkill java ZooKeeper 进程的终结导致了它父进程的终止。由于容器的 RestartPolicy 是 Always，父进程被重启。\nNAME READY STATUS RESTARTS AGE zk-0 1/1 Running 0 21m zk-1 1/1 Running 0 20m zk-2 1/1 Running 0 19m NAME READY STATUS RESTARTS AGE zk-0 0/1 Error 0 29m zk-0 0/1 Running 1 29m zk-0 1/1 Running 1 29m 如果你的应用使用一个脚本（例如 zkServer.sh）来启动一个实现了应用业务逻辑的进程，这个脚本必须和子进程一起结束。这保证了当实现应用业务逻辑的进程故障时，Kubernetes 会重启这个应用的容器。\n你的应用配置为自动重启故障进程，但这对于保持一个分布式系统的健康来说是不够的。许多场景下，一个系统进程可以是活动状态但不响应请求，或者是不健康状态。你应该使用 liveness probes 来通知 Kubernetes 你的应用进程处于不健康状态，需要被重启。\nzk StatefulSet 的 Pod 的 template 一节指定了一个 liveness probe。\nlivenessProbe: exec: command: - \u0026#34;zkOk.sh\u0026#34; initialDelaySeconds: 15 timeoutSeconds: 5 这个探针调用一个简单的 bash 脚本，使用 ZooKeeper 的四字缩写 ruok 来测试服务的健康状态。\nZK_CLIENT_PORT=${ZK_CLIENT_PORT:-2181} OK=$(echo ruok | nc 127.0.0.1 $ZK_CLIENT_PORT) if [ \u0026#34;$OK\u0026#34; == \u0026#34;imok\u0026#34; ]; then exit 0 else exit 1 fi 在一个终端窗口观察 zk StatefulSet 中的 Pods。\nkubectl get pod -w -l app=zk 在另一个窗口中，从 Pod zk-0 的文件系统中删除 zkOk.sh 脚本。\nkubectl exec zk-0 -- rm /opt/zookeeper/bin/zkOk.sh 当 ZooKeeper 进程的 liveness probe 失败时，Kubernetes 将会为你自动重启这个进程，从而保证 ensemble 中不健康状态的进程都被重启。\nkubectl get pod -w -l app=zk NAME READY STATUS RESTARTS AGE zk-0 1/1 Running 0 1h zk-1 1/1 Running 0 1h zk-2 1/1 Running 0 1h NAME READY STATUS RESTARTS AGE zk-0 0/1 Running 0 1h zk-0 0/1 Running 1 1h zk-0 1/1 Running 1 1h 可读性测试 可读性不同于存活性。如果一个进程是存活的，它是可调度和健康的。如果一个进程是就绪的，它应该能够处理输入。存活性是可读性的必要非充分条件。在许多场景下，特别是初始化和终止过程中，一个进程可以是存活但没有就绪。\n如果你指定了一个可读性探针，Kubernetes将保证在可读性检查通过之前，你的应用不会接收到网络流量。\n对于一个 ZooKeeper 服务来说，存活性实现了可读性。因此 zookeeper.yaml 清单中的可读性探针和存活性探针完全相同。\nreadinessProbe: exec: command: - \u0026#34;zkOk.sh\u0026#34; initialDelaySeconds: 15 timeoutSeconds: 5 虽然存活性探针和可读性探针是相同的，但同时指定它们两者仍然重要。这保证了 ZooKeeper ensemble 中唯一健康的服务能够接收网络流量。\n容忍节点故障 ZooKeeper 需要一个服务的 quorum 来成功的提交数据变动。对于一个 3 个服务的 ensemble，必须有两个是健康的写入才能成功。在基于 quorum 的系统里，成员被部署在故障域之间以保证可用性。为了防止由于某台机器断连引起服务中断，最佳实践是防止应用的多个示例在相同的机器上共存。\n默认情况下，Kubernetes 可以把 StatefulSet 的 Pods 部署在相同节点上。对于你创建的 3 个服务的 ensemble 来说，如果有两个服务并存于相同的节点上并且该节点发生故障时，你的 ZooKeeper 服务客户端将不能使用服务，至少一个 Pods 被重新调度后才能恢复。\n你应该总是提供额外的容量以允许关键系统进程在节点故障时能够被重新调度。如果你这样做了，服务故障就只会持续到 Kubernetes 调度器重新调度 ZooKeeper 服务之前。但是，如果希望你的服务在容忍节点故障时无停服时间，你应该设置 podAntiAffinity。\n获取 zk Stateful Set 中的 Pods 的节点。\nfor i in 0 1 2; do kubectl get pod zk-$i --template {{.spec.nodeName}}; echo \u0026#34;\u0026#34;; done zk StatefulSe 中所有的 Pods 都被部署在不同的节点。\nkubernetes-minion-group-cxpk kubernetes-minion-group-a5aq kubernetes-minion-group-2g2d 这是因为 zk StatefulSet 中的 Pods 指定了 PodAntiAffinity。\naffinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: \u0026#34;app\u0026#34; operator: In values: - zk topologyKey: \u0026#34;kubernetes.io/hostname\u0026#34; requiredDuringSchedulingRequiredDuringExecution 告诉 Kubernetes 调度器，在以 topologyKey 指定的域中，绝对不要把带有键为 app，值为 zk 的标签的两个 Pods 调度到相同的节点。topologyKey kubernetes.io/hostname 表示这个域是一个单独的节点。使用不同的 rules、labels 和 selectors，你能够通过这种技术把你的 ensemble 在物理、网络和电力故障域之间分布。\n存活管理 在本节中你将会 cordon 和 drain 节点。如果你是在一个共享的集群里使用本教程，请保证不会影响到其他租户\n上一小节展示了如何在节点之间分散 Pods 以在计划外的节点故障时存活。但是你也需要为计划内维护引起的临时节点故障做准备。\n获取你集群中的节点。\nkubectl get nodes 使用 [kubectl cordon](/zh/docs/user-guide/kubectl/. param \u0026ldquo;version\u0026rdquo; \u0026gt;}}/#cordon) cordon 你的集群中除4个节点以外的所有节点。\nkubectl cordon \u0026lt; node name \u0026gt; 获取 zk-budget PodDisruptionBudget。\nkubectl get poddisruptionbudget zk-budget min-available 字段指示 Kubernetes 在任何时候，zk StatefulSet 至少有两个 Pods 必须是可用的。\nNAME MIN-AVAILABLE ALLOWED-DISRUPTIONS AGE zk-budget 2 1 1h 在一个终端观察 zk StatefulSet 中的 Pods。\nkubectl get pods -w -l app=zk 在另一个终端获取 Pods 当前调度的节点。\nfor i in 0 1 2; do kubectl get pod zk-$i --template {{.spec.nodeName}}; echo \u0026#34;\u0026#34;; done kubernetes-minion-group-pb41 kubernetes-minion-group-ixsl kubernetes-minion-group-i4c4 使用 [kubectl drain](/zh/docs/user-guide/kubectl/. param \u0026ldquo;version\u0026rdquo; \u0026gt;}}/#drain) 来 cordon 和 drain zk-0 Pod 调度的节点。\nkubectl drain $(kubectl get pod zk-0 --template {{.spec.nodeName}}) --ignore-daemonsets --force --delete-local-data node \u0026#34;kubernetes-minion-group-pb41\u0026#34; cordoned WARNING: Deleting pods not managed by ReplicationController, ReplicaSet, Job, or DaemonSet: fluentd-cloud-logging-kubernetes-minion-group-pb41, kube-proxy-kubernetes-minion-group-pb41; Ignoring DaemonSet-managed pods: node-problem-detector-v0.1-o5elz pod \u0026#34;zk-0\u0026#34; deleted node \u0026#34;kubernetes-minion-group-pb41\u0026#34; drained 由于你的集群中有4个节点, kubectl drain 执行成功，`zk-0 被调度到其它节点。\nNAME READY STATUS RESTARTS AGE zk-0 1/1 Running 2 1h zk-1 1/1 Running 0 1h zk-2 1/1 Running 0 1h NAME READY STATUS RESTARTS AGE zk-0 1/1 Terminating 2 2h zk-0 0/1 Terminating 2 2h zk-0 0/1 Terminating 2 2h zk-0 0/1 Terminating 2 2h zk-0 0/1 Pending 0 0s zk-0 0/1 Pending 0 0s zk-0 0/1 ContainerCreating 0 0s zk-0 0/1 Running 0 51s zk-0 1/1 Running 0 1m 在第一个终端持续观察 StatefulSet 的 Pods并 drain zk-1 调度的节点。\nkubectl drain $(kubectl get pod zk-1 --template {{.spec.nodeName}}) --ignore-daemonsets --force --delete-local-data \u0026#34;kubernetes-minion-group-ixsl\u0026#34; cordoned WARNING: Deleting pods not managed by ReplicationController, ReplicaSet, Job, or DaemonSet: fluentd-cloud-logging-kubernetes-minion-group-ixsl, kube-proxy-kubernetes-minion-group-ixsl; Ignoring DaemonSet-managed pods: node-problem-detector-v0.1-voc74 pod \u0026#34;zk-1\u0026#34; deleted node \u0026#34;kubernetes-minion-group-ixsl\u0026#34; drained zk-1 Pod 不能被调度。由于 zk StatefulSet 包含了一个防止 Pods 共存的 PodAntiAffinity 规则，而且只有两个节点可用于调度，这个 Pod 将保持在 Pending 状态。\nkubectl get pods -w -l app=zk NAME READY STATUS RESTARTS AGE zk-0 1/1 Running 2 1h zk-1 1/1 Running 0 1h zk-2 1/1 Running 0 1h NAME READY STATUS RESTARTS AGE zk-0 1/1 Terminating 2 2h zk-0 0/1 Terminating 2 2h zk-0 0/1 Terminating 2 2h zk-0 0/1 Terminating 2 2h zk-0 0/1 Pending 0 0s zk-0 0/1 Pending 0 0s zk-0 0/1 ContainerCreating 0 0s zk-0 0/1 Running 0 51s zk-0 1/1 Running 0 1m zk-1 1/1 Terminating 0 2h zk-1 0/1 Terminating 0 2h zk-1 0/1 Terminating 0 2h zk-1 0/1 Terminating 0 2h zk-1 0/1 Pending 0 0s zk-1 0/1 Pending 0 0s 继续观察 stateful set 的 Pods 并 drain zk-2 调度的节点。\nkubectl drain $(kubectl get pod zk-2 --template {{.spec.nodeName}}) --ignore-daemonsets --force --delete-local-data node \u0026#34;kubernetes-minion-group-i4c4\u0026#34; cordoned WARNING: Deleting pods not managed by ReplicationController, ReplicaSet, Job, or DaemonSet: fluentd-cloud-logging-kubernetes-minion-group-i4c4, kube-proxy-kubernetes-minion-group-i4c4; Ignoring DaemonSet-managed pods: node-problem-detector-v0.1-dyrog WARNING: Ignoring DaemonSet-managed pods: node-problem-detector-v0.1-dyrog; Deleting pods not managed by ReplicationController, ReplicaSet, Job, or DaemonSet: fluentd-cloud-logging-kubernetes-minion-group-i4c4, kube-proxy-kubernetes-minion-group-i4c4 There are pending pods when an error occurred: Cannot evict pod as it would violate the pod\u0026#39;s disruption budget. pod/zk-2 使用 CRTL-C 终止 kubectl。\n你不能 drain 第三个节点，因为删除 zk-2 将和 zk-budget 冲突。然而这个节点仍然保持 cordoned。\n使用 zkCli.sh 从 zk-0 取回你的健康检查中输入的数值。\nkubectl exec zk-0 zkCli.sh get /hello 由于遵守了 PodDisruptionBudget，服务仍然可用。\nWatchedEvent state:SyncConnected type:None path:null world cZxid = 0x200000002 ctime = Wed Dec 07 00:08:59 UTC 2016 mZxid = 0x200000002 mtime = Wed Dec 07 00:08:59 UTC 2016 pZxid = 0x200000002 cversion = 0 dataVersion = 0 aclVersion = 0 ephemeralOwner = 0x0 dataLength = 5 numChildren = 0 使用 [kubectl uncordon](/zh/docs/user-guide/kubectl/. param \u0026ldquo;version\u0026rdquo; \u0026gt;}}/#uncordon) 来取消对第一个节点的隔离。\nkubectl uncordon kubernetes-minion-group-pb41 node \u0026#34;kubernetes-minion-group-pb41\u0026#34; uncordoned zk-1 被重新调度到了这个节点。等待 zk-1 变为 Running 和 Ready 状态。\nkubectl get pods -w -l app=zk NAME READY STATUS RESTARTS AGE zk-0 1/1 Running 2 1h zk-1 1/1 Running 0 1h zk-2 1/1 Running 0 1h NAME READY STATUS RESTARTS AGE zk-0 1/1 Terminating 2 2h zk-0 0/1 Terminating 2 2h zk-0 0/1 Terminating 2 2h zk-0 0/1 Terminating 2 2h zk-0 0/1 Pending 0 0s zk-0 0/1 Pending 0 0s zk-0 0/1 ContainerCreating 0 0s zk-0 0/1 Running 0 51s zk-0 1/1 Running 0 1m zk-1 1/1 Terminating 0 2h zk-1 0/1 Terminating 0 2h zk-1 0/1 Terminating 0 2h zk-1 0/1 Terminating 0 2h zk-1 0/1 Pending 0 0s zk-1 0/1 Pending 0 0s zk-1 0/1 Pending 0 12m zk-1 0/1 ContainerCreating 0 12m zk-1 0/1 Running 0 13m zk-1 1/1 Running 0 13m 尝试 drain zk-2 调度的节点。\nkubectl drain $(kubectl get pod zk-2 --template {{.spec.nodeName}}) --ignore-daemonsets --force --delete-local-data node \u0026#34;kubernetes-minion-group-i4c4\u0026#34; already cordoned WARNING: Deleting pods not managed by ReplicationController, ReplicaSet, Job, or DaemonSet: fluentd-cloud-logging-kubernetes-minion-group-i4c4, kube-proxy-kubernetes-minion-group-i4c4; Ignoring DaemonSet-managed pods: node-problem-detector-v0.1-dyrog pod \u0026#34;heapster-v1.2.0-2604621511-wht1r\u0026#34; deleted pod \u0026#34;zk-2\u0026#34; deleted node \u0026#34;kubernetes-minion-group-i4c4\u0026#34; drained 这次 kubectl drain 执行成功。\nUncordon 第二个节点以允许 zk-2 被重新调度。\nkubectl uncordon kubernetes-minion-group-ixsl node \u0026#34;kubernetes-minion-group-ixsl\u0026#34; uncordoned 你可以同时使用 kubectl drain 和 PodDisruptionBudgets 来保证你的服务在维护过程中仍然可用。如果使用 drain 来隔离节点并在此之前删除 pods 使节点进入离线维护状态，如果服务表达了 disruption budget，这个 budget 将被遵守。你应该总是为关键服务分配额外容量，这样它们的 Pods 就能够迅速的重新调度。\n. heading \u0026ldquo;cleanup\u0026rdquo; %}}  使用 kubectl uncordon 解除你集群中所有节点的隔离。 你需要删除在本教程中使用的 PersistentVolumes 的持久存储媒介。请遵循必须的步骤，基于你的环境、存储配置和准备方法，保证回收所有的存储。  "
},
{
	"uri": "https://lijun.in/tasks/run-application/run-single-instance-stateful-application/",
	"title": "运行一个单实例有状态应用",
	"tags": [],
	"description": "",
	"content": "本文介绍在 Kubernetes 中使用 PersistentVolume 和 Deployment 如何运行一个单实例有状态应用. 该应用是 MySQL.\n. heading \u0026ldquo;objectives\u0026rdquo; %}}  在环境中通过磁盘创建一个PersistentVolume. 创建一个MySQL Deployment. 在集群内以一个已知的 DNS 名将 MySQL 暴露给其他 pods.  . heading \u0026ldquo;prerequisites\u0026rdquo; %}}   . include \u0026ldquo;task-tutorial-prereqs.md\u0026rdquo; \u0026gt;}} . version-check \u0026gt;}}\n  . include \u0026ldquo;default-storage-class-prereqs.md\u0026rdquo; \u0026gt;}}\n  部署MySQL 注意: 在配置的 yaml 文件中定义密码的做法是不安全的. 具体安全解决方案请参考 Kubernetes Secrets.\n. codenew file=\u0026quot;application/mysql/mysql-deployment.yaml\u0026rdquo; \u0026gt;}} . codenew file=\u0026quot;application/mysql/mysql-pv.yaml\u0026rdquo; \u0026gt;}}\n  部署 YAML 文件中定义的 PV 和 PVC：\n kubectl apply -f https://k8s.io/examples/application/mysql/mysql-pv.yaml    部署 YAML 文件中定义的 Deployment：\n kubectl apply -f https://k8s.io/examples/application/mysql/mysql-deployment.yaml    展示 Deployment 相关信息:\n kubectl describe deployment mysql Name: mysql Namespace: default CreationTimestamp: Tue, 01 Nov 2016 11:18:45 -0700 Labels: app=mysql Annotations: deployment.kubernetes.io/revision=1 Selector: app=mysql Replicas: 1 desired | 1 updated | 1 total | 0 available | 1 unavailable StrategyType: Recreate MinReadySeconds: 0 Pod Template: Labels: app=mysql Containers: mysql: Image: mysql:5.6 Port: 3306/TCP Environment: MYSQL_ROOT_PASSWORD: password Mounts: /var/lib/mysql from mysql-persistent-storage (rw) Volumes: mysql-persistent-storage: Type: PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace) ClaimName: mysql-pv-claim ReadOnly: false Conditions: Type Status Reason ---- ------ ------ Available False MinimumReplicasUnavailable Progressing True ReplicaSetUpdated OldReplicaSets: \u0026lt;none\u0026gt; NewReplicaSet: mysql-63082529 (1/1 replicas created) Events: FirstSeen LastSeen Count From SubobjectPath Type Reason Message --------- -------- ----- ---- ------------- -------- ------ ------- 33s 33s 1 {deployment-controller } Normal ScalingReplicaSet Scaled up replica set mysql-63082529 to 1    列举出 Deployment 创建的 pods:\n kubectl get pods -l app=mysql NAME READY STATUS RESTARTS AGE mysql-63082529-2z3ki 1/1 Running 0 3m    查看 PersistentVolumeClaim:\n kubectl describe pvc mysql-pv-claim Name: mysql-pv-claim Namespace: default StorageClass: Status: Bound Volume: mysql-pv-volume Labels: \u0026lt;none\u0026gt; Annotations: pv.kubernetes.io/bind-completed=yes pv.kubernetes.io/bound-by-controller=yes Capacity: 20Gi Access Modes: RWO Events: \u0026lt;none\u0026gt;    访问 MySQL 实例 前面 YAML 文件中创建了一个允许集群内其他 pods 访问数据库的服务. 该服务中选项 clusterIP: None 让服务 DNS 名称直接解析为 Pod 的 IP 地址. 当在一个服务下只有一个 pod 并且不打算增加 pods 的数量这是最好的.\n运行 MySQL 客户端以连接到服务器:\nkubectl run -it --rm --image=mysql:5.6 --restart=Never mysql-client -- mysql -h mysql -ppassword 此命令在集群内创建一个新的 Pod 并运行 MySQL 客户端,并通过 Service 将其连接到服务器.如果连接成功,你就知道有状态的 MySQL 数据库正处于运行状态.\nWaiting for pod default/mysql-client-274442439-zyp6i to be running, status is Pending, pod ready: false If you don't see a command prompt, try pressing enter. mysql\u0026gt; 更新 Deployment 中镜像或其他部分同往常一样可以通过 kubectl apply 命令更新. 以下是 特定于有状态应用的一些注意事项:\n 不要弹性伸缩. 弹性伸缩仅适用于单实例应用. 下层的 PersistentVolume 仅只能挂载一个 pod. 对于集群级有状态应用, 请参考 StatefulSet 文档 StatefulSet documentation. 在 Deployment 的 YAML 文件中使用 strategy: type: Recreate . 该选项指示 Kubernetes 不使用滚动升级. 滚动升级将无法工作, 由于一次不能运行多个 pod. 在更新配置文件 创建一个新的 pod 前 Recreate 策略将先停止第一个 pod.  删除 Deployment 通过名称删除部署的对象:\nkubectl delete deployment,svc mysql kubectl delete pvc mysql-pv-claim kubectl delete pv mysql-pv-volume 如果通过手动的方式分配 PersistentVolume, 那么也需要手动的删除它，以及释放下层资源. 如果是用过动态分配 PersistentVolume 的方式，在删除 PersistentVolumeClaim 后 PersistentVolume 将被自动的删除. 一些存储服务(比如 EBS 和 PD)也会在 PersistentVolume 被删除时自动回收下层资源.\n. heading \u0026ldquo;whatsnext\u0026rdquo; %}}   了解更多 Deployment 对象请参考 Deployment objects.\n  了解更多 Deployment 应用请参考 Deploying applications\n  kubectl run 文档请参考kubectl run documentation\n  卷和持久卷请参考 Volumes 和 Persistent Volumes\n  "
},
{
	"uri": "https://lijun.in/reference/kubectl/docker-cli-to-kubectl/",
	"title": "适用于 Docker 用户的 kubectl",
	"tags": [],
	"description": "",
	"content": "您可以使用 Kubernetes 命令行工具 kubectl 与 API 服务器进行交互。如果您熟悉 Docker 命令行工具，则使用 kubectl 非常简单。但是，docker 命令和 kubectl 命令之间有一些区别。以下显示了 docker 子命令，并描述了等效的 kubectl 命令。\ndocker run 要运行 nginx 部署并将其暴露，请参见 kubectl 运行。\ndocker:\ndocker run -d --restart=always -e DOMAIN=cluster --name nginx-app -p 80:80 nginx 55c103fa129692154a7652490236fee9be47d70a8dd562281ae7d2f9a339a6db docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 55c103fa1296 nginx \u0026quot;nginx -g 'daemon of…\u0026quot; 9 seconds ago Up 9 seconds 0.0.0.0:80-\u0026gt;80/tcp nginx-app kubectl:\n# 启动运行 nginx 的 Pod kubectl run --image=nginx nginx-app --port=80 --env=\u0026#34;DOMAIN=cluster\u0026#34; deployment \u0026quot;nginx-app\u0026quot; created note \u0026gt;}}\nkubectl 命令打印创建或突变资源的类型和名称，然后可以在后续命令中使用。部署后，您可以公开新服务。 /note \u0026gt;}}\n# 通过服务公开端口 kubectl expose deployment nginx-app --port=80 --name=nginx-http service \u0026quot;nginx-http\u0026quot; exposed 在 kubectl 命令中，我们创建了一个 Deployment，这将保证有 N 个运行 nginx 的 pod(N 代表 spec 中声明的 replica 数，默认为 1)。我们还创建了一个 service，其选择器与容器标签匹配。查看使用服务访问群集中的应用程序 获取更多信息。\n默认情况下镜像会在后台运行，与 docker run -d ... 类似，如果您想在前台运行，使用:\nkubectl run [-i] [--tty] --attach \u0026lt;name\u0026gt; --image=\u0026lt;image\u0026gt; 与 docker run ... 不同的是，如果指定了 --attach ，我们将连接到 stdin，stdout 和 stderr，而不能控制具体连接到哪个输出流（docker -a ...）。要从容器中退出，可以输入 Ctrl + P，然后按 Ctrl + Q。\n因为我们使用 Deployment 启动了容器，如果您终止连接到的进程（例如 ctrl-c），容器将会重启，这跟 docker run -it 不同。 如果想销毁该 Deployment（和它的 pod），您需要运行 kubectl delete deployment \u0026lt;name\u0026gt;。\ndocker ps 如何列出哪些正在运行？查看 kubectl get。\n使用 docker 命令：\ndocker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 14636241935f ubuntu:16.04 \u0026quot;echo test\u0026quot; 5 seconds ago Exited (0) 5 seconds ago cocky_fermi 55c103fa1296 nginx \u0026quot;nginx -g 'daemon of…\u0026quot; About a minute ago Up About a minute 0.0.0.0:80-\u0026gt;80/tcp nginx-app 使用 kubectl 命令：\nkubectl get po NAME READY STATUS RESTARTS AGE nginx-app-8df569cb7-4gd89 1/1 Running 0 3m ubuntu 0/1 Completed 0 20s docker attach 如何连接到已经运行在容器中的进程？查看 kubectl attach。\n使用 docker 命令：\ndocker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 55c103fa1296 nginx \u0026quot;nginx -g 'daemon of…\u0026quot; 5 minutes ago Up 5 minutes 0.0.0.0:80-\u0026gt;80/tcp nginx-app docker attach 55c103fa1296 ... kubectl:\nkubectl get pods NAME READY STATUS RESTARTS AGE nginx-app-5jyvm 1/1 Running 0 10m kubectl attach -it nginx-app-5jyvm ... 要从容器中分离，可以输入 Ctrl + P，然后按 Ctrl + Q。\ndocker exec 如何在容器中执行命令？查看 kubectl exec。\n使用 docker 命令：\ndocker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 55c103fa1296 nginx \u0026quot;nginx -g 'daemon of…\u0026quot; 6 minutes ago Up 6 minutes 0.0.0.0:80-\u0026gt;80/tcp nginx-app docker exec 55c103fa1296 cat /etc/hostname 55c103fa1296 使用 kubectl 命令：\nkubectl get po NAME READY STATUS RESTARTS AGE nginx-app-5jyvm 1/1 Running 0 10m kubectl exec nginx-app-5jyvm -- cat /etc/hostname nginx-app-5jyvm 执行交互式命令怎么办？\n使用 docker 命令：\ndocker exec -ti 55c103fa1296 /bin/sh # exit kubectl:\nkubectl exec -ti nginx-app-5jyvm -- /bin/sh # exit 更多信息请查看获取运行中容器的 Shell 环境。\ndocker logs 如何查看运行中进程的 stdout/stderr？查看 kubectl logs。\n使用 docker 命令：\ndocker logs -f a9e 192.168.9.1 - - [14/Jul/2015:01:04:02 +0000] \u0026quot;GET / HTTP/1.1\u0026quot; 200 612 \u0026quot;-\u0026quot; \u0026quot;curl/7.35.0\u0026quot; \u0026quot;-\u0026quot; 192.168.9.1 - - [14/Jul/2015:01:04:03 +0000] \u0026quot;GET / HTTP/1.1\u0026quot; 200 612 \u0026quot;-\u0026quot; \u0026quot;curl/7.35.0\u0026quot; \u0026quot;-\u0026quot; 使用 kubectl 命令：\nkubectl logs -f nginx-app-zibvs 10.240.63.110 - - [14/Jul/2015:01:09:01 +0000] \u0026quot;GET / HTTP/1.1\u0026quot; 200 612 \u0026quot;-\u0026quot; \u0026quot;curl/7.26.0\u0026quot; \u0026quot;-\u0026quot; 10.240.63.110 - - [14/Jul/2015:01:09:02 +0000] \u0026quot;GET / HTTP/1.1\u0026quot; 200 612 \u0026quot;-\u0026quot; \u0026quot;curl/7.26.0\u0026quot; \u0026quot;-\u0026quot; 现在是时候提一下 pod 和容器之间的细微差别了；默认情况下如果 pod 中的进程退出 pod 也不会终止，相反它将会重启该进程。这类似于 docker run 时的 --restart=always 选项， 这是主要差别。在 docker 中，进程的每个调用的输出都是被连接起来的，但是对于 kubernetes，每个调用都是分开的。要查看以前在 kubernetes 中执行的输出，请执行以下操作：\nkubectl logs --previous nginx-app-zibvs 10.240.63.110 - - [14/Jul/2015:01:09:01 +0000] \u0026quot;GET / HTTP/1.1\u0026quot; 200 612 \u0026quot;-\u0026quot; \u0026quot;curl/7.26.0\u0026quot; \u0026quot;-\u0026quot; 10.240.63.110 - - [14/Jul/2015:01:09:02 +0000] \u0026quot;GET / HTTP/1.1\u0026quot; 200 612 \u0026quot;-\u0026quot; \u0026quot;curl/7.26.0\u0026quot; \u0026quot;-\u0026quot; 查看日志架构获取更多信息。\ndocker stop and docker rm 如何停止和删除运行中的进程？查看 kubectl delete。\n使用 docker 命令：\ndocker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES a9ec34d98787 nginx \u0026quot;nginx -g 'daemon of\u0026quot; 22 hours ago Up 22 hours 0.0.0.0:80-\u0026gt;80/tcp, 443/tcp nginx-app docker stop a9ec34d98787 a9ec34d98787 docker rm a9ec34d98787 a9ec34d98787 使用 kubectl 命令：\nkubectl get deployment nginx-app NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE nginx-app 1 1 1 1 2m kubectl get po -l run=nginx-app NAME READY STATUS RESTARTS AGE nginx-app-2883164633-aklf7 1/1 Running 0 2m kubectl delete deployment nginx-app deployment \u0026quot;nginx-app\u0026quot; deleted kubectl get po -l run=nginx-app # Return nothing note \u0026gt;}}\n请注意，我们不直接删除 pod。使用 kubectl 命令，我们要删除拥有该 pod 的 Deployment。如果我们直接删除 pod，Deployment 将会重新创建该 pod。 /note \u0026gt;}}\ndocker login 在 kubectl 中没有对 docker login 的直接模拟。如果您有兴趣在私有镜像仓库中使用 Kubernetes，请参阅使用私有镜像仓库。\ndocker version 如何查看客户端和服务端的版本？查看 kubectl version。\n使用 docker 命令：\ndocker version Client version: 1.7.0 Client API version: 1.19 Go version (client): go1.4.2 Git commit (client): 0baf609 OS/Arch (client): linux/amd64 Server version: 1.7.0 Server API version: 1.19 Go version (server): go1.4.2 Git commit (server): 0baf609 OS/Arch (server): linux/amd64 使用 kubectl 命令：\nkubectl version Client Version: version.Info{Major:\u0026quot;1\u0026quot;, Minor:\u0026quot;6\u0026quot;, GitVersion:\u0026quot;v1.6.9+a3d1dfa6f4335\u0026quot;, GitCommit:\u0026quot;9b77fed11a9843ce3780f70dd251e92901c43072\u0026quot;, GitTreeState:\u0026quot;dirty\u0026quot;, BuildDate:\u0026quot;2017-08-29T20:32:58Z\u0026quot;, OpenPaasKubernetesVersion:\u0026quot;v1.03.02\u0026quot;, GoVersion:\u0026quot;go1.7.5\u0026quot;, Compiler:\u0026quot;gc\u0026quot;, Platform:\u0026quot;linux/amd64\u0026quot;} Server Version: version.Info{Major:\u0026quot;1\u0026quot;, Minor:\u0026quot;6\u0026quot;, GitVersion:\u0026quot;v1.6.9+a3d1dfa6f4335\u0026quot;, GitCommit:\u0026quot;9b77fed11a9843ce3780f70dd251e92901c43072\u0026quot;, GitTreeState:\u0026quot;dirty\u0026quot;, BuildDate:\u0026quot;2017-08-29T20:32:58Z\u0026quot;, OpenPaasKubernetesVersion:\u0026quot;v1.03.02\u0026quot;, GoVersion:\u0026quot;go1.7.5\u0026quot;, Compiler:\u0026quot;gc\u0026quot;, Platform:\u0026quot;linux/amd64\u0026quot;} docker info 如何获取有关环境和配置的各种信息？查看 kubectl cluster-info。\n使用 docker 命令：\ndocker info Containers: 40 Images: 168 Storage Driver: aufs Root Dir: /usr/local/google/docker/aufs Backing Filesystem: extfs Dirs: 248 Dirperm1 Supported: false Execution Driver: native-0.2 Logging Driver: json-file Kernel Version: 3.13.0-53-generic Operating System: Ubuntu 14.04.2 LTS CPUs: 12 Total Memory: 31.32 GiB Name: k8s-is-fun.mtv.corp.google.com ID: ADUV:GCYR:B3VJ:HMPO:LNPQ:KD5S:YKFQ:76VN:IANZ:7TFV:ZBF4:BYJO WARNING: No swap limit support 使用 kubectl 命令：\nkubectl cluster-info Kubernetes master is running at https://108.59.85.141 KubeDNS is running at https://108.59.85.141/api/v1/namespaces/kube-system/services/kube-dns/proxy kubernetes-dashboard is running at https://108.59.85.141/api/v1/namespaces/kube-system/services/kubernetes-dashboard/proxy Grafana is running at https://108.59.85.141/api/v1/namespaces/kube-system/services/monitoring-grafana/proxy Heapster is running at https://108.59.85.141/api/v1/namespaces/kube-system/services/monitoring-heapster/proxy InfluxDB is running at https://108.59.85.141/api/v1/namespaces/kube-system/services/monitoring-influxdb/proxy "
},
{
	"uri": "https://lijun.in/tasks/administer-cluster/namespaces/",
	"title": "通过命名空间共享集群",
	"tags": [],
	"description": "",
	"content": "本页展示了如何查看、使用和删除. glossary_tooltip text=\u0026quot;namespaces\u0026rdquo; term_id=\u0026quot;namespace\u0026rdquo; \u0026gt;}}。本页同时展示了如何使用 Kubernetes 命名空间去细分集群。\n. heading \u0026ldquo;prerequisites\u0026rdquo; %}}  您已拥有一个 配置好的 Kubernetes 集群. 您已对 Kubernetes 的 Pods, Services, 和 Deployments 有基本理解。  查看命名空间  列出集群中现有的命名空间：  kubectl get namespaces NAME STATUS AGE default Active 11d kube-system Active 11d kube-public Active 11d 初始状态下，Kubernetes 具有三个名字空间：\n default 无命名空间对象的默认命名空间 kube-system 由 Kubernetes 系统创建的对象的命名空间 kube-public 自动创建且被所有用户可读的命名空间（包括未经身份认证的）。此命名空间通常在某些资源在整个集群中可见且可公开读取时被集群使用。此命名空间的公共方面只是一个约定，而不是一个必要条件。  您还可以通过下列命令获取特定命名空间的摘要：\nkubectl get namespaces \u0026lt;name\u0026gt; 或获取详细信息：\nkubectl describe namespaces \u0026lt;name\u0026gt; Name: default Labels: \u0026lt;none\u0026gt; Annotations: \u0026lt;none\u0026gt; Status: Active No resource quota. Resource Limits Type Resource Min Max Default ---- -------- --- --- --- Container cpu - - 100m 请注意，这些详情同时显示了资源配额（如果存在）以及资源限制区间。\n资源配额跟踪并聚合 Namespace 中资源的使用情况，并允许集群运营者定义 Namespace 可能消耗的 Hard 资源使用限制。\n限制区间定义了单个实体在一个 Namespace 中可使用的最小/最大资源量约束。\n参阅 准入控制: 限制区间\n命名空间可以处于下列两个阶段中的一个:\n Active 命名空间使用中 Terminating 命名空间正在被删除，且不能被用于新对象。  参见 设计文档 查看更多细节。\n创建命名空间  新建一个名为 my-namespace.yaml 的 YAML 文件，并写入下列内容：  apiVersion: v1 kind: Namespace metadata: name: \u0026lt;insert-namespace-name-here\u0026gt; 然后运行：\nkubectl create -f ./my-namespace.yaml  或者，你可以使用下面的命令创建命名空间：\nkubectl create namespace \u0026lt;insert-namespace-name-here\u0026gt;   请注意，命名空间的名称必须是 DNS 兼容的标签。\n可选字段 finalizers 允许观察者们在命名空间被删除时清除资源。记住如果指定了一个不存在的终结器，命名空间仍会被创建，但如果用户试图删除它，它将陷入 Terminating 状态。\n更多有关 finalizers 的信息请查阅 设计文档 中命名空间部分。\n删除命名空间  删除命名空间使用命令  kubectl delete namespaces \u0026lt;insert-some-namespace-name\u0026gt; . warning \u0026gt;}}\n这会删除命名空间下的 所有内容 ！\n. /warning \u0026gt;}}\n删除是异步的,所以有一段时间你会看到命名空间处于 Terminating 状态。\n使用 Kubernetes 命名空间细分您的集群  理解默认命名空间  默认情况下，Kubernetes 集群会在配置集群时实例化一个默认命名空间，用以存放集群所使用的默认 Pods、Services 和 Deployments 集合。\n假设您有一个新的集群，您可以通过执行以下操作来内省可用的命名空间\nkubectl get namespaces NAME STATUS AGE default Active 13m 创建新的命名空间  在本练习中，我们将创建两个额外的 Kubernetes 命名空间来保存我们的内容。\n在某组织使用共享的 Kubernetes 集群进行开发和生产的场景中：\n开发团队希望在集群中维护一个空间，以便他们可以查看用于构建和运行其应用程序的 Pods、Services 和 Deployments 列表。在这个空间里，Kubernetes 资源被自由地加入或移除，对谁能够或不能修改资源的限制被放宽，以实现敏捷开发。\n运维团队希望在集群中维护一个空间，以便他们可以强制实施一些严格的规程，对谁可以或不可以操作运行生产站点的 Pods、Services 和 Deployments 集合进行控制。\n该组织可以遵循的一种模式是将 Kubernetes 集群划分为两个命名空间：development 和 production。\n让我们创建两个新的命名空间来保存我们的工作。\n文件 namespace-dev.json 描述了 development 命名空间:\n. codenew language=\u0026quot;json\u0026rdquo; file=\u0026quot;admin/namespace-dev.json\u0026rdquo; \u0026gt;}}\n使用 kubectl 创建 development 命名空间。\nkubectl create -f https://k8s.io/examples/admin/namespace-dev.json 让我们使用 kubectl 创建 production 命名空间。\nkubectl create -f https://k8s.io/examples/admin/namespace-prod.json 为了确保一切正常，列出集群中的所有命名空间。\nkubectl get namespaces --show-labels NAME STATUS AGE LABELS default Active 32m \u0026lt;none\u0026gt; development Active 29s name=development production Active 23s name=production 在每个命名空间中创建 pod  Kubernetes 命名空间为集群中的 Pods、Services 和 Deployments 提供了作用域。\n与一个命名空间交互的用户不会看到另一个命名空间中的内容。\n为了演示这一点，让我们在 development 命名空间中启动一个简单的 Deployment 和 Pod。\n我们首先检查一下当前的上下文：\nkubectl config view apiVersion: v1 clusters: - cluster: certificate-authority-data: REDACTED server: https://130.211.122.180 name: lithe-cocoa-92103_kubernetes contexts: - context: cluster: lithe-cocoa-92103_kubernetes user: lithe-cocoa-92103_kubernetes name: lithe-cocoa-92103_kubernetes current-context: lithe-cocoa-92103_kubernetes kind: Config preferences: {} users: - name: lithe-cocoa-92103_kubernetes user: client-certificate-data: REDACTED client-key-data: REDACTED token: 65rZW78y8HbwXXtSXuUw9DbP4FLjHi4b - name: lithe-cocoa-92103_kubernetes-basic-auth user: password: h5M0FtUUIflBSdI7 username: admin kubectl config current-context lithe-cocoa-92103_kubernetes 下一步是为 kubectl 客户端定义一个上下文，以便在每个命名空间中工作。\u0026ldquo;cluster\u0026rdquo; 和 \u0026ldquo;user\u0026rdquo; 字段的值将从当前上下文中复制。\nkubectl config set-context dev --namespace=development --cluster=lithe-cocoa-92103_kubernetes --user=lithe-cocoa-92103_kubernetes kubectl config set-context prod --namespace=production --cluster=lithe-cocoa-92103_kubernetes --user=lithe-cocoa-92103_kubernetes 上述命令提供了两个可以替代的请求上下文，具体取决于您希望使用的命名空间。\n让我们切换到 development 命名空间进行操作。\nkubectl config use-context dev 您可以使用下列命令验证当前上下文：\nkubectl config current-context dev 此时，我们从命令行向 Kubernetes 集群发出的所有请求都限定在 development 命名空间中。\n让我们创建一些内容。\nkubectl run snowflake --image=k8s.gcr.io/serve_hostname --replicas=2 我们刚刚创建了一个副本大小为 2 的 deployment，该 deployment 运行名为 snowflake 的 pod，其中包含一个仅提供主机名服务的基本容器。请注意，kubectl run 仅在 Kubernetes 集群版本 \u0026gt;= v1.2 时创建 deployment。如果您运行在旧版本上，则会创建 replication controllers。如果期望执行旧版本的行为，请使用 --generator=run/v1 创建 replication controllers。 参见 kubectl run 获取更多细节。\nkubectl get deployment NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE snowflake 2 2 2 2 2m kubectl get pods -l run=snowflake NAME READY STATUS RESTARTS AGE snowflake-3968820950-9dgr8 1/1 Running 0 2m snowflake-3968820950-vgc4n 1/1 Running 0 2m 这很棒，开发人员可以做他们想要的事情，而不必担心影响 production 命名空间中的内容。\n让我们切换到 production 命名空间，展示一个命名空间中的资源如何对另一个命名空间不可见。\nkubectl config use-context prod production 命名空间应该是空的，下列命令应该返回的内容为空。\nkubectl get deployment kubectl get pods 生产环境需要运行 cattle，让我们创建一些名为 cattle 的 pods。\nkubectl run cattle --image=k8s.gcr.io/serve_hostname --replicas=5 kubectl get deployment NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE cattle 5 5 5 5 10s kubectl get pods -l run=cattle NAME READY STATUS RESTARTS AGE cattle-2263376956-41xy6 1/1 Running 0 34s cattle-2263376956-kw466 1/1 Running 0 34s cattle-2263376956-n4v97 1/1 Running 0 34s cattle-2263376956-p5p3i 1/1 Running 0 34s cattle-2263376956-sxpth 1/1 Running 0 34s 此时，应该很清楚的展示了用户在一个命名空间中创建的资源对另一个命名空间是隐藏的。\n随着 Kubernetes 中的策略支持的发展，我们将扩展此场景，以展示如何为每个命名空间提供不同的授权规则。\n理解使用命名空间的动机 单个集群应该能满足多个用户及用户组的需求（以下称为 “用户社区”）。\nKubernetes 命名空间 帮助不同的项目、团队或客户去共享 Kubernetes 集群。\n名字空间通过以下方式实现这点：\n 为名字设置作用域. 为集群中的部分资源关联鉴权和策略的机制。  使用多个命名空间是可选的。\n每个用户社区都希望能够与其他社区隔离开展工作。\n每个用户社区都有:\n 资源（pods, services, replication controllers, 等等） 策略（谁能或不能在他们的社区里执行操作） 约束（该社区允许多少配额，等等）  集群运营者可以为每个唯一用户社区创建命名空间。\n命名空间为下列内容提供唯一的作用域：\n 命名资源（避免基本的命名冲突） 将管理权限委派给可信用户 限制社区资源消耗的能力  用例包括:\n 作为集群运营者, 我希望能在单个集群上支持多个用户社区。 作为集群运营者，我希望将集群分区的权限委派给这些社区中的受信任用户。 作为集群运营者，我希望能限定每个用户社区可使用的资源量，以限制对使用同一集群的其他用户社区的影响。 作为群集用户，我希望与我的用户社区相关的资源进行交互，而与其他用户社区在该集群上执行的操作无关。  理解命名空间和 DNS 当您创建 Service 时，它会创建相应的 DNS 条目。此条目的格式为 \u0026lt;service-name\u0026gt;。\u0026lt;namespace-name\u0026gt; .svc.cluster.local，这意味着如果容器只使用 \u0026lt;service-name\u0026gt;，它将解析为本地服务到命名空间。 这对于在多个命名空间（如开发，暂存和生产）中使用相同的配置非常有用。 如果要跨命名空间访问，则需要使用完全限定的域名（FQDN）。\n. heading \u0026ldquo;whatsnext\u0026rdquo; %}}  了解更多 设置命名空间首选项 的内容。 了解更多 设置请求的命名空间 的内容。 参见 [命名空间设计](https://github.com/kubernetes/community/blob/. param \u0026ldquo;githubbranch\u0026rdquo; \u0026gt;}}/contributors/design-proposals/architecture/namespaces.md)。  "
},
{
	"uri": "https://lijun.in/tasks/inject-data-application/downward-api-volume-expose-pod-information/",
	"title": "通过文件将Pod信息呈现给容器",
	"tags": [],
	"description": "",
	"content": "此页面描述Pod如何使用DownwardAPIVolumeFile把自己的信息呈现给pod中运行的容器。DownwardAPIVolumeFile可以呈现pod的字段和容器字段。\n. heading \u0026ldquo;prerequisites\u0026rdquo; %}} . include \u0026ldquo;task-tutorial-prereqs.md\u0026rdquo; \u0026gt;}} . version-check \u0026gt;}}\nDownward API 有两种方式可以将Pod和Container字段呈现给运行中的容器：\n 环境变量 DownwardAPIVolumeFile  这两种呈现Pod和Container字段的方式都称为Downward API。\n存储Pod字段 在这个练习中，你将创建一个包含一个容器的pod。这是该pod的配置文件：\n. codenew file=\u0026quot;pods/inject/dapi-volume.yaml\u0026rdquo; \u0026gt;}}\n在配置文件中，你可以看到Pod有一个downwardAPI类型的Volume，并且挂载到容器中的/etc。\n查看downwardAPI下面的items数组。每个数组元素都是一个[DownwardAPIVolumeFile](/docs/resources-reference/. param \u0026ldquo;version\u0026rdquo; \u0026gt;}}/#downwardapivolumefile-v1-core)。 第一个元素指示Pod的metadata.labels字段的值保存在名为labels的文件中。 第二个元素指示Pod的annotations字段的值保存在名为annotations的文件中。\n. note \u0026gt;}} 本示例中的字段是Pod字段，不是Pod中容器的字段。 . /note \u0026gt;}}\n创建 Pod：\nkubectl apply -f https://k8s.io/examples/pods/inject/dapi-volume.yaml 验证Pod中的容器运行正常：\nkubectl get pods 查看容器的日志：\nkubectl logs kubernetes-downwardapi-volume-example 输出显示 labels 和 annotations 文件的内容：\ncluster=\u0026#34;test-cluster1\u0026#34; rack=\u0026#34;rack-22\u0026#34; zone=\u0026#34;us-est-coast\u0026#34; build=\u0026#34;two\u0026#34; builder=\u0026#34;john-doe\u0026#34; 进入Pod中运行的容器，打开一个shell：\nkubectl exec -it kubernetes-downwardapi-volume-example -- sh 在该shell中，查看labels文件：\n/# cat /etc/labels 输出显示Pod的所有labels都已写入labels文件。\ncluster=\u0026#34;test-cluster1\u0026#34; rack=\u0026#34;rack-22\u0026#34; zone=\u0026#34;us-est-coast\u0026#34; 同样，查看annotations文件：\n/# cat /etc/annotations 查看/etc目录下的文件：\n/# ls -laR /etc 在输出中可以看到，labels 和 annotations文件都在一个临时子目录中：这个例子，..2019_12_05_07_00_34.813117769。在/etc目录中，..data是一个指向临时子目录 的符号链接。/etc目录中，labels 和 annotations也是符号链接。\n/etc/podinfo # ls -alRL .: total 8 drwxrwxrwt 3 root root 120 Dec 5 07:00 . drwxr-xr-x 1 root root 21 Dec 5 07:00 .. drwxr-xr-x 2 root root 80 Dec 5 07:00 ..2019_12_05_07_00_34.813117769 drwxr-xr-x 2 root root 80 Dec 5 07:00 ..data -rw-r--r-- 1 root root 1123 Dec 5 07:00 annotations -rw-r--r-- 1 root root 39 Dec 5 07:00 labels ./..2019_12_05_07_00_34.813117769: total 8 drwxr-xr-x 2 root root 80 Dec 5 07:00 . drwxrwxrwt 3 root root 120 Dec 5 07:00 .. -rw-r--r-- 1 root root 1123 Dec 5 07:00 annotations -rw-r--r-- 1 root root 39 Dec 5 07:00 labels ./..data: total 8 drwxr-xr-x 2 root root 80 Dec 5 07:00 . drwxrwxrwt 3 root root 120 Dec 5 07:00 .. -rw-r--r-- 1 root root 1123 Dec 5 07:00 annotations -rw-r--r-- 1 root root 39 Dec 5 07:00 labels 用符号链接可实现元数据的动态原子刷新；更新将写入一个新的临时目录，然后..data符号链接完成原子更新，通过使用rename(2)。\n退出shell：\n/# exit 存储容器字段 前面的练习中，你将Pod字段保存到DownwardAPIVolumeFile中。接下来这个练习，你将存储容器字段。这里是包含一个容器的pod的配置文件：\n. codenew file=\u0026quot;pods/inject/dapi-volume-resources.yaml\u0026rdquo; \u0026gt;}}\n在这个配置文件中，你可以看到Pod有一个downwardAPI类型的Volume,并且挂载到容器的/etc目录。\n查看downwardAPI下面的items数组。每个数组元素都是一个DownwardAPIVolumeFile。\n第一个元素指定名为client-container的容器中limits.cpu字段的值应保存在名为cpu_limit的文件中。\n创建Pod：\nkubectl apply -f https://k8s.io/examples/pods/inject/dapi-volume-resources.yaml 进入Pod中运行的容器，打开一个shell：\nkubectl exec -it kubernetes-downwardapi-volume-example-2 -- sh 在shell中，查看cpu_limit文件：\n/# cat /etc/cpu_limit 你可以使用同样的命令查看cpu_request, mem_limit 和mem_request 文件.\nCapabilities of the Downward API 下面这些信息可以通过环境变量和DownwardAPIVolumeFiles提供给容器：\n能通过fieldRef获得的：\n metadata.name - Pod名称 metadata.namespace - Pod名字空间 metadata.uid - Pod的UID, 版本要求 v1.8.0-alpha.2 metadata.labels['\u0026lt;KEY\u0026gt;'] - 单个 pod 标签值 \u0026lt;KEY\u0026gt; (例如, metadata.labels['mylabel']); 版本要求 Kubernetes 1.9+ metadata.annotations['\u0026lt;KEY\u0026gt;'] - 单个 pod 的标注值 \u0026lt;KEY\u0026gt; (例如, metadata.annotations['myannotation']); 版本要求 Kubernetes 1.9+  能通过resourceFieldRef获得的：\n 容器的CPU约束值 容器的CPU请求值 容器的内存约束值 容器的内存请求值 容器的临时存储约束值, 版本要求 v1.8.0-beta.0 容器的临时存储请求值, 版本要求 v1.8.0-beta.0  此外，以下信息可通过DownwardAPIVolumeFiles从fieldRef获得：\n metadata.labels - all of the pod’s labels, formatted as label-key=\u0026quot;escaped-label-value\u0026quot; with one label per line metadata.annotations - all of the pod’s annotations, formatted as annotation-key=\u0026quot;escaped-annotation-value\u0026quot; with one annotation per line metadata.labels - 所有Pod的标签，以label-key=\u0026quot;escaped-label-value\u0026quot;格式显示，每行显示一个label metadata.annotations - Pod的注释，以annotation-key=\u0026quot;escaped-annotation-value\u0026quot;格式显示，每行显示一个标签  以下信息可通过环境变量从fieldRef获得：\n status.podIP - 节点IP spec.serviceAccountName - Pod服务帐号名称, 版本要求 v1.4.0-alpha.3 spec.nodeName - 节点名称, 版本要求 v1.4.0-alpha.3 status.hostIP - 节点IP, 版本要求 v1.7.0-alpha.1  . note \u0026gt;}} 如果容器未指定CPU和memory limits，则Downward API默认为节点可分配值。 . /note \u0026gt;}}\n投射密钥到指定路径并且指定文件权限 你可以将密钥投射到指定路径并且指定每个文件的访问权限。更多信息，请参阅Secrets.\nDownward API的动机 对于容器来说，有时候拥有自己的信息是很有用的，可避免与Kubernetes过度耦合。Downward API使得容器使用自己或者集群的信息，而不必通过Kubernetes客户端或API服务器。\n一个例子是有一个现有的应用假定要用一个非常熟悉的环境变量来保存一个唯一标识。一种可能是给应用增加处理层，但这样是冗余和易出错的，而且它违反了低耦合的目标。更好的选择是使用Pod名称作为标识，把Pod名称注入这个环境变量中。\n. heading \u0026ldquo;whatsnext\u0026rdquo; %}}  [PodSpec](/docs/resources-reference/. param \u0026ldquo;version\u0026rdquo; \u0026gt;}}/#podspec-v1-core) [Volume](/docs/resources-reference/. param \u0026ldquo;version\u0026rdquo; \u0026gt;}}/#volume-v1-core) [DownwardAPIVolumeSource](/docs/resources-reference/. param \u0026ldquo;version\u0026rdquo; \u0026gt;}}/#downwardapivolumesource-v1-core) [DownwardAPIVolumeFile](/docs/resources-reference/. param \u0026ldquo;version\u0026rdquo; \u0026gt;}}/#downwardapivolumefile-v1-core) [ResourceFieldSelector](/docs/resources-reference/. param \u0026ldquo;version\u0026rdquo; \u0026gt;}}/#resourcefieldselector-v1-core)  "
},
{
	"uri": "https://lijun.in/tasks/inject-data-application/environment-variable-expose-pod-information/",
	"title": "通过环境变量将Pod信息呈现给容器",
	"tags": [],
	"description": "",
	"content": "此页面显示了Pod如何使用环境变量把自己的信息呈现给pod中运行的容器。环境变量可以呈现pod的字段和容器字段。\n有两种方式可以将Pod和Container字段呈现给运行中的容器： 环境变量 和[DownwardAPIVolumeFiles](/docs/resources-reference/. param \u0026ldquo;version\u0026rdquo; \u0026gt;}}/#downwardapivolumefile-v1-core). 这两种呈现Pod和Container字段的方式都称为Downward API。\n. heading \u0026ldquo;prerequisites\u0026rdquo; %}} . include \u0026ldquo;task-tutorial-prereqs.md\u0026rdquo; \u0026gt;}} . version-check \u0026gt;}}\nDownward API 有两种方式可以将Pod和Container字段呈现给运行中的容器：\n 环境变量 [DownwardAPIVolumeFiles](/docs/resources-reference/. param \u0026ldquo;version\u0026rdquo; \u0026gt;}}/#downwardapivolumefile-v1-core)  这两种呈现Pod和Container字段的方式都称为Downward API。\n用Pod字段作为环境变量的值 在这个练习中，你将创建一个包含一个容器的pod。这是该pod的配置文件：\n. codenew file=\u0026quot;pods/inject/dapi-envars-pod.yaml\u0026rdquo; \u0026gt;}}\n这个配置文件中，你可以看到五个环境变量。env字段是一个[EnvVars](/docs/resources-reference/. param \u0026ldquo;version\u0026rdquo; \u0026gt;}}/#envvar-v1-core)类型的数组。 数组中第一个元素指定MY_NODE_NAME这个环境变量从Pod的spec.nodeName字段获取变量值。同样，其它环境变量也是从Pod的字段获取它们的变量值。\n. note \u0026gt;}} 本示例中的字段是Pod字段，不是Pod中容器的字段。 . /note \u0026gt;}}\n创建Pod：\nkubectl apply -f https://k8s.io/examples/pods/inject/dapi-envars-pod.yaml 验证Pod中的容器运行正常：\nkubectl get pods 查看容器日志：\nkubectl logs dapi-envars-fieldref 输出信息显示了所选择的环境变量的值：\nminikube dapi-envars-fieldref default 172.17.0.4 default 要了解为什么这些值在日志中，请查看配置文件中的command 和 args字段。 当容器启动时，它将五个环境变量的值写入stdout。每十秒重复执行一次。\n接下来，进入Pod中运行的容器，打开一个shell：\nkubectl exec -it dapi-envars-fieldref -- sh 在shell中，查看环境变量：\n/# printenv 输出信息显示环境变量已经指定为Pod的字段的值。\nMY_POD_SERVICE_ACCOUNT=default ... MY_POD_NAMESPACE=default MY_POD_IP=172.17.0.4 ... MY_NODE_NAME=minikube ... MY_POD_NAME=dapi-envars-fieldref 用容器字段作为环境变量的值 前面的练习中，你将Pod字段作为环境变量的值。接下来这个练习，你将用容器字段作为环境变量的值。这里是包含一个容器的pod的配置文件：\n. codenew file=\u0026quot;pods/inject/dapi-envars-container.yaml\u0026rdquo; \u0026gt;}}\n这个配置文件中，你可以看到四个环境变量。env字段是一个[EnvVars](/docs/resources-reference/. param \u0026ldquo;version\u0026rdquo; \u0026gt;}}/#envvar-v1-core) 类型的数组。数组中第一个元素指定MY_CPU_REQUEST这个环境变量从容器的requests.cpu字段获取变量值。同样，其它环境变量也是从容器的字段获取它们的变量值。\n创建Pod：\nkubectl apply -f https://k8s.io/examples/pods/inject/dapi-envars-container.yaml 验证Pod中的容器运行正常：\nkubectl get pods 查看容器日志：\nkubectl logs dapi-envars-resourcefieldref 输出信息显示了所选择的环境变量的值：\n1 1 33554432 67108864 . heading \u0026ldquo;whatsnext\u0026rdquo; %}}  给容器定义环境变量 [PodSpec](/docs/resources-reference/. param \u0026ldquo;version\u0026rdquo; \u0026gt;}}/#podspec-v1-core) [Container](/docs/resources-reference/. param \u0026ldquo;version\u0026rdquo; \u0026gt;}}/#container-v1-core) [EnvVar](/docs/resources-reference/. param \u0026ldquo;version\u0026rdquo; \u0026gt;}}/#envvar-v1-core) [EnvVarSource](/docs/resources-reference/. param \u0026ldquo;version\u0026rdquo; \u0026gt;}}/#envvarsource-v1-core) [ObjectFieldSelector](/docs/resources-reference/. param \u0026ldquo;version\u0026rdquo; \u0026gt;}}/#objectfieldselector-v1-core) [ResourceFieldSelector](/docs/resources-reference/. param \u0026ldquo;version\u0026rdquo; \u0026gt;}}/#resourcefieldselector-v1-core)  "
},
{
	"uri": "https://lijun.in/tasks/administer-cluster/kubelet-config-file/",
	"title": "通过配置文件设置 Kubelet 参数",
	"tags": [],
	"description": "",
	"content": ". feature-state state=\u0026quot;beta\u0026rdquo; \u0026gt;}}\n通过保存在硬盘的配置文件设置 Kubelet 的配置参数子集，可以作为命令行参数的替代。此功能在 v1.10 中为 beta 版。\n建议通过配置文件的方式提供参数，因为这样可以简化节点部署和配置管理。\n. heading \u0026ldquo;prerequisites\u0026rdquo; %}}  需要安装 1.10 或更高版本的 Kubelet 二进制文件，才能实现 beta 功能。  创建配置文件 KubeletConfiguration 结构体定义了可以通过文件配置的 Kubelet 配置子集，该结构体在 [这里（v1beta1）](https://github.com/kubernetes/kubernetes/blob/. param \u0026ldquo;docsbranch\u0026rdquo; \u0026gt;}}/staging/src/k8s.io/kubelet/config/v1beta1/types.go) 可以找到, 配置文件必须是这个结构体中参数的 JSON 或 YAML 表现形式。\n在单独的文件夹中创建一个名为 kubelet 的文件，并保证 Kubelet 可以读取该文件夹及文件。您应该在这个 kubelet 文件中编写 Kubelet 配置。\n这是一个 Kubelet 配置文件示例：\nkind: KubeletConfiguration apiVersion: kubelet.config.k8s.io/v1beta1 evictionHard: memory.available: \u0026quot;200Mi\u0026quot; 在这个示例中, 当可用内存低于200Mi 时, Kubelet 将会开始驱逐 Pods。 没有声明的其余配置项都将使用默认值, 命令行中的 flags 将会覆盖配置文件中的对应值。\n作为一个小技巧，您可以从活动节点生成配置文件，相关方法请查看 重新配置活动集群节点的 Kubelet。\n启动通过配置文件配置的 Kubelet 进程 启动 Kubelet 需要将 --config 参数设置为 Kubelet 配置文件的路径。Kubelet 将从此文件加载其配置。\n请注意，命令行参数与配置文件有相同的值时，就会覆盖配置文件中的该值。这有助于确保命令行 API 的向后兼容性。\n请注意，Kubelet 配置文件中的相对文件路径是相对于 Kubelet 配置文件的位置解析的，而命令行参数中的相对路径是相对于 Kubelet 的当前工作目录解析的。\n请注意，命令行参数和 Kubelet 配置文件的某些默认值不同。如果设置了 --config，并且没有通过命令行指定值，则 KubeletConfiguration 版本的默认值生效。在上面的例子中，version 是 kubelet.config.k8s.io/v1beta1。\n与动态 Kubelet 配置的关系 如果您正在使用 动态 Kubelet 配置 特性，那么自动回滚机制将认为是 \u0026ldquo;最后已知正常（last known good）\u0026rdquo; 的配置，通过 --config 提供的配置与覆盖这些值的任何参数的结合。\n"
},
{
	"uri": "https://lijun.in/tasks/administer-cluster/quota-api-object/",
	"title": "配置 API 对象配额",
	"tags": [],
	"description": "",
	"content": "本文讨论如何为 API 对象配置配额，包括 PersistentVolumeClaims 和 Services。 配额限制了可以在命名空间中创建的特定类型对象的数量。 您可以在 [ResourceQuota](/docs/reference/generated/kubernetes-api/. param \u0026ldquo;version\u0026rdquo; \u0026gt;}}/#resourcequota-v1-core) 对象中指定配额。\n. heading \u0026ldquo;prerequisites\u0026rdquo; %}} . include \u0026ldquo;task-tutorial-prereqs.md\u0026rdquo; \u0026gt;}} . version-check \u0026gt;}}\n创建命名空间 创建一个命名空间以便本例中创建的资源和集群中的其余部分相隔离。\nkubectl create namespace quota-object-example 创建 ResourceQuota 下面是一个 ResourceQuota 对象的配置文件：\n. codenew file=\u0026quot;admin/resource/quota-objects.yaml\u0026rdquo; \u0026gt;}}\n创建 ResourceQuota\nkubectl create -f https://k8s.io/examples/admin/resource/quota-objects.yaml --namespace=quota-object-example 查看 ResourceQuota 的详细信息：\nkubectl get resourcequota object-quota-demo --namespace=quota-object-example --output=yaml 输出结果表明在 quota-object-example 命名空间中，至多只能有一个 PersistentVolumeClaim，最多两个 LoadBalancer 类型的服务，不能有 NodePort 类型的服务。\nstatus: hard: persistentvolumeclaims: \u0026#34;1\u0026#34; services.loadbalancers: \u0026#34;2\u0026#34; services.nodeports: \u0026#34;0\u0026#34; used: persistentvolumeclaims: \u0026#34;0\u0026#34; services.loadbalancers: \u0026#34;0\u0026#34; services.nodeports: \u0026#34;0\u0026#34; 创建 PersistentVolumeClaim 下面是一个 PersistentVolumeClaim 对象的配置文件：\n. codenew file=\u0026quot;admin/resource/quota-objects-pvc.yaml\u0026rdquo; \u0026gt;}}\n创建 PersistentVolumeClaim：\nkubectl create -f https://k8s.io/examples/admin/resource/quota-objects-pvc.yaml --namespace=quota-object-example 确认已创建完 PersistentVolumeClaim：\nkubectl get persistentvolumeclaims --namespace=quota-object-example 输出信息表明 PersistentVolumeClaim 存在并且处于 Pending 状态：\nNAME STATUS pvc-quota-demo Pending 尝试创建第二个 PersistentVolumeClaim 下面是第二个 PersistentVolumeClaim 的配置文件：\n. codenew file=\u0026quot;admin/resource/quota-objects-pvc-2.yaml\u0026rdquo; \u0026gt;}}\n尝试创建第二个 PersistentVolumeClaim：\nkubectl create -f https://k8s.io/examples/admin/resource/quota-objects-pvc-2.yaml --namespace=quota-object-example 输出信息表明第二个 PersistentVolumeClaim 没有创建成功，因为这会超出命名空间的配额。\npersistentvolumeclaims \u0026quot;pvc-quota-demo-2\u0026quot; is forbidden: exceeded quota: object-quota-demo, requested: persistentvolumeclaims=1, used: persistentvolumeclaims=1, limited: persistentvolumeclaims=1 注意事项 下面这些字符串可被用来标识那些能被配额限制的 API 资源：\n清理 删除您的命名空间：\nkubectl delete namespace quota-object-example . heading \u0026ldquo;whatsnext\u0026rdquo; %}} 集群管理员参考   为命名空间配置默认的内存请求和限制\n  为命名空间配置默认的 CPU 请求和限制\n  为命名空间配置内存的最小和最大限制\n  为命名空间配置 CPU 的最小和最大限制\n  为命名空间配置 CPU 和内存配额\n  为命名空间配置 Pod 配额\n  应用开发者参考  为容器和 Pod 分配内存资源 为容器和 Pod 分配 CPU 资源 为 Pod 配置服务质量  "
},
{
	"uri": "https://lijun.in/tasks/administer-cluster/configure-multiple-schedulers/",
	"title": "配置多个调度器",
	"tags": [],
	"description": "",
	"content": "Kubernetes 自带了一个默认调度器，其详细描述请查阅这里。\n如果默认调度器不适合您的需求，您可以实现自己的调度器。\n不仅如此，您甚至可以伴随着默认调度器同时运行多个调度器，并告诉 Kubernetes 为每个 pod 使用什么调度器。 让我们通过一个例子讲述如何在 Kubernetes 中运行多个调度器。\n关于实现调度器的具体细节描述超出了本文范围。 请参考 kube-scheduler 的实现，规范示例代码位于 [pkg/scheduler](https://github.com/kubernetes/kubernetes/tree/. param \u0026ldquo;githubbranch\u0026rdquo; \u0026gt;}}/pkg/scheduler)。\n. heading \u0026ldquo;prerequisites\u0026rdquo; %}} . include \u0026ldquo;task-tutorial-prereqs.md\u0026rdquo; \u0026gt;}} . version-check \u0026gt;}}\n打包调度器 将调度器二进制文件打包到容器镜像中。出于示例目的，我们就使用默认调度器（kube-scheduler）作为我们的第二个调度器。\n从 Github 克隆 Kubernetes 源代码，并编译构建源代码。\ngit clone https://github.com/kubernetes/kubernetes.git cd kubernetes make 创建一个包含 kube-scheduler 二进制文件的容器镜像。用于构建镜像的 Dockerfile 内容如下：\nFROMbusyboxADD ./_output/dockerized/bin/linux/amd64/kube-scheduler /usr/local/bin/kube-scheduler将文件保存为 Dockerfile，构建镜像并将其推送到镜像仓库。 此示例将镜像推送到 Google 容器镜像仓库（GCR）。\n有关详细信息，请阅读 GCR 文档。\ndocker build -t gcr.io/my-gcp-project/my-kube-scheduler:1.0 . gcloud docker -- push gcr.io/my-gcp-project/my-kube-scheduler:1.0 为调度器定义 Kubernetes Deployment 现在我们将调度器放在容器镜像中，我们可以为它创建一个 pod 配置，并在我们的 Kubernetes 集群中运行它。 但是与其在集群中直接创建一个 pod，不如使用 Deployment。 Deployment 管理一个 Replica Set，Replica Set 再管理 pod，从而使调度器能够适应故障。 以下是 Deployment 配置，被保存为 my-scheduler.yaml：\n. codenew file=\u0026quot;admin/sched/my-scheduler.yaml\u0026rdquo; \u0026gt;}}\n这里需要注意的是，在该部署文件中 Container 的 spec 配置的调度器启动命令参数（\u0026ndash;scheduler-name）指定的调度器名称应该是惟一的。 这个名称应该与 pods 上的可选参数 spec.schedulerName 的值相匹配，也就是说调度器名称的匹配关系决定了 pods 的调度任务由哪个调度器负责。\n还要注意，我们创建了一个专用服务帐户 my-scheduler 并将集群角色 system:kube-scheduler 绑定到它，以便它可以获得与 kube-scheduler 相同的权限。\n请参阅 kube-scheduler 文档以获取其他命令行参数的详细说明。\n在集群中运行第二个调度器 为了在 Kubernetes 集群中运行我们的第二个调度器，只需在 Kubernetes 集群中创建上面配置中指定的 Deployment：\nkubectl create -f my-scheduler.yaml 验证调度器 pod 正在运行：\n$ kubectl get pods --namespace=kube-system NAME READY STATUS RESTARTS AGE .... my-scheduler-lnf4s-4744f 1/1 Running 0 2m ... 此列表中，除了默认的 kube-scheduler pod 之外，您应该还能看到处于 “Running” 状态的 my-scheduler pod。\n要在启用了 leader 选举的情况下运行多调度器，您必须执行以下操作：\n首先，更新上述 Deployment YAML（my-scheduler.yaml）文件中的以下字段：\n --leader-elect=true --lock-object-namespace=lock-object-namespace --lock-object-name=lock-object-name  如果在集群上启用了 RBAC，则必须更新 system：kube-scheduler 集群角色。将调度器名称添加到应用于端点资源的规则的 resourceNames，如以下示例所示：\n$ kubectl edit clusterrole system:kube-scheduler - apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: annotations: rbac.authorization.kubernetes.io/autoupdate: \u0026quot;true\u0026quot; labels: kubernetes.io/bootstrapping: rbac-defaults name: system:kube-scheduler rules: - apiGroups: - \u0026quot;\u0026quot; resourceNames: - kube-scheduler - my-scheduler resources: - endpoints verbs: - delete - get - patch - update 指定 pod 的调度器 现在我们的第二个调度器正在运行，让我们创建一些 pod，并指定它们由默认调度器或我们刚部署的调度器进行调度。 为了使用特定的调度器调度给定的 pod，我们在那个 pod 的 spec 中指定调度器的名称。让我们看看三个例子。\n Pod spec 没有任何调度器名称  . codenew file=\u0026quot;admin/sched/pod1.yaml\u0026rdquo; \u0026gt;}}\n如果未提供调度器名称，则会使用 default-scheduler 自动调度 pod。\n将此文件另存为 pod1.yaml，并将其提交给 Kubernetes 集群。\nkubectl create -f pod1.yaml  Pod spec 设置为 default-scheduler  . codenew file=\u0026quot;admin/sched/pod2.yaml\u0026rdquo; \u0026gt;}}\n通过将调度器名称作为 spec.schedulerName 参数的值来指定调度器。在这种情况下，我们提供默认调度器的名称，即 default-scheduler。\n将此文件另存为 pod2.yaml，并将其提交给 Kubernetes 集群。\nkubectl create -f pod2.yaml  Pod spec 设置为 my-scheduler  . codenew file=\u0026quot;admin/sched/pod3.yaml\u0026rdquo; \u0026gt;}}\n在这种情况下，我们指定此 pod 使用我们部署的 my-scheduler 来调度。 请注意，spec.schedulerName 参数的值应该与 Deployment 中配置的提供给 scheduler 命令的参数名称匹配。\n将此文件另存为 pod3.yaml，并将其提交给 Kubernetes 集群。\nkubectl create -f pod3.yaml 确认所有三个 pod 都在运行。\nkubectl get pods 验证是否使用所需的调度器调度了 pod 为了更容易地完成这些示例，我们没有验证 pod 实际上是使用所需的调度程序调度的。 我们可以通过更改 pod 的顺序和上面的部署配置提交来验证这一点。 如果我们在提交调度器部署配置之前将所有 pod 配置提交给 Kubernetes 集群，我们将看到注解了 annotation-second-scheduler 的 pod 始终处于 “Pending” 状态，而其他两个 pod 被调度。 一旦我们提交调度器部署配置并且我们的新调度器开始运行，注解了 annotation-second-scheduler 的 pod 就能被调度。\n或者，可以查看事件日志中的 “Scheduled” 条目，以验证是否由所需的调度器调度了 pod。\nkubectl get events "
},
{
	"uri": "https://lijun.in/tasks/access-application-cluster/configure-access-multiple-clusters/",
	"title": "配置对多集群的访问",
	"tags": [],
	"description": "",
	"content": "本文展示如何使用配置文件来配置对多个集群的访问。 在将集群、用户和上下文定义在一个或多个配置文件中之后，用户可以使用 kubectl config use-context 命令快速地在集群之间进行切换。\n. note \u0026gt;}} 用于配置集群访问的文件有时被称为 kubeconfig 文件。 这是一种引用配置文件的通用方式，并不意味着存在一个名为 kubeconfig 的文件。 . /note \u0026gt;}}\n. heading \u0026ldquo;prerequisites\u0026rdquo; %}} 需要安装 kubectl 命令行工具。\n定义集群、用户和上下文 假设用户有两个集群，一个用于正式开发工作，一个用于其它临时用途（scratch）。 在 development 集群中，前端开发者在名为 frontend 的名字空间下工作， 存储开发者在名为 storage 的名字空间下工作。 在 scratch 集群中， 开发人员可能在默认名字空间下工作，也可能视情况创建附加的名字空间。 访问开发集群需要通过证书进行认证。 访问其它临时用途的集群需要通过用户名和密码进行认证。\n创建名为 config-exercise 的目录。 在 config-exercise 目录中，创建名为 config-demo 的文件，其内容为：\napiVersion: v1 kind: Config preferences: {} clusters: - cluster: name: development - cluster: name: scratch users: - name: developer - name: experimenter contexts: - context: name: dev-frontend - context: name: dev-storage - context: name: exp-scratch 配置文件描述了集群、用户名和上下文。 config-demo 文件中含有描述两个集群、两个用户和三个上下文的框架。\n进入 config-exercise 目录。 输入以下命令，将群集详细信息添加到配置文件中：\nkubectl config --kubeconfig=config-demo set-cluster development --server=https://1.2.3.4 --certificate-authority=fake-ca-file kubectl config --kubeconfig=config-demo set-cluster scratch --server=https://5.6.7.8 --insecure-skip-tls-verify 将用户详细信息添加到配置文件中：\nkubectl config --kubeconfig=config-demo set-credentials developer --client-certificate=fake-cert-file --client-key=fake-key-seefile kubectl config --kubeconfig=config-demo set-credentials experimenter --username=exp --password=some-password 将上下文详细信息添加到配置文件中：\nkubectl config --kubeconfig=config-demo set-context dev-frontend --cluster=development --namespace=frontend --user=developer kubectl config --kubeconfig=config-demo set-context dev-storage --cluster=development --namespace=storage --user=developer kubectl config --kubeconfig=config-demo set-context exp-scratch --cluster=scratch --namespace=default --user=experimenter 打开 config-demo 文件查看添加的详细信息。 也可以使用 config view 命令进行查看：\nkubectl config --kubeconfig=config-demo view 输出展示了两个集群、两个用户和三个上下文：\napiVersion: v1 clusters: - cluster: certificate-authority: fake-ca-file server: https://1.2.3.4 name: development - cluster: insecure-skip-tls-verify: true server: https://5.6.7.8 name: scratch contexts: - context: cluster: development namespace: frontend user: developer name: dev-frontend - context: cluster: development namespace: storage user: developer name: dev-storage - context: cluster: scratch namespace: default user: experimenter name: exp-scratch current-context: \u0026#34;\u0026#34; kind: Config preferences: {} users: - name: developer user: client-certificate: fake-cert-file client-key: fake-key-file - name: experimenter user: password: some-password username: exp 每个上下文包含三部分（集群、用户和名字空间），例如， dev-frontend 上下文表明：使用 developer 用户的凭证来访问 development 集群的 frontend 名字空间。\n设置当前上下文：\nkubectl config --kubeconfig=config-demo use-context dev-frontend 现在当输入 kubectl 命令时，相应动作会应用于 dev-frontend 上下文中所列的集群和名字空间，同时，命令会使用 dev-frontend 上下文中所列用户的凭证。\n使用 --minify 参数，来查看与当前上下文相关联的配置信息。\nkubectl config --kubeconfig=config-demo view --minify 输出结果展示了 dev-frontend 上下文相关的配置信息：\napiVersion: v1 clusters: - cluster: certificate-authority: fake-ca-file server: https://1.2.3.4 name: development contexts: - context: cluster: development namespace: frontend user: developer name: dev-frontend current-context: dev-frontend kind: Config preferences: {} users: - name: developer user: client-certificate: fake-cert-file client-key: fake-key-file 现在假设用户希望在其它临时用途集群中工作一段时间。\n将当前上下文更改为 exp-scratch：\nkubectl config --kubeconfig=config-demo use-context exp-scratch 现在用户 kubectl 下达的任何命令都将应用于 scratch 集群的默认名字空间。 同时，命令会使用 exp-scratch 上下文中所列用户的凭证。\n查看更新后的当前上下文 exp-scratch 相关的配置：\nkubectl config --kubeconfig=config-demo view --minify 最后，假设用户希望在 development 集群中的 storage 名字空间下工作一段时间。\n将当前上下文更改为 dev-storage：\nkubectl config --kubeconfig=config-demo use-context dev-storage 查看更新后的当前上下文 dev-storage 相关的配置：\nkubectl config --kubeconfig=config-demo view --minify 创建第二个配置文件 在 config-exercise 目录中，创建名为 config-demo-2 的文件，其中包含以下内容：\napiVersion: v1 kind: Config preferences: {} contexts: - context: cluster: development namespace: ramp user: developer name: dev-ramp-up 上述配置文件定义了一个新的上下文，名为 dev-ramp-up。\n设置 KUBECONFIG 环境变量 查看是否有名为 KUBECONFIG 的环境变量。 如有，保存 KUBECONFIG 环境变量当前的值，以便稍后恢复。 例如，在 Linux 中：\nexport KUBECONFIG_SAVED=$KUBECONFIG KUBECONFIG 环境变量是配置文件路径的列表，该列表在 Linux 和 Mac 中以冒号分隔，在 Windows 中以分号分隔。 如果有 KUBECONFIG 环境变量，请熟悉列表中的配置文件。\n临时添加两条路径到 KUBECONFIG 环境变量中。 例如，在 Linux 中：\nexport KUBECONFIG=$KUBECONFIG:config-demo:config-demo-2 在 config-exercise 目录中输入以下命令：\nkubectl config view 输出展示了 KUBECONFIG 环境变量中所列举的所有文件合并后的信息。 特别地， 注意合并信息中包含来自 config-demo-2 文件的 dev-ramp-up 上下文和来自 config-demo 文件的三个上下文：\ncontexts: - context: cluster: development namespace: frontend user: developer name: dev-frontend - context: cluster: development namespace: ramp user: developer name: dev-ramp-up - context: cluster: development namespace: storage user: developer name: dev-storage - context: cluster: scratch namespace: default user: experimenter name: exp-scratch 更多关于 kubeconfig 文件如何合并的信息，请参考 使用 kubeconfig 文件组织集群访问\n探索 $HOME/.kube 目录 如果用户已经拥有一个集群，可以使用 kubectl 与集群进行交互。 那么很可能在 $HOME/.kube 目录下有一个名为 config 的文件。\n进入 $HOME/.kube 目录， 看看那里有什么文件。 通常会有一个名为 config 的文件，目录中可能还有其他配置文件。 请简单地熟悉这些文件的内容。\n将 $HOME/.kube/config 追加到 KUBECONFIG 环境变量中 如果有 $HOME/.kube/config 文件，并且还未列在 KUBECONFIG 环境变量中， 那么现在将它追加到 KUBECONFIG 环境变量中。 例如，在 Linux 中：\nexport KUBECONFIG=$KUBECONFIG:$HOME/.kube/config 在配置练习目录中输入以下命令，来查看当前 KUBECONFIG 环境变量中列举的所有文件合并后的配置信息：\nkubectl config view 清理 将 KUBECONFIG 环境变量还原为原始值。 例如，在 Linux 中：\nexport KUBECONFIG=$KUBECONFIG_SAVED . heading \u0026ldquo;whatsnext\u0026rdquo; %}}  使用 kubeconfig 文件组织集群访问 [kubectl 配置](/docs/user-guide/kubectl/. param \u0026ldquo;version\u0026rdquo; \u0026gt;}}/)  "
},
{
	"uri": "https://lijun.in/tasks/administer-cluster/out-of-resource/",
	"title": "配置资源不足时的处理方式",
	"tags": [],
	"description": "",
	"content": "本页介绍了如何使用kubelet配置资源不足时的处理方式。\n当可用计算资源较少时，kubelet需要保证节点稳定性。这在处理如内存和硬盘之类的不可压缩资源时尤为重要。如果任意一种资源耗尽，节点将会变得不稳定。\n驱逐策略 kubelet 能够主动监测和防止计算资源的全面短缺。在那种情况下，kubelet可以主动地结束一个或多个 pod 以回收短缺的资源。当 kubelet 结束一个 pod 时，它将终止 pod 中的所有容器，而 pod 的 PodPhase 将变为 Failed。\n如果被驱逐的 Pod 由 Deployment 管理，这个 Deployment 会创建另一个 Pod 给 Kubernetes 来调度。\n驱逐信号 kubelet 支持按照以下表格中描述的信号触发驱逐决定。每个信号的值在 description 列描述，基于 kubelet 摘要 API。\n   驱逐信号 描述     memory.available memory.available := node.status.capacity[memory] - node.stats.memory.workingSet   nodefs.available nodefs.available := node.stats.fs.available   nodefs.inodesFree nodefs.inodesFree := node.stats.fs.inodesFree   imagefs.available imagefs.available := node.stats.runtime.imagefs.available   imagefs.inodesFree imagefs.inodesFree := node.stats.runtime.imagefs.inodesFree    上面的每个信号都支持字面值或百分比的值。基于百分比的值的计算与每个信号对应的总容量相关。\nmemory.available 的值从 cgroupfs 获取，而不是通过类似 free -m 的工具。这很重要，因为 free -m 不能在容器中工作，并且如果用户使用了 可分配节点特性，资源不足的判定将同时在本地 cgroup 层次结构的终端用户 pod 部分和根节点做出。这个 脚本复现了与 kubelet 计算 memory.available 相同的步骤。kubelet将inactive_file（意即活动 LRU 列表上基于文件后端的内存字节数）从计算中排除，因为它假设内存在出现压力时将被回收。\nkubelet 只支持两种文件系统分区。\n nodefs 文件系统，kubelet 将其用于卷和守护程序日志等。 imagefs 文件系统，容器运行时用于保存镜像和容器可写层。  imagefs可选。kubelet使用 cAdvisor 自动发现这些文件系统。kubelet不关心其它文件系统。当前不支持配置任何其它类型。例如，在专用文件系统中存储卷和日志是不可以的。\n在将来的发布中，kubelet将废除当前存在的 垃圾回收 机制，这种机制目前支持将驱逐操作作为对磁盘压力的响应。\n驱逐阈值 kubelet支持指定驱逐阈值，用于触发 kubelet 回收资源。\n每个阈值形式如下：\n[eviction-signal][operator][quantity]\n 合法的 eviction-signal 标志如上所示。 operator 是所需的关系运算符，例如 \u0026lt;。 quantity 是驱逐阈值值标志，例如 1Gi。合法的标志必须匹配 Kubernetes 使用的数量表示。驱逐阈值也可以使用 % 标记表示百分比。  举例说明，如果一个节点有 10Gi 内存，希望在可用内存下降到 1Gi 以下时引起驱逐操作，则驱逐阈值可以使用下面任意一种方式指定（但不是两者同时）。\n memory.available\u0026lt;10% memory.available\u0026lt;1Gi  软驱逐阈值 软驱逐阈值使用一对由驱逐阈值和管理员必须指定的宽限期组成的配置对。在超过宽限期前，kubelet不会采取任何动作回收和驱逐信号关联的资源。如果没有提供宽限期，kubelet启动时将报错。\n此外，如果达到了软驱逐阈值，操作员可以指定从节点驱逐 pod 时，在宽限期内允许结束的 pod 的最大数量。如果指定了 pod.Spec.TerminationGracePeriodSeconds 值，kubelet将使用它和宽限期二者中较小的一个。如果没有指定，kubelet将立即终止 pod，而不会优雅结束它们。\n软驱逐阈值的配置支持下列标记：\n eviction-soft 描述了驱逐阈值的集合（例如 memory.available\u0026lt;1.5Gi），如果在宽限期之外满足条件将触发 pod 驱逐。 eviction-soft-grace-period 描述了驱逐宽限期的集合（例如 memory.available=1m30s），对应于在驱逐 pod 前软驱逐阈值应该被控制的时长。 eviction-max-pod-grace-period 描述了当满足软驱逐阈值并终止 pod 时允许的最大宽限期值（秒数）。  硬驱逐阈值 硬驱逐阈值没有宽限期，一旦察觉，kubelet将立即采取行动回收关联的短缺资源。如果满足硬驱逐阈值，kubelet将立即结束 pod 而不是优雅终止。\n硬驱逐阈值的配置支持下列标记：\n eviction-hard 描述了驱逐阈值的集合（例如 memory.available\u0026lt;1Gi），如果满足条件将触发 pod 驱逐。  kubelet 有如下所示的默认硬驱逐阈值：\n memory.available\u0026lt;100Mi nodefs.available\u0026lt;10% nodefs.inodesFree\u0026lt;5% imagefs.available\u0026lt;15%  驱逐监控时间间隔 kubelet 根据其配置的整理时间间隔计算驱逐阈值。\n housekeeping-interval 是容器管理时间间隔。  节点状态 kubelet 会将一个或多个驱逐信号映射到对应的节点状态。\n如果满足硬驱逐阈值，或者满足独立于其关联宽限期的软驱逐阈值时，kubelet将报告节点处于压力下的状态。\n下列节点状态根据相应的驱逐信号定义。\n   节点状态 驱逐信号 描述     MemoryPressure memory.available Available memory on the node has satisfied an eviction threshold   DiskPressure nodefs.available, nodefs.inodesFree, imagefs.available, or imagefs.inodesFree Available disk space and inodes on either the node\u0026rsquo;s root filesystem or image filesystem has satisfied an eviction threshold    kubelet 将以 --node-status-update-frequency 指定的频率连续报告节点状态更新，其默认值为 10s。\n节点状态振荡 如果节点在软驱逐阈值的上下振荡，但没有超过关联的宽限期时，将引起对应节点的状态持续在 true 和 false 间跳变，并导致不好的调度结果。\n为了防止这种振荡，可以定义下面的标志，用于控制 kubelet 从压力状态中退出之前必须等待的时间。\n eviction-pressure-transition-period 是 kubelet 从压力状态中退出之前必须等待的时长。  kubelet 将确保在设定的时间段内没有发现和指定压力条件相对应的驱逐阈值被满足时，才会将状态变回 false。\n回收节点层级资源 如果满足驱逐阈值并超过了宽限期，kubelet将启动回收压力资源的过程，直到它发现低于设定阈值的信号为止。\nkubelet将尝试在驱逐终端用户 pod 前回收节点层级资源。发现磁盘压力时，如果节点针对容器运行时配置有独占的 imagefs，kubelet回收节点层级资源的方式将会不同。\n使用 Imagefs 如果 nodefs 文件系统满足驱逐阈值，kubelet通过驱逐 pod 及其容器来释放磁盘空间。\n如果 imagefs 文件系统满足驱逐阈值，kubelet通过删除所有未使用的镜像来释放磁盘空间。\n未使用 Imagefs 如果 nodefs 满足驱逐阈值，kubelet将以下面的顺序释放磁盘空间：\n 删除停止运行的 pod/container 删除全部没有使用的镜像  驱逐最终用户的 pod 如果 kubelet 在节点上无法回收足够的资源，kubelet将开始驱逐 pod。\nkubelet 首先根据他们对短缺资源的使用是否超过请求来排除 pod 的驱逐行为，然后通过 优先级，然后通过相对于 pod 的调度请求消耗急需的计算资源。\nkubelet 按以下顺序对要驱逐的 pod 排名：\n BestEffort 或 Burstable，其对短缺资源的使用超过了其请求，此类 pod 按优先级排序，然后使用高于请求。 Guaranteed pod 和 Burstable pod，其使用率低于请求，最后被驱逐。Guaranteedpod 只有为所有的容器指定了要求和限制并且它们相等时才能得到保证。由于另一个 pod 的资源消耗，这些 pod 保证永远不会被驱逐。如果系统守护进程（例如 kubelet、docker、和 journald）消耗的资源多于通过 system-reserved 或 kube-reserved 分配保留的资源，并且该节点只有 Guaranteed 或 Burstable pod 使用少于剩余的请求，然后节点必须选择驱逐这样的 pod 以保持节点的稳定性并限制意外消耗对其他 pod 的影响。在这种情况下，它将首先驱逐优先级最低的 pod。  必要时，kubelet会在遇到 DiskPressure 时驱逐一个 pod 来回收磁盘空间。如果 kubelet 响应 inode 短缺，它会首先驱逐服务质量最低的 pod 来回收 inodes。如果 kubelet 响应缺少可用磁盘，它会将 pod 排在服务质量范围内，该服务会消耗大量的磁盘并首先结束这些磁盘。\n使用 imagefs 如果是 nodefs 触发驱逐，kubelet将按 nodefs 用量 - 本地卷 + pod 的所有容器日志的总和对其排序。\n如果是 imagefs 触发驱逐，kubelet将按 pod 所有可写层的用量对其进行排序。\n未使用 imagefs 如果是 nodefs 触发驱逐，kubelet会根据磁盘的总使用情况对 pod 进行排序 - 本地卷 + 所有容器的日志及其可写层。\n最小驱逐回收 在某些场景，驱逐 pod 会导致回收少量资源。这将导致 kubelet 反复碰到驱逐阈值。除此之外，对如 disk 这类资源的驱逐时比较耗时的。\n为了减少这类问题，kubelet可以为每个资源配置一个 minimum-reclaim。当 kubelet 发现资源压力时，kubelet将尝试至少回收驱逐阈值之下 minimum-reclaim 数量的资源。\n例如使用下面的配置：\n--eviction-hard=memory.available\u0026lt;500Mi,nodefs.available\u0026lt;1Gi,imagefs.available\u0026lt;100Gi --eviction-minimum-reclaim=\u0026quot;memory.available=0Mi,nodefs.available=500Mi,imagefs.available=2Gi\u0026quot;` 如果 memory.available 驱逐阈值被触发，kubelet将保证 memory.available 至少为 500Mi。对于 nodefs.available，kubelet将保证 nodefs.available 至少为 1.5Gi。对于 imagefs.available，kubelet将保证 imagefs.available 至少为 102Gi，直到不再有相关资源报告压力为止。\n所有资源的默认 eviction-minimum-reclaim 值为 0。\n调度器 当资源处于压力之下时，节点将报告状态。调度器将那种状态视为一种信号，阻止更多 pod 调度到这个节点上。\n   节点状态 调度器行为     MemoryPressure No new BestEffort Pods are scheduled to the node.   DiskPressure No new Pods are scheduled to the node.    节点 OOM 行为 如果节点在 kubelet 回收内存之前经历了系统 OOM（内存不足）事件，它将基于 oom-killer 做出响应。\nkubelet 基于 pod 的 service 质量为每个容器设置一个 oom_score_adj 值。\n   Service 质量 oom_score_adj     Guaranteed -998   BestEffort 1000   Burstable min(max(2, 1000 - (1000 * memoryRequestBytes) / machineMemoryCapacityBytes), 999)    如果 kubelet 在节点经历系统 OOM 之前无法回收内存，oom_killer将基于它在节点上使用的内存百分比算出一个 oom_score，并加上 oom_score_adj 得到容器的有效 oom_score，然后结束得分最高的容器。\n预期的行为应该是拥有最低 service 质量并消耗和调度请求相关内存量最多的容器第一个被结束，以回收内存。\n和 pod 驱逐不同，如果一个 pod 的容器是被 OOM 结束的，基于其 RestartPolicy，它可能会被 kubelet 重新启动。\n最佳实践 以下部分描述了资源外处理的最佳实践。\n可调度资源和驱逐策略 考虑以下场景：\n 节点内存容量：10Gi 操作员希望为系统守护进程保留 10% 内存容量（内核、kubelet等）。 操作员希望在内存用量达到 95% 时驱逐 pod，以减少对系统的冲击并防止系统 OOM 的发生。  为了促成这个场景，kubelet将像下面这样启动：\n--eviction-hard=memory.available\u0026lt;500Mi --system-reserved=memory=1.5Gi 这个配置的暗示是理解系统保留应该包含被驱逐阈值覆盖的内存数量。\n要达到这个容量，要么某些 pod 使用了超过它们请求的资源，要么系统使用的内存超过 1.5Gi - 500Mi = 1Gi。\n这个配置将保证在 pod 使用量都不超过它们配置的请求值时，如果可能立即引起内存压力并触发驱逐时，调度器不会将 pod 放到这个节点上。\nDaemonSet 我们永远都不希望 kubelet 驱逐一个从 DaemonSet 派生的 pod，因为这个 pod 将立即被重建并调度回相同的节点。\n目前，kubelet没有办法区分一个 pod 是由 DaemonSet 还是其他对象创建。如果/当这个信息可用时，kubelet可能会预先将这些 pod 从提供给驱逐策略的候选集合中过滤掉。\n总之，强烈推荐 DaemonSet 不要创建 BestEffort 的 pod，防止其被识别为驱逐的候选 pod。相反，理想情况下 DaemonSet 应该启动 Guaranteed 的 pod。\n弃用现有特性标签以回收磁盘 kubelet 已经按需求清空了磁盘空间以保证节点稳定性。\n当磁盘驱逐成熟时，下面的 kubelet 标志将被标记为废弃的，以简化支持驱逐的配置。\n   现有标签 新标签     --image-gc-high-threshold --eviction-hard or eviction-soft   --image-gc-low-threshold --eviction-minimum-reclaim   --maximum-dead-containers deprecated   --maximum-dead-containers-per-container deprecated   --minimum-container-ttl-duration deprecated   --low-diskspace-threshold-mb --eviction-hard or eviction-soft   --outofdisk-transition-frequency --eviction-pressure-transition-period    已知问题 以下部分描述了与资源外处理有关的已知问题。\nkubelet 可能无法立即发现内存压力 kubelet当前通过以固定的时间间隔轮询 cAdvisor 来收集内存使用数据。如果内存使用在那个时间窗口内迅速增长，kubelet可能不能足够快的发现 MemoryPressure，OOMKiller将不会被调用。我们准备在将来的发行版本中通过集成 memcg 通知 API 来减小这种延迟。当超过阈值时，内核将立即告诉我们。\n如果您想处理可察觉的超量使用而不要求极端精准，可以设置驱逐阈值为大约 75% 容量作为这个问题的变通手段。这将增强这个特性的能力，防止系统 OOM，并提升负载卸载能力，以再次平衡集群状态。\nkubelet 可能会驱逐超过需求数量的 pod 由于状态采集的时间差，驱逐操作可能驱逐比所需的更多的 pod。将来可通过添加从根容器获取所需状态的能力 https://github.com/google/cadvisor/issues/1247 来减缓这种状况。\n"
},
{
	"uri": "https://lijun.in/tasks/administer-cluster/limit-storage-consumption/",
	"title": "限制存储消耗",
	"tags": [],
	"description": "",
	"content": "此示例演示了一种限制命名空间中存储使用量的简便方法。\n演示中用到了以下资源：ResourceQuota，LimitRange 和 PersistentVolumeClaim。\n. heading \u0026ldquo;prerequisites\u0026rdquo; %}}  . include \u0026ldquo;task-tutorial-prereqs.md\u0026rdquo; \u0026gt;}} . version-check \u0026gt;}}  场景：限制存储消耗 集群管理员代表用户群操作集群，管理员希望控制单个名称空间可以消耗多少存储空间以控制成本。\n管理员想要限制：\n 命名空间中持久卷申领（persistent volume claims）的数量 每个申领（claim）可以请求的存储量 命名空间可以具有的累计存储量  使用 LimitRange 限制存储请求 将 LimitRange 添加到命名空间会为存储请求大小强制设置最小值和最大值。存储是通过 PersistentVolumeClaim 来发起请求的。执行限制范围控制的准入控制器会拒绝任何高于或低于管理员所设阈值的 PVC。\n在此示例中，请求 10Gi 存储的 PVC 将被拒绝，因为它超过了最大 2Gi。\napiVersion: v1 kind: LimitRange metadata: name: storagelimits spec: limits: - type: PersistentVolumeClaim max: storage: 2Gi min: storage: 1Gi 当底层存储提供程序需要某些最小值时，将会用到所设置最小存储请求值。例如，AWS EBS volumes 的最低要求为 1Gi。\n使用 StorageQuota 限制 PVC 数目和累计存储容量 管理员可以限制某个命名空间中的 PVCs 个数以及这些 PVCs 的累计容量。新 PVCs 请求如果超过任一上限值将被拒绝。\n在此示例中，命名空间中的第 6 个 PVC 将被拒绝，因为它超过了最大计数 5。或者，当与上面的 2Gi 最大容量限制结合在一起时，意味着 5Gi 的最大配额不能支持 3 个都是 2Gi 的 PVC。后者实际上是向命名空间请求 6Gi 容量，而该命令空间已经设置上限为 5Gi。\napiVersion: v1 kind: ResourceQuota metadata: name: storagequota spec: hard: persistentvolumeclaims: \u0026quot;5\u0026quot; requests.storage: \u0026quot;5Gi\u0026quot; 小结 限制范围对象可以用来设置可请求的存储量上限，而资源配额对象则可以通过申领计数和累计存储容量有效地限制命名空间耗用的存储量。这两种机制使得集群管理员能够规划其集群存储预算而不会发生任一项目超量分配的风险。\n"
},
{
	"uri": "https://lijun.in/tasks/administer-cluster/dns-horizontal-autoscaling/",
	"title": "集群 DNS 服务自动伸缩",
	"tags": [],
	"description": "",
	"content": "本页展示了如何在集群中启用和配置 DNS 服务的自动伸缩功能。\n. heading \u0026ldquo;prerequisites\u0026rdquo; %}}  . include \u0026ldquo;task-tutorial-prereqs.md\u0026rdquo; \u0026gt;}} . version-check \u0026gt;}}    本指南假设您的节点使用 AMD64 或 Intel 64 CPU 架构\n  确保已启用 DNS 功能本身。\n  建议使用 Kubernetes 1.4.0 或更高版本。\n  确定是否 DNS 水平 水平自动伸缩特性已经启用 在 kube-system 命名空间中列出集群中的 . glossary_tooltip text=\u0026quot;Deployments\u0026rdquo; term_id=\u0026quot;deployment\u0026rdquo; \u0026gt;}} ：\nkubectl get deployment --namespace=kube-system 输出类似如下这样：\nNAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE ... dns-autoscaler 1 1 1 1 ... ...  如果在输出中看到 “dns-autoscaler”，说明 DNS 水平自动伸缩已经启用，可以跳到 调优自动伸缩参数。\n获取 DNS Deployment 或 ReplicationController 的名称 列出集群内 kube-system namespace 中的 Deployment：\nkubectl get deployment --namespace=kube-system 输出类似如下这样：\nNAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE ... coredns 2 2 2 2 ... ...  在早于 1.12 的 Kubernetes 版本中，DNS 部署称为 “kube-dns”。\nKubernetes 1.5 或之前版本，DNS 通过使用 ReplicationController 来实现，而不是 Deployment。 所以看不到 kube-dns 或者类似的名称，在之前的输出中，列出了集群内 kube-system namespace 中的 ReplicationController：\nkubectl get rc --namespace=kube-system 输出类似如下这样：\nNAME DESIRED CURRENT READY AGE ... kube-dns-v20 1 1 1 ... ...  确定伸缩目标 如果有一个 DNS Deployment，伸缩目标是：\nDeployment/\u0026lt;your-deployment-name\u0026gt;  其中 \u0026lt;your-deployment-name\u0026gt; 是 DNS 部署的名称。例如，如果您的 DNS 部署名称是 coredns，则您的扩展目标是 Deployment/coredns。\n如果有一个 DNS ReplicationController，那么伸缩目标为：\nReplicationController/\u0026lt;your-rc-name\u0026gt;  这里 \u0026lt;your-rc-name\u0026gt; 是 DNS ReplicationController 的名称。 例如，DNS ReplicationController 的名称是 kube-dns-v20，则伸缩目标为 ReplicationController/kube-dns-v20。\n启用 DNS 水平自动伸缩 在本段，我们创建一个 Deployment。Deployment 中的 Pod 运行一个基于 cluster-proportional-autoscaler-amd64 镜像的容器。\n创建文件 dns-horizontal-autoscaler.yaml，内容如下所示：\n. codenew file=\u0026quot;admin/dns/dns-horizontal-autoscaler.yaml\u0026rdquo; \u0026gt;}}\n在文件中，将 \u0026lt;SCALE_TARGET\u0026gt; 替换成 scale 目标。\n进入到包含配置文件的目录中，输入如下命令创建 Deployment：\nkubectl apply -f dns-horizontal-autoscaler.yaml 一个成功的命令输出是：\ndeployment.apps/dns-autoscaler created  DNS 水平自动伸缩在已经启用了。\n调优自动伸缩参数 验证 dns-autoscaler . glossary_tooltip text=\u0026quot;ConfigMap\u0026rdquo; term_id=\u0026quot;configmap\u0026rdquo; \u0026gt;}} 是否存在：\nkubectl get configmap --namespace=kube-system 输出类似如下所示：\nNAME DATA AGE ... dns-autoscaler 1 ... ...  修改该 ConfigMap 中的数据：\nkubectl edit configmap dns-autoscaler --namespace=kube-system 找到如下这行内容：\nlinear: \u0026#39;{\u0026#34;coresPerReplica\u0026#34;:256,\u0026#34;min\u0026#34;:1,\u0026#34;nodesPerReplica\u0026#34;:16}\u0026#39; 根据需要修改对应的字段。“min” 字段说明 DNS 后端的最小数量。实际后端的数量，通过使用如下公式来计算：\nreplicas = max( ceil( cores * 1/coresPerReplica ) , ceil( nodes * 1/nodesPerReplica ) )  注意 coresPerReplica 和 nodesPerReplica 的值都是整数。\n想法是，当一个集群使用具有很多核心的节点时，由 coresPerReplica 来控制。 当一个集群使用具有较少核心的节点时，由 nodesPerReplica 来控制。\n其它的伸缩模式也是支持的，详情查看 cluster-proportional-autoscaler。\n禁用 DNS 水平自动伸缩 有几个 DNS 水平自动伸缩的选项。具体使用哪个选项因环境而异。\n选项 1：调小 dns-autoscaler deployment 至 0 个副本 该选项适用于所有场景。运行如下命令：\nkubectl scale deployment --replicas=0 dns-autoscaler --namespace=kube-system 输出如下所示：\ndeployment.extensions/dns-autoscaler scaled  验证当前副本数为 0：\nkubectl get deployment --namespace=kube-system 输出内容中，在 DESIRED 和 CURRENT 列显示为 0：\nNAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE ... dns-autoscaler 0 0 0 0 ... ...  选项 2：删除 dns-autoscaler Deployment 如果 dns-autoscaler 为您所控制，该选项可以正常工作，也就说没有人会去重新创建它：\nkubectl delete deployment dns-autoscaler --namespace=kube-system 输出内容如下所示：\ndeployment.extensions \u0026quot;dns-autoscaler\u0026quot; deleted  选项 3：从 master 节点删除 dns-autoscaler manifest 文件 如果 dns-autoscaler 在插件管理器的控制之下，该选项可以工作，并且具有操作 master 节点的写权限。\n登录到 master 节点，删除对应的 manifest 文件。 dns-autoscaler 的路径一般为：\n/etc/kubernetes/addons/dns-horizontal-autoscaler/dns-horizontal-autoscaler.yaml  当 manifest 文件删除后，插件管理器将删除 dns-autoscaler Deployment。\n理解 DNS 水平自动伸缩工作原理   cluster-proportional-autoscaler 应用独立于 DNS service 部署。\n  autoscaler Pod 运行一个客户端，它通过轮询 Kubernetes API server 获取集群中节点和核心的数量。\n  一个期望的副本数会被计算，并根据当前可调度的节点、核心数、给定伸缩参数，被应用到 DNS 后端。\n  伸缩参数和数据点会基于一个 ConfigMap 来提供给 autoscaler，它会在每次轮询时刷新它的参数表，以与最近期望的伸缩参数保持一致。\n  允许对伸缩参数进行修改，而不需要重建或重启 autoscaler Pod。\n  autoscaler 提供了一个控制器接口来支持两种控制模式：linear 和 ladder。\n  未来功能增强 控制模式，除了 linear 和 ladder，正在考虑未来将开发自定义 metric。\n基于 DNS 特定 metric 的 DNS 后端的伸缩，考虑未来会开发。当前实现是使用集群中节点和核心的数量是受限制的。\n支持自定义 metric，类似于 Horizontal Pod 自动伸缩 所提供的，考虑未来进行开发。\n. heading \u0026ldquo;whatsnext\u0026rdquo; %}}  了解更多关于 cluster-proportional-autoscaler 实现的相关信息。  "
},
{
	"uri": "https://lijun.in/tasks/administer-cluster/securing-a-cluster/",
	"title": "集群安全",
	"tags": [],
	"description": "",
	"content": "本文档涉及与保护集群免受意外或恶意访问有关的主题，并对总体安全性提出建议。\n. heading \u0026ldquo;prerequisites\u0026rdquo; %}}  . include \u0026ldquo;task-tutorial-prereqs.md\u0026rdquo; \u0026gt;}} . version-check \u0026gt;}}  控制对 Kubernetes API 的访问 因为 Kubernetes 是完全通过 API 驱动的，所以，控制和限制谁可以通过 API 访问集群，以及允许这些访问者执行什么样的 API 动作，就成为了安全控制的第一道防线。\n为 API 交互提供传输层安全 （TLS） Kubernetes 期望集群中所有的 API 通信在默认情况下都使用 TLS 加密，大多数安装方法也允许创建所需的证书并且分发到集群组件中。请注意，某些组件和安装方法可能使用 HTTP 来访问本地端口， 管理员应该熟悉每个组件的设置，以识别潜在的不安全的流量。\nAPI 认证 安装集群时，选择一个 API 服务器的身份验证机制，去使用与之匹配的公共访问模式。例如，小型的单用户集群可能希望使用简单的证书或静态承载令牌方法。更大的集群则可能希望整合现有的、OIDC、LDAP 等允许用户分组的服务器。\n所有 API 客户端都必须经过身份验证，即使它是基础设施的一部分，比如节点、代理、调度程序和卷插件。这些客户端通常使用 服务帐户 或 X509 客户端证书，并在集群启动时自动创建或是作为集群安装的一部分进行设置。\n如果您希望获取更多信息，请参考 认证参考文档。\nAPI 授权 一旦使用授权，每个 API 的调用都将通过授权检查。Kubernetes 集成 基于访问控制（RBAC） 的组件，将传入的用户或组与一组绑定到角色的权限匹配。这些权限将动作（get，create，delete）和资源（pod，service, node）在命名空间或者集群范围内结合起来，根据客户可能希望执行的操作，提供了一组提供合理的违约责任分离的外包角色。建议您将节点 和 RBAC 一起作为授权者，再与 NodeRestriction 准入插件结合使用。\n与身份验证一样，简单而广泛的角色可能适合于较小的集群，但是随着更多的用户与集群交互，可能需要将团队划分成有更多角色限制的单独的命名空间。\n使用授权时，理解怎么样更新一个对象可能导致在其它地方的发生什么样的行为是非常重要的。例如，用户可能不能直接创建 pod，但允许他们通过创建一个 deployment 来创建这些 pod，这将让他们间接创建这些 pod。同样地，从 API 删除一个节点将导致调度到这些节点上的 pod 被中止，并在其他节点上重新创建。外包角色代表了灵活性和常见用例之间的平衡，但有限制的角色应该仔细审查，以防止意外升级。如果外包角色不满足您的需求，则可以为用例指定特定的角色。\n如果您希望获取更多信息，请参考 授权参考部分。\n控制对 Kubelet 的访问 Kubelet 公开 HTTPS 端点，这些端点授予节点和容器强大的控制权。默认情况下，Kubelet 允许对此 API 进行未经身份验证的访问。\n生产级别的集群应启用 Kubelet 身份验证和授权。\n如果您希望获取更多信息，请参考 Kubelet 身份验证 / 授权参考。\n控制运行时负载或用户的能力 Kubernetes 中的授权故意设置为了高层级，它侧重于对资源的粗行为。更强大的控制是以通过用例限制这些对象如何作用于集群、自身和其他资源上的策略存在的。\n限制集群上的资源使用 资源配额限制了授予命名空间的资源的数量或容量。这通常用于限制命名空间可以分配的 CPU、内存或持久磁盘的数量，但也可以控制每个命名空间中有多少个 pod、服务或卷的存在。\n限制范围限制了上述某些资源的最大值或者最小值，以防止用户使用类似内存这样的通用保留资源时请求不合理的过高或过低的值，或者在没有指定的情况下提供默认限制。\n控制容器运行的特权 pod 定义包含了一个安全上下文，用于描述允许它请求访问某个节点上的特定 Linux 用户（如 root）、获得特权或访问主机网络、以及允许它在主机节点上不受约束地运行的其它控件。Pod 安全策略可以限制哪些用户或服务帐户可以提供危险的安全上下文设置。例如，pod 的安全策略可以限制卷挂载，尤其是 hostpath，这些都是 pod 应该控制的一些方面。\n一般来说，大多数应用程序需要限制对主机资源的访问，他们可以在不能访问主机信息的情况下成功以根进程（UID 0）运行。但是，考虑到与 root 用户相关的特权，在编写应用程序容器时，您应该使用非 root 用户运行。类似地，希望阻止客户端应用程序逃避其容器的管理员，应该使用限制性的 pod 安全策略。\n限制网络访问 基于命名空间的网络策略允许应用程序作者限制其它命名空间中的哪些 pod 可以访问它们命名空间内的 pod 和端口。现在已经有许多支持网络策略的 Kubernetes 网络供应商。\n对于可以控制用户的应用程序是否在集群之外可见的许多集群，配额和限制范围也可用于控制用户是否可以请求节点端口或负载均衡服务。\n在插件或者环境基础上控制网络规则可以增加额外的保护措施，比如节点防火墙、物理分离群集节点以防止串扰、或者高级的网络策略。\n限制云 metadata API 访问 云平台（AWS, Azure, GCE 等）经常讲 metadate 本地服务暴露给实例。默认情况下，这些 API 可由运行在实例上的 pod 访问，并且可以包含该云节点的凭据或配置数据（如 kubelet 凭据）。这些凭据可以用于在集群内升级或在同一账户下升级到其他云服务。\n在云平台上运行 Kubernetes 时，限制对实例凭据的权限，使用网络策略限制对 metadate API 的 pod 访问，并避免使用配置数据来传递机密。\n控制 pod 可以访问那些节点 默认情况下，对哪些节点可以运行 pod 没有任何限制。Kubernetes 给最终用户提供了一组丰富的策略用于控制 pod 放在节点上的位置，以及基于 pod 位置和驱逐的污点。对于许多集群，可以约定由作者采用或者强制通过工具使用这些策略来分离工作负载。\n作为管理员，β 特性的准入插件 PodNodeSelector 可用于强制命名空间中的 pod 使用默认或需要使用特定的节点选择器。如果最终用户无法改变命名空间，这可以强烈地限制所有的 pod 在特定工作负载的位置。\n保护集群组件免受破坏 本节描述保护集群免受破坏的一些常见模式。\n限制访问 etcd 对于 API 来说，拥有 etcd 后端的写访问权限，相当于获得了整个集群的 root 权限，并且可以使用写访问权限来相当快速地升级。从 API 服务器访问它们的 etcd 服务器，管理员应该使用广受信任的凭证，如通过 TLS 客户端证书的相互认证。往往，我们建议将 etcd 服务器隔离到只有API服务器可以访问的防火墙后面。\n. caution \u0026gt;}} 允许集群中其它组件拥有读或写全空间的权限去访问 etcd 实例，相当于授予群集管理员访问的权限。对于非 master 组件，强烈推荐使用单独的 etcd 实例，或者使用 etcd 的访问控制列表去限制只能读或者写空间的一个子集。 . /caution \u0026gt;}}\n开启审计日志 审计日志是 β 特性，记录了 API 在发生破坏时进行后续分析的操作。建议启用审计日志，并将审计文件归档到安全服务器上。\n限制使用 α 和 β 特性 Kubernetes 的 α 和 β 特性还在积极发展当中，可能存在导致安全漏洞的缺陷或错误。要始终评估 α 和 β 特性可能为您的安全态势带来的风险。当您怀疑存在风险时，可以禁用那些不需要使用的特性。\n频繁回收基础证书 一个 secret 或凭据的寿命越短，攻击者就越难使用该凭据，在证书上设置短生命周期并实现自动回收，是控制安全的一个好方法。因此，使用身份验证提供程序时，应该要求可以控制发布令牌的可用时间，并尽可能使用短寿命。如果在外部集成中使用服务帐户令牌，则应该频繁地回收这些令牌。例如，一旦引导阶段完成，就应该撤销用于设置节点的引导令牌，或者取消它的授权。\n在启用第三方集成之前，请先审查它们 许多集成到 Kubernetes 的第三方都可以改变您集群的安全配置。启用集成时，在授予访问权限之前，您应该始终检查扩展请求的权限。例如，许多安全集成可以请求访问来查看集群上的所有 secret，从而有效地使该组件成为集群管理。当有疑问时，如果可能的话，将集成限制在单个命名空间中运行。\n如果组件创建的 pod 能够在命名空间中做一些类似 kube-system 命名空间中的事情，那么它也可能是出乎意料的强大。因为这些 pod 可以访问服务账户的 secret，或者，如果这些服务帐户被授予访问许可的 pod 安全策略的权限，它们能以高权限运行。\n使用加密的 rest 一般情况下，etcd 数据库包含了通过 Kubernetes API 可以访问到的所有信息，并且可以授予攻击者对集群状态的可见性。始终使用经过良好审查的备份和加密解决方案来加密备份，并考虑在可能的情况下使用全磁盘加密。\nKubernetes 1.7 包含了 rest 加密，它是一个 α 特性，会加密 etcd 里面的 Secret 资源，以防止某一方通过查看这些 secret 的内容获取 etcd 的备份。虽然目前这还只是实验性的功能，但是在备份没有加密或者攻击者获取到 etcd 的读访问权限的时候，它能提供额外的防御层级。\n接收安全更新和报告漏洞的警报 加入 kubernetes-announce 组，能够获取有关安全公告的邮件。有关如何报告漏洞的更多信息，请参见 安全报告页面。\n"
},
{
	"uri": "https://lijun.in/tasks/debug-application-cluster/debug-cluster/",
	"title": "集群故障排查",
	"tags": [],
	"description": "",
	"content": "本篇文档是介绍集群故障排查的；我们假设对于你碰到的问题，你已经排除了是由应用程序造成的。 对于应用的调试，请参阅应用故障排查指南。 你也可以访问troubleshooting document来获取更多的信息。\n显示出集群的节点列表 调试的第一步是查看所有的节点是否都正确的注册。\n运行\nkubectl get nodes 接下来，验证你的所有节点都能够显示出来，并且都处于Ready状态。\n查看logs 现在，挖掘出集群更深层的信息就需要登录到相关的机器上。下面是相关log文件所在的位置。 (注意，对于基于systemd的系统，你可能需要使用journalctl)\nMaster  /var/log/kube-apiserver.log - API Server, 提供API服务 /var/log/kube-scheduler.log - Scheduler, 负责调度决策 /var/log/kube-controller-manager.log - 管理replication controllers的控制器  Worker Nodes  /var/log/kubelet.log - Kubelet, 管控节点上运行的容器 /var/log/kube-proxy.log - Kube Proxy, 负责服务的负载均衡  集群故障模式的概述 下面是一个不完整的列表，列举了一些可能出错的场景，以及通过调整集群配置来解决相关问题的方法。\n根本原因：\n VM(s)关机 集群之间，或者集群和用户之间网络分裂 Kubernetes软件本身崩溃了 数据丢失或者持久化存储不可用(如:GCE PD 或 AWS EBS卷) 操作错误，如：Kubernetes或者应用程序配置错误  具体情况:\n  Apiserver所在的VM关机或者apiserver崩溃\n 结果  不能停止，更新，或者启动新的pods，services，replication controller 现有的pods和services在不依赖Kubernetes API的情况下应该能继续正常工作      Apiserver 后端存储丢失\n 结果  apiserver应该不能起来 kubelets将不能访问它，但是能够继续运行之前的Pods和提供相同的服务代理 在apiserver重启之前，需要手动恢复或者重创apiserver的状态      Kubernetes服务组件(节点控制器，副本控制器，调度器等等)所在的VM关机或者崩溃\n 当前，这些控制器是和apiserver共存的，它们不可用的现象是与apiserver类似的 将来，这些控制器也会复制为多份，并且可能为非共存的 它们没有自己的持久状态    单个节点(VM或者物理机)关机\n 结果  此节点上的所有Pods都停止运行      网络分裂(Network partition)\n 结果  partition A认为partition B中所有的节点都down掉了；partition B认为apiserver是down掉了(假定master所在的VM位于partition A内)。      Kubelet软件故障\n 结果  崩溃的kubelet就不能在其所在的节点上启动新的pods kubelet可能删掉pods或者不删 节点被标识为非健康态 副本控制器会在其它的节点上启动新的pods      集群操作错误\n 结果  丢失pods，服务等等 丢失apiserver后端存储 用户无法读取API 等等      缓解措施:\n  措施：对于IaaS上的VMs，使用IaaS的自动VM重启功能\n 缓解：Apiserver VM关机或apiserver崩溃 缓解：Kubernetes服务组件所在的VM关机或崩溃    措施: 对于具有apiserver+etcd的VM，使用IaaS提供的可靠的存储（例如GCE PD或者AWS EBS卷）\n 缓解：Apiserver后端存储的丢失    措施：使用（实验）高可用性的配置\n 缓解：master VM关机或者master组件(scheduler, API server, controller-managing)崩馈  将容许一个或多个节点或组件同时出现故障   缓解：apiserver后端存储(例如etcd的数据目录)丢失  假定你使用了集群化的etcd。      措施：定期的对apiserver的PDs/EBS卷进行快照\n 缓解：apiserver后端存储丢失 缓解：一些操作错误的场景 缓解：一些Kubernetes软件本身故障的场景    措施：在pods的前面使用副本控制器或服务\n 缓解：节点关机 缓解：Kubelet软件故障    措施：应用（容器）设计成容许异常重启\n 缓解：节点关机 缓解：Kubelet软件故障    措施：多个独立的集群(并且避免一次性地对所有的集群进行有风险性的修改)\n 缓解：以上列出的所有情况    "
},
{
	"uri": "https://lijun.in/tasks/administer-cluster/cluster-management/",
	"title": "集群管理",
	"tags": [],
	"description": "",
	"content": ". toc \u0026gt;}}\n本文描述了和集群生命周期相关的几个主题：创建新集群、更新集群的 master 和 worker 节点、执行节点维护（例如升级内核）以及升级运行中集群的 Kubernetes API 版本。\n创建和配置集群 要在一组机器上安装 Kubernetes， 请根据您的环境，查阅现有的 入门指南\n升级集群 集群升级当前是配套提供的，某些发布版本在升级时可能需要特殊处理。推荐管理员在升级他们的集群前，同时查阅 发行说明 和版本具体升级说明。\n 升级到 1.6  升级 Google Compute Engine 集群 Google Compute Engine Open Source (GCE-OSS) 通过删除和重建 master 来支持 master 升级。通过维持相同的 Persistent Disk (PD) 以保证在升级过程中保留数据。\nGCE 的 Node 升级采用 管理实例组，每个节点将被顺序的删除，然后使用新软件重建。任何运行在那个节点上的 Pod 需要用 Replication Controller 控制，或者在扩容之后手动重建。\n开源 Google Compute Engine (GCE) 集群上的升级过程由 cluster/gce/upgrade.sh 脚本控制。\n运行 cluster/gce/upgrade.sh -h 获取使用说明。\n例如，只将 master 升级到一个指定的版本 (v1.0.2):\ncluster/gce/upgrade.sh -M v1.0.2 或者，将整个集群升级到最新的稳定版本：\ncluster/gce/upgrade.sh release/stable 升级 Google Kubernetes Engine 集群 Google Kubernetes Engine 自动升级 master 组件（例如 kube-apiserver、kube-scheduler）至最新版本。它还负责 master 运行的操作系统和其它组件。\n节点升级过程由用户初始化，Google Kubernetes Engine 文档 里有相关描述。\n在其他平台上升级集群 不同的供应商和工具管理升级的过程各不相同。建议您查阅它们有关升级的主要文档。\n kops kubespray CoreOS Tectonic \u0026hellip;  调整集群大小 如果集群资源短缺，您可以轻松的添加更多的机器，如果集群正运行在节点自注册模式下的话。如果正在使用的是 GCE 或者 Google Kubernetes Engine，这将通过调整管理节点的实例组的大小完成。在 Google Cloud Console page 的 Compute \u0026gt; Compute Engine \u0026gt; Instance groups \u0026gt; your group \u0026gt; Edit group 下修改实例数量或使用 gcloud CLI 都可以完成这个任务。\ngcloud compute instance-groups managed resize kubernetes-minion-group --size 42 --zone $ZONE 实例组将负责在新机器上放置恰当的镜像并启动它们。Kubelet 将向 API server 注册它的节点以使其可以用于调度。如果您对 instance group 进行缩容，系统将会随机选取节点来终止。\n在其他环境上，您可能需要手动配置机器并告诉 Kubelet API server 在哪台机器上运行。\n集群自动伸缩 如果正在使用 GCE 或者 Google Kubernetes Engine，您可以配置您的集群，使其能够基于 pod 需求自动重新调整大小。\n如 Compute Resource 所述，用户可以控制预留多少 CPU 和内存来分配给 pod。这个信息被 Kubernetes scheduler 用来寻找一个运行 pod 的地方。如果没有一个节点有足够的空闲容量（或者不能满足其他 pod 的需求），这个 pod 就需要等待某些 pod 结束，或者一个新的节点被添加。\n集群 autoscaler 查找不能被调度的 pod 并检查添加一个新节点（和集群中其它节点类似的）是否有帮助。如果是的话，它将调整集群的大小以容纳等待调度的 pod。\n如果发现在一段延时时间内（默认10分钟，将来有可能改变）某些节点不再需要，集群 autoscaler 也会缩小集群。\n集群 autoscaler 在每一个实例组（GCE）或节点池（Google Kubernetes Engine）上配置。\n如果您使用 GCE，那么您可以在使用 kube-up.sh 脚本创建集群的时候启用它。要想配置集群 autoscaler，您需要设置三个环境变量：\n KUBE_ENABLE_CLUSTER_AUTOSCALER - 如果设置为 true 将启用集群 autoscaler。 KUBE_AUTOSCALER_MIN_NODES - 集群的最小节点数量。 KUBE_AUTOSCALER_MAX_NODES - 集群的最大节点数量。  示例：\nKUBE_ENABLE_CLUSTER_AUTOSCALER=true KUBE_AUTOSCALER_MIN_NODES=3 KUBE_AUTOSCALER_MAX_NODES=10 NUM_NODES=5 ./cluster/kube-up.sh 在 Google Kubernetes Engine 上，您可以在创建、更新集群或创建一个特别的节点池（您希望自动伸缩的）时，通过给对应的 gcloud 命令传递 --enable-autoscaling --min-nodes 和 --max-nodes 来配置集群 autoscaler。\n示例：\ngcloud container clusters create mytestcluster --zone=us-central1-b --enable-autoscaling --min-nodes=3 --max-nodes=10 --num-nodes=5 gcloud container clusters update mytestcluster --enable-autoscaling --min-nodes=1 --max-nodes=15 集群 autoscaler 期望节点未被手动修改过（例如通过 kubectl 添加标签），因为那些属性可能不能被传递到相同节点组中的新节点上。\n维护节点 如果需要重启节点（例如内核升级、libc 升级、硬件维修等），且停机时间很短时，当 Kubelet 重启后，它将尝试重启调度到节点上的 pod。如果重启花费较长时间（默认时间为 5 分钟，由 controller-manager 的 --pod-eviction-timeout 控制），节点控制器将会结束绑定到这个不可用节点上的 pod。如果存在对应的 replica set（或者 replication controller）时，则将在另一个节点上启动 pod 的新副本。所以，如果所有的 pod 都是复制而来，那么在不是所有节点都同时停机的前提下，升级可以在不需要特殊调整情况下完成。\n如果您希望更多的控制升级过程，可以使用下面的工作流程：\n使用 kubectl drain 优雅的结束节点上的所有 pod 并同时标记节点为不可调度：\nkubectl drain $NODENAME 在您正试图使节点离线时，这将阻止新的 pod 落到它们上面。\n对于有 replica set 的 pod 来说，它们将会被新的 pod 替换并且将被调度到一个新的节点。此外，如果 pod 是一个 service 的一部分，则客户端将被自动重定向到新的 pod。\n对于没有 replica set 的 pod，您需要手动启动 pod 的新副本，并且如果它不是 service 的一部分，您需要手动将客户端重定向到这个 pod。\n在节点上执行维护工作。\n重新使节点可调度：\nkubectl uncordon $NODENAME 如果删除了节点的虚拟机实例并重新创建，那么一个新的可调度节点资源将被自动创建（只在您使用支持节点发现的云服务提供商时；当前只有 Google Compute Engine，不包括在 Google Compute Engine 上使用 kube-register 的 CoreOS）。相关详细信息，请查阅 节点。\n高级主题 升级到不同的 API 版本 当新的 API 版本发布时，您可能需要升级集群支持新的 API 版本（例如当 \u0026lsquo;v2\u0026rsquo; 发布时从 \u0026lsquo;v1\u0026rsquo; 切换到 \u0026lsquo;v2\u0026rsquo;）。\n这不是一个经常性的事件，但需要谨慎的处理。这里有一系列升级到新 API 版本的步骤。\n 1. 开启新 API 版本。 2. 升级集群存储来使用新版本。 3. 升级所有配置文件。识别使用旧 API 版本 endpoint 的用户。 4. 运行 `cluster/update-storage-objects.sh` 升级存储中的现有对象为新版本。 5. 关闭旧 API 版本。  打开或关闭集群的 API 版本 可以在启动 API server 时传递 --runtime-config=api/\u0026lt;version\u0026gt; 标志来打开或关闭特定的 API 版本。例如：要关闭 v1 API，请传递 --runtime-config=api/v1=false。运行时配置还支持两个特殊键值：api/all 和 api/legacy，分别控制全部和遗留 API。例如要关闭除 v1 外全部 API 版本，请传递 --runtime-config=api/all=false,api/v1=true。对于这些标志来说，legacy API 指那些被显式废弃的 API（例如 v1beta3）。\n切换集群存储的 API 版本 存储于磁盘中，用于在集群内部代表 Kubernetes 活跃资源的对象使用特定的 API 版本书写。当支撑的 API 改变时，这些对象可能需要使用更新的 API 重写。重写失败将最终导致资源不再能够被 Kubernetes API server 解析或使用。\nkube-apiserver 二进制文件的 KUBE_API_VERSIONS 环境变量控制了集群支持的 API 版本。列表中的第一个版本被用作集群的存储版本。因此，要设置特定的版本为存储版本，请将其放在 KUBE_API_VERSIONS 参数值版本列表的最前面。您需要重启 kube-apiserver 二进制以使这个变量的改动生效。\n切换配置文件为新 API 版本 可以使用 kubectl convert 命令对不同 API 版本的配置文件进行转换。\nkubectl convert -f pod.yaml --output-version v1 更多选项请参考 kubectl convert 命令用法。\n"
},
{
	"uri": "https://lijun.in/tasks/administer-cluster/encrypt-data/",
	"title": "静态加密 Secret 数据",
	"tags": [],
	"description": "",
	"content": "本文展示如何启用和配置静态 Secret 数据的加密\n. heading \u0026ldquo;prerequisites\u0026rdquo; %}}  . include \u0026ldquo;task-tutorial-prereqs.md\u0026rdquo; \u0026gt;}} . version-check \u0026gt;}}    需要 Kubernetes 1.7.0 或者更高版本\n  需要 etcd v3 或者更高版本\n  静态数据加密在 1.7.0 中仍然是 alpha 版本，这意味着它可能会在没有通知的情况下进行更改。在升级到 1.8.0 之前，用户可能需要解密他们的数据。\n  . toc \u0026gt;}}\n. heading \u0026ldquo;prerequisites\u0026rdquo; %}} . include \u0026ldquo;task-tutorial-prereqs.md\u0026rdquo; \u0026gt;}} . version-check \u0026gt;}}\n配置并确定是否已启用静态数据加密 kube-apiserver 的参数 --experimental-encryption-provider-config 控制 API 数据在 etcd 中的加密方式。 下面提供一个配置示例。\n理解静态数据加密 kind: EncryptionConfiguration apiVersion: apiserver.config.k8s.io/v1 resources: - resources: - secrets providers: - identity: {} - aesgcm: keys: - name: key1 secret: c2VjcmV0IGlzIHNlY3VyZQ== - name: key2 secret: dGhpcyBpcyBwYXNzd29yZA== - aescbc: keys: - name: key1 secret: c2VjcmV0IGlzIHNlY3VyZQ== - name: key2 secret: dGhpcyBpcyBwYXNzd29yZA== - secretbox: keys: - name: key1 secret: YWJjZGVmZ2hpamtsbW5vcHFyc3R1dnd4eXoxMjM0NTY= 每个 resources 数组项目是一个单独的完整的配置。 resources.resources 字段是要加密的 Kubernetes 资源名称（resource 或 resource.group）的数组。 providers 数组是可能的加密 provider 的有序列表。每个条目只能指定一个 provider 类型（可以是 identity 或 aescbc，但不能在同一个项目中同时指定）。\n列表中的第一个提供者用于加密进入存储的资源。当从存储器读取资源时，与存储的数据匹配的所有提供者将尝试按顺序解密数据。 如果由于格式或密钥不匹配而导致提供者无法读取存储的数据，则会返回一个错误，以防止客户端访问该资源。\n重要： 如果通过加密配置无法读取资源（因为密钥已更改），唯一的方法是直接从基础 etcd 中删除该密钥。任何尝试读取资源的调用将会失败，直到它被删除或提供有效的解密密钥。\nProviders:    名称 加密类型 强度 速度 密钥长度 其它事项     identity 无 N/A N/A N/A 不加密写入的资源。当设置为第一个 provider 时，资源将在新值写入时被解密。   aescbc 填充 PKCS#7 的 AES-CBC 最强 快 32字节 建议使用的加密项，但可能比 secretbox 稍微慢一些。   secretbox XSalsa20 和 Poly1305 强 更快 32字节 较新的标准，在需要高度评审的环境中可能不被接受。   aesgcm 带有随机数的 AES-GCM 必须每 200k 写入一次 最快 16, 24, 或者 32字节 建议不要使用，除非实施了自动密钥循环方案。   kms 使用信封加密方案：数据使用带有 PKCS#7 填充的 AES-CBC 通过 data encryption keys（DEK）加密，DEK 根据 Key Management Service（KMS）中的配置通过 key encryption keys（KEK）加密 最强 快 32字节 建议使用第三方工具进行密钥管理。为每个加密生成新的 DEK，并由用户控制 KEK 轮换来简化密钥轮换。配置 KMS 提供程序    每个 provider 都支持多个密钥 - 在解密时会按顺序使用密钥，如果是第一个 provider，则第一个密钥用于加密。\n加密您的数据 创建一个新的加密配置文件：\nkind: EncryptionConfiguration apiVersion: apiserver.config.k8s.io/v1 resources: - resources: - secrets providers: - aescbc: keys: - name: key1 secret: \u0026lt;BASE 64 ENCODED SECRET\u0026gt; - identity: {} 遵循如下步骤来创建一个新的 secret：\n  生成一个 32 字节的随机密钥并进行 base64 编码。如果您在 Linux 或 Mac OS X 上，请运行以下命令：\nhead -c 32 /dev/urandom | base64   将这个值放入到 secret 字段中。 设置 kube-apiserver 的 --experimental-encryption-provider-config 参数，将其指定到配置文件所在位置。 重启您的 API server。  重要： 您的配置文件包含可以解密 etcd 内容的密钥，因此您必须正确限制主设备的权限，以便只有能运行 kube-apiserver 的用户才能读取它。\n验证数据是否被加密 数据在写入 etcd 时会被加密。重新启动你的 kube-apiserver 后，任何新创建或更新的密码在存储时都应该被加密。 如果想要检查，你可以使用 etcdctl 命令行程序来检索你的加密内容。\n  创建一个新的 secret，名称为 secret1，命名空间为 default：\nkubectl create secret generic secret1 -n default --from-literal=mykey=mydata    使用 etcdctl 命令行，从 etcd 中读取 secret：\n   ETCDCTL_API=3 etcdctl get /registry/secrets/default/secret1 [\u0026hellip;] | hexdump -C ```\n这里的 `[...]` 是用来连接 etcd 服务的额外参数。   验证存储的密钥前缀是否为 k8s:enc:aescbc:v1:，这表明 aescbc provider 已加密结果数据。\n  通过 API 检索，验证 secret 是否被正确解密：\nkubectl describe secret secret1 -n default   必须匹配 `mykey: mydata`  确保所有 secret 都被加密 由于 secret 是在写入时被加密，因此对 secret 执行更新也会加密该内容。\nkubectl get secrets --all-namespaces -o json | kubectl replace -f - 上面的命令读取所有 secret，然后使用服务端加密来进行更新。 如果由于冲突写入而发生错误，请重试该命令。 对于较大的集群，您可能希望通过命名空间或更新脚本来分割 secret。\n回滚解密密钥 在不发生停机的情况下更改 secret 需要多步操作，特别是在有多个 kube-apiserver 进程正在运行的高可用部署的情况下。\n 生成一个新密钥并将其添加为所有服务器上当前提供程序的第二个密钥条目 重新启动所有 kube-apiserver 进程以确保每台服务器都可以使用新密钥进行解密 将新密钥设置为 keys 数组中的第一个条目，以便在配置中使用其进行加密 重新启动所有 kube-apiserver 进程以确保每个服务器现在都使用新密钥进行加密 运行 kubectl get secrets --all-namespaces -o json | kubectl replace -f - 以用新密钥加密所有现有的秘密 在使用新密钥备份 etcd 后，从配置中删除旧的解密密钥并更新所有密钥  如果只有一个 kube-apiserver，第 2 步可能可以忽略。\n解密所有数据 要禁用 rest 加密，请将 identity provider 作为配置中的第一个条目：\nkind: EncryptionConfiguration apiVersion: apiserver.config.k8s.io/v1 resources: - resources: - secrets providers: - identity: {} - aescbc: keys: - name: key1 secret: \u0026lt;BASE 64 ENCODED SECRET\u0026gt; 并重新启动所有 kube-apiserver 进程。然后运行命令 kubectl get secrets --all-namespaces -o json | kubectl replace -f - 强制解密所有 secret。\n"
},
{
	"uri": "https://lijun.in/tasks/network/validate-dual-stack/",
	"title": "验证 IPv4/IPv6 双协议栈",
	"tags": [],
	"description": "",
	"content": "这篇文章分享了如何验证 IPv4/IPv6 双协议栈的 Kubernetes 集群。\n. heading \u0026ldquo;prerequisites\u0026rdquo; %}}  Kubernetes 1.16 或更高版本 提供程序对双协议栈网络的支持 (云供应商或其他方式必须能够为 Kubernetes 节点提供可路由的 IPv4/IPv6 网络接口) Kubenet 网络插件 Kube-proxy 在 IPVS 模式下运行 启用双协议栈 集群  验证寻址 验证节点寻址 每个双协议栈节点应分配一个 IPv4 块和一个 IPv6 块。 通过运行以下命令来验证是否配置了 IPv4/IPv6 Pod 地址范围。 将示例节点名称替换为集群中的有效双协议栈节点。 在此示例中，节点的名称为 k8s-linuxpool1-34450317-0：\nkubectl get nodes k8s-linuxpool1-34450317-0 -o go-template --template=\u0026#39;{{range .spec.podCIDRs}}{{printf \u0026#34;%s\\n\u0026#34; .}}{{end}}\u0026#39; 10.244.1.0/24 a00:100::/24 应该分配一个 IPv4 块和一个 IPv6 块。\n验证节点是否检测到 IPv4 和 IPv6 接口（用集群中的有效节点替换节点名称。在此示例中，节点名称为 k8s-linuxpool1-34450317-0）：\nkubectl get nodes k8s-linuxpool1-34450317-0 -o go-template --template=\u0026#39;{{range .status.addresses}}{{printf \u0026#34;%s: %s \\n\u0026#34; .type .address}}{{end}}\u0026#39; Hostname: k8s-linuxpool1-34450317-0 InternalIP: 10.240.0.5 InternalIP: 2001:1234:5678:9abc::5 验证 Pod 寻址 验证 Pod 已分配了 IPv4 和 IPv6 地址。（用集群中的有效 Pod 替换 Pod 名称。在此示例中， Pod 名称为 pod01）\nkubectl get pods pod01 -o go-template --template=\u0026#39;{{range .status.podIPs}}{{printf \u0026#34;%s \\n\u0026#34; .ip}}{{end}}\u0026#39; 10.244.1.4 a00:100::4 您也可以通过 status.podIPs 使用 Downward API 验证 Pod IP。以下代码段演示了如何通过容器内称为 MY_POD_IPS 的环境变量公开 Pod 的 IP 地址。\n env: - name: MY_POD_IPS valueFrom: fieldRef: fieldPath: status.podIPs 使用以下命令打印出容器内部 MY_POD_IPS 环境变量的值。该值是一个逗号分隔的列表，与 Pod 的 IPv4 和 IPv6 地址相对应。\nkubectl exec -it pod01 -- set | grep MY_POD_IPS MY_POD_IPS=10.244.1.4,a00:100::4 Pod 的 IP 地址也将被写入容器内的 /etc/hosts 文件中。在双栈 Pod 上执行 cat /etc/hosts 命令操作。从输出结果中，您可以验证 Pod 的 IPv4 和 IPv6 地址。\nkubectl exec -it pod01 -- cat /etc/hosts # Kubernetes-managed hosts file. 127.0.0.1 localhost ::1 localhost ip6-localhost ip6-loopback fe00::0 ip6-localnet fe00::0 ip6-mcastprefix fe00::1 ip6-allnodes fe00::2 ip6-allrouters 10.244.1.4 pod01 a00:100::4 pod01 验证服务 在不设置 ipFamily 字段的情况下创建以下服务。 如果未设置此字段，则服务会通过 kube-controller-manager 上的 --service-cluster-ip-range 标志从第一个配置的范围中获取 IP。\n. codenew file=\u0026quot;service/networking/dual-stack-default-svc.yaml\u0026rdquo; \u0026gt;}}\n通过查看该服务的 YAML ，您可以观察到该服务的 ipFamily 字段已设置为反映通过 kube-controller-manager 上的 --service-cluster-ip-range 标志设置的第一个配置范围的地址族。\nkubectl get svc my-service -o yaml apiVersion: v1 kind: Service metadata: creationTimestamp: \u0026#34;2019-09-03T20:45:13Z\u0026#34; labels: app: MyApp name: my-service namespace: default resourceVersion: \u0026#34;485836\u0026#34; selfLink: /api/v1/namespaces/default/services/my-service uid: b6fa83ef-fe7e-47a3-96a1-ac212fa5b030 spec: clusterIP: 10.0.29.179 ipFamily: IPv4 ports: - port: 80 protocol: TCP targetPort: 9376 selector: app: MyApp sessionAffinity: None type: ClusterIP status: loadBalancer: {} 在 ipFamily 字段设置为 IPv6 的情况下创建一下服务。\n. codenew file=\u0026quot;service/networking/dual-stack-ipv6-svc.yaml\u0026rdquo; \u0026gt;}}\n验证服务是否是 IPv6 地址块获取集群 IP 地址。 然后，您可以通过 IP 和端口验证对服务的访问。\n kubectl get svc -l app=MyApp NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE my-service ClusterIP fe80:20d::d06b \u0026lt;none\u0026gt; 80/TCP 9s 创建双协议栈负载均衡服务 如果云提供商支持配置启用 IPv6 的外部负载均衡器，则将 ipFamily 字段设置为 IPv6 并将 type 字段设置为 LoadBalancer的方式创建以下服务\n. codenew file=\u0026quot;service/networking/dual-stack-ipv6-lb-svc.yaml\u0026rdquo; \u0026gt;}}\n验证服务是否从 IPv6 地址块中接收到 CLUSTER-IP 地址以及 EXTERNAL-IP。 然后，您可以通过 IP 和端口验证对服务的访问。\n kubectl get svc -l app=MyApp NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE my-service ClusterIP fe80:20d::d06b 2001:db8:f100:4002::9d37:c0d7 80:31868/TCP 30s "
},
{
	"uri": "https://lijun.in/tasks/extend-kubectl/kubectl-plugins/",
	"title": "😝 - 用插件扩展 kubectl",
	"tags": [],
	"description": "使用 kubectl 插件，你可以通过添加新的子命令来扩展 kubectl 命令的功能。",
	"content": ". feature-state state=\u0026quot;stable\u0026rdquo; \u0026gt;}}\n本指南演示了如何为 kubectl 安装和编写扩展。 通过将核心 kubectl 命令看作与 Kubernetes 集群交互的基本构建块，集群管理员可以将插件视为一种利用这些构建块创建更复杂行为的方法。 插件用新的子命令扩展了 kubectl，允许新的和自定义的特性不包括在 kubectl 的主要发行版中。\n. heading \u0026ldquo;prerequisites\u0026rdquo; %}} 您需要安装一个工作的二进制 kubectl。\n. note \u0026gt;}}\n插件在 v1.8.0 版本中正式作为 alpha 特性引入。它们已经在 v1.12.0 版本中工作，以支持更广泛的用例。因此，虽然在以前的版本中已经提供了部分插件特性，但如果您遵循这些文档，建议使用 1.12.0 或更高版本的 kubectl。\n. /note \u0026gt;}}\n安装 kubectl 插件 插件只不过是一个独立的可执行文件，名称以 kubectl- 开头。要安装插件，只需将此可执行文件移动到路径上的任何位置。\n. note \u0026gt;}}\nKubernetes 不提供包管理器或任何类似于安装或更新插件的东西。你有责任确保插件可执行文件的文件名以 kubectl- 开头，并将它们放在你路径的某个位置。 . /note \u0026gt;}}\n发现插件 kubectl 提供一个命令 kubectl plugin list，用于搜索路径查找有效的插件可执行文件。 执行此命令将遍历路径中的所有文件。任何以 kubectl- 开头的可执行文件都将在这个命令的输出中以它们在路径中出现的顺序显示。 任何以 kubectl- 开头的文件如果不可执行，都将包含一个警告。 对于任何相同的有效插件文件，都将包含一个警告。\n限制 目前无法创建覆盖现有 kubectl 命令的插件，例如，创建一个插件 kubectl-version 将导致该插件永远不会被执行，因为现有的 kubectl-version 命令总是优先于它执行。 由于这个限制，也不可能使用插件将新的子命令添加到现有的 kubectl 命令中。例如，通过将插件命名为 kubectl-create-foo 来添加子命令 kubectl create foo 将导致该插件被忽略。对于任何试图这样做的有效插件 kubectl plugin list 的输出中将显示警告。\n编写 kubectl 插件 你可以用任何编程语言或脚本编写插件，允许您编写命令行命令。\n不需要安装插件或预加载，插件可执行程序从 kubectl 二进制文件接收继承的环境，插件根据其名称确定它希望实现的命令路径。 例如，一个插件想要提供一个新的命令 kubectl foo，它将被简单地命名为 kubectl-foo，并且位于用户路径的某个位置。\n示例插件 #!/bin/bash # optional argument handling if [[ \u0026quot;$1\u0026quot; == \u0026quot;version\u0026quot; ]] then echo \u0026quot;1.0.0\u0026quot; exit 0 fi # optional argument handling if [[ \u0026quot;$1\u0026quot; == \u0026quot;config\u0026quot; ]] then echo $KUBECONFIG exit 0 fi echo \u0026quot;I am a plugin named kubectl-foo\u0026quot; 使用插件 要使用上面的插件，只需使其可执行：\nsudo chmod +x ./kubectl-foo 并将它放在你的路径中的任何地方：\nsudo mv ./kubectl-foo /usr/local/bin 你现在可以调用你的插件作为 kubectl 命令：\nkubectl foo I am a plugin named kubectl-foo 所有参数和标记按原样传递给可执行文件：\nkubectl foo version 1.0.0 所有环境变量也按原样传递给可执行文件：\nexport KUBECONFIG=~/.kube/config kubectl foo config /home/\u0026lt;user\u0026gt;/.kube/config KUBECONFIG=/etc/kube/config kubectl foo config /etc/kube/config 此外，传递给插件的第一个参数总是调用它的位置的绝对路径（在上面的例子中，$0 将等于 /usr/local/bin/kubectl-foo）。\n命名插件 如上面的例子所示，插件根据文件名确定要实现的命令路径，插件所针对的命令路径中的每个子命令都由破折号（-）分隔。 例如，当用户调用命令 kubectl foo bar baz 时，希望调用该命令的插件的文件名为 kubectl-foo-bar-baz。\n参数和标记处理 . note \u0026gt;}}\n与以前版本的 kubectl 不同，插件机制不会为插件进程创建任何定制的、特定于插件的值或环境变量，这意味着像 KUBECTL_PLUGINS_CURRENT_NAMESPACE 这样的环境变量不再提供给插件。 插件必须解析用户传递给它们的所有参数，并将参数验证作为它们自己实现的一部分处理。对于用 Go 编写的插件，在 k8s.io/cli-runtime 下提供了一组实用程序来帮助实现这一点。\n. /note \u0026gt;}}\n从上面的场景中使用我们的 kubectl-foo-bar-baz 插件，我们将进一步研究用户在提供额外标记和参数的同时调用我们的插件的其他情况。 例如，在用户调用命令 kubectl foo bar baz arg1 --flag=value arg2 的情况下，插件机制将首先尝试找到名称可能最长的插件，在本例中是 kubectl-foo-bar-baz-arg1。 当没有找到这个插件时，它就会将最后一个以破折号分隔的值视为参数（在本例中为 arg1），并尝试找到下一个最长的名称 kubectl-foo-bar-baz。 在找到具有此名称的插件后，它将调用该插件，并在其名称之后将所有参数和标志传递给插件可执行文件。\n示例：\n# create a plugin echo -e \u0026#39;#!/bin/bash\\n\\necho \u0026#34;My first command-line argument was $1\u0026#34;\u0026#39; \u0026gt; kubectl-foo-bar-baz sudo chmod +x ./kubectl-foo-bar-baz # \u0026#34;install\u0026#34; our plugin by placing it on our PATH sudo mv ./kubectl-foo-bar-baz /usr/local/bin # ensure our plugin is recognized by kubectl kubectl plugin list The following kubectl-compatible plugins are available: /usr/local/bin/kubectl-foo-bar-baz # test that calling our plugin via a \u0026quot;kubectl\u0026quot; command works # even when additional arguments and flags are passed to our # plugin executable by the user. kubectl foo bar baz arg1 --meaningless-flag=true My first command-line argument was arg1 正如你所看到的，我们的插件是基于用户指定的 kubectl 命令找到的，所有额外的参数和标记都是按原样传递给插件可执行文件的。\n带有破折号和下划线的名称 虽然 kubectl 插件机制在插件文件名中使用破折号（-）分隔插件处理的子命令序列，但是仍然可以通过在文件名中使用下划线（-）来创建命令行中包含破折号的插件命令。\n例子：\n# create a plugin containing an underscore in its filename echo -e \u0026#39;#!/bin/bash\\n\\necho \u0026#34;I am a plugin with a dash in my name\u0026#34;\u0026#39; \u0026gt; ./kubectl-foo_bar sudo chmod +x ./kubectl-foo_bar # move the plugin into your PATH sudo mv ./kubectl-foo_bar /usr/local/bin # our plugin can now be invoked from `kubectl` like so: kubectl foo-bar I am a plugin with a dash in my name 请注意，在插件文件名中引入下划线并不会阻止我们使用 kubectl foo_bar 之类的命令。可以使用破折号（-）或下划线（-）调用上面示例中的命令:\n# our plugin can be invoked with a dash kubectl foo-bar I am a plugin with a dash in my name # it can also be invoked using an underscore kubectl foo_bar I am a plugin with a dash in my name 命名冲突和弊端 可以在路径的不同位置使用多个文件名相同的插件， 例如，给定一个路径的值为: PATH=/usr/local/bin/plugins:/usr/local/bin/moreplugins，在 /usr/local/bin/plugins 和 /usr/local/bin/moreplugins 中可以存在一个插件 kubectl-foo 的副本，这样 kubectl plugin list 命令的输出就是:\nPATH=/usr/local/bin/plugins:/usr/local/bin/moreplugins kubectl plugin list The following kubectl-compatible plugins are available: /usr/local/bin/plugins/kubectl-foo /usr/local/bin/moreplugins/kubectl-foo - warning: /usr/local/bin/moreplugins/kubectl-foo is overshadowed by a similarly named plugin: /usr/local/bin/plugins/kubectl-foo error: one plugin warning was found 在上面的场景中 /usr/local/bin/moreplugins/kubectl-foo 下的警告告诉我们这个插件永远不会被执行。相反，首先出现在我们路径中的可执行文件 /usr/local/bin/plugins/kubectl-foo 总是首先被 kubectl 插件机制找到并执行。\n解决这个问题的一种方法是你确保你希望与 kubectl 一起使用的插件的位置总是在你的路径中首先出现。 例如，如果我们总是想使用 /usr/local/bin/moreplugins/kubectl foo，那么在调用 kubectl 命令 kubectl foo 时，我们只需将路径的值更改为 PATH=/usr/local/bin/moreplugins:/usr/local/bin/plugins。\n调用最长的可执行文件名 对于插件文件名而言还有另一种弊端，给定用户路径中的两个插件 kubectl-foo-bar 和 kubectl-foo-bar-baz ，kubectl 插件机制总是为给定的用户命令选择尽可能长的插件名称。下面的一些例子进一步的说明了这一点：\n# for a given kubectl command, the plugin with the longest possible filename will always be preferred kubectl foo bar baz Plugin kubectl-foo-bar-baz is executed kubectl foo bar Plugin kubectl-foo-bar is executed kubectl foo bar baz buz Plugin kubectl-foo-bar-baz is executed, with \u0026quot;buz\u0026quot; as its first argument kubectl foo bar buz Plugin kubectl-foo-bar is executed, with \u0026quot;buz\u0026quot; as its first argument 这种设计选择确保插件子命令可以跨多个文件实现，如果需要，这些子命令可以嵌套在\u0026quot;父\u0026quot;插件命令下：\nls ./plugin_command_tree kubectl-parent kubectl-parent-subcommand kubectl-parent-subcommand-subsubcommand 检查插件警告 你可以使用前面提到的 kubectl plugin list 命令来确保你的插件可以被 kubectl 看到，并且验证没有警告防止它被称为 kubectl 命令。\nkubectl plugin list The following kubectl-compatible plugins are available: test/fixtures/pkg/kubectl/plugins/kubectl-foo /usr/local/bin/kubectl-foo - warning: /usr/local/bin/kubectl-foo is overshadowed by a similarly named plugin: test/fixtures/pkg/kubectl/plugins/kubectl-foo plugins/kubectl-invalid - warning: plugins/kubectl-invalid identified as a kubectl plugin, but it is not executable error: 2 plugin warnings were found 使用命令行 runtime 包 作为 v1.12.0 版本中插件机制更新的一部分，插件作者可以使用另外一组实用程序。这些实用程序在， k8s.io/cli-runtime 存储库，并且可以被编写在 Go 中的插件用来解析和更新， 用户的 KUBECONFIG 文件，获取 REST 客户端与 API 服务器通信，并自动绑定与配置和打印相关的参数。\n插件不必须用 Go 编写才能被 kubectl 识别为有效的插件，但是它们必须使用 Go 才能使用的 CLI Runtime 存储库中的工具和实用程序。\n参见 CLI 插件示例了解 CLI Runtime 存储库中提供的工具的使用示例。\n. heading \u0026ldquo;whatsnext\u0026rdquo; %}}  查看 CLI 插件库示例，查看用 Go 编写的插件的详细示例 如有任何问题，请随时联系 CLI SIG 小组 二进制插件是 beta 版的特性，所以现在是时候为代码库贡献一些想法和改进了。我们也很高兴听到您计划用插件实现什么，所以让我们知道！  "
},
{
	"uri": "https://lijun.in/tasks/manage-hugepages/scheduling-hugepages/",
	"title": "😝 - 管理巨页（HugePages）",
	"tags": [],
	"description": "",
	"content": ". feature-state state=\u0026quot;stable\u0026rdquo; \u0026gt;}}\n作为 GA 特性，Kubernetes 支持在 Pod 应用中使用预先分配的巨页。本文描述了用户如何使用巨页，以及当前的限制。\n. heading \u0026ldquo;prerequisites\u0026rdquo; %}}  为了使节点能够上报巨页容量，Kubernetes 节点必须预先分配巨页。每个节点只能预先分配一种特定规格的巨页。  节点会自动发现全部巨页资源，并作为可供调度的资源进行上报。\nAPI 用户可以通过在容器级别的资源需求中使用资源名称 hugepages-\u0026lt;size\u0026gt; 来使用巨页，其中的 size 是特定节点上支持的以整数值表示的最小二进制单位。 例如，如果节点支持 2048KiB 的页面规格， 它将暴露可供调度的资源 hugepages-2Mi。 与 CPU 或内存不同，巨页不支持过量使用（overcommit）。 注意，在请求巨页资源时，还必须请求内存或 CPU 资源。\napiVersion: v1 kind: Pod metadata: generateName: hugepages-volume- spec: containers: - image: fedora:latest command: - sleep - inf name: example volumeMounts: - mountPath: /hugepages name: hugepage resources: limits: hugepages-2Mi: 100Mi memory: 100Mi requests: memory: 100Mi volumes: - name: hugepage emptyDir: medium: HugePages  巨页的资源请求值必须等于其限制值。该条件在指定了资源限制，而没有指定请求的情况下默认成立。 巨页是被隔离在 pod 作用域的，计划在将来的迭代中实现容器级别的隔离。 巨页可用于 EmptyDir 卷，不过 EmptyDir 卷所使用的巨页数量不能够超出 Pod 请求的巨页数量。 通过带有 SHM_HUGETLB 的 shmget() 使用巨页的应用，必须运行在一个与 proc/sys/vm/hugetlb_shm_group 匹配的补充组下。 通过 ResourceQuota 资源，可以使用 hugepages-\u0026lt;size\u0026gt; 标记控制每个命名空间下的巨页使用量， 类似于使用 cpu 或 memory 来控制其他计算资源。  待实现的特性  在 pod 级别隔离的基础上，支持巨页在容器级别的隔离。 作为服务质量特性，保证巨页的 NUMA 局部性。 支持 LimitRange 。  "
},
{
	"uri": "https://lijun.in/tasks/manage-gpus/scheduling-gpus/",
	"title": "😝 - 调度 GPUs",
	"tags": [],
	"description": "",
	"content": "Kubernetes 支持对节点上的 AMD 和 NVIDA GPU 进行管理，目前处于实验状态。对 NVIDIA GPU 的支持在 v1.6 中加入，已经经历了多次不向后兼容的迭代。而对 AMD GPU 的支持则在 v1.9 中通过 设备插件 加入。\n这个页面介绍了用户如何在不同的 Kubernetes 版本中使用 GPU，以及当前存在的一些限制。\n从 v1.8 起 从 1.8 版本开始，我们推荐通过 设备插件 的方式来使用 GPU。\n在 1.10 版本之前，为了通过设备插件开启 GPU 的支持，我们需要在系统中将 DevicePlugins 这一特性开关显式地设置为 true：--feature-gates=\u0026quot;DevicePlugins=true\u0026quot;。不过， 从 1.10 版本开始，我们就不需要这一步骤了。\n接着你需要在主机节点上安装对应厂商的 GPU 驱动并运行对应厂商的设备插件 (AMD、NVIDIA)。\n当上面的条件都满足，Kubernetes 将会暴露 nvidia.com/gpu 或 amd.com/gpu 来作为 一种可调度的资源。\n你也能通过像请求 cpu 或 memory 一样请求 \u0026lt;vendor\u0026gt;.com/gpu 来在容器中使用 GPU。然而，当你要通过指定资源请求来使用 GPU 时，存在着以下几点限制：\n GPU 仅仅支持在 limits 部分被指定，这表明：  你可以仅仅指定 GPU 的 limits 字段而不必须指定 requests 字段，因为 Kubernetes 会默认使用 limit 字段的值来作为 request 字段的默认值。 你能同时指定 GPU 的 limits 和 requests 字段，但这两个值必须相等。 你不能仅仅指定 GPU 的 request 字段而不指定 limits。   容器（以及 pod）并不会共享 GPU，也不存在对 GPU 的过量使用。 每一个容器能够请求一个或多个 GPU。然而只请求一个 GPU 的一部分是不允许的。  下面是一个例子:\napiVersion: v1 kind: Pod metadata: name: cuda-vector-add spec: restartPolicy: OnFailure containers: - name: cuda-vector-add # https://github.com/kubernetes/kubernetes/blob/v1.7.11/test/images/nvidia-cuda/Dockerfile image: \u0026#34;k8s.gcr.io/cuda-vector-add:v0.1\u0026#34; resources: limits: nvidia.com/gpu: 1 # requesting 1 GPU 部署 AMD GPU 设备插件 官方的 AMD GPU 设备插件 有以下要求：\n Kubernetes 节点必须预先安装 AMD GPU 的 Linux 驱动。  如果你的集群已经启动并且满足上述要求的话，可以这样部署 AMD 设备插件：\n# 针对 Kubernetes v1.9 kubectl create -f https://raw.githubusercontent.com/RadeonOpenCompute/k8s-device-plugin/r1.9/k8s-ds-amdgpu-dp.yaml # 针对 Kubernetes v1.10 kubectl create -f https://raw.githubusercontent.com/RadeonOpenCompute/k8s-device-plugin/r1.10/k8s-ds-amdgpu-dp.yaml 请到 RadeonOpenCompute/k8s-device-plugin 报告有关此设备插件的问题。\n部署 NVIDIA GPU 设备插件 对于 NVIDIA GPUs，目前存在两种设备插件的实现：\n官方的 NVIDIA GPU 设备插件 官方的 NVIDIA GPU 设备插件 有以下要求:\n Kubernetes 的节点必须预先安装了 NVIDIA 驱动 Kubernetes 的节点必须预先安装 nvidia-docker 2.0 Docker 的默认运行时必须设置为 nvidia-container-runtime，而不是 runc NVIDIA 驱动版本 ~= 361.93  如果你的集群已经启动并且满足上述要求的话，可以这样部署 NVIDIA 设备插件：\n# 针对 Kubernetes v1.8 kubectl create -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/v1.8/nvidia-device-plugin.yml # 针对 Kubernetes v1.9 kubectl create -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/v1.9/nvidia-device-plugin.yml 请到 NVIDIA/k8s-device-plugin 报告有关此设备插件的问题。\nGCE 中使用的 NVIDIA GPU 设备插件 GCE 使用的 NVIDIA GPU 设备插件 并不要求使用 nvidia-docker，并且对于任何实现了 Kubernetes CRI 的容器运行时，都应该能够使用。这一实现已经在 Container-Optimized OS 上进行了测试，并且在 1.9 版本之后会有对于 Ubuntu 的实验性代码。\n在你 1.12 版本的集群上，你能使用下面的命令来安装 NVIDIA 驱动以及设备插件：\n# 在容器优化的操作系统上安装 NVIDIA 驱动: kubectl create -f https://raw.githubusercontent.com/GoogleCloudPlatform/container-engine-accelerators/stable/daemonset.yaml # 在 Ubuntu 上安装 NVIDIA 驱动 (实验性质): kubectl create -f https://raw.githubusercontent.com/GoogleCloudPlatform/container-engine-accelerators/stable/nvidia-driver-installer/ubuntu/daemonset.yaml # 安装设备插件: kubectl create -f https://raw.githubusercontent.com/kubernetes/kubernetes/release-1.12/cluster/addons/device-plugins/nvidia-gpu/daemonset.yaml 请到 GoogleCloudPlatform/container-engine-accelerators 报告有关此设备插件以及安装方法的问题。\n集群内存在不同类型的 NVIDIA GPU 如果集群内部的不同节点上有不同类型的 NVIDIA GPU，那么你可以使用 节点标签和节点选择器 来将 pod 调度到合适的节点上。\n例如：\n# 为你的节点加上它们所拥有的加速器类型的标签 kubectl label nodes \u0026lt;node-with-k80\u0026gt; accelerator=nvidia-tesla-k80 kubectl label nodes \u0026lt;node-with-p100\u0026gt; accelerator=nvidia-tesla-p100 对于 AMD GPUs，您可以部署 节点标签器，它会自动给节点打上 GPU 属性标签。目前支持的属性：\n 设备 ID (-device-id) VRAM 大小 (-vram) SIMD 数量(-simd-count) 计算单位数量(-cu-count) 固件和特性版本 (-firmware) GPU 系列，两个字母的首字母缩写(-family)  SI - Southern Islands CI - Sea Islands KV - Kaveri VI - Volcanic Islands CZ - Carrizo AI - Arctic Islands RV - Raven    示例:\n$ kubectl describe node cluster-node-23 Name: cluster-node-23 Roles: \u0026lt;none\u0026gt; Labels: beta.amd.com/gpu.cu-count.64=1 beta.amd.com/gpu.device-id.6860=1 beta.amd.com/gpu.family.AI=1 beta.amd.com/gpu.simd-count.256=1 beta.amd.com/gpu.vram.16G=1 beta.kubernetes.io/arch=amd64 beta.kubernetes.io/os=linux kubernetes.io/hostname=cluster-node-23 Annotations: kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock node.alpha.kubernetes.io/ttl: 0 ......  在 pod 的 spec 字段中指定 GPU 的类型：\napiVersion: v1 kind: Pod metadata: name: cuda-vector-add spec: restartPolicy: OnFailure containers: - name: cuda-vector-add # https://github.com/kubernetes/kubernetes/blob/v1.7.11/test/images/nvidia-cuda/Dockerfile image: \u0026#34;k8s.gcr.io/cuda-vector-add:v0.1\u0026#34; resources: limits: nvidia.com/gpu: 1 nodeSelector: accelerator: nvidia-tesla-p100 # or nvidia-tesla-k80 etc. 这能够保证 pod 能够被调度到你所指定类型的 GPU 的节点上去。\n"
}]