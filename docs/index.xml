<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>文档 on Hugo 主题的 Learn 文档</title>
    <link>https://lijun.in/</link>
    <description>Recent content in 文档 on Hugo 主题的 Learn 文档</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-US</language>
    
	<atom:link href="https://lijun.in/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>容器概述</title>
      <link>https://lijun.in/concepts/containers/overview/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/concepts/containers/overview/</guid>
      <description>容器是一种用来打包已经编译好的代码以及运行时需要的各个依赖项的技术。您运行的每个容器都是可以重复运行的；包含依赖项的标准化意味着您在任何地点运行它都会得到相同的结果。
容器将应用程序和底层主机架构解耦，这使得在不同的云或OS环境中部署应用更加容易。
容器镜像 容器镜像是一个现成的软件包，包含了运行应用程序时所需要的一切：代码和任何运行时所需的东西，应用程序和系统库，以及任何基本设置的默认值。
根据设计，容器是不可变的：你不能更改已经在运行的容器中的代码。如果您有一个容器化的应用程序，想要做一些更改，您需要构建一个新的容器，来包含所做的更改，然后使用已经更新过的镜像来重新创建容器。
##容器运行时
term_id=&amp;quot;container-runtime&amp;rdquo; length=&amp;quot;all&amp;rdquo; &amp;gt;}}
  阅读有关容器镜像 阅读有关 Pods  </description>
    </item>
    
    <item>
      <title>😍 - 标准化词汇表</title>
      <link>https://lijun.in/reference/glossary/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/glossary/</guid>
      <description></description>
    </item>
    
    <item>
      <title>😎 - 你好 Minikube</title>
      <link>https://lijun.in/tutorials/hello-minikube/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tutorials/hello-minikube/</guid>
      <description>本教程向您展示如何使用 Minikube 和 Katacoda 在 Kubernetes 上运行一个简单的 “Hello World” Node.js 应用程序。Katacoda 提供免费的浏览器内 Kubernetes 环境。
. note &amp;gt;}}
如果您已在本地安装 Minikube，也可以按照本教程操作。
. /note &amp;gt;}}
heading &amp;ldquo;objectives&amp;rdquo; %}}  将 &amp;ldquo;Hello World&amp;rdquo; 应用程序部署到 Minikube。 运行应用程序。 查看应用日志  . heading &amp;ldquo;prerequisites&amp;rdquo; %}} 本教程提供了从以下文件构建的容器镜像：
. codenew language=&amp;quot;js&amp;rdquo; file=&amp;quot;minikube/server.js&amp;rdquo; &amp;gt;}}
. codenew language=&amp;quot;conf&amp;rdquo; file=&amp;quot;minikube/Dockerfile&amp;rdquo; &amp;gt;}}
有关 docker build 命令的更多信息，请参阅 Docker 文档。
创建 Minikube 集群   点击 启动终端
. kat-button &amp;gt;}}
. note &amp;gt;}}If you installed Minikube locally, run minikube start.</description>
    </item>
    
    <item>
      <title>Endpoint Slices</title>
      <link>https://lijun.in/concepts/services-networking/endpoint-slices/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/concepts/services-networking/endpoint-slices/</guid>
      <description>feature-state for_k8s_version=&amp;quot;v1.17&amp;rdquo; state=&amp;quot;beta&amp;rdquo;
Endpoint Slices 提供了一种简单的方法来跟踪 Kubernetes 集群中的网络端点（network endpoints）。它们为 Endpoints 提供了一种可伸缩和可拓展的替代方案。
Endpoint Slice 资源 在 Kubernetes 中，EndpointSlice 包含对一组网络端点的引用。指定选择器后，EndpointSlice 控制器会自动为 Kubernetes 服务创建 EndpointSlice。这些 EndpointSlice 将包含对与服务选择器匹配的所有 Pod 的引用。EndpointSlice 通过唯一的服务和端口组合将网络端点组织在一起。
例如，这里是 Kubernetes服务 example 的示例 EndpointSlice 资源。
apiVersion: discovery.k8s.io/v1beta1 kind: EndpointSlice metadata: name: example-abc labels: kubernetes.io/service-name: example addressType: IPv4 ports: - name: http protocol: TCP port: 80 endpoints: - addresses: - &amp;#34;10.1.2.3&amp;#34; conditions: ready: true hostname: pod-1 topology: kubernetes.io/hostname: node-1 topology.kubernetes.io/zone: us-west2-a 默认情况下，由 EndpointSlice 控制器管理的 Endpoint Slice 将有不超过 100 个 endpoints。低于此比例时，Endpoint Slices 应与 Endpoints 和服务进行 1：1 映射，并具有相似的性能。</description>
    </item>
    
    <item>
      <title>kubeadm 概述</title>
      <link>https://lijun.in/reference/setup-tools/kubeadm/kubeadm/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/setup-tools/kubeadm/kubeadm/</guid>
      <description>kubeadm 通过执行必要的操作来启动和运行一个最小可用的集群。它被故意设计为只关心启动集群，而不是准备节点环境的工作。同样的，诸如安装各种各样的可有可无的插件，例如 Kubernetes 控制面板、监控解决方案以及特定云提供商的插件，这些都不在它负责的范围。
相反，我们期望由一个基于 kubeadm 从更高层设计的更加合适的工具来做这些事情；并且，理想情况下，使用 kubeadm 作为所有部署的基础将会使得创建一个符合期望的集群变得容易。
接下可以做什么  kubeadm init 启动一个 Kubernetes 主节点   kubeadm join 启动一个 Kubernetes 工作节点并且将其加入到集群   kubeadm upgrade 更新一个 Kubernetes 集群到新版本   kubeadm config 如果你使用 kubeadm v1.7.x 或者更低版本，你需要对你的集群做一些配置以便使用 kubeadm upgrade 命令   kubeadm token 使用 kubeadm join 来管理令牌   kubeadm reset 还原之前使用 kubeadm init 或者 kubeadm join 对节点产生的改变   kubeadm version 打印出 kubeadm 版本   kubeadm alpha 预览一组可用的新功能以便从社区搜集反馈  </description>
    </item>
    
    <item>
      <title>Kubernetes API 总览</title>
      <link>https://lijun.in/reference/using-api/api-overview/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/using-api/api-overview/</guid>
      <description>此页提供 Kubernetes API 的总览
REST API 是 Kubernetes 的基础架构。组件之间的所有操作和通信，以及外部用户命令都是 API Server 处理的 REST API 调用。因此，Kubernetes 平台中的所有资源被视为 API 对象，并且在 [API](/docs/reference/generated/kubernetes-api/. param &amp;ldquo;version&amp;rdquo; &amp;gt;}}/) 中都有对应的定义项。
大多数操作可以通过 kubectl 命令行界面或其他命令行工具执行，例如 kubeadm，它们本身也使用 API。但是，您也可以使用 REST 调用直接访问 API。
如果您正在使用 Kubernetes API 编写应用程序，请考虑使用 客户端库。
API 版本控制 为了消除字段或重组资源表示形式，Kubernetes 支持多个 API 版本，每个版本在不同的 API 路径下。例如：/api/v1 或者 /apis/extensions/v1beta1。
版本是在 API 级别而非资源或字段级别配置的：
 确保 API 呈现出清晰一致的系统资源和行为视图。 允许控制对已寿终正寝的 API 和/或实验性 API 的访问。  JSON 和 Protobuf 序列化模式在出现模式变更时均遵循这些准则。以下说明同时适用于这两种格式。
. note &amp;gt;}} API 版本和软件版本是间接相关的。API 和发布版本建议 描述了 API 版本和软件版本之间的关系。 .</description>
    </item>
    
    <item>
      <title>Kubernetes 是什么？</title>
      <link>https://lijun.in/concepts/overview/what-is-kubernetes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/concepts/overview/what-is-kubernetes/</guid>
      <description>此页面是 Kubernetes 的概述。
Kubernetes 是一个可移植的、可扩展的开源平台，用于管理容器化的工作负载和服务，可促进声明式配置和自动化。Kubernetes 拥有一个庞大且快速增长的生态系统。Kubernetes 的服务、支持和工具广泛可用。
名称 Kubernetes 源于希腊语，意为 &amp;ldquo;舵手&amp;rdquo; 或 &amp;ldquo;飞行员&amp;rdquo;。Google 在 2014 年开源了 Kubernetes 项目。Kubernetes 建立在 Google 在大规模运行生产工作负载方面拥有十几年的经验的基础上，结合了社区中最好的想法和实践。
言归正传 让我们回顾一下为什么 Kubernetes 如此有用。
传统部署时代： 早期，组织在物理服务器上运行应用程序。无法为物理服务器中的应用程序定义资源边界，这会导致资源分配问题。例如，如果在物理服务器上运行多个应用程序，则可能会出现一个应用程序占用大部分资源的情况，结果可能导致其他应用程序的性能下降。一种解决方案是在不同的物理服务器上运行每个应用程序，但是由于资源利用不足而无法扩展，并且组织维护许多物理服务器的成本很高。
虚拟化部署时代： 作为解决方案，引入了虚拟化功能，它允许您在单个物理服务器的 CPU 上运行多个虚拟机（VM）。虚拟化功能允许应用程序在 VM 之间隔离，并提供安全级别，因为一个应用程序的信息不能被另一应用程序自由地访问。
因为虚拟化可以轻松地添加或更新应用程序、降低硬件成本等等，所以虚拟化可以更好地利用物理服务器中的资源，并可以实现更好的可伸缩性。
每个 VM 是一台完整的计算机，在虚拟化硬件之上运行所有组件，包括其自己的操作系统。
容器部署时代： 容器类似于 VM，但是它们具有轻量级的隔离属性，可以在应用程序之间共享操作系统（OS）。因此，容器被认为是轻量级的。容器与 VM 类似，具有自己的文件系统、CPU、内存、进程空间等。由于它们与基础架构分离，因此可以跨云和 OS 分发进行移植。
容器因具有许多优势而变得流行起来。下面列出了容器的一些好处：
 敏捷应用程序的创建和部署：与使用 VM 镜像相比，提高了容器镜像创建的简便性和效率。 持续开发、集成和部署：通过快速简单的回滚(由于镜像不可变性)，提供可靠且频繁的容器镜像构建和部署。 关注开发与运维的分离：在构建/发布时而不是在部署时创建应用程序容器镜像，从而将应用程序与基础架构分离。 可观察性不仅可以显示操作系统级别的信息和指标，还可以显示应用程序的运行状况和其他指标信号。 跨开发、测试和生产的环境一致性：在便携式计算机上与在云中相同地运行。 云和操作系统分发的可移植性：可在 Ubuntu、RHEL、CoreOS、本地、Google Kubernetes Engine 和其他任何地方运行。 以应用程序为中心的管理：提高抽象级别，从在虚拟硬件上运行 OS 到使用逻辑资源在 OS 上运行应用程序。 松散耦合、分布式、弹性、解放的微服务：应用程序被分解成较小的独立部分，并且可以动态部署和管理 - 而不是在一台大型单机上整体运行。 资源隔离：可预测的应用程序性能。 资源利用：高效率和高密度。  为什么需要 Kubernetes，它能做什么?</description>
    </item>
    
    <item>
      <title>Kubernetes 问题追踪</title>
      <link>https://lijun.in/reference/issues-security/issues/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/issues-security/issues/</guid>
      <description>要报告安全问题，请遵循 Kubernetes 安全问题公开流程。
使用 GitHub Issues 跟踪 Kubernetes 编码工作和公开问题。
 CVE 相关问题  与安全性相关的公告请发送到 kubernetes-security-announce@googlegroups.com 邮件列表。</description>
    </item>
    
    <item>
      <title>Pod 概览</title>
      <link>https://lijun.in/concepts/workloads/pods/pod-overview/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/concepts/workloads/pods/pod-overview/</guid>
      <description>本节提供了 Pod 的概览信息，Pod 是最小可部署的 Kubernetes 对象模型。
理解 Pod Pod 是 Kubernetes 应用程序的基本执行单元，即它是 Kubernetes 对象模型中创建或部署的最小和最简单的单元。Pod 表示在 glossary_tooltip term_id=&amp;quot;cluster&amp;rdquo; &amp;gt;}} 上运行的进程。
Pod 封装了应用程序容器（或者在某些情况下封装多个容器）、存储资源、唯一网络 IP 以及控制容器应该如何运行的选项。 Pod 表示部署单元：Kubernetes 中应用程序的单个实例，它可能由单个glossary_tooltip text=&amp;quot;容器&amp;rdquo; term_id=&amp;quot;container&amp;rdquo; &amp;gt;}} 或少量紧密耦合并共享资源的容器组成。
Docker 是 Kubernetes Pod 中最常用的容器运行时，但 Pod 也能支持其他的容器运行时。
Kubernetes 集群中的 Pod 可被用于以下两个主要用途：
 运行单个容器的 Pod。&amp;ldquo;每个 Pod 一个容器&amp;quot;模型是最常见的 Kubernetes 用例；在这种情况下，可以将 Pod 看作单个容器的包装器，并且 Kubernetes 直接管理 Pod，而不是容器。 运行多个协同工作的容器的 Pod。 Pod 可能封装由多个紧密耦合且需要共享资源的共处容器组成的应用程序。 这些位于同一位置的容器可能形成单个内聚的服务单元 —— 一个容器将文件从共享卷提供给公众，而另一个单独的“挂斗”（sidecar）容器则刷新或更新这些文件。 Pod 将这些容器和存储资源打包为一个可管理的实体。 Kubernetes 博客 上有一些其他的 Pod 用例信息。更多信息请参考：   分布式系统工具包：容器组合的模式 容器设计模式  每个 Pod 表示运行给定应用程序的单个实例。如果希望横向扩展应用程序（例如，运行多个实例），则应该使用多个 Pod，每个应用实例使用一个 Pod 。在 Kubernetes 中，这通常被称为 副本。通常使用一个称为控制器的抽象来创建和管理一组副本 Pod。更多信息请参见 Pod 和控制器。</description>
    </item>
    
    <item>
      <title>ReplicaSet</title>
      <link>https://lijun.in/concepts/workloads/controllers/replicaset/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/concepts/workloads/controllers/replicaset/</guid>
      <description>ReplicaSet 是下一代的 Replication Controller。 ReplicaSet 和 Replication Controller 的唯一区别是选择器的支持。ReplicaSet 支持新的基于集合的选择器需求，这在标签用户指南中有描述。而 Replication Controller 仅支持基于相等选择器的需求。
怎样使用 ReplicaSet 大多数支持 Replication Controllers 的kubectl命令也支持 ReplicaSets。但rolling-update 命令是个例外。如果您想要滚动更新功能请考虑使用 Deployment。rolling-update 命令是必需的，而 Deployment 是声明性的，因此我们建议通过 rollout命令使用 Deployment。
虽然 ReplicaSets 可以独立使用，但今天它主要被Deployments 用作协调 Pod 创建、删除和更新的机制。 当您使用 Deployment 时，您不必担心还要管理它们创建的 ReplicaSet。Deployment 会拥有并管理它们的 ReplicaSet。
什么时候使用 ReplicaSet ReplicaSet 确保任何时间都有指定数量的 Pod 副本在运行。 然而，Deployment 是一个更高级的概念，它管理 ReplicaSet，并向 Pod 提供声明式的更新以及许多其他有用的功能。 因此，我们建议使用 Deployment 而不是直接使用 ReplicaSet，除非您需要自定义更新业务流程或根本不需要更新。
这实际上意味着，您可能永远不需要操作 ReplicaSet 对象：而是使用 Deployment，并在 spec 部分定义您的应用。
示例 codenew file=&amp;quot;controllers/frontend.yaml&amp;rdquo; &amp;gt;}}
将此清单保存到 frontend.yaml 中，并将其提交到 Kubernetes 集群，应该就能创建 yaml 文件所定义的 ReplicaSet 及其管理的 Pod。</description>
    </item>
    
    <item>
      <title>Service 拓扑</title>
      <link>https://lijun.in/concepts/services-networking/service-topology/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/concepts/services-networking/service-topology/</guid>
      <description>feature-state for_k8s_version=&amp;quot;v1.17&amp;rdquo; state=&amp;quot;alpha&amp;rdquo; &amp;gt;}}
Service 拓扑可以让一个服务基于集群的 Node 拓扑进行流量路由。例如，一个服务可以指定流量是被优先路由到一个和客户端在同一个 Node 或者在同一可用区域的端点。
介绍 默认情况下，发往 ClusterIP 或者 NodePort 服务的流量可能会被路由到任意一个服务后端的地址上。从 Kubernetes 1.7 开始，可以将“外部”流量路由到节点上运行的 pod 上，但不支持 ClusterIP 服务，更复杂的拓扑 — 比如分区路由 — 也还不支持。通过允许 Service 创建者根据源 Node 和目的 Node 的标签来定义流量路由策略，Service 拓扑特性实现了服务流量的路由。
通过对源 Node 和目的 Node 标签的匹配，运营者可以使用任何符合运营者要求的度量值来指定彼此“较近”和“较远”的节点组。例如，对于在公有云上的运营者来说，更偏向于把流量控制在同一区域内，因为区域间的流量是有费用成本的，而区域内的流量没有。其它常用需求还包括把流量路由到由 DaemonSet 管理的本地 Pod 上，或者把保持流量在连接同一机架交换机的 Node 上，以获得低延时。
前提条件 为了启用拓扑感知服务路由功能，必须要满足以下一些前提条件：
 Kubernetes 的版本不低于 1.17 Kube-proxy 运行在 iptables 模式或者 IPVS 模式 启用 端点切片功能  启用 Service 拓扑 要启用 Service 拓扑，就要给 kube-apiserver 和 kube-proxy 启用 ServiceTopology 功能：</description>
    </item>
    
    <item>
      <title>Services</title>
      <link>https://lijun.in/concepts/services-networking/service/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/concepts/services-networking/service/</guid>
      <description>apiVersion: v1 kind: Service # string Required 资源类型 metadata: # Object Required 元数据 name: string # string Required Service名称 namespace: string # string Required 命名空间，默认default labels: # list Required 自定义标签属性列表 - name: string annotations: # list Required 自定义注解属性列表 - name: string spec: # Object Required 详细描述 selector: [] # list Required Label Selector配置，将选择具有指定Label标签的Pod作为管理范围 type: string # string Required Service的类型，指定Service的访问方式，默认为ClusterIP # ClusterIP：虚拟的服务IP地址，改地址用于Kubernetes集群内部的Pod访问，在Node上kube-proxy通过设置iptables规则进行转发 # NodePort: 使用宿主机的端口，使能够访问个Node的外部客户端通过Node的IP地址和端口号就能访问服务 # LoadBalancer: 使用外接负载均衡完成到服务的负载分发，需要在spec.status.loadBalancer字段指定外部负载均衡器的IP地址， # 并同时定义nodePort和clusterIP，用于公有云环境 clusterIP: string # string 虚拟服务IP地址，当type=ClusterIP时，如果不指定，则系统自动分配，也可以手动指定；当type=LoaderBalancer需要指定 sessionAffinity: string # string 是否支持Session，可选值为ClientIP，默认为空 # ClientIP: 表示将同一个客户端（根据IP地址决定）的访问请求都转发到同一个后端Pod ports: # list 需要暴露的端口列表 - name: string # 端口名称 protocol: string # 端口协议 支持tcp和udp，默认为tcp port: int # 服务监听的端口号 targetPort: int # 需要转发到后端Pod的端口号 nodePort: int # 当spec.</description>
    </item>
    
    <item>
      <title>Using a Service to Expose Your App</title>
      <link>https://lijun.in/tutorials/kubernetes-basics/expose/expose-intro/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tutorials/kubernetes-basics/expose/expose-intro/</guid>
      <description>Objectives  Learn about a Service in Kubernetes Understand how labels and LabelSelector objects relate to a Service Expose an application outside a Kubernetes cluster using a Service   Overview of Kubernetes Services Kubernetes Pods are mortal. Pods in fact have a lifecycle. When a worker node dies, the Pods running on the Node are also lost. A ReplicaSet might then dynamically drive the cluster back to desired state via creation of new Pods to keep your application running.</description>
    </item>
    
    <item>
      <title>Volumes</title>
      <link>https://lijun.in/concepts/storage/volumes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/concepts/storage/volumes/</guid>
      <description>容器中的文件在磁盘上是临时存放的，这给容器中运行的特殊应用程序带来一些问题。 首先，当容器崩溃时，kubelet 将重新启动容器，容器中的文件将会丢失——因为容器会以干净的状态重建。 其次，当在一个 Pod 中同时运行多个容器时，常常需要在这些容器之间共享文件。 Kubernetes 抽象出 Volume 对象来解决这两个问题。
阅读本文前建议您熟悉一下 Pods。
背景 Docker 也有 Volume 的概念，但对它只有少量且松散的管理。 在 Docker 中，Volume 是磁盘上或者另外一个容器内的一个目录。 直到最近，Docker 才支持对基于本地磁盘的 Volume 的生存期进行管理。 虽然 Docker 现在也能提供 Volume 驱动程序，但是目前功能还非常有限（例如，截至 Docker 1.7，每个容器只允许有一个 Volume 驱动程序，并且无法将参数传递给卷）。
另一方面，Kubernetes 卷具有明确的生命周期——与包裹它的 Pod 相同。 因此，卷比 Pod 中运行的任何容器的存活期都长，在容器重新启动时数据也会得到保留。 当然，当一个 Pod 不再存在时，卷也将不再存在。也许更重要的是，Kubernetes 可以支持许多类型的卷，Pod 也能同时使用任意数量的卷。
卷的核心是包含一些数据的目录，Pod 中的容器可以访问该目录。 特定的卷类型可以决定这个目录如何形成的，并能决定它支持何种介质，以及目录中存放什么内容。
使用卷时, Pod 声明中需要提供卷的类型 (.spec.volumes 字段)和卷挂载的位置 (.spec.containers.volumeMounts 字段).
容器中的进程能看到由它们的 Docker 镜像和卷组成的文件系统视图。 Docker 镜像 位于文件系统层次结构的根部，并且任何 Volume 都挂载在镜像内的指定路径上。 卷不能挂载到其他卷，也不能与其他卷有硬链接。 Pod 中的每个容器必须独立地指定每个卷的挂载位置。
Volume 的类型 Kubernetes 支持下列类型的卷：</description>
    </item>
    
    <item>
      <title>为命名空间配置默认的内存请求和限制</title>
      <link>https://lijun.in/tasks/administer-cluster/manage-resources/memory-default-namespace/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/administer-cluster/manage-resources/memory-default-namespace/</guid>
      <description>本文介绍怎样给命名空间配置默认的内存请求和限制。如果在一个有默认内存限制的命名空间创建容器，该容器没有声明自己的内存限制时，将会被指定默认内存限制。Kubernetes 还为某些情况指定了默认的内存请求，本章后面会进行介绍。
. heading &amp;ldquo;prerequisites&amp;rdquo; %}} . include &amp;ldquo;task-tutorial-prereqs.md&amp;rdquo; &amp;gt;}} . version-check &amp;gt;}}
你的集群中的每个节点必须至少有2 GiB的内存。
创建命名空间 创建一个命名空间，以便本练习中所建的资源与集群的其余资源相隔离。
kubectl create namespace default-mem-example 创建 LimitRange 和 Pod 这里给出了一个限制范围对象的配置文件。该配置声明了一个默认的内存请求和一个默认的内存限制。
. codenew file=&amp;quot;admin/resource/memory-defaults.yaml&amp;rdquo; &amp;gt;}}
在 default-mem-example 命名空间创建限制范围：
kubectl create -f https://k8s.io/examples/admin/resource/memory-defaults.yaml --namespace=default-mem-example 现在，如果在 default-mem-example 命名空间创建容器，并且该容器没有声明自己的内存请求和限制值，它将被指定一个默认的内存请求256 MiB和一个默认的内存限制512 Mib。
. codenew file=&amp;quot;admin/resource/memory-defaults-pod.yaml&amp;rdquo; &amp;gt;}}
创建 Pod
kubectl create -f https://k8s.io/examples/admin/resource/memory-defaults-pod.yaml --namespace=default-mem-example 查看 Pod 的详情：
kubectl get pod default-mem-demo --output=yaml --namespace=default-mem-example 输出内容显示该Pod的容器有一个256 MiB的内存请求和一个512 MiB的内存限制。这些都是限制范围声明的默认值。
containers: - image: nginx imagePullPolicy: Always name: default-mem-demo-ctr resources: limits: memory: 512Mi requests: memory: 256Mi 删除你的 Pod：</description>
    </item>
    
    <item>
      <title>为容器和 Pod 分配内存资源</title>
      <link>https://lijun.in/tasks/configure-pod-container/assign-memory-resource/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/configure-pod-container/assign-memory-resource/</guid>
      <description>此页面显示如何将内存 请求 （request）和内存 限制 （limit）分配给一个容器。我们保障容器拥有它请求数量的内存，但不允许使用超过限制数量的内存。
. heading &amp;ldquo;prerequisites&amp;rdquo; %}} . include &amp;ldquo;task-tutorial-prereqs.md&amp;rdquo; &amp;gt;}} . version-check &amp;gt;}}
您集群中的每个节点必须拥有至少 300 MiB 的内存。
该页面上的一些步骤要求您在集群中运行 metrics-server 服务。如果您已经有在运行中的 metrics-server，则可以跳过这些步骤。
如果您运行的是 Minikube，可以运行下面的命令启用 metrics-server：
minikube addons enable metrics-server 要查看 metrics-server 或资源指标 API (metrics.k8s.io) 是否已经运行，请运行以下命令：
kubectl get apiservices 如果资源指标 API 可用，则输出结果将包含对 metrics.k8s.io 的引用信息。
NAME v1beta1.metrics.k8s.io 创建命名空间 创建一个命名空间，以便将本练习中创建的资源与集群的其余部分隔离。
kubectl create namespace mem-example 指定内存请求和限制 要为容器指定内存请求，请在容器资源清单中包含 resources：requests 字段。 同理，要指定内存限制，请包含 resources：limits。
在本练习中，您将创建一个拥有一个容器的 Pod。 容器将会请求 100 MiB 内存，并且内存会被限制在 200 MiB 以内。 这是 Pod 的配置文件：</description>
    </item>
    
    <item>
      <title>为容器设置启动时要执行的命令及其入参</title>
      <link>https://lijun.in/tasks/inject-data-application/define-command-argument-container/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/inject-data-application/define-command-argument-container/</guid>
      <description>本页将展示如何为 . glossary_tooltip term_id=&amp;quot;pod&amp;rdquo; &amp;gt;}} 中的容器设置启动时要执行的命令及其入参。
. heading &amp;ldquo;prerequisites&amp;rdquo; %}} . include &amp;ldquo;task-tutorial-prereqs.md&amp;rdquo; &amp;gt;}} . version-check &amp;gt;}}
创建 Pod 时设置命令及入参 创建 Pod 时，可以为其下的容器设置启动时要执行的命令及其入参。如果要设置命令，就填写在配置文件的 command 字段下，如果要设置命令的入参，就填写在配置文件的 args 字段下。一旦 Pod 创建完成，该命令及其入参就无法再进行更改了。
如果在配置文件中设置了容器启动时要执行的命令及其入参，那么容器镜像中自带的命令与入参将会被覆盖而不再执行。如果配置文件中只是设置了入参，却没有设置其对应的命令，那么容器镜像中自带的命令会使用该新入参作为其执行时的入参。
. note &amp;gt;}} 在有些容器运行时中，command 字段对应 entrypoint，请参阅下面的 注意。 . /note &amp;gt;}}
本示例中，将创建一个只包含单个容器的 Pod。在 Pod 配置文件中设置了一个命令与两个入参：
. codenew file=&amp;quot;pods/commands.yaml&amp;rdquo; &amp;gt;}}
  基于 YAML 文件创建一个 Pod：
kubectl apply -f https://k8s.io/examples/pods/commands.yaml    获取正在运行的 Pods：
kubectl get pods ``
查询结果显示在 command-demo 这个 Pod 下运行的容器已经启动完成。</description>
    </item>
    
    <item>
      <title>使用 Calico 作为 NetworkPolicy</title>
      <link>https://lijun.in/tasks/administer-cluster/network-policy-provider/calico-network-policy/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/administer-cluster/network-policy-provider/calico-network-policy/</guid>
      <description>本页展示了两种在 Kubernetes 上快速创建 Calico 集群的方法。
. heading &amp;ldquo;prerequisites&amp;rdquo; %}} 决定您想部署一个云 还是 本地 集群。
在 Google Kubernetes Engine (GKE) 上创建一个 Calico 集群 先决条件: gcloud
  启动一个带有 Calico 的 GKE 集群，只需加上flag --enable-network-policy。
语法
gcloud container clusters create [CLUSTER_NAME] --enable-network-policy 示例
gcloud container clusters create my-calico-cluster --enable-network-policy   使用如下命令验证部署是否正确。
kubectl get pods --namespace=kube-system Calico 的 pods 名以 calico 打头，检查确认每个 pods 状态为 Running。
  使用 kubeadm 创建一个本地 Calico 集群 在15分钟内使用 kubeadm 得到一个本地单主机 Calico 集群，请参考 Calico 快速入门。</description>
    </item>
    
    <item>
      <title>使用 CronJob 运行自动化任务</title>
      <link>https://lijun.in/tasks/job/automated-tasks-with-cron-jobs/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/job/automated-tasks-with-cron-jobs/</guid>
      <description>你可以利用 CronJobs 执行基于时间调度的任务。这些自动化任务和 Linux 或者 Unix 系统的 Cron 任务类似。
CronJobs 在创建周期性以及重复性的任务时很有帮助，例如执行备份操作或者发送邮件。CronJobs 也可以在特定时间调度单个任务，例如你想调度低活跃周期的任务。
. note &amp;gt;}}
从集群版本1.8开始，batch/v2alpha1 API 组中的 CronJob 资源已经被废弃。 你应该切换到 API 服务器默认启用的 batch/v1beta1 API 组。本文中的所有示例使用了batch/v1beta1。 . /note &amp;gt;}}
CronJobs 有一些限制和特点。 例如，在特定状况下，同一个 CronJob 可以创建多个任务。 因此，任务应该是幂等的。 查看更多限制，请参考 CronJobs。
. heading &amp;ldquo;prerequisites&amp;rdquo; %}}  . include &amp;ldquo;task-tutorial-prereqs.md&amp;rdquo; &amp;gt;}} . version-check &amp;gt;}}   你需要一个版本 &amp;gt;=1.8 且工作正常的 Kubernetes 集群。对于更早的版本（ &amp;lt;1.8 ），你需要对 API 服务器设置 --runtime-config=batch/v2alpha1=true 来开启 batch/v2alpha1 API，(更多信息请查看 为你的集群开启或关闭 API 版本 ), 然后重启 API 服务器和控制管理器。  创建 CronJob CronJob 需要一个配置文件。 本例中 CronJob 的.</description>
    </item>
    
    <item>
      <title>使用 kubectl 创建 Deployment</title>
      <link>https://lijun.in/tutorials/kubernetes-basics/deploy-app/deploy-intro/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tutorials/kubernetes-basics/deploy-app/deploy-intro/</guid>
      <description>Objectives -- 目标 Learn about application Deployments. Deploy your first app on Kubernetes with kubectl.  --  学习了解应用的部署 使用 kubectl 在 Kubernetes 上部署第一个应用   Kubernetes Deployments -- Kubernetes 部署 Once you have a running Kubernetes cluster, you can deploy your containerized applications on top of it. To do so, you create a Kubernetes Deployment configuration. The Deployment instructs Kubernetes how to create and update instances of your application.</description>
    </item>
    
    <item>
      <title>使用 Minikube 创建集群</title>
      <link>https://lijun.in/tutorials/kubernetes-basics/create-cluster/cluster-intro/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tutorials/kubernetes-basics/create-cluster/cluster-intro/</guid>
      <description>目标  了解 Kubernetes 集群。 了解 Minikube 。 使用在线终端开启一个 Kubernetes 集群。    Kubernetes 集群   Kubernetes 协调一个高可用计算机集群，每个计算机作为独立单元互相连接工作。 Kubernetes 中的抽象允许您将容器化的应用部署到群集，而无需将它们绑定到某个特定的独立计算机。为了使用这种新的部署模型，应用需要以将应用与单个主机分离的方式打包：它们需要被容器化。与过去的那种应用直接以包的方式深度与主机集成的部署模型相比，容器化应用更灵活、更可用。 Kubernetes 以更高效的方式跨群集自动分发和调度应用容器。 Kubernetes 是一个开源平台，并且可应用于生产环境。 一个 Kubernetes 集群包含两种类型的资源:   Master 调度整个集群  Nodes 负责运行应用   总结:   Kubernetes 集群  Minikube     Kubernetes 是一个生产级别的开源平台，可协调在计算机集群内和跨计算机集群的应用容器的部署（调度）和执行. 
   集群图      Master 负责管理整个集群。 Master 协调集群中的所有活动，例如调度应用、维护应用的所需状态、应用扩容以及推出新的更新。
 Node 是一个虚拟机或者物理机，它在 Kubernetes 集群中充当工作机器的角色 每个Node都有 Kubelet , 它管理 Node 而且是 Node 与 Master 通信的代理。 Node 还应该具有用于​​处理容器操作的工具，例如 Docker 或 rkt 。处理生产级流量的 Kubernetes 集群至少应具有三个 Node 。</description>
    </item>
    
    <item>
      <title>公开外部 IP 地址以访问集群中应用程序</title>
      <link>https://lijun.in/tutorials/stateless-application/expose-external-ip-address/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tutorials/stateless-application/expose-external-ip-address/</guid>
      <description>此页面显示如何创建公开外部 IP 地址的 Kubernetes 服务对象。
. heading &amp;ldquo;prerequisites&amp;rdquo; %}}   安装 kubectl.
  使用 Google Kubernetes Engine 或 Amazon Web Services 等云供应商创建 Kubernetes 群集。 本教程创建了一个外部负载均衡器，需要云供应商。
  配置 kubectl 与 Kubernetes API 服务器通信。有关说明，请参阅云供应商文档。
  . heading &amp;ldquo;objectives&amp;rdquo; %}}  运行 Hello World 应用程序的五个实例。 创建一个公开外部 IP 地址的 Service 对象。 使用 Service 对象访问正在运行的应用程序。  为一个在五个 pod 中运行的应用程序创建服务  在集群中运行 Hello World 应用程序：  . codenew file=&amp;quot;service/load-balancer-example.yaml&amp;rdquo; &amp;gt;}}
kubectl apply -f https://k8s.</description>
    </item>
    
    <item>
      <title>安装 kubeadm</title>
      <link>https://lijun.in/setup/production-environment/tools/kubeadm/install-kubeadm/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/setup/production-environment/tools/kubeadm/install-kubeadm/</guid>
      <description>本页面显示如何安装 kubeadm 工具箱。 有关在执行此安装过程后如何使用 kubeadm 创建集群的信息，请参见使用 kubeadm 创建集群 页面。
. heading &amp;ldquo;prerequisites&amp;rdquo; %}}  一台或多台运行着下列系统的机器：  Ubuntu 16.04+ Debian 9+ CentOS 7 Red Hat Enterprise Linux (RHEL) 7 Fedora 25+ HypriotOS v1.0.1+ Container Linux (测试 1800.6.0 版本)   每台机器 2 GB 或更多的 RAM (如果少于这个数字将会影响您应用的运行内存) 2 CPU 核或更多 集群中的所有机器的网络彼此均能相互连接(公网和内网都可以) 节点之中不可以有重复的主机名、MAC 地址或 product_uuid。请参见这里 了解更多详细信息。 开启机器上的某些端口。请参见这里 了解更多详细信息。 禁用交换分区。为了保证 kubelet 正常工作，您 必须 禁用交换分区。  确保每个节点上 MAC 地址和 product_uuid 的唯一性  您可以使用命令 ip link 或 ifconfig -a 来获取网络接口的 MAC 地址 可以使用 sudo cat /sys/class/dmi/id/product_uuid 命令对 product_uuid 校验  一般来讲，硬件设备会拥有唯一的地址，但是有些虚拟机的地址可能会重复。Kubernetes 使用这些值来唯一确定集群中的节点。 如果这些值在每个节点上不唯一，可能会导致安装失败。</description>
    </item>
    
    <item>
      <title>安装并设置 kubectl</title>
      <link>https://lijun.in/tasks/tools/install-kubectl/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/tools/install-kubectl/</guid>
      <description>在 Kubernetes 上使用 Kubernetes 命令行工具 kubectl 部署和管理应用程序。使用 kubectl，您可以检查集群资源；创建、删除和更新组件；查看您的新集群；并启动实例应用程序。
. heading &amp;ldquo;prerequisites&amp;rdquo; %}} 您必须使用与集群小版本号差别为一的 kubectl 版本。例如，1.2版本的客户端应该与1.1版本、1.2版本和1.3版本的主节点一起使用。使用最新版本的 kubectl 有助于避免无法预料的问题。
安装 kubectl 以下是一些安装 kubectl 的方法。
使用本地软件包管理软件安装 kubectl 二进制文件 . tabs name=&amp;quot;kubectl_install&amp;rdquo; &amp;gt;}} . tab name=&amp;quot;Ubuntu, Debian or HypriotOS&amp;rdquo; codelang=&amp;quot;bash&amp;rdquo; &amp;gt;}} sudo apt-get update &amp;amp;&amp;amp; sudo apt-get install -y apt-transport-https curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add - echo &amp;ldquo;deb https://apt.kubernetes.io/ kubernetes-xenial main&amp;rdquo; | sudo tee -a /etc/apt/sources.list.d/kubernetes.list sudo apt-get update sudo apt-get install -y kubectl .</description>
    </item>
    
    <item>
      <title>容器运行时</title>
      <link>https://lijun.in/setup/production-environment/container-runtimes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/setup/production-environment/container-runtimes/</guid>
      <description>. feature-state for_k8s_version=&amp;quot;v1.6&amp;rdquo; state=&amp;quot;stable&amp;rdquo; &amp;gt;}}
Kubernetes 使用容器运行时来实现在 pod 中运行容器。 这是各种运行时的安装说明。
. caution &amp;gt;}}
我们发现 runc 在运行容器，处理系统文件描述符时存在一个漏洞。 恶意容器可以利用此漏洞覆盖 runc 二进制文件的内容，并以此在主机系统的容器上运行任意的命令。
请参考此链接以获取有关此问题的更多信息 [cve-2019-5736 : runc vulnerability ] (https://access.redhat.com/security/cve/cve-2019-5736) . /caution &amp;gt;}}
适用性 . note &amp;gt;}}
本文档是为在 Linux 上安装 CRI 的用户编写的。 对于其他操作系统，请查找特定于您平台的文档。 . /note &amp;gt;}}
您应该以 root 身份执行本指南中的所有命令。 例如，使用 sudo 前缀命令，或者成为 root 并以该用户身份运行命令。
Cgroup 驱动程序 当某个 Linux 系统发行版使用 systemd 作为其初始化系统时，初始化进程会生成并使用一个 root 控制组 （cgroup），并充当 cgroup 管理器。 systemd 与 cgroup 集成紧密，并将为每个进程分配 cgroup。 您也可以配置容器运行时和 kubelet 使用 cgroupfs。 连同 systemd 一起使用 cgroupfs 意味着将有两个不同的 cgroup 管理器。</description>
    </item>
    
    <item>
      <title>执行滚动更新</title>
      <link>https://lijun.in/tutorials/kubernetes-basics/update/update-intro/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tutorials/kubernetes-basics/update/update-intro/</guid>
      <description>Objectives  Perform a rolling update using kubectl. -- 使用 kubectl 执行滚动更新。   Updating an application -- 更新应用程序 Users expect applications to be available all the time and developers are expected to deploy new versions of them several times a day. In Kubernetes this is done with rolling updates. Rolling updates allow Deployments&#39; update to take place with zero downtime by incrementally updating Pods instances with new ones.</description>
    </item>
    
    <item>
      <title>扩展 Kubernetes 集群</title>
      <link>https://lijun.in/concepts/extend-kubernetes/extend-cluster/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/concepts/extend-kubernetes/extend-cluster/</guid>
      <description>Kubernetes 是高度可配置和可扩展的。因此，极少需要分发或提交补丁代码给 Kubernetes 项目。
本文档介绍自定义 Kubernetes 集群的选项。本文档的目标读者 text=&amp;quot;cluster operators&amp;rdquo; term_id=&amp;quot;cluster-operator&amp;rdquo; &amp;gt;}} 是希望了解如何使 Kubernetes 集群满足其业务环境需求的集群运维人员。Kubernetes 项目的贡献者 text=&amp;quot;Contributors&amp;rdquo; term_id=&amp;quot;contributor&amp;rdquo; &amp;gt;}} 或潜在的平台开发人员 text=&amp;quot;Platform Developers&amp;rdquo; term_id=&amp;quot;platform-developer&amp;rdquo; &amp;gt;}} 也可以从本文找到有用的信息，如对已存在扩展点和模式的介绍，以及它们的权衡和限制。
概述 定制方法可以大致分为 配置 和 扩展 。配置 只涉及更改标志参数、本地配置文件或 API 资源；扩展 涉及运行额外的程序或服务。本文档主要内容是关于扩展。
配置 关于 配置文件 和 标志 的说明文档位于在线文档的参考部分，按照二进制组件各自描述：
 kubelet kube-apiserver kube-controller-manager kube-scheduler.  在托管的 Kubernetes 服务或受控安装的 Kubernetes 版本中，标志和配置文件可能并不总是可以更改的。而且当它们可以进行更改时，它们通常只能由集群管理员进行更改。此外，标志和配置文件在未来的 Kubernetes 版本中可能会发生变化，并且更改设置后它们可能需要重新启动进程。出于这些原因，只有在没有其他选择的情况下才使用它们。
内置策略 API ，例如 ResourceQuota、PodSecurityPolicy、NetworkPolicy 和基于角色的权限控制 (RBAC)，是内置的 Kubernetes API。API 通常与托管的 Kubernetes 服务和受控的 Kubernetes 安装一起使用。 它们是声明性的，并使用与其他 Kubernetes 资源（如 Pod ）相同的约定，所以新的集群配置可以重复使用，并以与应用程序相同的方式进行管理。而且，当他们变稳定后，他们和其他 Kubernetes API 一样享受定义支持政策。出于这些原因，在合适的情况下它们优先于 配置文件 和 标志 被使用。</description>
    </item>
    
    <item>
      <title>扩展资源的资源箱打包</title>
      <link>https://lijun.in/concepts/configuration/resource-bin-packing/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/concepts/configuration/resource-bin-packing/</guid>
      <description>for_k8s_version=&amp;quot;1.16&amp;rdquo; state=&amp;quot;alpha&amp;rdquo; &amp;gt;}}
可以将 kube-scheduler 配置为使用 RequestedToCapacityRatioResourceAllocation 优先级函数启用资源箱打包以及扩展资源。 优先级函数可用于根据自定义需求微调 kube-scheduler 。
使用 RequestedToCapacityRatioResourceAllocation 启用装箱 在 Kubernetes 1.15 之前，Kube-scheduler 用于允许根据主要资源，如 CPU 和内存对容量之比的请求对节点进行评分。 Kubernetes 1.16 在优先级函数中添加了一个新参数，该参数允许用户指定资源以及每个资源的权重，以便根据容量之比的请求为节点评分。 这允许用户通过使用适当的参数来打包扩展资源，从而提高了大型集群中稀缺资源的利用率。 RequestedToCapacityRatioResourceAllocation 优先级函数的行为可以通过名为 requestedToCapacityRatioArguments 的配置选项进行控制。 这个论证由两个参数 shape 和 resources 组成。 Shape 允许用户根据 utilization 和 score 值将功能调整为要求最少或要求最高的功能。 资源由 name 和 weight 组成，name 指定评分时要考虑的资源，weight 指定每种资源的权重。
以下是一个配置示例，该配置将 requestedToCapacityRatioArguments 设置为扩展资源 intel.com/foo 和 intel.com/bar 的装箱行为
{ &amp;#34;kind&amp;#34; : &amp;#34;Policy&amp;#34;, &amp;#34;apiVersion&amp;#34; : &amp;#34;v1&amp;#34;, ... &amp;#34;priorities&amp;#34; : [ ... { &amp;#34;name&amp;#34;: &amp;#34;RequestedToCapacityRatioPriority&amp;#34;, &amp;#34;weight&amp;#34;: 2, &amp;#34;argument&amp;#34;: { &amp;#34;requestedToCapacityRatioArguments&amp;#34;: { &amp;#34;shape&amp;#34;: [ {&amp;#34;utilization&amp;#34;: 0, &amp;#34;score&amp;#34;: 0}, {&amp;#34;utilization&amp;#34;: 100, &amp;#34;score&amp;#34;: 10} ], &amp;#34;resources&amp;#34;: [ {&amp;#34;name&amp;#34;: &amp;#34;intel.</description>
    </item>
    
    <item>
      <title>查看 pod 和工作节点</title>
      <link>https://lijun.in/tutorials/kubernetes-basics/explore/explore-intro/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tutorials/kubernetes-basics/explore/explore-intro/</guid>
      <description>Objectives -- 目标  Learn about Kubernetes Pods. Learn about Kubernetes Nodes. Troubleshoot deployed applications. -- 了解 Kubernetes Pod。 了解 Kubernetes 工作节点。 对已部署的应用故障排除。   Kubernetes Pods -- Kubernetes Pods When you created a Deployment in Module 2, Kubernetes created a Pod to host your application instance. A Pod is a Kubernetes abstraction that represents a group of one or more application containers (such as Docker or rkt), and some shared resources for those containers.</description>
    </item>
    
    <item>
      <title>特性门控</title>
      <link>https://lijun.in/reference/command-line-tools-reference/feature-gates/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/command-line-tools-reference/feature-gates/</guid>
      <description>本页详述了管理员可以在不同的 Kubernetes 组件上指定的各种特性门控。
关于特性各个阶段的说明，请参见特性阶段。
概述 特性门控是描述 Kubernetes 特性的一组键值对。您可以在 Kubernetes 的每一个组件中使用 --feature-gates flag 来启用或禁用这些特性。
每个 Kubernetes 组件都支持启用或禁用与该组件相关的一组特性门控。 使用 -h 参数来查看所有组件支持的完整特性门控。 要为诸如 kubelet 之类的组件设置特性门控，请使用 --feature-gates 参数，并向其传递一组特性：
--feature-gates=&amp;#34;...,DynamicKubeletConfig=true&amp;#34; 下表总结了在不同的 Kubernetes 组件上可以设置的特性门控。
 引入特性或更改其发布阶段后，&amp;ldquo;Since&amp;rdquo; 列将包含 Kubernetes 版本。 &amp;ldquo;Until&amp;rdquo; 列（如果不为空）包含最后一个 Kubernetes 版本，您仍可以在其中使用特性门控。 如果某个特性处于 Alpha 或 Beta 状态，您可以在 Alpha 和 Beta 特性门控表中找到该特性。 如果某个特性处于稳定状态，您可以在毕业和废弃特性门控表.中找到该特性的所有阶段。 毕业和废弃特性门控表 还列出了废弃的和已被移除的特性。  Alpha 和 Beta 的特性门控 . table caption=&amp;quot;处于 Alpha 或 Beta 状态的特性门控&amp;rdquo; &amp;gt;}}
   特性 默认值 状态 开始(Since) 结束(Until)     APIListChunking false Alpha 1.</description>
    </item>
    
    <item>
      <title>理解 Kubernetes 对象</title>
      <link>https://lijun.in/concepts/overview/working-with-objects/kubernetes-objects/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/concepts/overview/working-with-objects/kubernetes-objects/</guid>
      <description>本页说明了 Kubernetes 对象在 Kubernetes API 中是如何表示的，以及如何在 .yaml 格式的文件中表示。
理解 Kubernetes 对象 在 Kubernetes 系统中，Kubernetes 对象 是持久化的实体。Kubernetes 使用这些实体去表示整个集群的状态。特别地，它们描述了如下信息：
 哪些容器化应用在运行（以及在哪个 Node 上） 可以被应用使用的资源 关于应用运行时表现的策略，比如重启策略、升级策略，以及容错策略  Kubernetes 对象是 “目标性记录” —— 一旦创建对象，Kubernetes 系统将持续工作以确保对象存在。通过创建对象，本质上是在告知 Kubernetes 系统，所需要的集群工作负载看起来是什么样子的，这就是 Kubernetes 集群的 期望状态（Desired State）。
操作 Kubernetes 对象 —— 无论是创建、修改，或者删除 —— 需要使用 Kubernetes API。比如，当使用 kubectl 命令行接口时，CLI 会执行必要的 Kubernetes API 调用，也可以在程序中使用 客户端库 直接调用 Kubernetes API。
对象规约（Spec）与状态（Status） 每个 Kubernetes 对象包含两个嵌套的对象字段，它们负责管理对象的配置：对象 spec 和 对象 status 。 spec 是必需的，它描述了对象的 期望状态（Desired State） —— 希望对象所具有的特征。 status 描述了对象的 实际状态（Actual State） ，它是由 Kubernetes 系统提供和更新的。在任何时刻，Kubernetes 控制面一直努力地管理着对象的实际状态以与期望状态相匹配。</description>
    </item>
    
    <item>
      <title>网络插件</title>
      <link>https://lijun.in/concepts/extend-kubernetes/compute-storage-net/network-plugins/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/concepts/extend-kubernetes/compute-storage-net/network-plugins/</guid>
      <description>state=&amp;quot;alpha&amp;rdquo; &amp;gt;}}
Alpha 特性迅速变化。
Kubernetes中的网络插件有几种类型：
 CNI 插件： 遵守 appc/CNI 规约，为互操作性设计。 Kubenet 插件：使用 bridge 和 host-local CNI 插件实现了基本的 cbr0。  安装 kubelet 有一个单独的默认网络插件，以及一个对整个集群通用的默认网络。 它在启动时探测插件，记住找到的内容，并在 pod 生命周期的适当时间执行所选插件（这仅适用于 Docker，因为 rkt 管理自己的 CNI 插件）。 在使用插件时，需要记住两个 Kubelet 命令行参数：
 cni-bin-dir： Kubelet 在启动时探测这个目录中的插件 network-plugin： 要使用的网络插件来自 cni-bin-dir。它必须与从插件目录探测到的插件报告的名称匹配。对于 CNI 插件，其值为 &amp;ldquo;cni&amp;rdquo;。  网络插件要求 除了提供[NetworkPlugin 接口](https://github.com/kubernetes/kubernetes/tree/param &amp;ldquo;fullversion&amp;rdquo; &amp;gt;}}/pkg/kubelet/dockershim/network/plugins.go)来配置和清理 pod 网络之外，该插件还可能需要对 kube-proxy 的特定支持。 iptables 代理显然依赖于 iptables，插件可能需要确保 iptables 能够监控容器的网络通信。 例如，如果插件将容器连接到 Linux 网桥，插件必须将 net/bridge/bridge-nf-call-iptables 系统参数设置为1，以确保 iptables 代理正常工作。 如果插件不使用 Linux 网桥（而是类似于 Open vSwitch 或者其它一些机制），它应该确保为代理对容器通信执行正确的路由。</description>
    </item>
    
    <item>
      <title>网页界面 (Dashboard)</title>
      <link>https://lijun.in/tasks/access-application-cluster/web-ui-dashboard/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/access-application-cluster/web-ui-dashboard/</guid>
      <description>Dashboard 是基于网页的 Kubernetes 用户界面。您可以使用 Dashboard 将容器应用部署到 Kubernetes 集群中，也可以对容器应用排错，还能管理集群资源。您可以使用 Dashboard 获取运行在集群中的应用的概览信息，也可以创建或者修改 Kubernetes 资源（如 Deployment，Job，DaemonSet 等等）。例如，您可以对 Deployment 实现弹性伸缩、发起滚动升级、重启 Pod 或者使用向导创建新的应用。
Dashboard 同时展示了 Kubernetes 集群中的资源状态信息和所有报错信息。
部署 Dashboard UI 默认情况下不会部署 Dashboard。可以通过以下命令部署：
kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0/aio/deploy/recommended.yaml 访问 Dashboard UI 为了保护您的集群数据，默认情况下，Dashboard 会使用最少的 RBAC 配置进行部署。 当前，Dashboard 仅支持使用 Bearer 令牌登录。 要为此样本演示创建令牌，您可以按照创建示例用户上的指南进行操作。
. warning &amp;gt;}} 在教程中创建的样本用户将具有管理特权，并且仅用于教育目的。 . /warning &amp;gt;}}
命令行代理 您可以使用 kubectl 命令行工具访问 Dashboard，命令如下：
kubectl proxy kubectl 会使得 Dashboard 可以通过 http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/ 访问。
UI 只能 通过执行这条命令的机器进行访问。更多选项参见 kubectl proxy --help。
. note &amp;gt;}} Kubeconfig 身份验证方法不支持外部身份提供程序或基于 x509 证书的身份验证。 .</description>
    </item>
    
    <item>
      <title>节点</title>
      <link>https://lijun.in/concepts/architecture/nodes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/concepts/architecture/nodes/</guid>
      <description>在 Kubernetes 中，节点（Node）是执行工作的机器，以前叫做 minion。根据你的集群环境，节点可以是一个虚拟机或者物理机器。每个节点都包含用于运行 pods 的必要服务，并由主控组件管理。节点上的服务包括 容器运行时、kubelet 和 kube-proxy。查阅架构设计文档中 Kubernetes 节点 一节获取更多细节。
节点状态 一个节点的状态包含以下信息:
 地址 条件 容量与可分配 信息  可以使用以下命令显示节点状态和有关节点的其他详细信息：
kubectl describe node &amp;lt;insert-node-name-here&amp;gt; 下面对每个章节进行详细描述。
地址 这些字段组合的用法取决于你的云服务商或者裸机配置。
 HostName：由节点的内核指定。可以通过 kubelet 的 --hostname-override 参数覆盖。 ExternalIP：通常是可以外部路由的节点 IP 地址（从集群外可访问）。 InternalIP：通常是仅可在集群内部路由的节点 IP 地址。  条件 conditions 字段描述了所有 Running 节点的状态。条件的示例包括：
   节点条件 描述     OutOfDisk True 表示节点的空闲空间不足以用于添加新 pods, 否则为 False   Ready 表示节点是健康的并已经准备好接受 pods；False 表示节点不健康而且不能接受 pods；Unknown 表示节点控制器在最近 40 秒内没有收到节点的消息   MemoryPressure True 表示节点存在内存压力 &amp;ndash; 即节点内存用量低，否则为 False   PIDPressure True 表示节点存在进程压力 &amp;ndash; 即进程过多；否则为 False   DiskPressure True 表示节点存在磁盘压力 &amp;ndash; 即磁盘可用量低，否则为 False   NetworkUnavailable True 表示节点网络配置不正确；否则为 False    节点条件使用一个 JSON 对象表示。例如，下面的响应描述了一个健康的节点。</description>
    </item>
    
    <item>
      <title>资源配额</title>
      <link>https://lijun.in/concepts/policy/resource-quotas/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/concepts/policy/resource-quotas/</guid>
      <description>当多个用户或团队共享具有固定节点数目的集群时，人们会担心有人使用超过其基于公平原则所分配到的资源量。
资源配额是帮助管理员解决这一问题的工具。
资源配额，通过 ResourceQuota 对象来定义，对每个命名空间的资源消耗总量提供限制。它可以限制命名空间中某种类型的对象的总数目上限，也可以限制命令空间中的 Pod 可以使用的计算资源的总上限。
资源配额的工作方式如下：
 不同的团队可以在不同的命名空间下工作，目前这是非约束性的，在未来的版本中可能会通过 ACL (Access Control List 访问控制列表) 来实现强制性约束。 集群管理员可以为每个命名空间创建一个或多个资源配额对象。 当用户在命名空间下创建资源（如 Pod、Service 等）时，Kubernetes 的配额系统会跟踪集群的资源使用情况，以确保使用的资源用量不超过资源配额中定义的硬性资源限额。 如果资源创建或者更新请求违反了配额约束，那么该请求会报错（HTTP 403 FORBIDDEN），并在消息中给出有可能违反的约束。 如果命名空间下的计算资源 （如 cpu 和 memory）的配额被启用，则用户必须为这些资源设定请求值（request）和约束值（limit），否则配额系统将拒绝 Pod 的创建。 提示: 可使用 LimitRanger 准入控制器来为没有设置计算资源需求的 Pod 设置默认值。 若想避免这类问题，请参考演练中的示例。  下面是使用命名空间和配额构建策略的示例：
 在具有 32 GiB 内存和 16 核 CPU 资源的集群中，允许 A 团队使用 20 GiB 内存 和 10 核的 CPU 资源，允许 B 团队使用 10 GiB 内存和 4 核的 CPU 资源，并且预留 2 GiB 内存和 2 核的 CPU 资源供将来分配。 限制 &amp;ldquo;testing&amp;rdquo; 命名空间使用 1 核 CPU 资源和 1GiB 内存。允许 &amp;ldquo;production&amp;rdquo; 命名空间使用任意数量。  在集群容量小于各命名空间配额总和的情况下，可能存在资源竞争。资源竞争时，Kubernetes 系统会遵循先到先得的原则。</description>
    </item>
    
    <item>
      <title>运行应用程序的多个实例</title>
      <link>https://lijun.in/tutorials/kubernetes-basics/scale/scale-intro/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tutorials/kubernetes-basics/scale/scale-intro/</guid>
      <description>Objectives -- 目的  Scale an app using kubectl. -- 用 kubectl 扩缩应用程序   Scaling an application -- 扩缩应用程序 In the previous modules we created a Deployment, and then exposed it publicly via a Service. The Deployment created only one Pod for running our application. When traffic increases, we will need to scale the application to keep up with user demand.
-- 在之前的模块中，我们创建了一个 Deployment，然后通过 Service让其可以开放访问。Deployment 仅为跑这个应用程序创建了一个 Pod。 当流量增加时，我们需要扩容应用程序满足用户需求。</description>
    </item>
    
    <item>
      <title>通过聚合层扩展 Kubernetes API</title>
      <link>https://lijun.in/concepts/extend-kubernetes/api-extension/apiserver-aggregation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/concepts/extend-kubernetes/api-extension/apiserver-aggregation/</guid>
      <description>聚合层允许 Kubernetes 通过额外的 API 进行扩展，而不局限于 Kubernetes 核心 API 提供的功能。
概述 聚合层使您的集群可以安装其他 Kubernetes 风格的 API。这些 API 可以是预编译的、第三方的解决方案提供的例如service-catalog、或者用户创建的类似apiserver-builder一样的API可以帮助你上手。
聚合层在 kube-apiserver 进程内运行。在扩展资源注册之前，聚合层不做任何事情。要注册 API，用户必须添加一个 APIService 对象，用它来申领 Kubernetes API 中的 URL 路径。自此以后，聚合层将会把发给该 API 路径的所有内容（例如 /apis/myextension.mycompany.io/v1/…）代理到已注册的 APIService。
正常情况下，APIService 会实现为运行于集群中某 Pod 内的 extension-apiserver。如果需要对增加的资源进行动态管理，extension-apiserver 经常需要和一个或多个控制器一起使用。因此，apiserver-builder 同时提供用来管理新资源的 API 框架和控制器框架。另外一个例子，当安装了 service-catalog 时，它会为自己提供的服务提供 extension-apiserver 和控制器。
extension-apiserver 与 kube-apiserver 之间的连接应具有低延迟。 特别是，发现请求需要在五秒钟或更短的时间内从 kube-apiserver 往返。 如果您的部署无法实现此目的，则应考虑如何进行更改。目前，在 kube-apiserver 上设置 EnableAggregatedDiscoveryTimeout=false 功能开关将禁用超时限制。它将在将来的版本中被删除。
 阅读配置聚合层 文档，了解如何在自己的环境中启用聚合器（aggregator）。 然后安装扩展的 api-server 来开始使用聚合层。 也可以学习怎样 使用自定义资源定义扩展 Kubernetes API。  </description>
    </item>
    
    <item>
      <title>配置最佳实践</title>
      <link>https://lijun.in/concepts/configuration/overview/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/concepts/configuration/overview/</guid>
      <description>本文档重点介绍并整合了整个用户指南、入门文档和示例中介绍的配置最佳实践。
这是一份活文件。 如果您认为某些内容不在此列表中但可能对其他人有用，请不要犹豫，提交问题或提交 PR。
一般配置提示  定义配置时，请指定最新的稳定 API 版本。   在推送到集群之前，配置文件应存储在版本控制中。 这允许您在必要时快速回滚配置更改。 它还有助于集群重新创建和恢复。   使用 YAML 而不是 JSON 编写配置文件。虽然这些格式几乎可以在所有场景中互换使用，但 YAML 往往更加用户友好。   只要有意义，就将相关对象分组到一个文件中。 一个文件通常比几个文件更容易管理。 请参阅[guestbook-all-in-one.yaml](https://github.com/kubernetes/examples/tree/param &amp;ldquo;githubbranch&amp;rdquo; &amp;gt;}}/guestbook/all-in-one/guestbook-all-in-one.yaml) 文件作为此语法的示例。   另请注意，可以在目录上调用许多kubectl命令。 例如，你可以在配置文件的目录中调用kubectl apply。   除非必要，否则不指定默认值：简单的最小配置会降低错误的可能性。   将对象描述放在注释中，以便更好地进行内省。  “Naked”Pods 与 ReplicaSet，Deployment 和 Jobs  如果您能避免，不要使用 naked Pods（即，Pod 未绑定到ReplicaSet 或Deployment）。 如果节点发生故障，将不会重新安排 Naked Pods。  Deployment，它创建一个 ReplicaSet 以确保所需数量的 Pod 始终可用，并指定替换 Pod 的策略(例如 RollingUpdate)，除了一些显式的restartPolicy: Never场景之外，几乎总是优先考虑直接创建 Pod。 Job 也可能是合适的。</description>
    </item>
    
    <item>
      <title>配置聚合层</title>
      <link>https://lijun.in/tasks/access-kubernetes-api/configure-aggregation-layer/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/access-kubernetes-api/configure-aggregation-layer/</guid>
      <description>配置 聚合层 允许 Kubernetes apiserver 使用其它 API 扩展，这些 API 不是核心 Kubernetes API 的一部分。
. heading &amp;ldquo;prerequisites&amp;rdquo; %}} . include &amp;ldquo;task-tutorial-prereqs.md&amp;rdquo; &amp;gt;}} . version-check &amp;gt;}}
. note &amp;gt;}}
要使聚合层在您的环境中正常工作以支持代理服务器和扩展 apiserver 之间的相互 TLS 身份验证，需要满足一些设置要求。Kubernetes 和 kube-apiserver 具有多个 CA，因此请确保代理是由聚合层 CA 签名的，而不是由主 CA 签名的。
. caution &amp;gt;}}
对不同的客户端类型重复使用相同的 CA 会对群集的功能产生负面影响。有关更多信息，请参见 CA重用和冲突。
. /caution &amp;gt;}} . /note &amp;gt;}}
认证流程 与自定义资源定义（CRD）不同，除标准的 Kubernetes apiserver 外，Aggregation API 还涉及另一个服务器：扩展 apiserver。Kubernetes apiserver 将需要与您的扩展 apiserver 通信，并且您的扩展 apiserver 也需要与 Kubernetes apiserver 通信。为了确保此通信的安全，Kubernetes apiserver 使用 x509 证书向扩展 apiserver 认证。</description>
    </item>
    
    <item>
      <title>镜像</title>
      <link>https://lijun.in/concepts/containers/images/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/concepts/containers/images/</guid>
      <description>创建 Docker 镜像并将其推送到仓库，然后在 Kubernetes pod 中引用它。
容器的 image 属性支持与 docker 命令相同的语法，包括私有仓库和标签。
升级镜像 默认的镜像拉取策略是 IfNotPresent，在镜像已经存在的情况下，kubelet 将不再去拉取镜像。如果总是想要拉取镜像，您可以执行以下操作：
 设置容器的 imagePullPolicy 为 Always。 省略 imagePullPolicy，并使用 :latest 作为要使用的镜像的标签。 省略 imagePullPolicy 和要使用的镜像标签。 启用 AlwaysPullImages 准入控制器（admission controller）。  注意应避免使用 :latest 标签，参见配置镜像最佳实践 获取更多信息。
使用清单（manifest）构建多架构镜像 Docker CLI 现在支持以下命令 docker manifest 以及 create、annotate、push 等子命令。这些命令可用于构建和推送清单。您可以使用 docker manifest inspect 来查看清单。
请在此处查看 docker 清单文档： https://docs.docker.com/edge/engine/reference/commandline/manifest/
查看有关如何在构建工具中使用清单的示例： https://cs.k8s.io/?q=docker%20manifest%20(create%7Cpush%7Cannotate)&amp;amp;i=nope&amp;amp;files=&amp;amp;repos=
这些命令依赖于 Docker CLI 并仅在 Docker CLI 上实现。需要编辑 $HOME/.docker/config.json 并将 experimental 设置为 enabled，或者仅在调用 CLI 命令时将 DOCKER_CLI_EXPERIMENTAL 环境变量设置为 enabled。</description>
    </item>
    
    <item>
      <title>限制范围</title>
      <link>https://lijun.in/concepts/policy/limit-range/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/concepts/policy/limit-range/</guid>
      <description>默认情况下， Kubernetes 集群上的容器运行使用的计算资源 没有限制。 使用资源配额，集群管理员可以以命名空间为单位，限制其资源的使用与创建。 在命名空间中，一个 Pod 或 Container 最多能够使用命名空间的资源配额所定义的 CPU 和内存用量。有人担心，一个 Pod 或 Container 会垄断所有可用的资源。LimitRange 是在命名空间内限制资源分配（给多个 Pod 或 Container）的策略对象。
一个 LimitRange 对象提供的限制能够做到：
 在一个命名空间中实施对每个 Pod 或 Container 最小和最大的资源使用量的限制。 在一个命名空间中实施对每个 PersistentVolumeClaim 能申请的最小和最大的存储空间大小的限制。 在一个命名空间中实施对一种资源的申请值和限制值的比值的控制。 设置一个命名空间中对计算资源的默认申请/限制值，并且自动的在运行时注入到多个 Container 中。  启用 LimitRange 对 LimitRange 的支持默认在多数 Kubernetes 发行版中启用。当 apiserver 的 --enable-admission-plugins 标志的参数包含 LimitRanger 准入控制器时即启用。
当一个命名空间中有 LimitRange 时，实施该 LimitRange 所定义的限制。
LimitRange 的名称必须是合法的 DNS 子域名。
限制范围总览  管理员在一个命名空间内创建一个 LimitRange 对象。 用户在命名空间内创建 Pod ，Container 和 PersistentVolumeClaim 等资源。 LimitRanger 准入控制器对所有没有设置计算资源需求的 Pod 和 Container 设置默认值与限制值，并跟踪其使用量以保证没有超出命名空间中存在的任意 LimitRange 对象中的最小、最大资源使用量以及使用量比值。 若创建或更新资源（Pod, Container, PersistentVolumeClaim）违反了 LimitRange 的约束，向 API 服务器的请求会失败，并返回 HTTP 状态码 403 FORBIDDEN 与描述哪一项约束被违反的消息。 若命名空间中的 LimitRange 启用了对 cpu 和 memory 的限制，用户必须指定这些值的需求使用量与限制使用量。否则，系统将会拒绝创建 Pod。 LimitRange 的验证仅在 Pod 准入阶段进行，不对正在运行的 Pod 进行验证。  能够使用限制范围创建策略的例子有：</description>
    </item>
    
    <item>
      <title>集群管理概述</title>
      <link>https://lijun.in/concepts/cluster-administration/cluster-administration-overview/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/concepts/cluster-administration/cluster-administration-overview/</guid>
      <description>集群管理概述面向任何创建和管理 Kubernetes 集群的读者人群。 我们假设你对用户指南中的概念大概了解。
规划集群 查阅 安装 中的指导，获取如何规划、建立以及配置 Kubernetes 集群的示例。本文所列的文章称为发行版 。
在选择一个指南前，有一些因素需要考虑：
 你是打算在你的电脑上尝试 Kubernetes，还是要构建一个高可用的多节点集群？请选择最适合你需求的发行版。 如果你正在设计一个高可用集群，请了解在多个 zones 中配置集群。 您正在使用 类似 Google Kubernetes Engine 这样的被托管的Kubernetes集群, 还是管理您自己的集群? 你的集群是在本地还是 云（IaaS） 上？ Kubernetes 不能直接支持混合集群。作为代替，你可以建立多个集群。 如果你在本地配置 Kubernetes，需要考虑哪种网络模型最适合。 你的 Kubernetes 在 裸金属硬件 还是 虚拟机（VMs） 上运行？ 你只想运行一个集群，还是打算活动开发 Kubernetes 项目代码？如果是后者，请选择一个活动开发的发行版。某些发行版只提供二进制发布版，但提供更多的选择。 让你自己熟悉运行一个集群所需的组件 。  请注意：不是所有的发行版都被积极维护着。请选择测试过最近版本的 Kubernetes 的发行版。
管理集群   管理集群叙述了和集群生命周期相关的几个主题：创建一个新集群、升级集群的 master 和 worker 节点、执行节点维护（例如内核升级）以及升级活动集群的 Kubernetes API 版本。
  学习如何 管理节点.
  学习如何设定和管理集群共享的 资源配额 。
  集群安全   Certificates 描述了使用不同的工具链生成证书的步骤。</description>
    </item>
    
    <item>
      <title>Kubernetes 对象管理</title>
      <link>https://lijun.in/concepts/overview/working-with-objects/object-management/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/concepts/overview/working-with-objects/object-management/</guid>
      <description>kubectl 命令行工具支持多种不同的方式来创建和管理 Kubernetes 对象。本文档概述了不同的方法。阅读 Kubectl book 来了解 kubectl 管理对象的详细信息。
管理技巧 应该只使用一种技术来管理 Kubernetes 对象。混合和匹配技术作用在同一对象上将导致未定义行为。
   Management technique Operates on Recommended environment Supported writers Learning curve     Imperative commands Live objects Development projects 1+ Lowest   Imperative object configuration Individual files Production projects 1 Moderate   Declarative object configuration Directories of files Production projects 1+ Highest    命令式命令 使用命令式命令时，用户可以在集群中的活动对象上进行操作。用户将操作传给 kubectl 命令作为参数或标志。
这是开始或者在集群中运行一次性任务的最简单方法。因为这个技术直接在活动对象上操作，所以它不提供以前配置的历史记录。
例子 通过创建 Deployment 对象来运行 nginx 容器的实例：</description>
    </item>
    
    <item>
      <title>设置一个扩展的 API server</title>
      <link>https://lijun.in/tasks/access-kubernetes-api/setup-extension-api-server/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/access-kubernetes-api/setup-extension-api-server/</guid>
      <description>设置一个扩展的 API server 来使用聚合层以让 Kubernetes apiserver 使用其它 API 进行扩展，这些 API 不是核心 Kubernetes API 的一部分。
. heading &amp;ldquo;prerequisites&amp;rdquo; %}}  您需要拥有一个运行的 Kubernetes 集群。 您必须 配置聚合层 并且启用 apiserver 的相关参数。  . heading &amp;ldquo;prerequisites&amp;rdquo; %}} . include &amp;ldquo;task-tutorial-prereqs.md&amp;rdquo; &amp;gt;}} . version-check &amp;gt;}}
设置一个扩展的 api-server 来使用聚合层 以下步骤描述如何 在一个高层次 设置一个扩展的 apiserver。无论您使用的是 YAML 配置还是使用 API，这些步骤都适用。目前我们正在尝试区分出两者的区别。有关使用 YAML 配置的具体示例，您可以在 Kubernetes 库中查看 sample-apiserver。
或者，您可以使用现有的第三方解决方案，例如 apiserver-builder，它将生成框架并自动执行以下所有步骤。
 确保启用了 APIService API（检查 --runtime-config）。默认应该是启用的，除非被特意关闭了。 您可能需要制定一个 RBAC 规则，以允许您添加 APIService 对象，或让您的集群管理员创建一个。（由于 API 扩展会影响整个集群，因此不建议在实时集群中对 API 扩展进行测试/开发/调试） 创建 Kubernetes 命名空间，扩展的 api-service 将运行在该命名空间中。 创建（或获取）用来签署服务器证书的 CA 证书，扩展 api-server 中将使用该证书做 HTTPS 连接。 为 api-server 创建一个服务端的证书（或秘钥）以使用 HTTPS。这个证书应该由上述的 CA 签署。同时应该还要有一个 Kube DNS 名称的 CN，这是从 Kubernetes 服务派生而来的，格式为 &amp;lt;service name&amp;gt;.</description>
    </item>
    
    <item>
      <title>ConfigMap</title>
      <link>https://lijun.in/concepts/configuration/configmap/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/concepts/configuration/configmap/</guid>
      <description>n term_id=&amp;quot;configmap&amp;rdquo; length=&amp;quot;all&amp;rdquo; &amp;gt;}}
ConfigMap 并不提供保密或者加密功能。如果你想存储的数据是机密的，请使用 text=&amp;quot;Secret&amp;rdquo; term_id=&amp;quot;secret&amp;rdquo; &amp;gt;}} ，或者使用其他第三方工具来保证你的数据的私密性，而不是用 ConfigMap。
动机 使用 ConfigMap 来将你的配置数据和应用程序代码分开。
比如，假设你正在开发一个应用，它可以在你自己的电脑上（用于开发）和在云上（用于实际流量）运行。你的代码里有一段是用于查看环境变量 DATABASE_HOST，在本地运行时，你将这个变量设置为 localhost，在云上，你将其设置为引用 Kubernetes 集群中的公开数据库 text=&amp;quot;Service&amp;rdquo; term_id=&amp;quot;service&amp;rdquo; &amp;gt;}} 中的组件。
这让您可以获取在云中运行的容器镜像，并且如果有需要的话，在本地调试完全相同的代码。
ConfigMap 对象 ConfigMap 是一个 API 对象，让你可以存储其他对象所需要使用的配置。和其他 Kubernetes 对象都有一个 spec 不同的是，ConfigMap 使用 data 块来存储元素（键名）和它们的值。
ConfigMap 的名字必须是一个合法的 DNS 子域名。
ConfigMaps 和 Pods 您可以写一个引用 ConfigMap 的 Pod 的 spec，并根据 ConfigMap 中的数据在该 Pod 中配置容器。这个 Pod 和 ConfigMap 必须要在同一个 text=&amp;quot;命名空间&amp;rdquo; term_id=&amp;quot;namespace&amp;rdquo; &amp;gt;}} 中。
这是一个 ConfigMap 的示例，它的一些键只有一个值，其他键的值看起来像是配置的片段格式。
apiVersion: v1 kind: ConfigMap metadata: Name: game-demo data: # 类属性键；每一个键都映射到一个简单的值 player_initial_lives: 3 ui_properties_file_name: &amp;#34;user-interface.</description>
    </item>
    
    <item>
      <title>kubeadm init</title>
      <link>https://lijun.in/reference/setup-tools/kubeadm/kubeadm-init/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/setup-tools/kubeadm/kubeadm-init/</guid>
      <description>此命令初始化一个 Kubernetes 控制平面节点。
. include &amp;ldquo;generated/kubeadm_init.md&amp;rdquo; &amp;gt;}}
Init 命令的工作流程 kubeadm init 命令通过执行下列步骤来启动一个 Kubernetes 控制平面节点。
 在做出变更前运行一系列的预检项来验证系统状态。一些检查项目仅仅触发警告，其它的则会被视为错误并且退出 kubeadm，除非问题得到解决或者用户指定了 --ignore-preflight-errors=&amp;lt;list-of-errors&amp;gt; 参数。  生成一个自签名的 CA 证书 (或者使用现有的证书，如果提供的话) 来为集群中的每一个组件建立身份标识。如果用户已经通过 --cert-dir 配置的证书目录（默认为 /etc/kubernetes/pki）提供了他们自己的 CA 证书以及/或者密钥， 那么将会跳过这个步骤，正如文档使用自定义证书所述。如果指定了 --apiserver-cert-extra-sans 参数, APIServer 的证书将会有额外的 SAN 条目，如果必要的话，将会被转为小写。  将 kubeconfig 文件写入 /etc/kubernetes/ 目录以便 kubelet、控制器管理器和调度器用来连接到 API 服务器，它们每一个都有自己的身份标识，同时生成一个名为 admin.conf 的独立的 kubeconfig 文件，用于管理操作。  为 API 服务器、控制器管理器和调度器生成静态 Pod 的清单文件。假使没有提供一个外部的 etcd 服务的话，也会为 etcd 生成一份额外的静态 Pod 清单文件。  静态 Pod 的清单文件被写入到 /etc/kubernetes/manifests 目录; kubelet 会监视这个目录以便在系统启动的时候创建 Pod。</description>
    </item>
    
    <item>
      <title>kubectl 概述</title>
      <link>https://lijun.in/reference/kubectl/overview/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/kubectl/overview/</guid>
      <description>Kubectl 是一个命令行接口，用于对 Kubernetes 集群运行命令。kubectl 在 $HOME/.kube 目录中寻找一个名为 config 的文件。您可以通过设置环境变量 KUBECONFIG 或设置 --kubeconfig 参数指定其它 kubeconfig 文件。
本文概述了 kubectl 语法和命令操作描述，并提供了常见的示例。有关每个命令的详细信息，包括所有受支持的参数和子命令，请参阅 kubectl 参考文档。有关安装说明，请参见 安装 kubectl 。
语法 使用以下语法 kubectl 从终端窗口运行命令：
kubectl [command] [TYPE] [NAME] [flags] 其中 command、TYPE、NAME 和 flags 分别是：
  command：指定要对一个或多个资源执行的操作，例如 create、get、describe、delete。
  TYPE：指定资源类型。资源类型不区分大小写，可以指定单数、复数或缩写形式。例如，以下命令输出相同的结果:
```shell kubectl get pod pod1 kubectl get pods pod1 kubectl get po pod1 ```      NAME：指定资源的名称。名称区分大小写。如果省略名称，则显示所有资源的详细信息 kubectl get pods。
在对多个资源执行操作时，您可以按类型和名称指定每个资源，或指定一个或多个文件：
    要按类型和名称指定资源：</description>
    </item>
    
    <item>
      <title>Kubernetes 安全和信息披露</title>
      <link>https://lijun.in/reference/issues-security/security/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/issues-security/security/</guid>
      <description>本页面介绍 Kubernetes 安全和信息披露相关的内容。
安全公告 加入 kubernets-announce 组，以获取关于安全性和主要 API 公告的电子邮件。
您也可以使用此链接订阅上述的 RSS 反馈。
报告一个漏洞 我们非常感谢向 Kubernetes 开源社区报告漏洞的安全研究人员和用户。 所有的报告都由社区志愿者进行彻底调查。
如需报告，请连同安全细节以及预期的所有 Kubernetes bug 报告详细信息电邮到security@kubernetes.io 列表。
您还可以通过电子邮件向私有 security@kubernetes.io 列表发送电子邮件，邮件中应该包含所有 Kubernetes 错误报告所需的详细信息。
您可以使用产品安全团队成员的 GPG 密钥加密您的电子邮件到此列表。 使用 GPG 加密不需要公开。
我应该在什么时候报告漏洞？  您认为在 Kubernetes 中发现了一个潜在的安全漏洞 您不确定漏洞如何影响 Kubernetes 您认为您在 Kubernetes 依赖的另一个项目中发现了一个漏洞  对于具有漏洞报告和披露流程的项目，请直接在该项目处报告    我什么时候不应该报告漏洞？  您需要帮助调整 Kubernetes 组件的安全性 您需要帮助应用与安全相关的更新 您的问题与安全无关  安全漏洞响应 每个报告在 3 个工作日内由产品安全团队成员确认和分析。这将启动安全发布过程。
与产品安全团队共享的任何漏洞信息都保留在 Kubernetes 项目中，除非有必要修复该问题，否则不会传播到其他项目。
随着安全问题从分类、识别修复、发布计划等方面的进展，我们将不断更新报告。
公开披露时间 公开披露日期由 Kubernetes 产品安全团队和 bug 提交者协商。我们倾向于在用户缓解措施可用时尽快完全披露该 bug。</description>
    </item>
    
    <item>
      <title>Kubernetes 组件</title>
      <link>https://lijun.in/concepts/overview/components/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/concepts/overview/components/</guid>
      <description>当你部署完 Kubernetes, 即拥有了一个完整的集群。 term_id=&amp;quot;cluster&amp;rdquo; length=&amp;quot;all&amp;rdquo; prepend=&amp;quot;一个 Kubernetes 集群包含&amp;quot;&amp;gt;}}
本文档概述了交付正常运行的 Kubernetes 集群所需的各种组件。
这张图表展示了包含所有相互关联组件的 Kubernetes 集群。
控制平面组件（Control Plane Components） 控制平面的组件对集群做出全局决策(比如调度)，以及检测和响应集群事件（例如，当不满足部署的 replicas 字段时，启动新的 text=&amp;quot;pod&amp;rdquo; term_id=&amp;quot;pod&amp;quot;&amp;gt;}}）。
控制平面组件可以在集群中的任何节点上运行。然而，为了简单起见，设置脚本通常会在同一个计算机上启动所有控制平面组件，并且不会在此计算机上运行用户容器。请参阅构建高可用性集群中对于多主机 VM 的设置示例。
kube-apiserver term_id=&amp;quot;kube-apiserver&amp;rdquo; length=&amp;quot;all&amp;rdquo; &amp;gt;}}
etcd term_id=&amp;quot;etcd&amp;rdquo; length=&amp;quot;all&amp;rdquo; &amp;gt;}}
kube-scheduler term_id=&amp;quot;kube-scheduler&amp;rdquo; length=&amp;quot;all&amp;rdquo; &amp;gt;}}
kube-controller-manager n term_id=&amp;quot;kube-controller-manager&amp;rdquo; length=&amp;quot;all&amp;rdquo; &amp;gt;}}
这些控制器包括:
 节点控制器（Node Controller）: 负责在节点出现故障时进行通知和响应。 副本控制器（Replication Controller）: 负责为系统中的每个副本控制器对象维护正确数量的 Pod。 端点控制器（Endpoints Controller）: 填充端点(Endpoints)对象(即加入 Service 与 Pod)。 服务帐户和令牌控制器（Service Account &amp;amp; Token Controllers）: 为新的命名空间创建默认帐户和 API 访问令牌.  云控制器管理器-(cloud-controller-manager) cloud-controller-manager 运行与基础云提供商交互的控制器。cloud-controller-manager 二进制文件是 Kubernetes 1.</description>
    </item>
    
    <item>
      <title>Pod 与 Service 的 DNS</title>
      <link>https://lijun.in/concepts/services-networking/dns-pod-service/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/concepts/services-networking/dns-pod-service/</guid>
      <description>该页面概述了Kubernetes对DNS的支持。
介绍 Kubernetes DNS 在群集上调度 DNS Pod 和服务，并配置 kubelet 以告知各个容器使用 DNS 服务的 IP 来解析 DNS 名称。
怎样获取 DNS 名字? 在集群中定义的每个 Service（包括 DNS 服务器自身）都会被指派一个 DNS 名称。 默认，一个客户端 Pod 的 DNS 搜索列表将包含该 Pod 自己的 Namespace 和集群默认域。 通过如下示例可以很好地说明：
假设在 Kubernetes 集群的 Namespace bar 中，定义了一个Service foo。 运行在Namespace bar 中的一个 Pod，可以简单地通过 DNS 查询 foo 来找到该 Service。 运行在 Namespace quux 中的一个 Pod 可以通过 DNS 查询 foo.bar 找到该 Service。
以下各节详细介绍了受支持的记录类型和支持的布局。 其中代码部分的布局，名称或查询命令均被视为实现细节，如有更改，恕不另行通知。 有关最新规范请查看 Kubernetes 基于 DNS 的服务发现.
支持的 DNS 模式 下面各段详细说明支持的记录类型和布局。 如果任何其它的布局、名称或查询，碰巧也能够使用，这就需要研究下它们的实现细节，以免后续修改它们又不能使用了。</description>
    </item>
    
    <item>
      <title>Pod 开销</title>
      <link>https://lijun.in/concepts/configuration/pod-overhead/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/concepts/configuration/pod-overhead/</guid>
      <description>for_k8s_version=&amp;quot;v1.18&amp;rdquo; state=&amp;quot;beta&amp;rdquo; &amp;gt;}}
在节点上运行 Pod 时，Pod 本身占用大量系统资源。这些资源是运行 Pod 内容器所需资源的附加资源。 POD 开销 是一个特性，用于计算 Pod 基础设施在容器请求和限制之上消耗的资源。
Pod 开销 在 Kubernetes 中，Pod 的开销是根据与 Pod 的 RuntimeClass 相关联的开销在 准入 时设置的。
当启用 Pod 开销时，在调度 Pod 时，除了考虑容器资源请求的总和外，还要考虑 Pod 开销。类似地，Kubelet 将在确定 Pod cgroup 的大小和执行 Pod 驱逐排序时包含 Pod 开销。
启用 Pod 开销 您需要确保在集群中启用了 PodOverhead 特性门（在 1.18 默认是开启的），以及一个用于定义 overhead 字段的 RuntimeClass。
使用示例 要使用 PodOverhead 特性，需要一个定义 overhead 字段的 RuntimeClass. 作为例子，可以在虚拟机和来宾操作系统中通过一个虚拟化容器运行时来定义 RuntimeClass 如下，其中每个 Pod 大约使用 120MiB:
--- kind: RuntimeClass apiVersion: node.k8s.io/v1beta1 metadata: name: kata-fc handler: kata-fc overhead: podFixed: memory: &amp;#34;120Mi&amp;#34; cpu: &amp;#34;250m&amp;#34; 通过指定 kata-fc RuntimeClass 处理程序创建的工作负载会将内存和 cpu 开销计入资源配额计算、节点调度以及 Pod cgroup 分级。</description>
    </item>
    
    <item>
      <title>Pods</title>
      <link>https://lijun.in/concepts/workloads/pods/pod/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/concepts/workloads/pods/pod/</guid>
      <description>Pod 是可以在 Kubernetes 中创建和管理的、最小的可部署的计算单元。
Pod 是什么？ Pod （就像在鲸鱼荚或者豌豆荚中）是一组（一个或多个） glossary_tooltip text=&amp;quot;容器&amp;rdquo; term_id=&amp;quot;container&amp;rdquo; &amp;gt;}}（例如 Docker 容器），这些容器共享存储、网络、以及怎样运行这些容器的声明。Pod 中的内容总是并置（colocated）的并且一同调度，在共享的上下文中运行。 Pod 所建模的是特定于应用的“逻辑主机”，其中包含一个或多个应用容器，这些容器是相对紧密的耦合在一起 — 在容器出现之前，在相同的物理机或虚拟机上运行意味着在相同的逻辑主机上运行。
虽然 Kubernetes 支持多种容器运行时，但 Docker 是最常见的一种运行时，它有助于使用 Docker 术语来描述 Pod。
Pod 的共享上下文是一组 Linux 命名空间、cgroups、以及其他潜在的资源隔离相关的因素，这些相同的东西也隔离了 Docker 容器。在 Pod 的上下文中，单个应用程序可能还会应用进一步的子隔离。
Pod 中的所有容器共享一个 IP 地址和端口空间，并且可以通过 localhost 互相发现。他们也能通过标准的进程间通信（如 SystemV 信号量或 POSIX 共享内存）方式进行互相通信。不同 Pod 中的容器的 IP 地址互不相同，没有 特殊配置 就不能使用 IPC 进行通信。这些容器之间经常通过 Pod IP 地址进行通信。
Pod 中的应用也能访问共享 glossary_tooltip text=&amp;quot;卷&amp;rdquo; term_id=&amp;quot;volume&amp;rdquo; &amp;gt;}}，共享卷是 Pod 定义的一部分，可被用来挂载到每个应用的文件系统上。
在 Docker 体系的术语中，Pod 被建模为一组具有共享命名空间和共享文件系统卷 的 Docker 容器。</description>
    </item>
    
    <item>
      <title>ReplicationController</title>
      <link>https://lijun.in/concepts/workloads/controllers/replicationcontroller/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/concepts/workloads/controllers/replicationcontroller/</guid>
      <description>现在推荐使用配置 ReplicaSet 的 Deployment 来建立副本管理机制。
ReplicationController 确保在任何时候都有特定数量的 pod 副本处于运行状态。 换句话说，ReplicationController 确保一个 pod 或一组同类的 pod 总是可用的。
ReplicationController 如何工作 当 pod 数量过多时，ReplicationController 会终止多余的 pod。当 pod 数量太少时，ReplicationController 将会启动新的 pod。 与手动创建的 pod 不同，由 ReplicationController 创建的 pod 在失败、被删除或被终止时会被自动替换。 例如，在中断性维护（如内核升级）之后，您的 pod 会在节点上重新创建。 因此，即使您的应用程序只需要一个 pod，您也应该使用 ReplicationController 创建 Pod。 ReplicationController 类似于进程管理器，但是 ReplicationController 不是监控单个节点上的单个进程，而是监控跨多个节点的多个 pod。
在讨论中，ReplicationController 通常缩写为 &amp;ldquo;rc&amp;rdquo;，并作为 kubectl 命令的快捷方式。
一个简单的示例是创建一个 ReplicationController 对象来可靠地无限期地运行 Pod 的一个实例。 更复杂的用例是运行一个多副本服务（如 web 服务器）的若干相同副本。
运行一个示例 ReplicationController 这个示例 ReplicationController 配置运行 nginx web 服务器的三个副本。
codenew file=&amp;quot;controllers/replication.yaml&amp;rdquo; &amp;gt;}}</description>
    </item>
    
    <item>
      <title>为 Windows 的 pod 和容器配置 RunAsUserName</title>
      <link>https://lijun.in/tasks/configure-pod-container/configure-runasusername/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/configure-pod-container/configure-runasusername/</guid>
      <description>. feature-state for_k8s_version=&amp;quot;v1.17&amp;rdquo; state=&amp;quot;beta&amp;rdquo; &amp;gt;}}
本页展示如何为运行在 Windows 节点上的 pod 和容器启用并使用 RunAsUserName 功能。此功能旨在成为 Windows 版的 runAsUser（Linux），允许用户使用与默认用户名不同的用户名运行容器 entrypoint。
. note &amp;gt;}}
该功能目前处于 beta 状态。 RunAsUserName 的整体功能不会出现变更，但是关于用户名验证的部分可能会有所更改。 . /note &amp;gt;}}
. heading &amp;ldquo;prerequisites&amp;rdquo; %}} 你必须有一个 Kubernetes 集群，并且 kubectl 必须能和集群通信。集群应该要有 Windows 工作节点，将在其中调度运行 Windows 工作负载的 pod 和容器。
为 Pod 设置 Username 要指定运行 Pod 容器时所使用的用户名，请在 Pod 声明中包含 securityContext （[PodSecurityContext](/docs/reference/generated/kubernetes-api/. param &amp;ldquo;version&amp;rdquo; &amp;gt;}}/#podsecuritycontext-v1-core)）字段，并在其内部包含 windowsOptions （[WindowsSecurityContextOptions](/docs/reference/generated/kubernetes-api/. param &amp;ldquo;version&amp;rdquo; &amp;gt;}}/#windowssecuritycontextoptions-v1-core)）字段的 runAsUserName 字段。
您为 Pod 指定的 Windows SecurityContext 选项适用于该 Pod 中（包括 init 容器）的所有容器。</description>
    </item>
    
    <item>
      <title>为命名空间配置默认的CPU请求和限制</title>
      <link>https://lijun.in/tasks/administer-cluster/manage-resources/cpu-default-namespace/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/administer-cluster/manage-resources/cpu-default-namespace/</guid>
      <description>本章介绍怎样为命名空间配置默认的 CPU 请求和限制。 一个 Kubernetes 集群可被划分为多个命名空间。如果在配置了 CPU 限制的命名空间创建容器，并且该容器没有声明自己的 CPU 限制，那么这个容器会被指定默认的 CPU 限制。Kubernetes 在一些特定情况还会指定 CPU 请求，本文后续章节将会对其进行解释。
. heading &amp;ldquo;prerequisites&amp;rdquo; %}} . include &amp;ldquo;task-tutorial-prereqs.md&amp;rdquo; &amp;gt;}} . version-check &amp;gt;}}
创建命名空间 创建一个命名空间，以便本练习中创建的资源和集群的其余部分相隔离。
kubectl create namespace default-cpu-example 创建 LimitRange 和 Pod
这里给出了 LimitRange 对象的配置文件。该配置声明了一个默认的 CPU 请求和一个默认的 CPU 限制。
. codenew file=&amp;quot;admin/resource/cpu-defaults.yaml&amp;rdquo; &amp;gt;}}
在命名空间 default-cpu-example 中创建 LimitRange 对象：
kubectl create -f https://k8s.io/examples/admin/resource/cpu-defaults.yaml --namespace=default-cpu-example 现在如果在 default-cpu-example 命名空间创建一个容器，该容器没有声明自己的 CPU 请求和限制时，将会给它指定默认的 CPU 请求0.5和默认的 CPU 限制值1.
这里给出了包含一个容器的 Pod 的配置文件。该容器没有声明 CPU 请求和限制。</description>
    </item>
    
    <item>
      <title>为容器和 Pods 分配 CPU 资源</title>
      <link>https://lijun.in/tasks/configure-pod-container/assign-cpu-resource/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/configure-pod-container/assign-cpu-resource/</guid>
      <description>此页面显示如何将 CPU request 和 CPU limit 分配给一个容器。容器使用的 CPU 不能超过配额限制。 如果系统有空闲的 CPU 时间，则可以保证根据请求给容器分配尽可能多的 CPU 资源。
. heading &amp;ldquo;prerequisites&amp;rdquo; %}} . include &amp;ldquo;task-tutorial-prereqs.md&amp;rdquo; &amp;gt;}} . version-check &amp;gt;}}
集群中的每个节点必须至少具有 1 个 CPU。
此页面上的一些步骤要求您在集群中运行metrics-server 服务。如果您的集群中已经有正在运行的 metrics-server 服务，那么您可以跳过这些步骤。
如果您正在运行. glossary_tooltip term_id=&amp;quot;minikube&amp;rdquo; &amp;gt;}}，请运行以下命令启用 metrics-server：
minikube addons enable metrics-server 查看是 metrics-server（或者其他资源度量 API 服务提供者，metrics.k8s.io ）是否正在运行，请键入以下命令：
kubectl get apiservices 如果资源指标 API 可用，则会输出将包含一个参考信息 metrics.k8s.io。
NAME v1beta1.metrics.k8s.io 创建一个命名空间 创建一个命名空间 . glossary_tooltip term_id=&amp;quot;namespace&amp;rdquo; &amp;gt;}}，以便在本练习中创建的资源与集群的其余部分资源隔离。
kubectl create namespace cpu-example 指定一个 CPU 请求和 CPU 限制 要为容器指定 CPU 请求，请包含 resources：requests 字段 在容器资源清单中。要指定 CPU 限制，请包含 resources：limits。</description>
    </item>
    
    <item>
      <title>为容器管理计算资源</title>
      <link>https://lijun.in/concepts/configuration/manage-compute-resources-container/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/concepts/configuration/manage-compute-resources-container/</guid>
      <description>当您定义 Pod 的时候可以选择为每个容器指定需要的 CPU 和内存（RAM）大小。当为容器指定了资源请求后，调度器就能够更好的判断出将容器调度到哪个节点上。如果您还为容器指定了资源限制，Kubernetes 就可以按照指定的方式来处理节点上的资源竞争。关于资源请求和限制的不同点和更多资料请参考 Resource QoS。
资源类型 CPU 和内存都是资源类型。资源类型具有基本单位。CPU 的单位是核心数，内存的单位是字节。
如果您使用的是 Kubernetes v1.14 或更高版本，则可以指定巨页资源。巨页是 Linux 特有的功能，节点内核在其中分配的内存块比默认页大小大得多。
例如，在默认页面大小为 4KiB 的系统上，您可以指定一个限制，hugepages-2Mi: 80Mi。如果容器尝试分配 40 个 2MiB 大页面（总共 80 MiB ），则分配失败。
您不能过量使用hugepages- *资源。 这与memory和cpu资源不同。
CPU和内存统称为计算资源，也可以称为资源。计算资源的数量是可以被请求、分配、消耗和可测量的。它们与 API 资源 不同。 API 资源（如 Pod 和 Service）是可通过 Kubernetes API server 读取和修改的对象。
Pod 和 容器的资源请求和限制 Pod 中的每个容器都可以指定以下的一个或者多个值：
 spec.containers[].resources.limits.cpu spec.containers[].resources.limits.memory spec.containers[].resources.requests.cpu spec.containers[].resources.requests.memory  尽管只能在个别容器上指定请求和限制，但是我们可以方便地计算出 Pod 资源请求和限制。特定资源类型的Pod 资源请求/限制是 Pod 中每个容器的该类型的资源请求/限制的总和。
CPU 的含义 CPU 资源的限制和请求以 cpu 为单位。
Kubernetes 中的一个 cpu 等于：</description>
    </item>
    
    <item>
      <title>交互式教程 - 创建集群</title>
      <link>https://lijun.in/tutorials/kubernetes-basics/create-cluster/cluster-interactive/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tutorials/kubernetes-basics/create-cluster/cluster-interactive/</guid>
      <description>   要与终端交互，请使用桌面/平板    Continue to Module 2› -- 继续阅读第二单元›       </description>
    </item>
    
    <item>
      <title>交互式教程 - 发布您的应用程序</title>
      <link>https://lijun.in/tutorials/kubernetes-basics/expose/expose-interactive/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tutorials/kubernetes-basics/expose/expose-interactive/</guid>
      <description>   要与终端交互，请使用台式机/平板电脑    Continue to Module 5›       </description>
    </item>
    
    <item>
      <title>交互式教程 - 更新应用</title>
      <link>https://lijun.in/tutorials/kubernetes-basics/update/update-interactive/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tutorials/kubernetes-basics/update/update-interactive/</guid>
      <description>  要与终端交互，请使用桌面/平板电脑版本    回到 Kubernetes 的基础›       </description>
    </item>
    
    <item>
      <title>交互式教程 - 部署应用程序</title>
      <link>https://lijun.in/tutorials/kubernetes-basics/deploy-app/deploy-interactive/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tutorials/kubernetes-basics/deploy-app/deploy-interactive/</guid>
      <description>   To interact with the Terminal, please use the desktop/tablet version  -- 要与终端进行交互，请使用桌面/平板电脑版本    Continue to Module 3› -- 继续阅读第3单元›      </description>
    </item>
    
    <item>
      <title>交互式教程-探索您的应用程序</title>
      <link>https://lijun.in/tutorials/kubernetes-basics/explore/explore-interactive/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tutorials/kubernetes-basics/explore/explore-interactive/</guid>
      <description>   To interact with the Terminal, please use the desktop/tablet version  -- 要与终端交互，请使用桌面/平板 版本    Continue to Module 4›       </description>
    </item>
    
    <item>
      <title>交互教程 - 缩放你的应用程序</title>
      <link>https://lijun.in/tutorials/kubernetes-basics/scale/scale-interactive/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tutorials/kubernetes-basics/scale/scale-interactive/</guid>
      <description>  与终端交互，请使用桌面/平板电脑版本    继续参阅第6单元›        </description>
    </item>
    
    <item>
      <title>使用 Cilium 作为 NetworkPolicy</title>
      <link>https://lijun.in/tasks/administer-cluster/network-policy-provider/cilium-network-policy/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/administer-cluster/network-policy-provider/cilium-network-policy/</guid>
      <description>本页展示了如何使用 Cilium 作为 NetworkPolicy。
关于 Cilium 的背景知识，请阅读 Cilium 介绍。
. heading &amp;ldquo;prerequisites&amp;rdquo; %}} . include &amp;ldquo;task-tutorial-prereqs.md&amp;rdquo; &amp;gt;}} . version-check &amp;gt;}}
在 Minikube 上部署 Cilium 用于基本测试 为了轻松熟悉 Cilium 您可以根据Cilium Kubernetes 入门指南在 minikube 中执行一个 cilium 的基本的 DaemonSet 安装。
在 minikube 中的安装配置使用一个简单的“一体化” YAML 文件，包括了 Cilium 的 DaemonSet 配置，连接 minikube 的 etcd 实例，以及适当的 RBAC 设置。
$ kubectl create -f https://raw.githubusercontent.com/cilium/cilium/master/examples/kubernetes/cilium.yaml configmap &amp;#34;cilium-config&amp;#34; created secret &amp;#34;cilium-etcd-secrets&amp;#34; created serviceaccount &amp;#34;cilium&amp;#34; created clusterrolebinding &amp;#34;cilium&amp;#34; created daemonset &amp;#34;cilium&amp;#34; created clusterrole &amp;#34;cilium&amp;#34; created 入门指南其余的部分用一个示例应用说明了如何强制执行L3/L4（即 IP 地址+端口）的安全策略以及L7 （如 HTTP）的安全策略。</description>
    </item>
    
    <item>
      <title>使用 Kops 安装 Kubernetes</title>
      <link>https://lijun.in/setup/production-environment/tools/kops/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/setup/production-environment/tools/kops/</guid>
      <description>本篇快速入门介绍了如何在 AWS 上轻松安装 Kubernetes 集群。 本篇使用了一个名为 kops 的工具。
kops 是一个自用的供应系统：
 全自动安装流程 使用 DNS 识别集群 自我修复：一切都在自动扩展组中运行 支持多种操作系统（如 Debian、Ubuntu 16.04、CentOS、RHEL、Amazon Linux 和 CoreOS） - 参考 images.md 支持高可用 - 参考 high_availability.md 可以直接提供或者生成 terraform 清单 - 参考 terraform.md  如果您有不同的观点，您可能更喜欢使用 kubeadm 作为构建工具来构建自己的集群。kops 建立在 kubeadm 工作的基础上。
创建集群 (1/5) 安装 kops 前提条件 您必须安装 kubectl 才能使 kops 工作。
安装 从下载页面下载 kops（从源代码构建也很容易）：
在 macOS 上：
curl -OL https://github.com/kubernetes/kops/releases/download/1.10.0/kops-darwin-amd64 chmod +x kops-darwin-amd64 mv kops-darwin-amd64 /usr/local/bin/kops # 您也可以使用 Homebrew 安装 kops brew update &amp;amp;&amp;amp; brew install kops 在 Linux 上：</description>
    </item>
    
    <item>
      <title>使用扩展进行并行处理</title>
      <link>https://lijun.in/tasks/job/parallel-processing-expansion/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/job/parallel-processing-expansion/</guid>
      <description>在这个示例中，我们将运行从一个公共模板创建的多个 Kubernetes Job。您可能需要先熟悉 Jobs 的基本概念、非并行以及如何使用它。
基本模板扩展 首先，将以下作业模板下载到名为 job-tmpl.yaml 的文件中。
. codenew file=&amp;quot;application/job/job-tmpl.yaml&amp;rdquo; &amp;gt;}}
与 pod 模板不同，我们的 job 模板不是 Kubernetes API 类型。它只是 Job 对象的 yaml 表示， YAML 文件有一些占位符，在使用它之前需要填充这些占位符。$ITEM 语法对 Kubernetes 没有意义。
在这个例子中，容器所做的唯一处理是 echo 一个字符串并睡眠一段时间。 在真实的用例中，处理将是一些重要的计算，例如渲染电影的一帧，或者处理数据库中的若干行。这时，$ITEM 参数将指定帧号或行范围。
这个 Job 及其 Pod 模板有一个标签: jobgroup=jobexample。这个标签在系统中没有什么特别之处。 这个标签使得我们可以方便地同时操作组中的所有作业。 我们还将相同的标签放在 pod 模板上，这样我们就可以用一个命令检查这些 Job 的所有 pod。 创建作业之后，系统将添加更多的标签来区分一个 Job 的 pod 和另一个 Job 的 pod。 注意，标签键 jobgroup 对 Kubernetes 并无特殊含义。您可以选择自己的标签方案。
下一步，将模板展开到多个文件中，每个文件对应要处理的项。
# 下载 job-templ.yaml curl -L -s -O https://k8s.io/examples/application/job/job-tmpl.yaml # 创建临时目录，并且在目录中创建 job yaml 文件 mkdir .</description>
    </item>
    
    <item>
      <title>创建大型集群</title>
      <link>https://lijun.in/setup/best-practices/cluster-large/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/setup/best-practices/cluster-large/</guid>
      <description>支持 在 . param &amp;ldquo;version&amp;rdquo; &amp;gt;}} 版本中， Kubernetes 支持的最大节点数为 5000。更具体地说，我们支持满足以下所有条件的配置：
 节点数不超过 5000 Pod 总数不超过 150000 容器总数不超过 300000 每个节点的 pod 数量不超过 100  . toc &amp;gt;}}
设定 集群是一组运行着 Kubernetes 代理的节点（物理机或者虚拟机），这些节点由主控节点（集群级控制面）控制。
通常，集群中的节点数由特定于云平台的配置文件 config-default.sh（可以参考 [GCE 平台的 config-default.sh](http://releases.k8s.io/. param &amp;ldquo;githubbranch&amp;rdquo; &amp;gt;}}/cluster/gce/config-default.sh)）中的 NUM_NODES 参数控制。
但是，在许多云供应商的平台上，仅将该值更改为非常大的值，可能会导致安装脚本运行失败。例如，在 GCE，由于配额问题，集群会启动失败。
因此，在创建大型 Kubernetes 集群时，必须考虑以下问题。
配额问题 为了避免遇到云供应商配额问题，在创建具有大规模节点的集群时，请考虑：
 增加诸如 CPU，IP 等资源的配额。  例如，在 GCE，您需要增加以下资源的配额：  CPUs VM 实例 永久磁盘总量 使用中的 IP 地址 防火墙规则 转发规则 路由 目标池     由于某些云供应商会对虚拟机的创建进行流控，因此需要对设置脚本进行更改，使其以较小的批次启动新的节点，并且之间有等待时间。  Etcd 存储 为了提高大规模集群的性能，我们将事件存储在专用的 etcd 实例中。</description>
    </item>
    
    <item>
      <title>卷快照</title>
      <link>https://lijun.in/concepts/storage/volume-snapshots/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/concepts/storage/volume-snapshots/</guid>
      <description>feature-state for_k8s_version=&amp;quot;1.17&amp;rdquo; state=&amp;quot;beta&amp;rdquo; &amp;gt;}}
在 Kubernetes 中，卷快照是一个存储系统上卷的快照，本文假设你已经熟悉了 Kubernetes 的 持久卷。
介绍 与 PersistentVolume 和 PersistentVolumeClaim 两个 API 资源用于给用户和管理员提供卷类似，VolumeSnapshotContent 和 VolumeSnapshot 两个 API 资源用于给用户和管理员创建卷快照。
VolumeSnapshotContent 是一种快照，从管理员已提供的集群中的卷获取。就像持久卷是集群的资源一样，它也是集群中的资源。
VolumeSnapshot 是用户对于卷的快照的请求。它类似于持久卷声明。
VolumeSnapshotClass 允许指定属于 VolumeSnapshot 的不同属性。在从存储系统的相同卷上获取的快照之间，这些属性可能有所不同，因此不能通过使用与 PersistentVolumeClaim 相同的 StorageClass 来表示。
当使用该功能时，用户需要注意以下几点：
 API 对象 VolumeSnapshot，VolumeSnapshotContent 和 VolumeSnapshotClass 是 glossary_tooltip term_id=&amp;quot;CustomResourceDefinition&amp;rdquo; text=&amp;quot;CRDs&amp;rdquo; &amp;gt;}}，不是核心 API 的部分。 VolumeSnapshot 支持仅可用于 CSI 驱动。 作为 beta 版本 VolumeSnapshot 部署过程的一部分，Kubernetes 团队提供了一个部署于控制平面的快照控制器，并且提供了一个叫做 csi-snapshotter 的 sidecar 帮助容器，它和 CSI 驱动程序部署在一起。快照控制器监视 VolumeSnapshot 和 VolumeSnapshotContent 对象，并且负责动态的创建和删除 VolumeSnapshotContent 对象。sidecar csi-snapshotter 监视 VolumeSnapshotContent 对象，并且触发针对 CSI 端点的 CreateSnapshot 和 DeleteSnapshot 的操作。 CSI 驱动可能实现，也可能没有实现卷快照功能。CSI 驱动可能会使用 csi-snapshotter 来提供对卷快照的支持。详见 CSI 驱动程序文档 Kubernetes 负责 CRDs 和快照控制器的安装。  卷快照和卷快照内容的生命周期 VolumeSnapshotContents 是集群中的资源。VolumeSnapshots 是对于这些资源的请求。VolumeSnapshotContents 和 VolumeSnapshots 之间的交互遵循以下生命周期：</description>
    </item>
    
    <item>
      <title>安装 Minikube</title>
      <link>https://lijun.in/tasks/tools/install-minikube/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/tools/install-minikube/</guid>
      <description>本页面讲述如何安装 Minikube，该工具用于在您电脑中的虚拟机上运行一个单节点的 Kubernetes 集群。
. heading &amp;ldquo;prerequisites&amp;rdquo; %}} . tabs name=&amp;quot;minikube_before_you_begin&amp;rdquo; &amp;gt;}} . tab name=&amp;quot;Linux&amp;rdquo; %}}
若要检查您的 Linux 是否支持虚拟化技术，请运行下面的命令并验证输出结果是否不为空：
grep -E --color &#39;vmx|svm&#39; /proc/cpuinfo . /tab %}}
. tab name=&amp;quot;macOS&amp;rdquo; %}}
若要检查您的 macOS 是否支持虚拟化技术，请运行下面的命令：
sysctl -a | grep -E --color &#39;machdep.cpu.features|VMX&#39; 如果你在输出结果中看到了 VMX （应该会高亮显示）的字眼，说明您的电脑已启用 VT-x 特性。
. /tab %}}
. tab name=&amp;quot;Windows&amp;rdquo; %}}
若要检查您的 Windows8 及以上的系统是否支持虚拟化技术，请终端或者 cmd 中运行以下命令：
systeminfo 如果您看到下面的输出，则表示该 Windows 支持虚拟化技术。
Hyper-V Requirements: VM Monitor Mode Extensions: Yes Virtualization Enabled In Firmware: Yes Second Level Address Translation: Yes Data Execution Prevention Available: Yes 如果您看到下面的输出，则表示您的操作系统已经安装了 Hypervisor，您可以跳过安装 Hypervisor 的步骤。</description>
    </item>
    
    <item>
      <title>容器环境</title>
      <link>https://lijun.in/concepts/containers/container-environment/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/concepts/containers/container-environment/</guid>
      <description>本页描述了在容器环境里容器可用的资源。
容器环境 Kubernetes 的容器环境给容器提供了几个重要的资源：
 文件系统，其中包含一个镜像 和一个或多个的卷。 容器自身的信息。 集群中其他对象的信息。  容器信息 容器的 hostname 是它所运行在的 pod 的名称。它可以通过 hostname 命令或者调用 libc 中的 gethostname 函数来获取。
Pod 名称和命名空间可以通过 downward API 使用环境变量。
Pod 定义中的用户所定义的环境变量也可在容器中使用，就像在 Docker 镜像中静态指定的任何环境变量一样。
集群信息 创建容器时正在运行的所有服务的列表都可用作该容器的环境变量。这些环境变量与 Docker 链接的语法匹配。
对于名为 foo 的服务，当映射到名为 bar 的容器时，以下变量是被定义了的：
FOO_SERVICE_HOST=&amp;lt;the host the service is running on&amp;gt; FOO_SERVICE_PORT=&amp;lt;the port the service is running on&amp;gt; Service 具有专用的 IP 地址。如果启用了 [DNS插件](http://releases.k8s.io/&amp;lt; param &amp;ldquo;githubbranch&amp;rdquo; &amp;gt;}}/cluster/addons/dns/)，就可以在容器中通过 DNS 来访问。
  学习更多有关容器生命周期钩子的知识。 动手获得经验将处理程序附加到容器生命周期事件。  </description>
    </item>
    
    <item>
      <title>容器运行时类(Runtime Class)</title>
      <link>https://lijun.in/concepts/containers/runtime-class/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/concepts/containers/runtime-class/</guid>
      <description>for_k8s_version=&amp;quot;v1.14&amp;rdquo; state=&amp;quot;beta&amp;rdquo; &amp;gt;}}
本页面描述了 RuntimeClass 资源和运行时的选择机制。
RuntimeClass 是一个用于选择容器运行时配置的特性，容器运行时配置用于运行 Pod 中的容器。
动机 您可以在不同的 pod 之间设置不同的 RuntimeClass，以提供性能与安全性之间的平衡。 例如，如果您的部分工作负载需要高级别的信息安全保证，那么您可以选择性地调度这些 pod， 使它们在使用硬件虚拟化的容器运行时中运行。 然后，您将从可选运行时的额外隔离中获益，代价是一些额外的开销。
您还可以使用 RuntimeClass 运行具有相同容器运行时但具有不同设置的pod。
设置 确保 RuntimeClass 特性开关处于开启状态（默认为开启状态）。 关于特性开关的详细介绍，请查阅 Feature Gates。 RuntimeClass 特性开关必须在 apiserver 和 kubelet 同时开启。
 在节点上配置 CRI 的实现（取决于所选用的运行时） 创建相应的 RuntimeClass 资源  1. 在节点上配置 CRI 实现 RuntimeClass 的配置依赖于 运行时接口（CRI）的实现。 根据你使用的 CRI 实现，查阅相关的文档（下方）来了解如何配置。
RuntimeClass 假设集群中的节点配置是同构的（换言之，所有的节点在容器运行时方面的配置是相同的）。 如果需要支持异构节点，配置方法请参阅下面的 调度。
所有这些配置都具有相应的 handler 名，并被 RuntimeClass 引用。 handler 必须符合 DNS-1123 命名规范（字母、数字、或 -）。
2. 创建相应的 RuntimeClass 资源 在上面步骤 1 中，每个配置都需要有一个用于标识配置的 handler。 针对每个 handler 需要创建一个 RuntimeClass 对象。</description>
    </item>
    
    <item>
      <title>对 kubeadm 进行故障排查</title>
      <link>https://lijun.in/setup/production-environment/tools/kubeadm/troubleshooting-kubeadm/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/setup/production-environment/tools/kubeadm/troubleshooting-kubeadm/</guid>
      <description>与任何程序一样，您可能会在安装或者运行 kubeadm 时遇到错误。 本文列举了一些常见的故障场景，并提供可帮助您理解和解决这些问题的步骤。
如果您的问题未在下面列出，请执行以下步骤：
  如果您认为问题是 kubeadm 的错误：
 转到 github.com/kubernetes/kubeadm 并搜索存在的问题。 如果没有问题，请 打开 并遵循问题模板。    如果您对 kubeadm 的工作方式有疑问，可以在 Slack 上的 #kubeadm 频道提问， 或者在 StackOverflow 上提问。 请加入相关标签，例如 #kubernetes 和 #kubeadm，这样其他人可以帮助您。
  在安装过程中没有找到 ebtables 或者其他类似的可执行文件 如果在运行 kubeadm init 命令时，遇到以下的警告
[preflight] WARNING: ebtables not found in system path [preflight] WARNING: ethtool not found in system path 那么或许在您的节点上缺失 ebtables、ethtool 或者类似的可执行文件。 您可以使用以下命令安装它们：
 对于 Ubuntu/Debian 用户，运行 apt install ebtables ethtool 命令。 对于 CentOS/Fedora 用户，运行 yum install ebtables ethtool 命令。  在安装过程中，kubeadm 一直等待控制平面就绪 如果您注意到 kubeadm init 在打印以下行后挂起：</description>
    </item>
    
    <item>
      <title>对象名称和IDs</title>
      <link>https://lijun.in/concepts/overview/working-with-objects/names/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/concepts/overview/working-with-objects/names/</guid>
      <description>集群中的每一个对象都一个名称 来标识在同类资源中的唯一性。
每个 Kubernetes 对象也有一个UID 来标识在整个集群中的唯一性。
比如，在同一个namespace中只能命名一个名为 myapp-1234 的 Pod, 但是可以命名一个 Pod 和一个 Deployment 同为 myapp-1234.
对于非唯一的用户提供的属性，Kubernetes 提供了标签和注释。
有关名称和 UID 的精确语法规则，请参见标识符设计文档。
名称 term_id=&amp;quot;name&amp;rdquo; length=&amp;quot;all&amp;rdquo; &amp;gt;}}
以下是比较常见的三种资源命名约束。
DNS 子域名 某些资源类型需要一个 name 来作为一个 DNS 子域名，见定义 RFC 1123。也就是命名必须满足如下规则：
 不能超过253个字符 只能包含字母数字，以及&amp;rsquo;-&amp;rsquo; 和 &amp;lsquo;.&amp;rsquo; 须以字母数字开头 须以字母数字结尾  DNS 标签名称 某些资源类型需要其名称遵循 DNS 标签的标准，见RFC 1123。也就是命名必须满足如下规则：
 最多63个字符 只能包含字母数字，以及&amp;rsquo;-&amp;rsquo; 须以字母数字开头 须以字母数字结尾  Path 部分名称 一些用与 Path 部分的资源类型要求名称能被安全的 encode。换句话说，其名称不能含有这些字符 &amp;ldquo;.&amp;quot;、&amp;rdquo;..&amp;quot;、&amp;quot;/&amp;ldquo;或&amp;rdquo;%&amp;quot;。
下面是一个名为nginx-demo的 Pod 的配置清单：
apiVersion: v1 kind: Pod metadata: name: nginx-demo spec: containers: - name: nginx image: nginx:1.</description>
    </item>
    
    <item>
      <title>示例：使用 Persistent Volumes 部署 WordPress 和 MySQL</title>
      <link>https://lijun.in/tutorials/stateful-application/mysql-wordpress-persistent-volume/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tutorials/stateful-application/mysql-wordpress-persistent-volume/</guid>
      <description>本示例描述了如何通过 Minikube 在 Kubernetes 上安装 WordPress 和 MySQL。这两个应用都使用 PersistentVolumes 和 PersistentVolumeClaims 保存数据。
PersistentVolume（PV）是一块集群里由管理员手动提供，或 kubernetes 通过 StorageClass 动态创建的存储。 PersistentVolumeClaim（PVC）是一个满足对 PV 存储需要的请求。PersistentVolumes 和 PersistentVolumeClaims 是独立于 Pod 生命周期而在 Pod 重启，重新调度甚至删除过程中保存数据。
. warning &amp;gt;}}
deployment 在生产场景中并不适合，它使用单实例 WordPress 和 MySQL Pods。考虑使用 WordPress Helm Chart 在生产场景中部署 WordPress。 . /warning &amp;gt;}}
. note &amp;gt;}}
本教程中提供的文件使用 GA Deployment API，并且特定于 kubernetes 1.9 或更高版本。如果您希望将本教程与 Kubernetes 的早期版本一起使用，请相应地更新 API 版本，或参考本教程的早期版本。 . /note &amp;gt;}}
. heading &amp;ldquo;objectives&amp;rdquo; %}}  创建 PersistentVolumeClaims 和 PersistentVolumes 创建 kustomization.</description>
    </item>
    
    <item>
      <title>示例：使用 Redis 部署 PHP 留言板应用程序</title>
      <link>https://lijun.in/tutorials/stateless-application/guestbook/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tutorials/stateless-application/guestbook/</guid>
      <description>本教程向您展示如何使用 Kubernetes 和 Docker 构建和部署 一个简单的多层 web 应用程序。本例由以下组件组成：
 单实例 Redis 主节点保存留言板条目 多个从 Redis 节点用来读取数据 多个 web 前端实例  . heading &amp;ldquo;objectives&amp;rdquo; %}}  启动 Redis 主节点。 启动 Redis 从节点。 启动留言板前端。 公开并查看前端服务。 清理。  . heading &amp;ldquo;prerequisites&amp;rdquo; %}} . include &amp;ldquo;task-tutorial-prereqs.md&amp;rdquo; &amp;gt;}}
. version-check &amp;gt;}}
启动 Redis 主节点 留言板应用程序使用 Redis 存储数据。它将数据写入一个 Redis 主实例，并从多个 Redis 读取数据。
创建 Redis 主节点的 Deployment 下面包含的清单文件指定了一个 Deployment 控制器，该控制器运行一个 Redis 主节点 Pod 副本。
. codenew file=&amp;quot;application/guestbook/redis-master-deployment.yaml&amp;rdquo; &amp;gt;}}</description>
    </item>
    
    <item>
      <title>设备插件</title>
      <link>https://lijun.in/concepts/extend-kubernetes/compute-storage-net/device-plugins/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/concepts/extend-kubernetes/compute-storage-net/device-plugins/</guid>
      <description>for_k8s_version=&amp;quot;v1.10&amp;rdquo; state=&amp;quot;beta&amp;rdquo; &amp;gt;}}
Kubernetes 提供了一个设备插件框架，您可以用来将系统硬件资源发布到 term_id=&amp;quot;kubelet&amp;rdquo; &amp;gt;}}。
供应商可以实现设备插件，由您手动部署或作为 term_id=&amp;quot;daemonset&amp;rdquo; &amp;gt;}} 来部署，而不必定制 Kubernetes 本身的代码。目标设备包括 GPU、高性能 NIC、FPGA、InfiniBand 适配器以及其他类似的、可能需要特定于供应商的初始化和设置的计算资源。
注册设备插件 kubelet 输出了一个 Registration 的 gRPC 服务：
service Registration { rpc Register(RegisterRequest) returns (Empty) {} } 设备插件可以通过此 gRPC 服务在 kubelet 进行注册。在注册期间，设备插件需要发送下面几样内容：
 设备插件的 Unix 套接字。 设备插件的 API 版本。 ResourceName 是需要公布的。这里 ResourceName 需要遵循扩展资源命名方案，类似于 vendor-domain/resourcetype。（比如 NVIDIA GPU 就被公布为 nvidia.com/gpu。）  成功注册后，设备插件就向 kubelet 发送他所管理的设备列表，然后 kubelet 负责将这些资源发布到 API 服务器，作为 kubelet 节点状态更新的一部分。
比如，设备插件在 kubelet 中注册了 hardware-vendor.example/foo 并报告了节点上的两个运行状况良好的设备后，节点状态将更新以通告该节点已安装2个 Foo 设备并且是可用的。</description>
    </item>
    
    <item>
      <title>访问集群</title>
      <link>https://lijun.in/tasks/access-application-cluster/access-cluster/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/access-application-cluster/access-cluster/</guid>
      <description>本文阐述多种与集群交互的方法。
. toc &amp;gt;}}
使用 kubectl 完成集群的第一次访问 当您第一次访问 Kubernetes API 的时候，我们建议您使用 Kubernetes CLI，kubectl。
访问集群时，您需要知道集群的地址并且拥有访问的凭证。通常，这些在您通过 Getting started guide 安装集群时都是自动安装好的，或者其他人安装时也应该提供了凭证和集群地址。
通过以下命令检查 kubectl 是否知道集群地址及凭证：
$ kubectl config view 有许多 例子 介绍了如何使用 kubectl，可以在 kubectl手册 中找到更完整的文档。
直接访问 REST API Kubectl 处理 apiserver 的定位和身份验证。 如果要使用 curl 或 wget 等 http 客户端或浏览器直接访问 REST API，可以通过多种方式查找和验证：
 以代理模式运行 kubectl。  推荐此方式。 使用已存储的 apiserver 地址。 使用自签名的证书来验证 apiserver 的身份。杜绝 MITM 攻击。 对 apiserver 进行身份验证。 未来可能会实现智能化的客户端负载均衡和故障恢复。   直接向 http 客户端提供位置和凭据。  可选的方案。 适用于代理可能引起混淆的某些客户端类型。 需要引入根证书到您的浏览器以防止 MITM 攻击。    使用 kubectl 代理 以下命令以反向代理的模式运行kubectl。它处理 apiserver 的定位和验证。 像这样运行：</description>
    </item>
    
    <item>
      <title>证书</title>
      <link>https://lijun.in/concepts/cluster-administration/certificates/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/concepts/cluster-administration/certificates/</guid>
      <description>当使用客户端证书进行认证时，用户可以使用现有部署脚本，或者通过 easyrsa、openssl 或 cfssl 手动生成证书。
easyrsa 使用 easyrsa 能够手动地为集群生成证书。
  下载、解压并初始化 easyrsa3 的补丁版本。
curl -LO https://storage.googleapis.com/kubernetes-release/easy-rsa/easy-rsa.tar.gz tar xzf easy-rsa.tar.gz cd easy-rsa-master/easyrsa3 ./easyrsa init-pki    生成 CA（通过 --batch 参数设置自动模式。 通过 --req-cn 设置默认使用的 CN）
./easyrsa --batch &amp;quot;--req-cn=${MASTER_IP}@`date +%s`&amp;quot; build-ca nopass    生成服务器证书和密钥。 参数 --subject-alt-name 设置了访问 API 服务器时可能使用的 IP 和 DNS 名称。 MASTER_CLUSTER_IP 通常为 --service-cluster-ip-range 参数中指定的服务 CIDR 的 首个 IP 地址，--service-cluster-ip-range 同时用于 API 服务器和控制器管理器组件。 --days 参数用于设置证书的有效期限。 下面的示例还假设用户使用 cluster.local 作为默认的 DNS 域名。</description>
    </item>
    
    <item>
      <title>JSONPath 支持</title>
      <link>https://lijun.in/reference/kubectl/jsonpath/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/kubectl/jsonpath/</guid>
      <description>Kubectl 支持 JSONPath 模板。
JSONPath 模板由 {} 包起来的 JSONPath 表达式组成。Kubectl 使用 JSONPath 表达式来过滤 JSON 对象中的特定字段并格式化输出。除了原始的 JSONPath 模板语法，以下函数和语法也是有效的:
 使用双引号将 JSONPath 表达式内的文本引起来。 使用 range，end 运算符来迭代列表。 使用负片索引后退列表。负索引不会&amp;quot;环绕&amp;quot;列表，并且只要 -index + listLength&amp;gt; = 0 就有效。  note &amp;gt;}}
  $ 运算符是可选的，因为默认情况下表达式总是从根对象开始。
  结果对象将作为其 String() 函数输出。
  /note &amp;gt;}}
给定 JSON 输入:
{ &amp;#34;kind&amp;#34;: &amp;#34;List&amp;#34;, &amp;#34;items&amp;#34;:[ { &amp;#34;kind&amp;#34;:&amp;#34;None&amp;#34;, &amp;#34;metadata&amp;#34;:{&amp;#34;name&amp;#34;:&amp;#34;127.0.0.1&amp;#34;}, &amp;#34;status&amp;#34;:{ &amp;#34;capacity&amp;#34;:{&amp;#34;cpu&amp;#34;:&amp;#34;4&amp;#34;}, &amp;#34;addresses&amp;#34;:[{&amp;#34;type&amp;#34;: &amp;#34;LegacyHostIP&amp;#34;, &amp;#34;address&amp;#34;:&amp;#34;127.0.0.1&amp;#34;}] } }, { &amp;#34;kind&amp;#34;:&amp;#34;None&amp;#34;, &amp;#34;metadata&amp;#34;:{&amp;#34;name&amp;#34;:&amp;#34;127.0.0.2&amp;#34;}, &amp;#34;status&amp;#34;:{ &amp;#34;capacity&amp;#34;:{&amp;#34;cpu&amp;#34;:&amp;#34;8&amp;#34;}, &amp;#34;addresses&amp;#34;:[ {&amp;#34;type&amp;#34;: &amp;#34;LegacyHostIP&amp;#34;, &amp;#34;address&amp;#34;:&amp;#34;127.</description>
    </item>
    
    <item>
      <title>kube-scheduler</title>
      <link>https://lijun.in/reference/command-line-tools-reference/kube-scheduler/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/command-line-tools-reference/kube-scheduler/</guid>
      <description>. heading &amp;ldquo;synopsis&amp;rdquo; %}} Kubernetes 调度器是一个策略丰富、拓扑感知、工作负载特定的功能，调度器显著影响可用性、性能和容量。调度器需要考虑个人和集体的资源要求、服务质量要求、硬件/软件/政策约束、亲和力和反亲和力规范、数据局部性、负载间干扰、完成期限等。工作负载特定的要求必要时将通过 API 暴露。
kube-scheduler [flags] . heading &amp;ldquo;options&amp;rdquo; %}} &amp;lt;tr&amp;gt; &amp;lt;td colspan=&amp;quot;2&amp;quot;&amp;gt;--add-dir-header&amp;lt;/td&amp;gt; &amp;lt;/tr&amp;gt; &amp;lt;tr&amp;gt; &amp;lt;td&amp;gt;&amp;lt;/td&amp;gt;&amp;lt;td style=&amp;quot;line-height: 130%; word-wrap: break-word;&amp;quot;&amp;gt; &amp;lt;!-- If true, adds the file directory to the header --&amp;gt; 如果为 true，则将文件目录添加到标题中 &amp;lt;/td&amp;gt; &amp;lt;/tr&amp;gt; &amp;lt;tr&amp;gt; &amp;lt;td colspan=&amp;quot;2&amp;quot;&amp;gt; &amp;lt;!-- --address string&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;Default: &amp;quot;0.0.0.0&amp;quot; --&amp;gt; --address string&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;默认: &amp;quot;0.0.0.0&amp;quot; &amp;lt;/td&amp;gt; &amp;lt;/tr&amp;gt; &amp;lt;tr&amp;gt; &amp;lt;td&amp;gt;&amp;lt;/td&amp;gt;&amp;lt;td style=&amp;quot;line-height: 130%; word-wrap: break-word;&amp;quot;&amp;gt; &amp;lt;!-- DEPRECATED: the IP address on which to listen for the --port port (set to 0.</description>
    </item>
    
    <item>
      <title>kubectl</title>
      <link>https://lijun.in/reference/kubectl/kubectl/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/kubectl/kubectl/</guid>
      <description>. heading &amp;ldquo;synopsis&amp;rdquo; %}} kubectl 管理控制 Kubernetes 集群。
获取更多信息，请访问 kubectl 概述。
kubectl [flags] . heading &amp;ldquo;options&amp;rdquo; %}} &amp;lt;tr&amp;gt; &amp;lt;td colspan=&amp;quot;2&amp;quot;&amp;gt;--add-dir-header&amp;lt;/td&amp;gt; &amp;lt;/tr&amp;gt; &amp;lt;tr&amp;gt; &amp;lt;td&amp;gt;&amp;lt;/td&amp;gt;&amp;lt;td style=&amp;quot;line-height: 130%; word-wrap: break-word;&amp;quot;&amp;gt; &amp;lt;!-- If true, adds the file directory to the header --&amp;gt; 设置为 true 表示添加文件目录到 header 中 &amp;lt;/td&amp;gt; &amp;lt;/tr&amp;gt; &amp;lt;tr&amp;gt; &amp;lt;td colspan=&amp;quot;2&amp;quot;&amp;gt;--alsologtostderr&amp;lt;/td&amp;gt; &amp;lt;/tr&amp;gt; &amp;lt;tr&amp;gt; &amp;lt;td&amp;gt;&amp;lt;/td&amp;gt;&amp;lt;td style=&amp;quot;line-height: 130%; word-wrap: break-word;&amp;quot;&amp;gt; &amp;lt;!-- log to standard error as well as files --&amp;gt; 表示将日志输出到文件的同时输出到 stderr &amp;lt;/td&amp;gt; &amp;lt;/tr&amp;gt; &amp;lt;tr&amp;gt; &amp;lt;td colspan=&amp;quot;2&amp;quot;&amp;gt;--as string&amp;lt;/td&amp;gt; &amp;lt;/tr&amp;gt; &amp;lt;tr&amp;gt; &amp;lt;td&amp;gt;&amp;lt;/td&amp;gt;&amp;lt;td style=&amp;quot;line-height: 130%; word-wrap: break-word;&amp;quot;&amp;gt; &amp;lt;!</description>
    </item>
    
    <item>
      <title>kubelet</title>
      <link>https://lijun.in/reference/command-line-tools-reference/kubelet/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/command-line-tools-reference/kubelet/</guid>
      <description>. heading &amp;ldquo;synopsis&amp;rdquo; %}} kubelet 是在每个 Node 节点上运行的主要 “节点代理”。它向 apiserver 注册节点时可以使用主机名（hostname）；可以提供用于覆盖主机名的参数；还可以执行特定于某云服务商的逻辑。
kubelet 是基于 PodSpec 来工作的。每个 PodSpec 是一个描述 Pod 的 YAML 或 JSON 对象。kubelet 接受通过各种机制（主要是通过 apiserver）提供的一组 PodSpec，并确保这些 PodSpec 中描述的容器处于运行状态且运行状况良好。kubelet 不管理不是由 Kubernetes 创建的容器。
除了来自 apiserver 的 PodSpec 之外，还可以通过以下三种方式将容器清单（manifest）提供给 kubelet。
File（文件）：利用命令行参数给定路径。kubelet 周期性地监视此路径下的文件是否有更新。监视周期默认为 20s，且可通过参数进行配置。
HTTP endpoint（HTTP 端点）：利用命令行参数指定 HTTP 端点。此端点每 20 秒被检查一次（也可以使用参数进行配置）。
HTTP server（HTTP 服务器）：kubelet 还可以侦听 HTTP 并响应简单的 API（当前未经过规范）来提交新的清单。
kubelet [flags] . heading &amp;ldquo;options&amp;rdquo; %}} </description>
    </item>
    
    <item>
      <title>CSI 卷克隆</title>
      <link>https://lijun.in/concepts/storage/volume-pvc-datasource/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/concepts/storage/volume-pvc-datasource/</guid>
      <description>本文档介绍 Kubernetes 中克隆现有 CSI 卷的概念。阅读前建议先熟悉卷。
介绍 glossary_tooltip text=&amp;quot;CSI&amp;rdquo; term_id=&amp;quot;csi&amp;rdquo; &amp;gt;}} 卷克隆功能增加了通过在 dataSource 字段中指定存在的 glossary_tooltip text=&amp;quot;PVC&amp;rdquo; term_id=&amp;quot;persistent-volume-claim&amp;rdquo; &amp;gt;}}s，来表示用户想要克隆的 glossary_tooltip term_id=&amp;quot;volume&amp;rdquo; &amp;gt;}}。
克隆，意思是为已有的 Kubernetes 卷创建副本，它可以像任何其它标准卷一样被使用。唯一的区别就是配置后，后端设备将创建指定完全相同的副本，而不是创建一个“新的”空卷。
从 Kubernetes API 的角度看，克隆的实现只是在创建新的 PVC 时，增加了指定一个现有 PVC 作为数据源的能力。源 PVC 必须是 bound 状态且可用的（不在使用中）。
用户在使用该功能时，需要注意以下事项：
 克隆支持（VolumePVCDataSource）仅适用于 CSI 驱动。 克隆支持仅适用于 动态供应器。 CSI 驱动可能实现，也可能未实现卷克隆功能。 仅当 PVC 与目标 PVC 存在于同一命名空间（源和目标 PVC 必须在相同的命名空间）时，才可以克隆 PVC。 仅在同一存储类中支持克隆。  目标卷必须和源卷具有相同的存储类 可以使用默认的存储类并且 storageClassName 字段在规格中忽略了   克隆只能在两个使用相同 VolumeMode 设置的卷中进行（如果请求克隆一个块存储模式的卷，源卷必须也是块存储模式）。  供应 克隆卷与其他任何 PVC 一样配置，除了需要增加 dataSource 来引用同一命名空间中现有的 PVC。</description>
    </item>
    
    <item>
      <title>Deployments</title>
      <link>https://lijun.in/concepts/workloads/controllers/deployment/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/concepts/workloads/controllers/deployment/</guid>
      <description>apiVersion: extensions/v1beta1 # 接口版本 kind: Deployment # 接口类型 metadata: name: cango-demo # Deployment名称 namespace: cango-prd # 命名空间 labels: app: cango-demo # 标签 spec: replicas: 3 strategy: # 部署策略 rollingUpdate: # 由于replicas为3,则整个升级,pod个数在2-4个之间 maxSurge: 1 # 滚动升级时会先启动1个pod maxUnavailable: 1 # 滚动升级时允许的最大Unavailable的pod个数 template: metadata: labels: app: cango-demo # 模板名称必填 sepc: # 定义容器模板,该模板可以包含多个容器 containers: - name: cango-demo # 镜像名称 image: swr.cn-east-2.myhuaweicloud.com/cango-prd/cango-demo:0.0.1-SNAPSHOT # 镜像地址 command: [ &amp;#34;/bin/sh&amp;#34;,&amp;#34;-c&amp;#34;,&amp;#34;cat /etc/config/path/to/special-key&amp;#34; ] # 启动命令 args: # 启动参数 - &amp;#39;-storage.</description>
    </item>
    
    <item>
      <title>kube-proxy</title>
      <link>https://lijun.in/reference/command-line-tools-reference/kube-proxy/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/command-line-tools-reference/kube-proxy/</guid>
      <description>. heading &amp;ldquo;synopsis&amp;rdquo; %}} Kubernetes 网络代理在每个节点上运行。网络代理反映了每个节点上 Kubernetes API 中定义的服务，并且可以执行简单的 TCP、UDP 和 SCTP 流转发，或者在一组后端进行循环 TCP、UDP 和 SCTP 转发。当前可通过 Docker-links-compatible 环境变量找到服务集群 IP 和端口，这些环境变量指定了服务代理打开的端口。有一个可选的插件，可以为这些集群 IP 提供集群 DNS。用户必须使用 apiserver API 创建服务才能配置代理。
kube-proxy [flags] . heading &amp;ldquo;options&amp;rdquo; %}} </description>
    </item>
    
    <item>
      <title>kubeadm join</title>
      <link>https://lijun.in/reference/setup-tools/kubeadm/kubeadm-join/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/setup-tools/kubeadm/kubeadm-join/</guid>
      <description>此命令用来初始化 Kubernetes 工作节点并将其加入集群。
. include &amp;ldquo;generated/kubeadm_join.md&amp;rdquo; &amp;gt;}}
加入流程 kubeadm join 初始化 Kubernetes 工作节点并将其加入集群。 该操作过程包含下面几个步骤：
  kubeadm 从 API 服务器下载必要的集群信息。 默认情况下，它使用引导令牌和 CA 密钥哈希来验证数据的真实性。 也可以通过文件或 URL 直接发现根 CA。
  如果调用 kubeadm 时启用了 --feature-gates=DynamicKubeletConfig，它首先从主机上检索 kubelet 初始化配置并将其写入磁盘。 当 kubelet 启动时，kubeadm 更新节点的 Node.spec.configSource 属性。 进一步了解动态 kubelet 配置 请参考 使用配置文件设置 Kubelet 参数 和 重新配置集群中节点的 Kubelet。
  一旦知道集群信息，kubelet 就可以开始 TLS 引导过程。 TLS 引导程序使用共享令牌与 Kubernetes API 服务器进行临时的身份验证，以提交证书签名请求 (CSR)； 默认情况下，控制平面自动对该 CSR 请求进行签名。
  最后，kubeadm 配置本地 kubelet 使用分配给节点的确定标识连接到 API 服务器。</description>
    </item>
    
    <item>
      <title>Kubernetes API</title>
      <link>https://lijun.in/concepts/overview/kubernetes-api/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/concepts/overview/kubernetes-api/</guid>
      <description>API协议文档描述了主系统和API概念。
API参考文档描述了API整体规范。
访问文档讨论了通过远程访问API的相关问题。
Kubernetes API是系统描述性配置的基础。 Kubectl 命令行工具被用于创建、更新、删除、获取API对象。
Kubernetes 通过API资源存储自己序列化状态(现在存储在etcd)。
Kubernetes 被分成多个组件，各部分通过API相互交互。
API 变更 根据经验，任何成功的系统都需要随着新的用例出现或现有用例发生变化的情况下，进行相应的进化与调整。因此，我们希望Kubernetes API也可以保持持续的进化和调整。同时，在较长一段时间内，我们也希望与现有客户端版本保持良好的向下兼容性。一般情况下，增加新的API资源和资源字段不会导致向下兼容性问题发生；但如果是需要删除一个已有的资源或者字段，那么必须通过API废弃流程来进行。
参考API变更文档，了解兼容性变更的要素以及如何变更API的流程。
OpenAPI 和 API Swagger 定义 完整的 API 详细文档使用 OpenAPI生成.
随着 Kubernetes 1.10 版本的正式启用，Kubernetes API 服务通过 /openapi/v2 接口提供 OpenAPI 规范。 通过设置 HTTP 标头的规定了请求的结构。
   Header Possible Values     Accept application/json, application/com.github.proto-openapi.spec.v2@v1.0+protobuf (the default content-type is application/json for */* or not passing this header)   Accept-Encoding gzip (not passing this header is acceptable)    在1.</description>
    </item>
    
    <item>
      <title>Kubernetes 版本及版本倾斜支持策略</title>
      <link>https://lijun.in/setup/release/version-skew-policy/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/setup/release/version-skew-policy/</guid>
      <description>本文描述 Kubernetes 各组件之间版本倾斜支持策略。 特定的集群部署工具可能会有额外的限制。
Supported versions Kubernetes 版本号格式为 x.y.z，其中 x 为大版本号，y 为小版本号，z 为补丁版本号。 版本号格式遵循 Semantic Versioning 规则。 更多信息，请参阅 Kubernetes Release Versioning。
Kubernetes 项目会维护最近的三个小版本分支。
一些 bug 修复，包括安全修复，根据其安全性和可用性，有可能会回合到这些分支。 补丁版本会定期或根据需要从这些分支中发布。 最终是否发布是由patch release team 来决定的。Patch release team同时也是release managers. 如需了解更多信息，请查看 Kubernetes Patch releases.
小版本大约每3个月发布一个，所以每个小版本分支会维护9个月。
Supported version skew kube-apiserver In highly-available (HA) clusters, the newest and oldest kube-apiserver instances must be within one minor version. 在 高可用（HA）集群 中， 多个 kube-apiserver 实例小版本号最多差1。
例如：
 最新的 kube-apiserver 版本号如果是 1.</description>
    </item>
    
    <item>
      <title>Operator 模式</title>
      <link>https://lijun.in/concepts/extend-kubernetes/operator/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/concepts/extend-kubernetes/operator/</guid>
      <description>Operator 是 Kubernetes 的扩展软件，它利用自定义资源管理应用及其组件。 Operator 遵循 Kubernetes 的理念，特别是在控制回路方面。
初衷 Operator 模式旨在捕获（正在管理一个或一组服务的）运维人员的关键目标。 负责特定应用和 service 的运维人员，在系统应该如何运行、如何部署以及出现问题时如何处理等方面有深入的了解。
在 Kubernetes 上运行工作负载的人们都喜欢通过自动化来处理重复的任务。Operator 模式会封装您编写的（Kubernetes 本身提供功能以外的）任务自动化代码。
Kubernetes 上的 Operator Kubernetes 为自动化而生。无需任何修改，您即可以从 Kubernetes 核心中获得许多内置的自动化功能。 您可以使用 Kubernetes 自动化部署和运行工作负载， 甚至 可以自动化 Kubernetes 自身。
Kubernetes text=&amp;quot;控制器&amp;rdquo; term_id=&amp;quot;controller&amp;rdquo; &amp;gt;}} 使您无需修改 Kubernetes 自身的代码，即可以扩展集群的行为。 Operator 是 Kubernetes API 的客户端，充当自定义资源的控制器。
Operator 示例 使用 Operator 可以自动化的事情包括：
 按需部署应用 获取/还原应用状态的备份 处理应用代码的升级以及相关改动。例如，数据库 schema 或额外的配置设置 发布一个 service，要求不支持 Kubernetes API 的应用也能发现它 模拟整个或部分集群中的故障以测试其稳定性 在没有内部成员选举程序的情况下，为分布式应用选择首领角色  想要更详细的了解 Operator？这儿有一个详细的示例：
 有一个名为 SampleDB 的自定义资源，您可以将其配置到集群中。 一个包含 Operator 控制器部分的 Deployment，用来确保 Pod 处于运行状态。 Operator 代码的容器镜像。 控制器代码，负责查询控制平面以找出已配置的 SampleDB 资源。 Operator 的核心是告诉 API 服务器，如何使现实与代码里配置的资源匹配。  如果添加新的 SampleDB，Operator 将设置 PersistentVolumeClaims 以提供持久化的数据库存储，设置 StatefulSet 以运行 SampleDB，并设置 Job 来处理初始配置。 如果您删除它，Operator 将建立快照，然后确保 StatefulSet 和 Volume 已被删除。   Operator 也可以管理常规数据库的备份。对于每个 SampleDB 资源，Operator 会确定何时创建（可以连接到数据库并进行备份的）Pod。这些 Pod 将依赖于 ConfigMap 和/或 具有数据库连接详细信息和凭据的 Secret。 由于 Operator 旨在为其管理的资源提供强大的自动化功能，因此它还需要一些额外的支持性代码。在这个示例中，代码将检查数据库是否正运行在旧版本上，如果是，则创建 Job 对象为您升级数据库。  部署 Operator 部署 Operator 最常见的方法是将自定义资源及其关联的控制器添加到您的集群中。跟运行容器化应用一样，Controller 通常会运行在 text=&amp;quot;控制平面&amp;rdquo; term_id=&amp;quot;control-plane&amp;rdquo; &amp;gt;}} 之外。例如，您可以在集群中将控制器作为 Deployment 运行。</description>
    </item>
    
    <item>
      <title>Storage Classes</title>
      <link>https://lijun.in/concepts/storage/storage-classes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/concepts/storage/storage-classes/</guid>
      <description>本文描述了 Kubernetes 中 StorageClass 的概念。建议先熟悉 卷 和 持久卷 的概念。
介绍 StorageClass 为管理员提供了描述存储 &amp;ldquo;类&amp;rdquo; 的方法。 不同的类型可能会映射到不同的服务质量等级或备份策略，或是由集群管理员制定的任意策略。 Kubernetes 本身并不清楚各种类代表的什么。这个类的概念在其他存储系统中有时被称为 &amp;ldquo;配置文件&amp;rdquo;。
StorageClass 资源 每个 StorageClass 都包含 provisioner、parameters 和 reclaimPolicy 字段， 这些字段会在 StorageClass 需要动态分配 PersistentVolume 时会使用到。
StorageClass 对象的命名很重要，用户使用这个命名来请求生成一个特定的类。 当创建 StorageClass 对象时，管理员设置 StorageClass 对象的命名和其他参数，一旦创建了对象就不能再对其更新。
管理员可以为没有申请绑定到特定 StorageClass 的 PVC 指定一个默认的存储类 ： 更多详情请参阅 PersistentVolumeClaim 章节。
apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: standard provisioner: kubernetes.io/aws-ebs parameters: type: gp2 reclaimPolicy: Retain allowVolumeExpansion: true mountOptions: - debug volumeBindingMode: Immediate 存储分配器 每个 StorageClass 都有一个分配器，用来决定使用哪个卷插件分配 PV。该字段必须指定。</description>
    </item>
    
    <item>
      <title>云控制器管理器的基础概念</title>
      <link>https://lijun.in/concepts/architecture/cloud-controller/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/concepts/architecture/cloud-controller/</guid>
      <description>云控制器管理器（cloud controller manager，CCM）这个概念 （不要与二进制文件混淆）创建的初衷是为了让特定的云服务供应商代码和 Kubernetes 核心相互独立演化。云控制器管理器与其他主要组件（如 Kubernetes 控制器管理器，API 服务器和调度程序）一起运行。它也可以作为 Kubernetes 的插件启动，在这种情况下，它会运行在 Kubernetes 之上。
云控制器管理器基于插件机制设计，允许新的云服务供应商通过插件轻松地与 Kubernetes 集成。目前已经有在 Kubernetes 上加入新的云服务供应商计划，并为云服务供应商提供从原先的旧模式迁移到新 CCM 模式的方案。
本文讨论了云控制器管理器背后的概念，并提供了相关功能的详细信息。
这是没有云控制器管理器的 Kubernetes 集群的架构：
设计 在上图中，Kubernetes 和云服务供应商通过几个不同的组件进行了集成，分别是：
 Kubelet Kubernetes 控制管理器 Kubernetes API 服务器  CCM 整合了前三个组件中的所有依赖于云的逻辑，以创建与云的单一集成点。CCM 的新架构如下所示：
CCM 的组成部分 CCM 打破了 Kubernetes 控制器管理器（KCM）的一些功能，并将其作为一个单独的进程运行。具体来说，它打破了 KCM 中依赖于云的控制器。KCM 具有以下依赖于云的控制器：
 节点控制器 卷控制器 路由控制器 服务控制器  在 1.9 版本中，CCM 运行前述列表中的以下控制器：
 节点控制器 路由控制器 服务控制器  注意卷控制器不属于 CCM，由于其中涉及到的复杂性和对现有供应商特定卷的逻辑抽象，因此决定了卷控制器不会被移动到 CCM 之中。
使用 CCM 支持 volume 的最初计划是使用 Flex volume 来支持可插拔卷，但是现在正在计划一项名为 CSI 的项目以取代 Flex。</description>
    </item>
    
    <item>
      <title>云驱动</title>
      <link>https://lijun.in/concepts/cluster-administration/cloud-providers/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/concepts/cluster-administration/cloud-providers/</guid>
      <description>本文介绍了如何管理运行在特定云驱动上的 Kubernetes 集群。
kubeadm kubeadm 是创建 kubernetes 集群的一种流行选择。 kubeadm 通过提供配置选项来指定云驱动的配置信息。例如，一个典型的适用于“树内”云驱动的 kubeadm 配置如下：
apiVersion: kubeadm.k8s.io/v1beta2 kind: InitConfiguration nodeRegistration: kubeletExtraArgs: cloud-provider: &amp;#34;openstack&amp;#34; cloud-config: &amp;#34;/etc/kubernetes/cloud.conf&amp;#34; --- apiVersion: kubeadm.k8s.io/v1beta2 kind: ClusterConfiguration kubernetesVersion: v1.13.0 apiServer: extraArgs: cloud-provider: &amp;#34;openstack&amp;#34; cloud-config: &amp;#34;/etc/kubernetes/cloud.conf&amp;#34; extraVolumes: - name: cloud hostPath: &amp;#34;/etc/kubernetes/cloud.conf&amp;#34; mountPath: &amp;#34;/etc/kubernetes/cloud.conf&amp;#34; controllerManager: extraArgs: cloud-provider: &amp;#34;openstack&amp;#34; cloud-config: &amp;#34;/etc/kubernetes/cloud.conf&amp;#34; extraVolumes: - name: cloud hostPath: &amp;#34;/etc/kubernetes/cloud.conf&amp;#34; mountPath: &amp;#34;/etc/kubernetes/cloud.conf&amp;#34; “树内”的云驱动通常需要在命令行中为 kube-apiserver、kube-controller-manager 和 kubelet 指定 --cloud-provider 和 --cloud-config。在 --cloud-config 中为每个供应商指定的文件的内容也同样需要写在下面。 对于所有外部云驱动，请遵循独立云存储库的说明，或浏览所有版本库清单
AWS 本节介绍在 Amazon Web Services 上运行 Kubernetes 时可以使用的所有配置。 如果希望使用此外部云驱动，其代码库位于 kubernetes/cloud-provider-aws</description>
    </item>
    
    <item>
      <title>使用 Kube-router 作为 NetworkPolicy</title>
      <link>https://lijun.in/tasks/administer-cluster/network-policy-provider/kube-router-network-policy/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/administer-cluster/network-policy-provider/kube-router-network-policy/</guid>
      <description>本页展示了如何使用 Kube-router 作为 NetworkPolicy。
. heading &amp;ldquo;prerequisites&amp;rdquo; %}} 您需要拥有一个正在运行的 Kubernetes 集群。如果您还没有集群，可以使用任意的集群安装器如 Kops，Bootkube，Kubeadm 等创建一个。
安装 Kube-router 插件 Kube-router 插件自带一个Network Policy 控制器，监视来自于Kubernetes API server 的 NetworkPolicy 和 pods 的变化，根据策略指示配置 iptables 规则和 ipsets 来允许或阻止流量。请根据 尝试通过集群安装器使用 Kube-router 指南安装 Kube-router 插件。
. heading &amp;ldquo;whatsnext&amp;rdquo; %}} 在您安装 Kube-router 插件后，可以根据 声明 Network Policy 去尝试使用 Kubernetes NetworkPolicy。</description>
    </item>
    
    <item>
      <title>使用准入控制器</title>
      <link>https://lijun.in/reference/access-authn-authz/admission-controllers/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/access-authn-authz/admission-controllers/</guid>
      <description>此页面概述了准入控制器。
什么是准入控制插件？ 准入控制器是一段代码，它会在请求通过认证和授权之后、对象被持久化之前拦截到达 API 服务器的请求。控制器由下面的列表组成，并编译进 kube-apiserver 二进制文件，并且只能由集群管理员配置。在该列表中，有两个特殊的控制器：MutatingAdmissionWebhook 和 ValidatingAdmissionWebhook。它们根据 API 中的配置，分别执行变更和验证准入控制 webhook。
准入控制器可以执行 “验证” 和/或 “变更” 操作。变更（mutating）控制器可以修改被其接受的对象；验证（validating）控制器则不行。
准入控制过程分为两个阶段。第一阶段，运行变更准入控制器。第二阶段，运行验证准入控制器。 再次提醒，某些控制器既是变更准入控制器又是验证准入控制器。
如果任何一个阶段的任何控制器拒绝了该请求，则整个请求将立即被拒绝，并向终端用户返回一个错误。
最后，除了对对象进行变更外，准入控制器还可以有其它作用：将相关资源作为请求处理的一部分进行变更。 增加使用配额就是一个典型的示例，说明了这样做的必要性。 此类用法都需要相应的回收或回调过程，因为任一准入控制器都无法确定某个请能否通过所有其它准入控制器。
为什么需要准入控制器？ Kubernetes 的许多高级功能都要求启用一个准入控制器，以便正确地支持该特性。因此，没有正确配置准入控制器的 Kubernetes API 服务器是不完整的，它无法支持您期望的所有特性。
如何启用一个准入控制器？ Kubernetes API 服务器的 enable-admission-plugins 标志，它指定了一个用于在集群修改对象之前调用的（以逗号分隔的）准入控制插件顺序列表。
例如，下面的命令就启用了 NamespaceLifecycle 和 LimitRanger 准入控制插件：
kube-apiserver --enable-admission-plugins=NamespaceLifecycle,LimitRanger ... . note &amp;gt;}}
根据您 Kubernetes 集群的部署方式以及 API 服务器的启动方式的不同，您可能需要以不同的方式应用设置。 例如，如果将 API 服务器部署为 systemd 服务，你可能需要修改 systemd 单元文件； 如果以自托管方式部署 Kubernetes，你可能需要修改 API 服务器的清单文件。 . /note &amp;gt;}}
怎么关闭准入控制器？ Kubernetes API 服务器的 disable-admission-plugins 标志，会将传入的（以逗号分隔的）准入控制插件列表禁用，即使是默认启用的插件也会被禁用。</description>
    </item>
    
    <item>
      <title>使用工作队列进行粗粒度并行处理</title>
      <link>https://lijun.in/tasks/job/coarse-parallel-processing-work-queue/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/job/coarse-parallel-processing-work-queue/</guid>
      <description>本例中，我们会运行包含多个并行工作进程的 Kubernetes Job。
本例中，每个 Pod 一旦被创建，会立即从任务队列中取走一个工作单元并完成它，然后将工作单元从队列中删除后再退出。
下面是本次示例的主要步骤：
  启动一个消息队列服务 本例中，我们使用 RabbitMQ，你也可以用其他的消息队列服务。在实际工作环境中，你可以创建一次消息队列服务然后在多个任务中重复使用。
  创建一个队列，放上消息数据 每个消息表示一个要执行的任务。本例中，每个消息是一个整数值。我们将基于这个整数值执行很长的计算操作。
  启动一个在队列中执行这些任务的 Job。该 Job 启动多个 Pod。每个 Pod 从消息队列中取走一个任务，处理它，然后重复执行，直到队列的队尾。
  . heading &amp;ldquo;prerequisites&amp;rdquo; %}} 要熟悉 Job 基本用法（非并行的），请参考 Job。
. include &amp;ldquo;task-tutorial-prereqs.md&amp;rdquo; &amp;gt;}} . version-check &amp;gt;}}
启动消息队列服务 本例使用了 RabbitMQ，使用其他 AMQP 类型的消息服务应该比较容易。
在实际工作中，在集群中一次性部署某个消息队列服务，之后在很多 Job 中复用，包括需要长期运行的服务。
按下面的方法启动 RabbitMQ：
$ kubectl create -f https://raw.githubusercontent.com/kubernetes/kubernetes/release-1.3/examples/celery-rabbitmq/rabbitmq-service.yaml service &amp;#34;rabbitmq-service&amp;#34; created $ kubectl create -f https://raw.githubusercontent.com/kubernetes/kubernetes/release-1.3/examples/celery-rabbitmq/rabbitmq-controller.yaml replicationcontroller &amp;#34;rabbitmq-controller&amp;#34; created 我们仅用到 celery-rabbitmq 示例 中描述的部分功能。</description>
    </item>
    
    <item>
      <title>卷快照类</title>
      <link>https://lijun.in/concepts/storage/volume-snapshot-classes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/concepts/storage/volume-snapshot-classes/</guid>
      <description>本文档描述了 Kubernetes 中 VolumeSnapshotClass 的概念。 建议熟悉卷快照（Volume Snapshots）和存储类（Storage Class）。
介绍 就像 StorageClass 为管理员提供了一种在配置卷时描述存储“类”的方法，VolumeSnapshotClass 提供了一种在配置卷快照时描述存储“类”的方法。
VolumeSnapshotClass 资源 每个 VolumeSnapshotClass 都包含 driver 、deletionPolicy 和 parameters 字段，当需要动态配置属于该类的 VolumeSnapshot 时使用。
VolumeSnapshotClass 对象的名称很重要，是用户可以请求特定类的方式。 管理员在首次创建 VolumeSnapshotClass 对象时设置类的名称和其他参数，对象一旦创建就无法更新。
管理员可以为不请求任何特定类绑定的 VolumeSnapshots 指定默认的 VolumeSnapshotClass。
apiVersion: snapshot.storage.k8s.io/v1beta1 kind: VolumeSnapshotClass metadata: name: csi-hostpath-snapclass driver: hostpath.csi.k8s.io deletionPolicy: Delete parameters: 驱动程序 卷快照类有一个驱动程序，用于确定配置 VolumeSnapshot 的 CSI 卷插件。 必须指定此字段。
删除策略 卷快照类具有 deletionPolicy 属性。用户可以配置当所绑定的 VolumeSnapshot 对象将被删除时，如何处理 VolumeSnapshotContent 对象。卷快照的这个策略可以是 Retain 或者 Delete。这个策略字段必须指定。
如果删除策略是 Delete，那么底层的存储快照会和 VolumeSnapshotContent 对象一起删除。如果删除策略是 Retain，那么底层快照和 VolumeSnapshotContent 对象都会被保留。</description>
    </item>
    
    <item>
      <title>命名空间</title>
      <link>https://lijun.in/concepts/overview/working-with-objects/namespaces/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/concepts/overview/working-with-objects/namespaces/</guid>
      <description>Kubernetes 支持多个虚拟集群，它们底层依赖于同一个物理集群。 这些虚拟集群被称为命名空间。
何时使用多个命名空间 命名空间适用于存在很多跨多个团队或项目的用户的场景。对于只有几到几十个用户的集群，根本不需要创建或考虑命名空间。当需要名称空间提供的功能时，请开始使用它们。
命名空间为名称提供了一个范围。资源的名称需要在命名空间内是唯一的，但不能跨命名空间。命名空间不能相互嵌套，每个 Kubernetes 资源只能在一个命名空间中。
命名空间是在多个用户之间划分集群资源的一种方法（通过资源配额）。
在 Kubernetes 未来版本中，相同命名空间中的对象默认将具有相同的访问控制策略。
不需要使用多个命名空间来分隔轻微不同的资源，例如同一软件的不同版本：使用 labels 来区分同一命名空间中的不同资源。
使用命名空间 命名空间的创建和删除已在命名空间的管理指南文档中进行了描述。
查看命名空间 您可以使用以下命令列出集群中现存的命名空间：
kubectl get namespace NAME STATUS AGE default Active 1d kube-system Active 1d kube-public Active 1d Kubernetes 会创建三个初始命名空间：
 default 没有指明使用其它命名空间的对象所使用的默认命名空间  kube-system The namespace for objects created by the Kubernetes system &amp;ndash;&amp;gt; kube-system Kubernetes 系统创建对象所使用的命名空间  kube-public This namespace is created automatically and is readable by all users (including those not authenticated).</description>
    </item>
    
    <item>
      <title>客户端库</title>
      <link>https://lijun.in/reference/using-api/client-libraries/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/using-api/client-libraries/</guid>
      <description>本页面包含基于各种编程语言使用 Kubernetes API 的客户端库概述。
在使用 Kubernetes REST API 编写应用程序时， 您并不需要自己实现 API 调用和 “请求/响应” 类型。 您可以根据自己的编程语言需要选择使用合适的客户端库。
客户端库通常为您处理诸如身份验证之类的常见任务。 如果 API 客户端在 Kubernetes 集群中运行，大多数客户端库可以发现并使用 Kubernetes 服务帐户进行身份验证， 或者能够理解 kubeconfig 文件 格式来读取凭据和 API 服务器地址。
官方支持的 Kubernetes 客户端库 以下客户端库由 Kubernetes SIG API Machinery 正式维护。
   语言 客户端库 样例程序     Go github.com/kubernetes/client-go/ 浏览   Python github.com/kubernetes-client/python/ 浏览   Java github.com/kubernetes-client/java 浏览   dotnet github.com/kubernetes-client/csharp 浏览   JavaScript github.com/kubernetes-client/javascript 浏览   Haskell github.</description>
    </item>
    
    <item>
      <title>容器生命周期钩子</title>
      <link>https://lijun.in/concepts/containers/container-lifecycle-hooks/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/concepts/containers/container-lifecycle-hooks/</guid>
      <description>这个页面描述了 kubelet 管理的容器如何使用容器生命周期钩子框架来运行在其管理生命周期中由事件触发的代码。
概述 类似于许多具有生命周期钩子组件的编程语言框架，例如 Angular、Kubernetes 为容器提供了生命周期钩子。 钩子使容器能够了解其管理生命周期中的事件，并在执行相应的生命周期钩子时运行在处理程序中实现的代码。
容器钩子 有两个钩子暴露在容器中:
PostStart
这个钩子在创建容器之后立即执行。 但是，不能保证钩子会在容器入口点之前执行。 没有参数传递给处理程序。
PreStop
在容器终止之前是否立即调用此钩子，取决于 API 的请求或者管理事件，类似活动探针故障、资源抢占、资源竞争等等。 如果容器已经完全处于终止或者完成状态，则对 preStop 钩子的调用将失败。 它是阻塞的，同时也是同步的，因此它必须在删除容器的调用之前完成。 没有参数传递给处理程序。
有关终止行为的更详细描述，请参见终止 Pod。
钩子处理程序的实现 容器可以通过实现和注册该钩子的处理程序来访问该钩子。 针对容器，有两种类型的钩子处理程序可供实现：
 Exec - 执行一个特定的命令，例如 pre-stop.sh，在容器的 cgroups 和名称空间中。 命令所消耗的资源根据容器进行计算。 HTTP - 对容器上的特定端点执行 HTTP 请求。  钩子处理程序执行 当调用容器生命周期管理钩子时，Kubernetes 管理系统在为该钩子注册的容器中执行处理程序。
钩子处理程序调用在包含容器的 Pod 上下文中是同步的。 这意味着对于 PostStart 钩子，容器入口点和钩子异步触发。 但是，如果钩子运行或挂起的时间太长，则容器无法达到 running 状态。
行为与 PreStop 钩子的行为类似。 如果钩子在执行过程中挂起，Pod 阶段将保持在 Terminating 状态，并在 Pod 结束的 terminationGracePeriodSeconds 之后被杀死。 如果 PostStart 或 PreStop 钩子失败，它会杀死容器。</description>
    </item>
    
    <item>
      <title>应用连接到 Service</title>
      <link>https://lijun.in/concepts/services-networking/connect-applications-service/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/concepts/services-networking/connect-applications-service/</guid>
      <description>Kubernetes 连接容器模型 既然有了一个持续运行、可复制的应用，我们就能够将它暴露到网络上。 在讨论 Kubernetes 网络连接的方式之前，非常值得与 Docker 中 “正常” 方式的网络进行对比。
默认情况下，Docker 使用私有主机网络连接，只能与同在一台机器上的容器进行通信。 为了实现容器的跨节点通信，必须在机器自己的 IP 上为这些容器分配端口，为容器进行端口转发或者代理。
多个开发人员之间协调端口的使用很难做到规模化，那些难以控制的集群级别的问题，都会交由用户自己去处理。 Kubernetes 假设 Pod 可与其它 Pod 通信，不管它们在哪个主机上。 我们给 Pod 分配属于自己的集群私有 IP 地址，所以没必要在 Pod 或映射到的容器的端口和主机端口之间显式地创建连接。 这表明了在 Pod 内的容器都能够连接到本地的每个端口，集群中的所有 Pod 不需要通过 NAT 转换就能够互相看到。 文档的剩余部分将详述如何在一个网络模型之上运行可靠的服务。
该指南使用一个简单的 Nginx server 来演示并证明谈到的概念。同样的原则也体现在一个更加完整的 Jenkins CI 应用 中。
在集群中暴露 Pod 我们在之前的示例中已经做过，然而再让我重试一次，这次聚焦在网络连接的视角。 创建一个 Nginx Pod，指示它具有一个容器端口的说明：
codenew file=&amp;quot;service/networking/run-my-nginx.yaml&amp;rdquo; &amp;gt;}}
这使得可以从集群中任何一个节点来访问它。检查节点，该 Pod 正在运行：
kubectl apply -f ./run-my-nginx.yaml kubectl get pods -l run=my-nginx -o wide NAME READY STATUS RESTARTS AGE IP NODE my-nginx-3800858182-jr4a2 1/1 Running 0 13s 10.</description>
    </item>
    
    <item>
      <title>控制器</title>
      <link>https://lijun.in/concepts/architecture/controller/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/concepts/architecture/controller/</guid>
      <description>在机器人技术和自动化中，控制环是一个控制系统状态的不终止的循环。
这是一个控制环的例子：房间里的温度自动调节器。
当你设置了温度，告诉了温度自动调节器你的期望状态。房间的实际温度是当前状态。通过对设备的开关控制，温度自动调节器让其当前状态接近期望状态。
控制器模式 一个控制器至少追踪一种类型的 Kubernetes 资源。这些对象有一个代表期望状态的指定字段。控制器负责确保其追踪的资源对象的当前状态接近期望状态。
控制器可能会自行执行操作；在 Kubernetes 中更常见的是一个控制器会发送信息给 API 服务器，这会有副作用。看下面这个例子。
一些内置的控制器，比如命名空间控制器，针对没有指定命名空间的对象。为了简单起见，这篇文章没有详细介绍这些细节。
通过 API 服务器来控制 控制器是一个 Kubernetes 内置控制器的例子。内置控制器通过和集群 API 服务器交互来管理状态。
Job 是一种 Kubernetes 资源，它运行一个 ，或者可能是多个 Pod，来执行一个任务然后停止。
（一旦被调度了），对 kubelet 来说 Pod 对象就会变成了期望状态的一部分。
在集群中，当 Job 控制器拿到新任务时，它会保证一组 Node 节点上的 kubelet 可以运行正确数量的 Pod 来完成工作。 Job 控制器不会自己运行任何的 Pod 或者容器。Job 控制器是通知 API 服务器来创建或者移除 Pod。 control-plane中的其它组件根据新的消息而反应（调度新的 Pod 并且运行它）并且最终完成工作。
创建新 Job 后，所期望的状态就是完成这个 Job。Job 控制器会让 Job 的当前状态不断接近期望状态：创建为 Job 要完成工作所需要的 Pod，使 Job 的状态接近完成。
控制器也会更新配置对象。例如：一旦 Job 的工作完成了，Job 控制器会更新 Job 对象的状态为 Finished。</description>
    </item>
    
    <item>
      <title>校验节点设置</title>
      <link>https://lijun.in/setup/best-practices/node-conformance/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/setup/best-practices/node-conformance/</guid>
      <description>. toc &amp;gt;}}
节点一致性测试 节点一致性测试 是一个容器化的测试框架，提供了针对节点的系统验证和功能测试。
该测试主要检测节点是否满足 Kubernetes 的最低要求，通过检测的节点有资格加入 Kubernetes 集群。
限制 在 Kubernetes 1.5 版中，节点一致性测试具有以下限制：
 节点一致性测试只支持 Docker 作为容器运行时环境。  节点的前提条件 要运行节点一致性测试，节点必须满足与标准 Kubernetes 节点相同的前提条件。节点至少应安装以下守护程序：
 容器运行时 (Docker) Kubelet  运行节点一致性测试 要运行节点一致性测试，请执行以下步骤：
 因为测试框架会启动一个本地的 master 来测试 Kubelet，所以将 Kubelet 指向本机 `&amp;ndash;api-servers=&amp;quot;http://localhost:8080&amp;rdquo;。 还有一些其他 Kubelet 参数可能需要注意：   --pod-cidr： 如果使用 kubenet， 需要为 Kubelet 任意指定一个 CIDR， 例如 --pod-cidr=10.180.0.0/24。 --cloud-provider： 如果使用 --cloud-provider=gce，需要移除这个参数来运行测试。  使用以下命令运行节点一致性测试：  # $CONFIG_DIR is the pod manifest path of your Kubelet. # $LOG_DIR is the test output path.</description>
    </item>
    
    <item>
      <title>用户自定义资源版本</title>
      <link>https://lijun.in/tasks/access-kubernetes-api/custom-resources/custom-resource-definition-versioning/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/access-kubernetes-api/custom-resources/custom-resource-definition-versioning/</guid>
      <description>本页介绍了如何添加版本信息到 [CustomResourceDefinitions](/docs/reference/generated/kubernetes-api/. param &amp;ldquo;version&amp;rdquo; &amp;gt;}}/#customresourcedefinition-v1beta1-apiextensions)，如何表示 CustomResourceDefinitions 的稳定水平或者用 API 之间的表征的转换提高您的 API 到一个新的版本。它还描述了如何将对象从一个版本升级到另一个版本。
. heading &amp;ldquo;prerequisites&amp;rdquo; %}} . include &amp;ldquo;task-tutorial-prereqs.md&amp;rdquo; &amp;gt;}} . version-check &amp;gt;}}
 确保您的 Kubernetes 集群的主版本为apiextensions.k8s.io/v1的1.16.0或更高版本，apiextensions.k8s.io/v1beta1的1.11.0或更高版本。   阅读 custom resources。  概览 . feature-state state=&amp;quot;stable&amp;rdquo; for_kubernetes_version=&amp;quot;1.16&amp;rdquo; &amp;gt;}}
CustomResourceDefinition API 提供了用于引入和升级的工作流程到 CustomResourceDefinition 的新版本。
创建 CustomResourceDefinition 时，会在 CustomResourceDefinition spec.versions 列表设置适当的稳定级和版本号。例如v1beta1表示第一个版本尚未稳定。所有自定义资源对象将首先存储在这个版本
创建 CustomResourceDefinition 后，客户端可以开始使用 v1beta1 API。
稍后可能需要添加新版本，例如 v1。
增加一个新版本：
 选择一种转化策略。由于自定义资源对象需要能够两种版本都可用，这意味着它们有时会以与存储版本不同的版本。为了能够做到这一点， 有时必须在它们存储的版本和提供的版本。如果转换涉及结构变更，并且需要自定义逻辑，转换应该使用 webhook。如果没有结构变更， 则使用 None 默认转换策略，不同版本时只有apiVersion字段有变更。 如果使用转换 Webhook，请创建并部署转换 Webhook。希望看到更多详细信息，请参见 Webhook conversion。 更新 CustomResourceDefinition，来将新版本包含在具有served：true的 spec.</description>
    </item>
    
    <item>
      <title>运行一个有状态的应用程序</title>
      <link>https://lijun.in/tasks/run-application/run-replicated-stateful-application/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/run-application/run-replicated-stateful-application/</guid>
      <description>该页面显示如何使用StatefulSet 控制器去运行一个有状态的应用程序。此例是一主多从的 MySQL 集群。
请注意 这不是生产配置。 重点是， MySQL 设置保留在不安全的默认值上，使重点放在 Kubernetes 中运行有状态应用程序的常规模式。
. heading &amp;ldquo;prerequisites&amp;rdquo; %}}  . include &amp;ldquo;task-tutorial-prereqs.md&amp;rdquo; &amp;gt;}} . version-check &amp;gt;}} . include &amp;ldquo;default-storage-class-prereqs.md&amp;rdquo; &amp;gt;}} 本教程假定您熟悉 PersistentVolumes 与 StatefulSets, 以及其他核心概念，例如Pods, Services, 与 ConfigMaps. 熟悉 MySQL 会有所帮助，但是本教程旨在介绍对其他系统应该有用的常规模式。  . heading &amp;ldquo;objectives&amp;rdquo; %}}  使用 StatefulSet 控制器部署复制的 MySQL 拓扑。 发送 MySQL 客户端流量。 观察对宕机的抵抗力。 缩放 StatefulSet 的大小。  部署 MySQL 部署 MySQL 示例，包含一个 ConfigMap，两个 Services，与一个 StatefulSet。
ConfigMap 从以下的 YAML 配置文件创建 ConfigMap ：</description>
    </item>
    
    <item>
      <title>配置 Pod 的服务质量</title>
      <link>https://lijun.in/tasks/configure-pod-container/quality-service-pod/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/configure-pod-container/quality-service-pod/</guid>
      <description>本文介绍怎样配置 Pod 让其获得特定的服务质量（QoS）类。Kubernetes 使用 QoS 类来决定 Pod 的调度和驱逐策略。
. heading &amp;ldquo;prerequisites&amp;rdquo; %}} . include &amp;ldquo;task-tutorial-prereqs.md&amp;rdquo; &amp;gt;}} . version-check &amp;gt;}}
QoS 类 Kubernetes 创建 Pod 时就给它指定了下列一种 QoS 类：
 Guaranteed Burstable BestEffort  创建命名空间 创建一个命名空间，以便将本练习所创建的资源与集群的其余资源相隔离。
kubectl create namespace qos-example 创建一个 QoS 类为 Guaranteed 的 Pod 对于 QoS 类为 Guaranteed 的 Pod：
 Pod 中的每个容器必须指定内存请求和内存限制，并且两者要相等。 Pod 中的每个容器必须指定 CPU 请求和 CPU 限制，并且两者要相等。  下面是包含一个容器的 Pod 配置文件。 容器设置了内存请求和内存限制，值都是 200 MiB。 容器设置了 CPU 请求和 CPU 限制，值都是 700 milliCPU：</description>
    </item>
    
    <item>
      <title>配置命名空间的最小和最大内存约束</title>
      <link>https://lijun.in/tasks/administer-cluster/manage-resources/memory-constraint-namespace/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/administer-cluster/manage-resources/memory-constraint-namespace/</guid>
      <description>此页面介绍如何设置在命名空间中运行的容器使用的内存的最小值和最大值。 您可以在 [LimitRange](/docs/reference/generated/kubernetes-api/. param &amp;ldquo;version&amp;rdquo; &amp;gt;}}/#limitrange-v1-core)对象中指定最小和最大内存值。 如果 Pod 不满足 LimitRange 施加的约束，则无法在命名空间中创建它。
. heading &amp;ldquo;prerequisites&amp;rdquo; %}} . include &amp;ldquo;task-tutorial-prereqs.md&amp;rdquo; &amp;gt;}} . version-check &amp;gt;}}
集群中每个节点必须至少要有1 GiB 的内存。
创建命名空间 创建一个命名空间，以便在此练习中创建的资源与群集的其余资源隔离。
kubectl create namespace constraints-mem-example 创建 LimitRange 和 Pod 下面是 LimitRange 的配置文件：
. codenew file=&amp;quot;admin/resource/memory-constraints.yaml&amp;rdquo; &amp;gt;}}
创建 LimitRange:
kubectl create -f https://k8s.io/examples/admin/resource/memory-constraints.yaml --namespace=constraints-mem-example 查看 LimitRange 的详情：
kubectl get limitrange mem-min-max-demo-lr --namespace=constraints-mem-example --output=yaml 输出显示预期的最小和最大内存约束。 但请注意，即使您没有在 LimitRange 的配置文件中指定默认值，也会自动创建它们。
 limits: - default: memory: 1Gi defaultRequest: memory: 1Gi max: memory: 1Gi min: memory: 500Mi type: Container 现在，只要在 constraints-mem-example 命名空间中创建容器，Kubernetes 就会执行下面的步骤：</description>
    </item>
    
    <item>
      <title>💖 - 使用 kubeadm 创建一个单主集群</title>
      <link>https://lijun.in/setup/independent/create-cluster-kubeadm/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/setup/independent/create-cluster-kubeadm/</guid>
      <description>kubeadm 能帮助您建立一个小型的符合最佳实践的 Kubernetes 集群。通过使用 kubeadm, 您的集群会符合 Kubernetes 合规性测试的要求. Kubeadm 也支持其他的集群生命周期操作，比如升级、降级和管理启动引导令牌。
因为您可以在不同类型的机器（比如笔记本、服务器和树莓派等）上安装 kubeadm，因此它非常适合与 Terraform 或 Ansible 这类自动化管理系统集成。
kubeadm 的简单便捷为大家带来了广泛的用户案例：
 新用户可以从 kubeadm 开始来试用 Kubernetes。 熟悉 Kubernetes 的用户可以使用 kubeadm 快速搭建集群并测试他们的应用。 大型的项目可以将 kubeadm 和其他的安装工具一起形成一个比较复杂的系统。  kubeadm 的设计初衷是为新用户提供一种便捷的方式来首次试用 Kubernetes， 同时也方便老用户搭建集群测试他们的应用。 此外 kubeadm 也可以跟其它生态系统与/或安装工具集成到一起，提供更强大的功能。
您可以很方便地在支持 rpm 或 deb 软件包的操作系统上安装 kubeadm。对应 kubeadm 的 SIG， SIG Cluster Lifecycle， 提供了预编译的这类安装包，当然您也可以自己基于源码为其它操作系统来构造安装包。
kubeadm 成熟程度    功能 成熟程度     命令行用户体验 beta   功能实现 beta   配置文件 API alpha   自托管 alpha   kubeadm alpha 子命令 alpha   CoreDNS GA   动态 Kubelet 配置 alpha    kubeadm 的整体功能目前还是 Beta 状态，然而很快在 2018 年就会转换成正式发布 (GA) 状态。 一些子功能，比如自托管或者配置文件 API 还在开发过程当中。 随着工具的发展，创建集群的方法可能会有所变化，但是整体部署方案还是比较稳定的。 在 kubeadm alpha 下面的任何命令都只是 alpha 状态，目前只提供初期阶段的服务。</description>
    </item>
    
    <item>
      <title>Ingress</title>
      <link>https://lijun.in/concepts/services-networking/ingress/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/concepts/services-networking/ingress/</guid>
      <description>feature-state for_k8s_version=&amp;quot;v1.1&amp;rdquo; state=&amp;quot;beta&amp;rdquo; &amp;gt;}} glossary_definition term_id=&amp;quot;ingress&amp;rdquo; length=&amp;quot;all&amp;rdquo; &amp;gt;}}
 节点（Node）: Kubernetes 集群中其中一台工作机器，是集群的一部分。 集群（Cluster）: 一组运行程序（这些程序是容器化的，被 Kubernetes 管理的）的节点。 在此示例中，和在大多数常见的Kubernetes部署方案，集群中的节点都不会是公共网络。 边缘路由器（Edge router）: 在集群中强制性执行防火墙策略的路由器（router）。可以是由云提供商管理的网关，也可以是物理硬件。 集群网络（Cluster network）: 一组逻辑或物理的链接，根据 Kubernetes 网络模型 在集群内实现通信。 服务（Service）：Kubernetes Service标签选择器（selectors）标识的一组 Pod。除非另有说明，否则假定服务只具有在集群网络中可路由的虚拟 IP。  可以将 Ingress 配置为提供服务外部可访问的 URL、负载均衡流量、终止 SSL / TLS，以及提供基于名称的虚拟主机。Ingress 控制器 通常负责通过负载均衡器来实现 Ingress，尽管它也可以配置边缘路由器或其他前端来帮助处理流量。
Ingress 不会公开任意端口或协议。 将 HTTP 和 HTTPS 以外的服务公开到 Internet 时，通常使用 Service.Type=NodePort 或者 Service.Type=LoadBalancer 类型的服务。
环境准备 您必须具有 ingress 控制器 才能满足 Ingress 的要求。仅创建 Ingress 资源无效。
您可能需要部署 Ingress 控制器，例如 ingress-nginx。您可以从许多Ingress 控制器 中进行选择。
理想情况下，所有 Ingress 控制器都应符合参考规范。但实际上，不同的 Ingress 控制器操作略有不同。</description>
    </item>
    
    <item>
      <title>Ingress 控制器</title>
      <link>https://lijun.in/concepts/services-networking/ingress-controllers/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/concepts/services-networking/ingress-controllers/</guid>
      <description>为了让 Ingress 资源工作，集群必须有一个正在运行的 Ingress 控制器。
与作为 kube-controller-manager 可执行文件的一部分运行的其他类型的控制器不同，Ingress 控制器不是随集群自动启动的。 基于此页面，您可选择最适合您的集群的 ingress 控制器实现。
Kubernetes 作为一个项目，目前支持和维护 GCE 和 nginx 控制器。
其他控制器  [AKS 应用程序网关 Ingress 控制器]使用 Azure 应用程序网关启用AKS 集群 ingress。 Ambassador API 网关， 一个基于 Envoy 的 ingress 控制器，有着来自社区 的支持和来自 Datawire 的商业 支持。 AppsCode Inc. 为最广泛使用的基于 HAProxy 的 ingress 控制器 Voyager 提供支持和维护。 AWS ALB Ingress 控制器通过 AWS 应用 Load Balancer 启用 ingress。 Contour 是一个基于 Envoy 的 ingress 控制器，它由 VMware 提供和支持。 Citrix 为其硬件（MPX），虚拟化（VPX）和 免费容器化 (CPX) ADC 提供了一个 Ingress 控制器，用于裸金属和云部署。 F5 Networks 为 用于 Kubernetes 的 F5 BIG-IP 控制器提供支持和维护。 Gloo 是一个开源的基于 Envoy 的 ingress 控制器，它提供了 API 网关功能，有着来自 solo.</description>
    </item>
    
    <item>
      <title>kubeadm upgrade</title>
      <link>https://lijun.in/reference/setup-tools/kubeadm/kubeadm-upgrade/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/setup-tools/kubeadm/kubeadm-upgrade/</guid>
      <description>kubeadm upgrade 是一个对用户友好的命令，它将复杂的升级逻辑包装在一个命令后面，支持升级的规划和实际执行。
kubeadm 升级指南 本文档概述了使用 kubeadm 执行升级的步骤。 有关 kubeadm 旧版本，请参阅 Kubernetes 网站的旧版文档。
您可以使用 kubeadm upgrade diff 来查看将应用于静态 pod 清单的更改。
要在 Kubernetes v1.13.0 及更高版本中使用 kube-dns 进行升级，请遵循本指南。
在 Kubernetes v1.15.0 和更高版本中，kubeadm upgrade apply 和 kubeadm upgrade node 也将自动续订该节点上的 kubeadm 托管证书，包括存储在 kubeconfig 文件中的证书。 要选择退出，可以传递参数 --certificate-renewal=false。有关证书续订的更多详细信息请参见证书管理文档。
kubeadm upgrade plan . include &amp;ldquo;generated/kubeadm_upgrade_plan.md&amp;rdquo; &amp;gt;}}
kubeadm upgrade apply  . include &amp;ldquo;generated/kubeadm_upgrade_apply.md&amp;rdquo; &amp;gt;}}
kubeadm upgrade diff . include &amp;ldquo;generated/kubeadm_upgrade_diff.md&amp;rdquo; &amp;gt;}}
kubeadm upgrade node . include &amp;ldquo;generated/kubeadm_upgrade_node.</description>
    </item>
    
    <item>
      <title>PKI 证书和要求</title>
      <link>https://lijun.in/setup/best-practices/certificates/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/setup/best-practices/certificates/</guid>
      <description>Kubernetes 需要 PKI 证书才能进行基于 TLS 的身份验证。如果您是使用 kubeadm 安装的 Kubernetes，则会自动生成集群所需的证书。您还可以生成自己的证书。例如，不将私钥存储在 API 服务器上，可以让私钥更加安全。此页面说明了集群必需的证书。
集群是如何使用证书的 Kubernetes 需要 PKI 才能执行以下操作：
 Kubelet 的客户端证书，用于 API 服务器身份验证 API 服务器端点的证书 集群管理员的客户端证书，用于 API 服务器身份认证 API 服务器的客户端证书，用于和 Kubelet 的会话 API 服务器的客户端证书，用于和 etcd 的会话 控制器管理器的客户端证书/kubeconfig，用于和 API server 的会话 调度器的客户端证书/kubeconfig，用于和 API server 的会话 前端代理 的客户端及服务端证书  . note &amp;gt;}}
只有当您运行 kube-proxy 并要支持扩展 API 服务器时，才需要 front-proxy 证书 . /note &amp;gt;}}
etcd 还实现了双向 TLS 来对客户端和对其他对等节点进行身份验证。
证书存放的位置 如果你是通过 kubeadm 安装的 Kubernetes，所有证书都存放在 /etc/kubernetes/pki 目录下。本文所有相关的路径都是基于该路径的相对路径。
手动配置证书 如果你不想通过 kubeadm 生成这些必需的证书，你可以通过下面两种方式之一来手动创建他们。</description>
    </item>
    
    <item>
      <title>StatefulSets</title>
      <link>https://lijun.in/concepts/workloads/controllers/statefulset/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/concepts/workloads/controllers/statefulset/</guid>
      <description>StatefulSet 是用来管理有状态应用的工作负载 API 对象。
glossary_definition term_id=&amp;quot;statefulset&amp;rdquo; length=&amp;quot;all&amp;rdquo; &amp;gt;}}
使用 StatefulSets StatefulSets 对于需要满足以下一个或多个需求的应用程序很有价值：
 稳定的、唯一的网络标识符。 稳定的、持久的存储。 有序的、优雅的部署和缩放。 有序的、自动的滚动更新。  在上面，稳定意味着 Pod 调度或重调度的整个过程是有持久性的。如果应用程序不需要任何稳定的标识符或有序的部署、删除或伸缩，则应该使用由一组无状态的副本控制器提供的工作负载来部署应用程序，比如 Deployment 或者 ReplicaSet 可能更适用于您的无状态应用部署需要。
限制  给定 Pod 的存储必须由 [PersistentVolume 驱动](https://github.com/kubernetes/examples/tree/&amp;lt; param &amp;ldquo;githubbranch&amp;rdquo; &amp;gt;}}/staging/persistent-volume-provisioning/README.md) 基于所请求的 storage class 来提供，或者由管理员预先提供。 删除或者收缩 StatefulSet 并不会删除它关联的存储卷。这样做是为了保证数据安全，它通常比自动清除 StatefulSet 所有相关的资源更有价值。 StatefulSet 当前需要 headless 服务 来负责 Pod 的网络标识。您需要负责创建此服务。 当删除 StatefulSets 时，StatefulSet 不提供任何终止 Pod 的保证。为了实现 StatefulSet 中的 Pod 可以有序和优雅的终止，可以在删除之前将 StatefulSet 缩放为 0。 在默认 Pod 管理策略(OrderedReady) 时使用 滚动更新，可能进入需要 人工干预 才能修复的损坏状态。  组件 下面的示例演示了 StatefulSet 的组件。</description>
    </item>
    
    <item>
      <title>Taint 和 Toleration</title>
      <link>https://lijun.in/concepts/configuration/taint-and-toleration/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/concepts/configuration/taint-and-toleration/</guid>
      <description>节点亲和性（详见这里），是 pod 的一种属性（偏好或硬性要求），它使 pod 被吸引到一类特定的节点。Taint 则相反，它使 节点 能够 排斥 一类特定的 pod。
Taint 和 toleration 相互配合，可以用来避免 pod 被分配到不合适的节点上。每个节点上都可以应用一个或多个 taint ，这表示对于那些不能容忍这些 taint 的 pod，是不会被该节点接受的。如果将 toleration 应用于 pod 上，则表示这些 pod 可以（但不要求）被调度到具有匹配 taint 的节点上。
概念 您可以使用命令 kubectl taint 给节点增加一个 taint。比如，
kubectl taint nodes node1 key=value:NoSchedule 给节点 node1 增加一个 taint，它的 key 是 key，value 是 value，effect 是 NoSchedule。这表示只有拥有和这个 taint 相匹配的 toleration 的 pod 才能够被分配到 node1 这个节点。您可以在 PodSpec 中定义 pod 的 toleration。下面两个 toleration 均与上面例子中使用 kubectl taint 命令创建的 taint 相匹配，因此如果一个 pod 拥有其中的任何一个 toleration 都能够被分配到 node1 ：</description>
    </item>
    
    <item>
      <title>为命名空间配置CPU最小和最大限制</title>
      <link>https://lijun.in/tasks/administer-cluster/manage-resources/cpu-constraint-namespace/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/administer-cluster/manage-resources/cpu-constraint-namespace/</guid>
      <description>本章介绍命名空间中可以被容器和Pod使用的CPU资源的最小和最大值。你可以通过 [LimitRange](/docs/reference/generated/kubernetes-api/. param &amp;ldquo;version&amp;rdquo; &amp;gt;}}/#limitrange-v1-core) 对象声明 CPU 的最小和最大值. 如果 Pod 不能满足 LimitRange 的限制，它就不能在命名空间中创建。
. heading &amp;ldquo;prerequisites&amp;rdquo; %}} . include &amp;ldquo;task-tutorial-prereqs.md&amp;rdquo; &amp;gt;}} . version-check &amp;gt;}}
你的集群中每个节点至少要有1个CPU。
创建命名空间 创建一个命名空间，以便本练习中创建的资源和集群的其余资源相隔离。
kubectl create namespace constraints-cpu-example 创建 LimitRange 和 Pod 这里给出了 LimitRange 的配置文件：
. codenew file=&amp;quot;admin/resource/cpu-constraints.yaml&amp;rdquo; &amp;gt;}}
创建 LimitRange:
kubectl create -f https://k8s.io/examples/admin/resource/cpu-constraints.yaml --namespace=constraints-cpu-example 查看 LimitRange 详情：
kubectl get limitrange cpu-min-max-demo-lr --output=yaml --namespace=constraints-cpu-example 输出结果显示 CPU 的最小和最大限制符合预期。但需要注意的是，尽管你在 LimitRange 的配置文件中你没有声明默认值，默认值也会被自动创建。
limits: - default: cpu: 800m defaultRequest: cpu: 800m max: cpu: 800m min: cpu: 200m type: Container 现在不管什么时候在 constraints-cpu-example 命名空间中创建容器，Kubernetes 都会执行下面这些步骤：</description>
    </item>
    
    <item>
      <title>为容器分派扩展资源</title>
      <link>https://lijun.in/tasks/configure-pod-container/extended-resource/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/configure-pod-container/extended-resource/</guid>
      <description>. feature-state state=&amp;quot;stable&amp;rdquo; &amp;gt;}}
本文介绍如何为容器指定扩展资源。
. feature-state state=&amp;quot;stable&amp;rdquo; &amp;gt;}}
. heading &amp;ldquo;prerequisites&amp;rdquo; %}} . include &amp;ldquo;task-tutorial-prereqs.md&amp;rdquo; &amp;gt;}} . version-check &amp;gt;}}
在您开始此练习前，请先练习为节点广播扩展资源。 在那个练习中将配置您的一个节点来广播 dongle 资源。
给 Pod 分派扩展资源 要请求扩展资源，需要在您的容器清单中包括 resources:requests 字段。 扩展资源可以使用任何完全限定名称，只是不能使用 *.kubernetes.io/。 有效的扩展资源名的格式为 example.com/foo，其中 example.com 应被替换为您的组织的域名，而 foo 则是描述性的资源名称。
下面是包含一个容器的 Pod 配置文件：
. codenew file=&amp;quot;pods/resource/extended-resource-pod.yaml&amp;rdquo; &amp;gt;}}
在配置文件中，您可以看到容器请求了 3 个 dongles。
创建 Pod：
kubectl apply -f https://k8s.io/examples/pods/resource/extended-resource-pod.yaml 检查 Pod 是否运行正常：
kubectl get pod extended-resource-demo 描述 Pod:
kubectl describe pod extended-resource-demo 输出结果显示 dongle 请求如下：</description>
    </item>
    
    <item>
      <title>使用 HTTP 代理访问 Kubernetes API</title>
      <link>https://lijun.in/tasks/access-kubernetes-api/http-proxy-access-api/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/access-kubernetes-api/http-proxy-access-api/</guid>
      <description>本文说明如何使用 HTTP 代理访问 Kubernetes API。
. heading &amp;ldquo;prerequisites&amp;rdquo; %}}  . include &amp;ldquo;task-tutorial-prereqs.md&amp;rdquo; &amp;gt;}} . version-check &amp;gt;}}   如果您的集群中还没有任何应用，使用如下命令启动一个 Hello World 应用：  kubectl run node-hello --image=gcr.io/google-samples/node-hello:1.0 --port=8080 使用 kubectl 启动代理服务器 使用如下命令启动 Kubernetes API 服务器的代理：
kubectl proxy --port=8080  探究 Kubernetes API 当代理服务器在运行时，你可以通过 curl、wget 或者浏览器访问 API。
获取 API 版本：
curl http://localhost:8080/api/ { &amp;quot;kind&amp;quot;: &amp;quot;APIVersions&amp;quot;, &amp;quot;versions&amp;quot;: [ &amp;quot;v1&amp;quot; ], &amp;quot;serverAddressByClientCIDRs&amp;quot;: [ { &amp;quot;clientCIDR&amp;quot;: &amp;quot;0.0.0.0/0&amp;quot;, &amp;quot;serverAddress&amp;quot;: &amp;quot;10.0.2.15:8443&amp;quot; } ] }  获取 Pod 列表：</description>
    </item>
    
    <item>
      <title>使用 kubeadm 定制控制平面配置</title>
      <link>https://lijun.in/setup/production-environment/tools/kubeadm/control-plane-flags/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/setup/production-environment/tools/kubeadm/control-plane-flags/</guid>
      <description>. feature-state for_k8s_version=&amp;quot;1.12&amp;rdquo; state=&amp;quot;stable&amp;rdquo; &amp;gt;}}
kubeadm ClusterConfiguration 对象公开了 extraArgs 字段，它可以覆盖传递给控制平面组件（如 APIServer、ControllerManager 和 Scheduler）的默认参数。各组件配置使用如下字段定义：
 apiServer controllerManager scheduler  extraArgs 字段由 key: value 对组成。 要覆盖控制平面组件的参数:
 将适当的字段添加到配置中。 向字段添加要覆盖的参数值。 用 --config &amp;lt;YOUR CONFIG YAML&amp;gt; 运行 kubeadm init。  有关配置中的每个字段的详细信息，您可以导航到我们的 API 参考页面。
. note &amp;gt;}}
您可以通过运行 kubeadm config print init-defaults 并将输出保存到您选择的文件中，以默认值形式生成 ClusterConfiguration 对象。 . /note &amp;gt;}}
APIServer 参数 有关详细信息，请参阅 kube-apiserver 参考文档。
使用示例：
apiVersion: kubeadm.k8s.io/v1beta2 kind: ClusterConfiguration kubernetesVersion: v1.16.0 apiServer: extraArgs: advertise-address: 192.168.0.103 anonymous-auth: &amp;#34;false&amp;#34; enable-admission-plugins: AlwaysPullImages,DefaultStorageClass audit-log-path: /home/johndoe/audit.</description>
    </item>
    
    <item>
      <title>使用 kubectl patch 更新 API 对象</title>
      <link>https://lijun.in/tasks/run-application/update-api-object-kubectl-patch/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/run-application/update-api-object-kubectl-patch/</guid>
      <description>这个任务展示了如何使用 kubectl patch 就地更新 API 对象。这个任务中的练习演示了一个策略性合并 patch 和一个 JSON 合并 patch。
. heading &amp;ldquo;prerequisites&amp;rdquo; %}} . include &amp;ldquo;task-tutorial-prereqs.md&amp;rdquo; &amp;gt;}} . version-check &amp;gt;}}
使用策略合并 patch 更新 Deployment 下面是具有两个副本的 Deployment 的配置文件。每个副本是一个 Pod，有一个容器：
. codenew file=&amp;quot;application/deployment-patch.yaml&amp;rdquo; &amp;gt;}}
创建 Deployment：
kubectl create -f https://k8s.io/examples/application/deployment-patch.yaml 查看与 Deployment 相关的 Pod：
kubectl get pods 输出显示 Deployment 有两个 Pod。1/1 表示每个 Pod 有一个容器:
NAME READY STATUS RESTARTS AGE patch-demo-28633765-670qr 1/1 Running 0 23s patch-demo-28633765-j5qs3 1/1 Running 0 23s 把运行的 Pod 的名字记下来。稍后，您将看到这些 Pod 被终止并被新的 Pod 替换。</description>
    </item>
    
    <item>
      <title>使用 Romana 作为 NetworkPolicy</title>
      <link>https://lijun.in/tasks/administer-cluster/network-policy-provider/romana-network-policy/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/administer-cluster/network-policy-provider/romana-network-policy/</guid>
      <description>本页展示如何使用 Romana 作为 NetworkPolicy。
. heading &amp;ldquo;prerequisites&amp;rdquo; %}} 完成kubeadm 入门指南中的1、2、3步。
使用 kubeadm 安装 Romana 按照容器化安装指南获取 kubeadm。
运用网络策略 使用以下的一种方式去运用网络策略：
 Romana 网络策略  Romana 网络策略例子   NetworkPolicy API  . heading &amp;ldquo;whatsnext&amp;rdquo; %}} Romana 安装完成后，您可以按照声明 Network Policy去尝试使用 Kubernetes NetworkPolicy。</description>
    </item>
    
    <item>
      <title>使用工作队列进行精细的并行处理</title>
      <link>https://lijun.in/tasks/job/fine-parallel-processing-work-queue/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/job/fine-parallel-processing-work-queue/</guid>
      <description>在这个例子中，我们会运行一个Kubernetes Job，其中的 Pod 会运行多个并行工作进程。
在这个例子中，当每个pod被创建时，它会从一个任务队列中获取一个工作单元，处理它，然后重复，直到到达队列的尾部。
下面是这个示例的步骤概述
 启动存储服务用于保存工作队列。 在这个例子中，我们使用 Redis 来存储工作项。在上一个例子中，我们使用了 RabbitMQ。在这个例子中，由于 AMQP 不能为客户端提供一个良好的方法来检测一个有限长度的工作队列是否为空，我们使用了 Redis 和一个自定义的工作队列客户端库。在实践中，您可能会设置一个类似于 Redis 的存储库，并将其同时用于多项任务或其他事务的工作队列。  创建一个队列，然后向其中填充消息。 每个消息表示一个将要被处理的工作任务。在这个例子中，消息只是一个我们将用于进行长度计算的整数。  启动一个 Job 对队列中的任务进行处理。这个 Job 启动了若干个 Pod 。每个 Pod 从消息队列中取出一个工作任务，处理它，然后重复，直到到达队列的尾部。  . toc &amp;gt;}}
. heading &amp;ldquo;prerequisites&amp;rdquo; %}} . include &amp;ldquo;task-tutorial-prereqs.md&amp;rdquo; &amp;gt;}} . version-check &amp;gt;}}
熟秋基础知识，非并行方式运行 Job。
启动 Redis 对于这个例子，为了简单起见，我们将启动一个单实例的 Redis。 了解如何部署一个可伸缩、高可用的 Redis 例子，请查看 Redis 样例
如果您在使用本文档库的源代码目录，您可以进入如下目录，然后启动一个临时的 Pod 用于运行 Redis 和 一个临时的 service 以便我们能够找到这个 Pod
$ cd content/en/examples/application/job/redis $ kubectl create -f .</description>
    </item>
    
    <item>
      <title>使用端口转发来访问集群中的应用</title>
      <link>https://lijun.in/tasks/access-application-cluster/port-forward-access-application-cluster/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/access-application-cluster/port-forward-access-application-cluster/</guid>
      <description>本文展示如何使用 kubectl port-forward 连接到在 Kubernetes 集群中运行的 Redis 服务。这种类型的连接对数据库调试很有用。
. heading &amp;ldquo;prerequisites&amp;rdquo; %}}  . include &amp;ldquo;task-tutorial-prereqs.md&amp;rdquo; &amp;gt;}} . version-check &amp;gt;}}   安装 redis-cli。  创建 Redis deployment 和服务   创建一个 Redis deployment：
 kubectl apply -f https://k8s.io/examples/application/guestbook/redis-master-deployment.yaml    查看输出是否成功，以验证是否成功创建 deployment： deployment.apps/redis-master created  查看 pod 状态，检查其是否准备就绪： kubectl get pods  输出显示创建的 pod： NAME READY STATUS RESTARTS AGE redis-master-765d459796-258hz 1/1 Running 0 50s  查看 deployment 状态： kubectl get deployment  输出显示创建的 deployment： NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE redis-master 1 1 1 1 55s  查看 replicaset 状态： kubectl get rs  输出显示创建的 replicaset： NAME DESIRED CURRENT READY AGE redis-master-765d459796 1 1 1 1m   创建一个 Redis 服务：</description>
    </item>
    
    <item>
      <title>使用配置文件对 Kubernetes 对象进行命令式管理</title>
      <link>https://lijun.in/tasks/manage-kubernetes-objects/imperative-config/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/manage-kubernetes-objects/imperative-config/</guid>
      <description>可以使用 kubectl 命令行工具以及用 YAML 或 JSON 编写的对象配置文件来创建、更新和删除 Kubernetes 对象。 本文档说明了如何使用配置文件定义和管理对象。
heading &amp;ldquo;prerequisites&amp;rdquo; %}} 安装 kubectl 。
include &amp;ldquo;task-tutorial-prereqs.md&amp;rdquo; &amp;gt;}} . version-check &amp;gt;}}
权衡 kubectl 工具支持三种对象管理：
 命令式命令 命令式对象配置 声明式对象配置  参看 Kubernetes 对象管理 讨论每种对象管理的优缺点。
如何创建对象 您可以使用 kubectl create -f 从配置文件创建一个对象。 请参考 [kubernetes API 参考](/docs/reference/generated/kubernetes-api/. param &amp;ldquo;version&amp;rdquo; &amp;gt;}}/) 有关详细信息。
 kubectl create -f &amp;lt;filename|url&amp;gt;  如何更新对象 . warning &amp;gt;}}
使用 replace 命令更新对象会删除所有未在配置文件中指定的规范的某些部分。 不应将其规范由集群部分管理的对象使用，比如类型为 LoadBalancer 的服务，其中 externalIPs 字段独立于配置文件进行管理。 必须将独立管理的字段复制到配置文件中，以防止 replace 删除它们。 . /warning &amp;gt;}}</description>
    </item>
    
    <item>
      <title>动态准入控制</title>
      <link>https://lijun.in/reference/access-authn-authz/extensible-admission-controllers/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/access-authn-authz/extensible-admission-controllers/</guid>
      <description>除了内置的 admission 插件，admission 插件可以作为扩展独立开发，并以运行时所配置的 webhook 的形式运行。 此页面描述了如何构建、配置、使用和监视 admission webhook。
什么是 admission webhook？ Admission webhook 是一种用于接收准入请求并对其进行处理的 HTTP 回调机制。 可以定义两种类型的 admission webhook，即 validating admission webhook 和 mutating admission webhook。 Mutating admission webhook 会先被调用。它们可以更改发送到 API 服务器的对象以执行自定义的设置默认值操作。
在完成了所有对象修改并且 API 服务器也验证了所传入的对象之后，validating admission webhook 会被调用，并通过拒绝请求的方式来强制实施自定义的策略。
. note &amp;gt;}} 如果 admission webhook 需要保证它们所看到的是对象的最终状态以实施某种策略。则应使用 validating admission webhook，因为对象被 mutating webhook 看到之后仍然可能被修改。 . /note &amp;gt;}}
尝试 admission webhook admission webhook 本质上是集群控制平面的一部分。您应该非常谨慎地编写和部署它们。 如果您打算编写或者部署生产级 admission webhook，请阅读用户指南以获取相关说明。 在下文中，我们将介绍如何快速试验 admission webhook。
先决条件   确保 Kubernetes 集群版本至少为 v1.</description>
    </item>
    
    <item>
      <title>动态卷供应</title>
      <link>https://lijun.in/concepts/storage/dynamic-provisioning/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/concepts/storage/dynamic-provisioning/</guid>
      <description>动态卷供应允许按需创建存储卷。 如果没有动态供应，集群管理员必须手动地联系他们的云或存储提供商来创建新的存储卷， 然后在 Kubernetes 集群创建 PersistentVolume 对象来表示这些卷。 动态供应功能消除了集群管理员预先配置存储的需要。 相反，它在用户请求时自动供应存储。
背景 动态卷供应的实现基于 storage.k8s.io API 组中的 StorageClass API 对象。 集群管理员可以根据需要定义多个 StorageClass 对象，每个对象指定一个卷插件（又名 provisioner）， 卷插件向卷供应商提供在创建卷时需要的数据卷信息及相关参数。
集群管理员可以在集群中定义和公开多种存储（来自相同或不同的存储系统），每种都具有自定义参数集。 该设计也确保终端用户不必担心存储供应的复杂性和细微差别，但仍然能够从多个存储选项中进行选择。
点击这里查阅有关存储类的更多信息。
启用动态卷供应 要启用动态供应功能，集群管理员需要为用户预先创建一个或多个 StorageClass 对象。 StorageClass 对象定义当动态供应被调用时，哪一个驱动将被使用和哪些参数将被传递给驱动。 以下清单创建了一个 StorageClass 存储类 &amp;ldquo;slow&amp;rdquo;，它提供类似标准磁盘的永久磁盘。
apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: slow provisioner: kubernetes.io/gce-pd parameters: type: pd-standard 以下清单创建了一个 &amp;ldquo;fast&amp;rdquo; 存储类，它提供类似 SSD 的永久磁盘。
apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: fast provisioner: kubernetes.io/gce-pd parameters: type: pd-ssd 使用动态卷供应 用户通过在 PersistentVolumeClaim 中包含存储类来请求动态供应的存储。 在 Kubernetes v1.</description>
    </item>
    
    <item>
      <title>服务目录</title>
      <link>https://lijun.in/concepts/extend-kubernetes/service-catalog/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/concepts/extend-kubernetes/service-catalog/</guid>
      <description>term_id=&amp;quot;service-catalog&amp;rdquo; length=&amp;quot;all&amp;rdquo; prepend=&amp;rdquo;&amp;rdquo; &amp;gt;}}
服务代理是由开放服务代理 API 规范定义的一组托管服务的终结点，由第三方提供并维护，其中的第三方可以是 AWS，GCP 或 Azure 等云服务提供商。 托管服务的一些示例是 Microsoft Azure Cloud Queue，Amazon Simple Queue Service 和 Google Cloud Pub/Sub，但它们是可以使用应用程序的任何软件产品。
使用服务目录，集群操作者可以浏览其提供的托管服务列表，提供托管服务实例并与之绑定，以使其可以被 Kubernetes 集群中的应用程序使用。
示例用例 应用开发者希望使用消息队列作为其在 Kubernetes 集群中运行的应用程序的一部分。 但是，它们不想承受建立这种服务的开销，也不想自行管理。幸运的是，有一家云服务提供商通过它们的服务代理将消息队列作为托管服务提供。
集群运维人员可以设置服务目录并使用它与云服务提供商的服务代理 通信，以此提供消息队列服务的实例并使其对 Kubernetes 中的应用程序可用。 因此，应用开发者可以不用关心消息队列的实现细节，也不用对其进行管理。它们的应用程序可以简单的将其作为服务使用。
架构 服务目录使用开放服务代理 API 与服务代理进行通信，并作为 Kubernetes API Server 的中介，以便协商首要规定并获取应用程序使用托管服务的必要凭据。
它被实现为一个扩展 API 服务和一个控制器管理器，使用 Etcd 作为存储。它还使用了 Kubernetes 1.7+ 版本中提供的 aggregation layer 来呈现其 API。
API 资源 服务目录安装 servicecatalog.k8s.io API 并提供以下 Kubernetes 资源：
 ClusterServiceBroker：服务目录的集群内代表，封装了它的服务连接细节。集群运维人员创建和管理这些资源，并希望使用该代理服务在集群中提供新类型的托管服务。 ClusterServiceClass：由特定服务代理提供的托管服务。当新的 ClusterServiceBroker 资源被添加到集群时，服务目录控制器将连接到服务代理以获取可用的托管服务列表。然后为每个托管服务创建对应的新 ClusterServiceClass 资源。 ClusterServicePlan：托管服务的特定产品。例如托管服务可能有不同的计划可用，如免费版本和付费版本，或者可能有不同的配置选项，例如使用 SSD 存储或拥有更多资源。与 ClusterServiceClass 类似，当一个新的 ClusterServiceBroker 被添加到集群时，服务目录会为每个托管服务的每个可用服务计划创建对应的新 ClusterServicePlan 资源。 ServiceInstance：ClusterServiceClass 提供的示例。由集群运维人员创建，以使托管服务的特定实例可供一个或多个集群内应用程序使用。当创建一个新的 ServiceInstance 资源时，服务目录控制器将连接到相应的服务代理并指示它调配服务实例。 ServiceBinding：ServiceInstance 的访问凭据。由希望其应用程序使用服务 ServiceInstance 的集群运维人员创建。创建之后，服务目录控制器将创建一个 Kubernetes Secret，其中包含服务实例的连接细节和凭据，可以挂载到 Pod 中。  认证 服务目录支持这些认证方法：</description>
    </item>
    
    <item>
      <title>标签和选择器</title>
      <link>https://lijun.in/concepts/overview/working-with-objects/labels/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/concepts/overview/working-with-objects/labels/</guid>
      <description>标签 是附加到 Kubernetes 对象（比如 Pods）上的键值对。 标签旨在用于指定对用户有意义且相关的对象的标识属性，但不直接对核心系统有语义含义。 标签可以用于组织和选择对象的子集。标签可以在创建时附加到对象，随后可以随时添加和修改。 每个对象都可以定义一组键/值标签。每个键对于给定对象必须是唯一的。
&amp;#34;metadata&amp;#34;: { &amp;#34;labels&amp;#34;: { &amp;#34;key1&amp;#34; : &amp;#34;value1&amp;#34;, &amp;#34;key2&amp;#34; : &amp;#34;value2&amp;#34; } } 我们最终将标签索引和反向索引，用于高效查询和监视，使用它们在 UI 和 CLI 中进行排序和分组等。我们不希望将非标识性的、尤其是大型或结构化数据用作标签，给后者带来污染。应使用 注解 记录非识别信息
动机 标签使用户能够以松散耦合的方式将他们自己的组织结构映射到系统对象，而无需客户端存储这些映射。
服务部署和批处理流水线通常是多维实体（例如，多个分区或部署、多个发行序列、多个层，每层多个微服务）。管理通常需要交叉操作，这打破了严格的层次表示的封装，特别是由基础设施而不是用户确定的严格的层次结构。
示例标签：
 &amp;quot;release&amp;quot; : &amp;quot;stable&amp;quot;, &amp;quot;release&amp;quot; : &amp;quot;canary&amp;quot; &amp;quot;environment&amp;quot; : &amp;quot;dev&amp;quot;, &amp;quot;environment&amp;quot; : &amp;quot;qa&amp;quot;, &amp;quot;environment&amp;quot; : &amp;quot;production&amp;quot; &amp;quot;tier&amp;quot; : &amp;quot;frontend&amp;quot;, &amp;quot;tier&amp;quot; : &amp;quot;backend&amp;quot;, &amp;quot;tier&amp;quot; : &amp;quot;cache&amp;quot; &amp;quot;partition&amp;quot; : &amp;quot;customerA&amp;quot;, &amp;quot;partition&amp;quot; : &amp;quot;customerB&amp;quot; &amp;quot;track&amp;quot; : &amp;quot;daily&amp;quot;, &amp;quot;track&amp;quot; : &amp;quot;weekly&amp;quot;  这些只是常用标签的例子; 您可以任意制定自己的约定。请记住，对于给定对象标签的键必须是唯一的。</description>
    </item>
    
    <item>
      <title>管理资源</title>
      <link>https://lijun.in/concepts/cluster-administration/manage-deployment/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/concepts/cluster-administration/manage-deployment/</guid>
      <description>您已经部署了应用并通过服务暴露它。然后呢？Kubernetes 提供了一些工具来帮助管理您的应用部署，包括缩扩容和更新。我们将更深入讨论的特性包括配置文件和标签。
组织资源配置 许多应用需要创建多个资源，例如 Deployment 和 Service。可以通过将多个资源组合在同一个文件中（在 YAML 中以 --- 分隔）来简化对它们的管理。例如：
file=&amp;quot;application/nginx-app.yaml&amp;rdquo; &amp;gt;}}
可以用创建单个资源相同的方式来创建多个资源：
kubectl apply -f https://k8s.io/examples/application/nginx-app.yaml service/my-nginx-svc created deployment.apps/my-nginx created 资源将按照它们在文件中的顺序创建。因此，最好先指定服务，这样在控制器（例如 Deployment）创建 Pod 时能够确保调度器可以将与服务关联的多个 Pod 分散到不同节点。
kubectl create 也接受多个 -f 参数:
kubectl apply -f https://k8s.io/examples/application/nginx/nginx-svc.yaml -f https://k8s.io/examples/application/nginx/nginx-deployment.yaml 还可以指定目录路径，而不用添加多个单独的文件：
kubectl apply -f https://k8s.io/examples/application/nginx/ kubectl 将读取任何后缀为 .yaml，.yml 或者 .json 的文件。
建议的做法是，将同一个微服务或同一应用层相关的资源放到同一个文件中，将同一个应用相关的所有文件按组存放到同一个目录中。如果应用的各层使用 DNS 相互绑定，那么您可以简单地将堆栈的所有组件一起部署。
还可以使用 URL 作为配置源，便于直接使用已经提交到 Github 上的配置文件进行部署：
kubectl apply -f https://raw.githubusercontent.com/kubernetes/website/master/content/zh/examples/application/nginx/nginx-deployment.yaml deployment.apps/my-nginx created kubectl 中的批量操作 资源创建并不是 kubectl 可以批量执行的唯一操作。kubectl 还可以从配置文件中提取资源名，以便执行其他操作，特别是删除您之前创建的资源：</description>
    </item>
    
    <item>
      <title>DaemonSet</title>
      <link>https://lijun.in/concepts/workloads/controllers/daemonset/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/concepts/workloads/controllers/daemonset/</guid>
      <description>DaemonSet 确保全部（或者某些）节点上运行一个 Pod 的副本。当有节点加入集群时， 也会为他们新增一个 Pod 。当有节点从集群移除时，这些 Pod 也会被回收。删除 DaemonSet 将会删除它创建的所有 Pod。
DaemonSet 的一些典型用法：
 在每个节点上运行集群存储 DaemonSet，例如 glusterd、ceph。 在每个节点上运行日志收集 DaemonSet，例如 fluentd、logstash。 在每个节点上运行监控 DaemonSet，例如 Prometheus Node Exporter、Flowmill、Sysdig 代理、collectd、Dynatrace OneAgent、AppDynamics 代理、Datadog 代理、New Relic 代理、Ganglia gmond 或者 Instana 代理。  一个简单的用法是在所有的节点上都启动一个 DaemonSet，将被作为每种类型的 daemon 使用。
一个稍微复杂的用法是单独对每种 daemon 类型使用多个 DaemonSet，但具有不同的标志， 并且对不同硬件类型具有不同的内存、CPU 要求。
编写 DaemonSet Spec 创建 DaemonSet 您可以在 YAML 文件中描述 DaemonSet。例如，下面的 daemonset.yaml 文件描述了一个运行 fluentd-elasticsearch Docker 镜像的 DaemonSet：
odenew file=&amp;quot;controllers/daemonset.yaml&amp;rdquo; &amp;gt;}}
 基于 YAML 文件创建 DaemonSet:  kubectl apply -f https://k8s.</description>
    </item>
    
    <item>
      <title>kubeadm config</title>
      <link>https://lijun.in/reference/setup-tools/kubeadm/kubeadm-config/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/setup-tools/kubeadm/kubeadm-config/</guid>
      <description>从 v1.8.0 开始，kubeadm 将集群的配置上传到名为 kube-system 的 ConfigMap 对象中，对象位于 kube-system 命名空间内。并在以后的升级中读取这个 ConfigMap 配置对象。 这样可以保证系统组件的正确配置，提供无缝的用户体验。
您可以执行 kubeadm config view 来查看 ConfigMap。如果使用 kubeadm v1.7.x 或更低版本来初始化群集，必须先使用 kubeadm config upload 创建 ConfigMap，然后才能使用 kubeadm upgrade。
在 Kubernetes v1.11.0 中，添加了一些新命令。你可以使用 kubeadm config print-default 打印默认配置，可以用 kubeadm config migrate 来将旧的配置文件转换到较新的版本，还可以使用 kubeadm config images list 和 kubeadm config images pull 列出并拉取 kubeadm 所需的镜像。
kubeadm config upload from-file kubeadm config view . include &amp;ldquo;generated/kubeadm_config_view.md&amp;rdquo; &amp;gt;}}
kubeadm config print init-defaults . include &amp;ldquo;generated/kubeadm_config_print_init-defaults.</description>
    </item>
    
    <item>
      <title>Kubernetes 调度器</title>
      <link>https://lijun.in/concepts/scheduling-eviction/kube-scheduler/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/concepts/scheduling-eviction/kube-scheduler/</guid>
      <description>在 Kubernetes 中，调度 是指将 text=&amp;quot;Pod&amp;rdquo; term_id=&amp;quot;pod&amp;rdquo; &amp;gt;}} 放置到合适的 text=&amp;quot;Node&amp;rdquo; term_id=&amp;quot;node&amp;rdquo; &amp;gt;}} 上，然后对应 Node 上的glossary_tooltip term_id=&amp;quot;kubelet&amp;rdquo; &amp;gt;}} 才能够运行这些 pod。
调度概览 调度器通过 kubernetes 的 watch 机制来发现集群中新创建且尚未被调度到 Node 上的 Pod。调度器会将发现的每一个未调度的 Pod 调度到一个合适的 Node 上来运行。调度器会依据下文的调度原则来做出调度选择。
如果你想要理解 Pod 为什么会被调度到特定的 Node 上，或者你想要尝试实现一个自定义的调度器，这篇文章将帮助你了解调度。
kube-scheduler kube-scheduler 是 Kubernetes 集群的默认调度器，并且是集群 text=&amp;quot;控制面&amp;rdquo; term_id=&amp;quot;control-plane&amp;rdquo; &amp;gt;}} 的一部分。如果你真的希望或者有这方面的需求，kube-scheduler 在设计上是允许你自己写一个调度组件并替换原有的 kube-scheduler。
对每一个新创建的 Pod 或者是未被调度的 Pod，kube-scheduler 会选择一个最优的 Node 去运行这个 Pod。然而，Pod 内的每一个容器对资源都有不同的需求，而且 Pod 本身也有不同的资源需求。因此，Pod 在被调度到 Node 上之前，根据这些特定的资源调度需求，需要对集群中的 Node 进行一次过滤。
在一个集群中，满足一个 Pod 调度请求的所有 Node 称之为 可调度节点。如果没有任何一个 Node 能满足 Pod 的资源请求，那么这个 Pod 将一直停留在未调度状态直到调度器能够找到合适的 Node。</description>
    </item>
    
    <item>
      <title>Pod Preset</title>
      <link>https://lijun.in/concepts/workloads/pods/podpreset/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/concepts/workloads/pods/podpreset/</guid>
      <description>本文提供了 PodPreset 的概述。 在 Pod 创建时，用户可以使用 PodPreset 对象将特定信息注入 Pod 中，这些信息可以包括 secret、 卷、卷挂载和环境变量。
理解 Pod Preset Pod Preset 是一种 API 资源，在 Pod 创建时，用户可以用它将额外的运行时需求信息注入 Pod。 使用标签选择器（label selector）来指定 Pod Preset 所适用的 Pod。
使用 Pod Preset 使得 Pod 模板编写者不必显式地为每个 Pod 设置信息。 这样，使用特定服务的 Pod 模板编写者不需要了解该服务的所有细节。
了解更多的相关背景信息，请参考 PodPreset 设计提案。
PodPreset 如何工作 Kubernetes 提供了准入控制器 (PodPreset)，该控制器被启用时，会将 Pod Preset 应用于接收到的 Pod 创建请求中。 当出现 Pod 创建请求时，系统会执行以下操作：
 检索所有可用 PodPresets 。 检查 PodPreset 的标签选择器与要创建的 Pod 的标签是否匹配。 尝试合并 PodPreset 中定义的各种资源，并注入要创建的 Pod。 发生错误时抛出事件，该事件记录了 pod 信息合并错误，同时在 不注入 PodPreset 信息的情况下创建 Pod。 为改动的 Pod spec 添加注解，来表明它被 PodPreset 所修改。 注解形如： podpreset.</description>
    </item>
    
    <item>
      <title>Pod 拓扑扩展约束</title>
      <link>https://lijun.in/concepts/workloads/pods/pod-topology-spread-constraints/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/concepts/workloads/pods/pod-topology-spread-constraints/</guid>
      <description>feature-state for_k8s_version=&amp;quot;v1.16&amp;rdquo; state=&amp;quot;alpha&amp;rdquo; &amp;gt;}}
可以使用拓扑扩展约束来控制glossary_tooltip text=&amp;quot;Pods&amp;rdquo; term_id=&amp;quot;Pod&amp;rdquo; &amp;gt;}} 在集群内故障域（例如地区，区域，节点和其他用户自定义拓扑域）之间的分布。这可以帮助实现高可用以及提升资源利用率。
先决条件 启用功能 确保 EvenPodsSpread 功能已开启（在 1.16 版本中该功能默认关闭）。阅读功能选项了解如何开启该功能。EvenPodsSpread 必须在 glossary_tooltip text=&amp;quot;API Server&amp;rdquo; term_id=&amp;quot;kube-apiserver&amp;rdquo; &amp;gt;}} 和 glossary_tooltip text=&amp;quot;scheduler&amp;rdquo; term_id=&amp;quot;kube-scheduler&amp;rdquo; &amp;gt;}} 中都要开启。
节点标签 拓扑扩展约束依赖于节点标签来标识每个节点所在的拓扑域。例如，一个节点可能具有标签：node=node1,zone=us-east-1a,region=us-east-1
假设你拥有一个具有以下标签的 4 节点集群：
NAME STATUS ROLES AGE VERSION LABELS node1 Ready &amp;lt;none&amp;gt; 4m26s v1.16.0 node=node1,zone=zoneA node2 Ready &amp;lt;none&amp;gt; 3m58s v1.16.0 node=node2,zone=zoneA node3 Ready &amp;lt;none&amp;gt; 3m17s v1.16.0 node=node3,zone=zoneB node4 Ready &amp;lt;none&amp;gt; 2m43s v1.16.0 node=node4,zone=zoneB 然后从逻辑上看集群如下：
+---------------+---------------+ | zoneA | zoneB | +-------+-------+-------+-------+ | node1 | node2 | node3 | node4 | +-------+-------+-------+-------+ 可以复用在大多数集群上自动创建和填充的知名标签，而不是手动添加标签。</description>
    </item>
    
    <item>
      <title>Secret</title>
      <link>https://lijun.in/concepts/configuration/secret/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/concepts/configuration/secret/</guid>
      <description>Secret 对象类型用来保存敏感信息，例如密码、OAuth 令牌和 ssh key。 将这些信息放在 secret 中比放在 term_id=&amp;quot;pod&amp;rdquo; &amp;gt;}} 的定义或者 text=&amp;quot;容器镜像&amp;rdquo; term_id=&amp;quot;image&amp;rdquo; &amp;gt;}} 中来说更加安全和灵活。 参阅 Secret 设计文档 获取更多详细信息。
Secret 概览 Secret 是一种包含少量敏感信息例如密码、token 或 key 的对象。这样的信息可能会被放在 Pod spec 中或者镜像中；将其放在一个 secret 对象中可以更好地控制它的用途，并降低意外暴露的风险。
用户可以创建 secret，同时系统也创建了一些 secret。
要使用 secret，pod 需要引用 secret。Pod 可以用两种方式使用 secret：作为 text=&amp;quot;volume&amp;rdquo; term_id=&amp;quot;volume&amp;rdquo; &amp;gt;}} 中的文件被挂载到 pod 中的一个或者多个容器里，或者当 kubelet 为 pod 拉取镜像时使用。
内置 secret Service Account 使用 API 凭证自动创建和附加 secret Kubernetes 自动创建包含访问 API 凭据的 secret，并自动修改您的 pod 以使用此类型的 secret。
如果需要，可以禁用或覆盖自动创建和使用 API 凭据。但是，如果您需要的只是安全地访问 apiserver，我们推荐这样的工作流程。</description>
    </item>
    
    <item>
      <title>v1.17</title>
      <link>https://lijun.in/reference/kubernetes-api/api-index/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/kubernetes-api/api-index/</guid>
      <description>Kubernetes API v1.17</description>
    </item>
    
    <item>
      <title>为命名空间配置内存和 CPU 配额</title>
      <link>https://lijun.in/tasks/administer-cluster/manage-resources/quota-memory-cpu-namespace/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/administer-cluster/manage-resources/quota-memory-cpu-namespace/</guid>
      <description>本文介绍怎样为命名空间设置容器可用的内存和 CPU 总量。你可以通过 [ResourceQuota](/docs/reference/generated/kubernetes-api/. param &amp;ldquo;version&amp;rdquo; &amp;gt;}}/#resourcequota-v1-core) 对象设置配额.
. heading &amp;ldquo;prerequisites&amp;rdquo; %}} . include &amp;ldquo;task-tutorial-prereqs.md&amp;rdquo; &amp;gt;}} . version-check &amp;gt;}}
集群中每个节点至少有1 GiB的内存。
创建命名空间 创建一个命名空间，以便本练习中创建的资源和集群的其余部分相隔离。
kubectl create namespace quota-mem-cpu-example 创建 ResourceQuota 这里给出一个 ResourceQuota 对象的配置文件：
. codenew file=&amp;quot;admin/resource/quota-mem-cpu.yaml&amp;rdquo; &amp;gt;}}
创建 ResourceQuota
kubectl create -f https://k8s.io/examples/admin/resource/quota-mem-cpu.yaml --namespace=quota-mem-cpu-example 查看 ResourceQuota 详情：
kubectl get resourcequota mem-cpu-demo --namespace=quota-mem-cpu-example --output=yaml ResourceQuota 在 quota-mem-cpu-example 命名空间中设置了如下要求：
 每个容器必须有内存请求和限制，以及 CPU 请求和限制。 所有容器的内存请求总和不能超过1 GiB。 所有容器的内存限制总和不能超过2 GiB。 所有容器的 CPU 请求总和不能超过1 cpu。 所有容器的 CPU 限制总和不能超过2 cpu。  创建 Pod 这里给出 Pod 的配置文件：</description>
    </item>
    
    <item>
      <title>使用 Weave Net 作为 NetworkPolicy</title>
      <link>https://lijun.in/tasks/administer-cluster/network-policy-provider/weave-network-policy/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/administer-cluster/network-policy-provider/weave-network-policy/</guid>
      <description>本页展示了如何使用使用 Weave Net 作为 NetworkPolicy。
. heading &amp;ldquo;prerequisites&amp;rdquo; %}} 您需要拥有一个 Kubernetes 集群。按照kubeadm 入门指南来引导一个。
安装 Weave Net 插件 按照通过插件集成Kubernetes指南。
Kubernetes 的 Weave Net 插件带有网络策略控制器，可自动监控 Kubernetes 所有名称空间中的任何 NetworkPolicy 注释。 配置iptables规则以允许或阻止策略指示的流量。
测试安装 验证 weave 是否有效。
输入以下命令：
kubectl get po -n kube-system -o wide 输出类似这样：
NAME READY STATUS RESTARTS AGE IP NODE weave-net-1t1qg 2/2 Running 0 9d 192.168.2.10 worknode3 weave-net-231d7 2/2 Running 1 7d 10.2.0.17 worknodegpu weave-net-7nmwt 2/2 Running 3 9d 192.168.2.131 masternode weave-net-pmw8w 2/2 Running 0 9d 192.</description>
    </item>
    
    <item>
      <title>将 Pod 分配给节点</title>
      <link>https://lijun.in/concepts/configuration/assign-pod-node/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/concepts/configuration/assign-pod-node/</guid>
      <description>你可以约束一个 text=&amp;quot;Pod&amp;rdquo; term_id=&amp;quot;pod&amp;rdquo; &amp;gt;}} 只能在特定的 text=&amp;quot;Node(s)&amp;rdquo; term_id=&amp;quot;node&amp;rdquo; &amp;gt;}} 上运行，或者优先运行在特定的节点上。有几种方法可以实现这点，推荐的方法都是用标签选择器来进行选择。通常这样的约束不是必须的，因为调度器将自动进行合理的放置（比如，将 pod 分散到节点上，而不是将 pod 放置在可用资源不足的节点上等等），但在某些情况下，你可以需要更多控制 pod 停靠的节点，例如，确保 pod 最终落在连接了 SSD 的机器上，或者将来自两个不同的服务且有大量通信的 pod 放置在同一个可用区。
nodeSelector nodeSelector 是节点选择约束的最简单推荐形式。nodeSelector 是 PodSpec 的一个字段。它指定键值对的映射。为了使 pod 可以在节点上运行，节点必须具有每个指定的键值对作为标签（它也可以具有其他标签）。最常用的是一对键值对。
让我们来看一个使用 nodeSelector 的例子。
步骤零：先决条件 本示例假设你已基本了解 Kubernetes 的 pod 并且已经建立一个 Kubernetes 集群。
步骤一：添加标签到节点 执行 kubectl get nodes 命令获取集群的节点名称。选择一个你要增加标签的节点，然后执行 kubectl label nodes &amp;lt;node-name&amp;gt; &amp;lt;label-key&amp;gt;=&amp;lt;label-value&amp;gt; 命令将标签添加到你所选择的节点上。例如，如果你的节点名称为 &amp;lsquo;kubernetes-foo-node-1.c.a-robinson.internal&amp;rsquo; 并且想要的标签是 &amp;lsquo;disktype=ssd&amp;rsquo;，则可以执行 kubectl label nodes kubernetes-foo-node-1.c.a-robinson.internal disktype=ssd 命令。
你可以通过重新运行 kubectl get nodes --show-labels 并且查看节点当前具有了一个标签来验证它是否有效。你也可以使用 kubectl describe node &amp;quot;nodename&amp;quot; 命令查看指定节点的标签完整列表。</description>
    </item>
    
    <item>
      <title>注解</title>
      <link>https://lijun.in/concepts/overview/working-with-objects/annotations/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/concepts/overview/working-with-objects/annotations/</guid>
      <description>你可以使用 Kubernetes 注解为对象附加任意的非标识的元数据。客户端程序（例如工具和库）能够获取这些元数据信息。
为对象附加元数据 您可以使用标签或注解将元数据附加到 Kubernetes 对象。 标签可以用来选择对象和查找满足某些条件的对象集合。 相反，注解不用于标识和选择对象。 注解中的元数据，可以很小，也可以很大，可以是结构化的，也可以是非结构化的，能够包含标签不允许的字符。
注解和标签一样，是键/值对:
&amp;#34;metadata&amp;#34;: { &amp;#34;annotations&amp;#34;: { &amp;#34;key1&amp;#34; : &amp;#34;value1&amp;#34;, &amp;#34;key2&amp;#34; : &amp;#34;value2&amp;#34; } } 以下是一些例子，用来说明哪些信息可以使用注解来记录:
 由声明性配置所管理的字段。 将这些字段附加为注解，能够将它们与客户端或服务端设置的默认值、自动生成的字段以及通过自动调整大小或自动伸缩系统设置的字段区分开来。   构建、发布或镜像信息（如时间戳、发布 ID、Git 分支、PR 数量、镜像哈希、仓库地址）。   指向日志记录、监控、分析或审计仓库的指针。   可用于调试目的的客户端库或工具信息：例如，名称、版本和构建信息。   用户或者工具/系统的来源信息，例如来自其他生态系统组件的相关对象的 URL。   推出的轻量级工具的元数据信息：例如，配置或检查点。   负责人员的电话或呼机号码，或指定在何处可以找到该信息的目录条目，如团队网站。  从用户到最终运行的指令，以修改行为或使用非标准功能。
您可以将这类信息存储在外部数据库或目录中而不使用注解，但这样做就使得开发人员很难生成用于部署、管理、自检的客户端共享库和工具。
语法和字符集 注解 存储的形式是键/值对。有效的注解键分为两部分：可选的前缀和名称，以斜杠（/）分隔。 名称段是必需项，并且必须在63个字符以内，以字母数字字符（[a-z0-9A-Z]）开头和结尾，并允许使用破折号（-），下划线（_），点（.）和字母数字。 前缀是可选的。 如果指定，则前缀必须是DNS子域：一系列由点（.）分隔的DNS标签，总计不超过253个字符，后跟斜杠（/）。 如果省略前缀，则假定注释键对用户是私有的。 由系统组件添加的注释（例如，kube-scheduler，kube-controller-manager，kube-apiserver，kubectl 或其他第三方组件），必须为终端用户添加注释前缀。
kubernetes.io / 和 k8s.io / 前缀是为Kubernetes核心组件保留的。
例如，这是Pod的配置文件，其注释为 imageregistry：https：// hub.docker.com / ：</description>
    </item>
    
    <item>
      <title>网络策略</title>
      <link>https://lijun.in/concepts/services-networking/network-policies/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/concepts/services-networking/network-policies/</guid>
      <description>网络策略（NetworkPolicy）是一种关于 text=&amp;quot;Pod&amp;rdquo; term_id=&amp;quot;pod&amp;quot;&amp;gt;}} 间及与其他网络端点间所允许的通信规则的规范。
NetworkPolicy 资源使用 text=&amp;quot;标签&amp;rdquo; term_id=&amp;quot;label&amp;quot;&amp;gt;}} 选择 Pod，并定义选定 Pod 所允许的通信规则。
前提 网络策略通过网络插件来实现。要使用网络策略，用户必须使用支持 NetworkPolicy 的网络解决方案。创建一个资源对象，而没有控制器来使它生效的话，是没有任何作用的。
隔离和非隔离的 Pod 默认情况下，Pod 是非隔离的，它们接受任何来源的流量。
Pod 可以通过相关的网络策略进行隔离。一旦命名空间中有网络策略选择了特定的 Pod，该 Pod 会拒绝网络策略所不允许的连接。 (命名空间下其他未被网络策略所选择的 Pod 会继续接收所有的流量)
网络策略不会冲突，它们是附加的。如果任何一个或多个策略选择了一个 Pod, 则该 Pod 受限于这些策略的 ingress/egress 规则的并集。因此评估的顺序并不会影响策略的结果。
NetworkPolicy 资源 查看 [网络策略](/docs/reference/generated/kubernetes-api/ param &amp;ldquo;version&amp;rdquo; &amp;gt;}}/#networkpolicy-v1-networking-k8s-io) 来了解完整的资源定义。
下面是一个 NetworkPolicy 的示例:
apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: test-network-policy namespace: default spec: podSelector: matchLabels: role: db policyTypes: - Ingress - Egress ingress: - from: - ipBlock: cidr: 172.</description>
    </item>
    
    <item>
      <title>配置 Pod 以使用卷进行存储</title>
      <link>https://lijun.in/tasks/configure-pod-container/configure-volume-storage/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/configure-pod-container/configure-volume-storage/</guid>
      <description>此页面展示了如何配置 Pod 以使用卷进行存储。
只要容器存在，容器的文件系统就会存在，因此当一个容器终止并重新启动，对该容器的文件系统改动将丢失。对于独立于容器的持久化存储，您可以使用卷。这对于有状态应用程序尤为重要，例如键值存储（如 Redis）和数据库。
. heading &amp;ldquo;prerequisites&amp;rdquo; %}} . include &amp;ldquo;task-tutorial-prereqs.md&amp;rdquo; &amp;gt;}} . version-check &amp;gt;}}
为 Pod 配置卷 在本练习中，您将创建一个运行 Pod，该 Pod 仅运行一个容器并拥有一个类型为 emptyDir 的卷，在整个 Pod 生命周期中一直存在，即使 Pod 中的容器被终止和重启。以下是 Pod 的配置：
. codenew file=&amp;quot;pods/storage/redis.yaml&amp;rdquo; &amp;gt;}}
 创建 Pod:  ```shell kubectl apply -f https://k8s.io/examples/pods/storage/redis.yaml ```   验证 Pod 中的容器是否正在运行，然后留意 Pod 的更改：  ```shell kubectl get pod redis --watch ``` 输出如下： ```shell NAME READY STATUS RESTARTS AGE redis 1/1 Running 0 13s ```   在另一个终端，用 shell 连接正在运行的容器：  ```shell kubectl exec -it redis -- /bin/bash ```   在您的 shell 终端中，切换到 /data/redis 目录下，然后创建一个文件：  ```shell root@redis:/data# cd /data/redis/ root@redis:/data/redis# echo Hello &amp;gt; test-file ```   在您的 shell 终端中，列出正在运行的进程：  ```shell root@redis:/data/redis# apt-get update root@redis:/data/redis# apt-get install procps root@redis:/data/redis# ps aux ``` 输出类似于： ```shell USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND redis 1 0.</description>
    </item>
    
    <item>
      <title>集群网络系统</title>
      <link>https://lijun.in/concepts/cluster-administration/networking/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/concepts/cluster-administration/networking/</guid>
      <description>集群网络系统是 Kubernetes 的核心部分，但是想要准确了解它的工作原理可是个不小的挑战。下面列出的是网络系统的的四个主要问题：
 高度耦合的容器间通信：这个已经被 pods 和 localhost 通信解决了。 Pod 间通信：这个是本文档的重点要讲述的。 Pod 和 Service 间通信：这个已经在 services 里讲述过了。 外部和 Service 间通信：这个也已经在 services 讲述过了。  Kubernetes 的宗旨就是在应用之间共享机器。通常来说，共享机器需要两个应用之间不能使用相同的端口，但是在多个应用开发者之间去大规模地协调端口是件很困难的事情，尤其是还要让用户暴露在他们控制范围之外的集群级别的问题上。
动态分配端口也会给系统带来很多复杂度 - 每个应用都需要设置一个端口的参数，而 API 服务器还需要知道如何将动态端口数值插入到配置模块中，服务也需要知道如何找到对方等等。与其去解决这些问题，Kubernetes 选择了其他不同的方法。
Kubernetes 网络模型 每一个 Pod 都有它自己的IP地址，这就意味着你不需要显式地在每个 Pod 之间创建链接，你几乎不需要处理容器端口到主机端口之间的映射。这将创建一个干净的、向后兼容的模型，在这个模型里，从端口分配、命名、服务发现、负载均衡、应用配置和迁移的角度来看，Pod 可以被视作虚拟机或者物理主机。
Kubernetes 对所有网络设施的实施，都需要满足以下的基本要求（除非有设置一些特定的网络分段策略）：
 节点上的 pods 可以不通过 NAT 和其他任何节点上的 pods 通信 节点上的代理（比如：系统守护进程、kubelet） 可以和节点上的所有pods通信  备注：仅针对那些支持 Pods 在主机网络中运行的平台(比如：Linux) ：
 那些运行在节点的主机网络里的 pods 可以不通过 NAT 和所有节点上的 pods 通信  这个模型不仅不复杂，而且还和 Kubernetes 的实现廉价的从虚拟机向容器迁移的初衷相兼容，如果你的工作开始是在虚拟机中运行的，你的虚拟机有一个 IP ，这样就可以和其他的虚拟机进行通信，这是基本相同的模型。</description>
    </item>
    
    <item>
      <title>高可用拓扑选项</title>
      <link>https://lijun.in/setup/production-environment/tools/kubeadm/ha-topology/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/setup/production-environment/tools/kubeadm/ha-topology/</guid>
      <description>本页面介绍了配置高可用（HA） Kubernetes 集群拓扑的两个选项。
您可以设置 HA 集群：
 使用堆叠（stacked）控制平面节点，其中 etcd 节点与控制平面节点共存 使用外部 etcd 节点，其中 etcd 在与控制平面不同的节点上运行  在设置 HA 集群之前，您应该仔细考虑每种拓扑的优缺点。
. note &amp;gt;}} kubeadm 静态引导 etcd 集群。 阅读 etcd 集群指南以获得更多详细信息。 . /note &amp;gt;}}
堆叠（Stacked） etcd 拓扑 堆叠（Stacked） HA 集群是一种这样的拓扑，其中 etcd 分布式数据存储集群堆叠在 kubeadm 管理的控制平面节点上，作为控制平面的一个组件运行。
每个控制平面节点运行 kube-apiserver，kube-scheduler 和 kube-controller-manager 实例。
kube-apiserver 使用负载均衡器暴露给工作节点。
每个控制平面节点创建一个本地 etcd 成员（member），这个 etcd 成员只与该节点的 kube-apiserver 通信。这同样适用于本地 kube-controller-manager 和 kube-scheduler 实例。
这种拓扑将控制平面和 etcd 成员耦合在同一节点上。相对使用外部 etcd 集群，设置起来更简单，而且更易于副本管理。
然而，堆叠集群存在耦合失败的风险。如果一个节点发生故障，则 etcd 成员和控制平面实例都将丢失，并且冗余会受到影响。您可以通过添加更多控制平面节点来降低此风险。
因此，您应该为 HA 集群运行至少三个堆叠的控制平面节点。</description>
    </item>
    
    <item>
      <title>kubeadm reset</title>
      <link>https://lijun.in/reference/setup-tools/kubeadm/kubeadm-reset/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/setup-tools/kubeadm/kubeadm-reset/</guid>
      <description>该命令尽力还原由 kubeadm init 或 kubeadm join 所做的更改。
. include &amp;ldquo;generated/kubeadm_reset.md&amp;rdquo; &amp;gt;}}
Reset 工作流程 kubeadm reset 负责从使用 kubeadm init 或 kubeadm join 命令创建的文件中清除节点本地文件系统。对于控制平面节点，reset 还从 etcd 集群中删除该节点的本地 etcd 堆成员，还从 kubeadm ClusterStatus 对象中删除该节点的信息。 ClusterStatus 是一个 kubeadm 管理的 Kubernetes API 对象，该对象包含 kube-apiserver 端点列表。
kubeadm reset phase 可用于执行上述工作流程的各个阶段。 要跳过阶段列表，您可以使用 --skip-phases 参数，该参数的工作方式类似于 kubeadm join 和 kubeadm init 阶段运行器。
外部 etcd 清理 如果使用了外部 etcd，kubeadm reset 将不会删除任何 etcd 中的数据。这意味着，如果再次使用相同的 etcd 端点运行 kubeadm init，您将看到先前集群的状态。
要清理 etcd 中的数据，建议您使用 etcdctl 这样的客户端，例如：</description>
    </item>
    
    <item>
      <title>Kubernetes 控制面的指标</title>
      <link>https://lijun.in/concepts/cluster-administration/monitoring/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/concepts/cluster-administration/monitoring/</guid>
      <description>系统组件的指标可以让我们更好的看清系统内部究竟发生了什么，尤其对于构建仪表盘和告警都非常有用。
Kubernetes 控制面板中的指标是以 prometheus 格式发出的，而且是易于阅读的。
Kubernetes 的指标 在大多数情况下，指标在 HTTP 服务器的 /metrics 端点使用，对于默认情况下不暴露端点的组件，可以使用 --bind-address 参数启用。
举例下面这些组件：
 term_id=&amp;quot;kube-controller-manager&amp;rdquo; text=&amp;quot;kube-controller-manager&amp;rdquo; &amp;gt;}} term_id=&amp;quot;kube-proxy&amp;rdquo; text=&amp;quot;kube-proxy&amp;rdquo; &amp;gt;}} term_id=&amp;quot;kube-apiserver&amp;rdquo; text=&amp;quot;kube-apiserver&amp;rdquo; &amp;gt;}} term_id=&amp;quot;kube-scheduler&amp;rdquo; text=&amp;quot;kube-scheduler&amp;rdquo; &amp;gt;}} term_id=&amp;quot;kubelet&amp;rdquo; text=&amp;quot;kubelet&amp;rdquo; &amp;gt;}}  在生产环境中，你可能需要配置 Prometheus Server 或其他指标收集器来定期收集这些指标，并使它们在某种时间序列数据库中可用。
请注意 term_id=&amp;quot;kubelet&amp;rdquo; text=&amp;quot;kubelet&amp;rdquo; &amp;gt;}} 同样在 /metrics/cadvisor、/metrics/resource 和 /metrics/probes 等端点提供性能指标。这些指标的生命周期并不相同。
如果你的集群还使用了 term_id=&amp;quot;rbac&amp;rdquo; text=&amp;quot;RBAC&amp;rdquo; &amp;gt;}} ，那读取指标数据的时候，还需要通过具有 ClusterRole 的用户、组或者 ServiceAccount 来进行授权，才有权限访问 /metrics 。
举例：
apiVersion: rbac.authorization.k8s.io/v1	kind: ClusterRole	metadata:	name: prometheus	rules:	- nonResourceURLs:	- &amp;quot;/metrics&amp;quot;	verbs:	- get 指标的生命周期 内测版指标 → 稳定版指标 → 弃用指标 → 隐藏指标 → 删除</description>
    </item>
    
    <item>
      <title>使用 HostAliases 向 Pod /etc/hosts 文件添加条目</title>
      <link>https://lijun.in/concepts/services-networking/add-entries-to-pod-etc-hosts-with-host-aliases/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/concepts/services-networking/add-entries-to-pod-etc-hosts-with-host-aliases/</guid>
      <description>当 DNS 配置以及其它选项不合理的时候，通过向 Pod 的 /etc/hosts 文件中添加条目，可以在 Pod 级别覆盖对主机名的解析。在 1.7 版本，用户可以通过 PodSpec 的 HostAliases 字段来添加这些自定义的条目。
建议通过使用 HostAliases 来进行修改，因为该文件由 Kubelet 管理，并且可以在 Pod 创建/重启过程中被重写。
默认 hosts 文件内容 让我们从一个 Nginx Pod 开始，给该 Pod 分配一个 IP：
kubectl run nginx --image nginx --generator=run-pod/v1 pod/nginx created 检查Pod IP：
kubectl get pods --output=wide NAME READY STATUS RESTARTS AGE IP NODE nginx 1/1 Running 0 13s 10.200.0.4 worker0 主机文件的内容如下所示：
kubectl exec nginx -- cat /etc/hosts # Kubernetes-managed hosts file.</description>
    </item>
    
    <item>
      <title>使用 kubeconfig 文件组织集群访问</title>
      <link>https://lijun.in/concepts/configuration/organize-cluster-access-kubeconfig/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/concepts/configuration/organize-cluster-access-kubeconfig/</guid>
      <description>使用 kubeconfig 文件来组织有关集群、用户、命名空间和身份认证机制的信息。kubectl 命令行工具使用 kubeconfig 文件来查找选择集群所需的信息，并与集群的 API 服务器进行通信。
用于配置集群访问的文件称为 kubeconfig 文件。这是引用配置文件的通用方法。这并不意味着有一个名为 kubeconfig 的文件
默认情况下，kubectl 在 $HOME/.kube 目录下查找名为 config 的文件。您可以通过设置 KUBECONFIG 环境变量或者设置--kubeconfig参数来指定其他 kubeconfig 文件。
有关创建和指定 kubeconfig 文件的分步说明，请参阅配置对多集群的访问。
支持多集群、用户和身份认证机制 假设您有多个集群，并且您的用户和组件以多种方式进行身份认证。比如：
 正在运行的 kubelet 可能使用证书在进行认证。 用户可能通过令牌进行认证。 管理员可能拥有多个证书集合提供给各用户。  使用 kubeconfig 文件，您可以组织集群、用户和命名空间。您还可以定义上下文，以便在集群和命名空间之间快速轻松地切换。
上下文（Context） 通过 kubeconfig 文件中的 context 元素，使用简便的名称来对访问参数进行分组。每个上下文都有三个参数：cluster、namespace 和 user。默认情况下，kubectl 命令行工具使用 当前上下文 中的参数与集群进行通信。
选择当前上下文
kubectl config use-context KUBECONFIG 环境变量 KUBECONFIG 环境变量包含一个 kubeconfig 文件列表。对于 Linux 和 Mac，列表以冒号分隔。对于 Windows，列表以分号分隔。KUBECONFIG 环境变量不是必要的。如果 KUBECONFIG 环境变量不存在，kubectl 使用默认的 kubeconfig 文件，$HOME/.kube/config。
如果 KUBECONFIG 环境变量存在，kubectl 使用 KUBECONFIG 环境变量中列举的文件合并后的有效配置。</description>
    </item>
    
    <item>
      <title>使用服务来访问集群中的应用</title>
      <link>https://lijun.in/tasks/access-application-cluster/service-access-application-cluster/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/access-application-cluster/service-access-application-cluster/</guid>
      <description>本文展示如何创建一个 Kubernetes 服务对象，能让外部客户端访问在集群中运行的应用。该服务为一个应用的两个运行实例提供负载均衡。
. heading &amp;ldquo;prerequisites&amp;rdquo; %}} . include &amp;ldquo;task-tutorial-prereqs.md&amp;rdquo; &amp;gt;}} . version-check &amp;gt;}}
. heading &amp;ldquo;objectives&amp;rdquo; %}}  运行 Hello World 应用的两个实例。 创建一个服务对象来暴露 node port。 使用服务对象来访问正在运行的应用。  为运行在两个 pod 中的应用创建一个服务 这是应用程序部署的配置文件：
. codenew file=&amp;quot;service/access/hello-application.yaml&amp;rdquo; &amp;gt;}}
 在您的集群中运行一个 Hello World 应用： 使用上面的文件创建应用程序 Deployment： kubectl apply -f https://k8s.io/examples/service/access/hello-application.yaml 上面的命令创建一个 Deployment 对象和一个关联的 ReplicaSet 对象。这个 ReplicaSet 有两个 Pod，每个 Pod 都运行着 Hello World 应用。
   展示 Deployment 的信息： kubectl get deployments hello-world kubectl describe deployments hello-world    展示您的 ReplicaSet 对象信息： kubectl get replicasets kubectl describe replicasets    创建一个服务对象来暴露 deployment： kubectl expose deployment hello-world --type=NodePort --name=example-service    展示服务信息： kubectl describe services example-service   输出类似于：</description>
    </item>
    
    <item>
      <title>删除 StatefulSet</title>
      <link>https://lijun.in/tasks/run-application/delete-stateful-set/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/run-application/delete-stateful-set/</guid>
      <description>本文介绍如何删除 StatefulSet。
. heading &amp;ldquo;prerequisites&amp;rdquo; %}}  本文假设在您的集群上已经运行了由 StatefulSet 创建的应用。  删除 StatefulSet 您可以像删除 Kubernetes 中的其他资源一样删除 StatefulSet：使用 kubectl delete 命令，并按文件或者名字指定 StatefulSet。
kubectl delete -f &amp;lt;file.yaml&amp;gt; kubectl delete statefulsets &amp;lt;statefulset-name&amp;gt; 删除 StatefulSet 之后，您可能需要单独删除关联的无头服务。
kubectl delete service &amp;lt;service-name&amp;gt; 通过 kubectl 删除 StatefulSet 会将其缩容为0，因此删除属于它的所有pods。 如果您只想删除 StatefulSet 而不删除 pods，使用 --cascade=false。
kubectl delete -f &amp;lt;file.yaml&amp;gt; --cascade=false 通过将 --cascade=false 传递给 kubectl delete，在删除 StatefulSet 对象之后，StatefulSet 管理的 pods 会被保留下来。如果 pods 有一个标签 app=myapp，则可以按照如下方式删除它们：
kubectl delete pods -l app=myapp Persistent Volumes 删除 StatefulSet 管理的 pods 并不会删除关联的卷。这是为了确保您有机会在删除卷之前从卷中复制数据。在pods离开终止状态后删除 PVC 可能会触发删除支持的 Persistent Volumes，具体取决于存储类和回收策略。声明删除后，您永远不应该假设能够访问卷。</description>
    </item>
    
    <item>
      <title>利用 kubeadm 创建高可用集群</title>
      <link>https://lijun.in/setup/production-environment/tools/kubeadm/high-availability/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/setup/production-environment/tools/kubeadm/high-availability/</guid>
      <description>本文讲述了使用 kubeadm 设置一个高可用的 Kubernetes 集群的两种不同方式：
 使用堆控制平面节点。这种方法所需基础设施较少。etcd 成员和控制平面节点位于同一位置。 使用外部集群。这种方法所需基础设施较多。控制平面的节点和 etcd 成员是分开的。  在下一步之前，您应该仔细考虑哪种方法更好的满足您的应用程序和环境的需求。 这是对比文档 讲述了每种方法的优缺点。
如果您在安装 HA 集群时遇到问题，请在 kubeadm 问题跟踪里向我们提供反馈。
您也可以阅读 升级文件。
. caution &amp;gt;}} 这篇文档没有讲述在云提供商上运行集群的问题。在云环境中，此处记录的方法不适用于类型为 LoadBalancer 的服务对象，或者具有动态的 PersistentVolumes。 . /caution &amp;gt;}}
. heading &amp;ldquo;prerequisites&amp;rdquo; %}} 对于这两种方法，您都需要以下基础设施：
 配置三台机器kubeadm 的最低要求给主节点 配置三台机器 kubeadm 的最低要求 给工作节点 在集群中，所有计算机之间的完全网络连接（公网或私网） 所有机器上的 sudo 权限 每台设备对系统中所有节点的 SSH 访问 在所有机器上安装 kubeadm 和 kubelet，kubectl 是可选的。  仅对于外部 etcd 集群来说，您还需要：
 给 etcd 成员使用的另外三台机器  这两种方法的第一步 为 kube-apiserver 创建负载均衡器 . note &amp;gt;}} 使用负载均衡器需要许多配置。您的集群搭建可能需要不同的配置。下面的例子只是其中的一方面配置。 .</description>
    </item>
    
    <item>
      <title>垃圾收集</title>
      <link>https://lijun.in/concepts/workloads/controllers/garbage-collection/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/concepts/workloads/controllers/garbage-collection/</guid>
      <description>Kubernetes 垃圾收集器的作用是删除某些曾经拥有所有者（owner）但现在不再拥有所有者的对象。
所有者和附属 某些 Kubernetes 对象是其它一些对象的所有者。例如，一个 ReplicaSet 是一组 Pod 的所有者。 具有所有者的对象被称为是所有者的 附属 。 每个附属对象具有一个指向其所属对象的 metadata.ownerReferences 字段。
有时，Kubernetes 会自动设置 ownerReference 的值。 例如，当创建一个 ReplicaSet 时，Kubernetes 自动设置 ReplicaSet 中每个 Pod 的 ownerReference 字段值。 在 Kubernetes 1.8 版本，Kubernetes 会自动为某些对象设置 ownerReference 的值，这些对象是由 ReplicationController、ReplicaSet、StatefulSet、DaemonSet、Deployment、Job 和 CronJob 所创建或管理。 也可以通过手动设置 ownerReference 的值，来指定所有者和附属之间的关系。
这里有一个配置文件，表示一个具有 3 个 Pod 的 ReplicaSet：
codenew file=&amp;quot;controllers/replicaset.yaml&amp;rdquo; &amp;gt;}}
如果创建该 ReplicaSet，然后查看 Pod 的 metadata 字段，能够看到 OwnerReferences 字段：
kubectl apply -f https://k8s.io/examples/controllers/replicaset.yaml kubectl get pods --output=yaml 输出显示了 Pod 的所有者是名为 my-repset 的 ReplicaSet：</description>
    </item>
    
    <item>
      <title>字段选择器</title>
      <link>https://lijun.in/concepts/overview/working-with-objects/field-selectors/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/concepts/overview/working-with-objects/field-selectors/</guid>
      <description>字段选择器（Field selectors）允许您根据一个或多个资源字段的值筛选 Kubernetes 资源。 下面是一些使用字段选择器查询的例子：
 metadata.name=my-service metadata.namespace!=default status.phase=Pending  下面这个 kubectl 命令将筛选出 status.phase 字段值为 Running 的所有 Pod：
kubectl get pods --field-selector status.phase=Running 字段选择器本质上是资源过滤器。默认情况下，字段选择器/过滤器是未被应用的，这意味着指定类型的所有资源都会被筛选出来。 这使得以下的两个 kubectl 查询是等价的：
kubectl get pods kubectl get pods --field-selector &amp;#34;&amp;#34; 支持的字段 不同的 Kubernetes 资源类型支持不同的字段选择器。所有资源类型都支持 metadata.name 和 metadata.namespace 字段。使用不被支持的字段选择器会产生错误，例如：
kubectl get ingress --field-selector foo.bar=baz Error from server (BadRequest): Unable to find &amp;quot;ingresses&amp;quot; that match label selector &amp;quot;&amp;quot;, field selector &amp;quot;foo.bar=baz&amp;quot;: &amp;quot;foo.bar&amp;quot; is not a known field selector: only &amp;quot;metadata.</description>
    </item>
    
    <item>
      <title>干扰</title>
      <link>https://lijun.in/concepts/workloads/pods/disruptions/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/concepts/workloads/pods/disruptions/</guid>
      <description>本指南针对的是希望构建高可用性应用程序的应用所有者，他们有必要了解可能发生在 pod 上的干扰类型。
文档同样适用于想要执行自动化集群操作（例如升级和自动扩展集群）的集群管理员。
自愿干扰和非自愿干扰 Pod 不会消失，除非有人（用户或控制器）将其销毁，或者出现了不可避免的硬件或软件系统错误。
我们把这些不可避免的情况称为应用的非自愿干扰。例如：
 节点下层物理机的硬件故障 集群管理员错误地删除虚拟机（实例） 云提供商或虚拟机管理程序中的故障导致的虚拟机消失 内核错误 节点由于集群网络隔离从集群中消失 由于节点资源不足导致 pod 被驱逐。  除了资源不足的情况，大多数用户应该都熟悉这些情况；它们不是特定于 Kubernetes 的。
我们称其他情况为自愿干扰。包括由应用程序所有者发起的操作和由集群管理员发起的操作。典型的应用程序所有者的 作包括：
 删除 deployment 或其他管理 pod 的控制器 更新了 deployment 的 pod 模板导致 pod 重启 直接删除 pod（例如，因为误操作）  集群管理员操作包括：
 排空（drain）节点进行修复或升级。 从集群中排空节点以缩小集群（了解集群自动扩缩）。 从节点中移除一个 pod，以允许其他 pod 使用该节点。  这些操作可能由集群管理员直接执行，也可能由集群管理员所使用的自动化工具执行，或者由集群托管提供商自动执行。
咨询集群管理员或联系云提供商，或者查询发布文档，以确定是否为集群启用了任何资源干扰源。如果没有启用，可以不用创建 Pod Disruption Budgets（Pod 干扰预算）
并非所有的自愿干扰都会受到 pod 干扰预算的限制。例如，删除 deployment 或 pod 的删除操作就会跳过 pod 干扰预算检查。
处理干扰 以下是减轻非自愿干扰的一些方法：
 确保 pod请求所需资源。 如果需要更高的可用性，请复制应用程序。（了解有关运行多副本的无状态和有状态应用程序的信息。） 为了在运行复制应用程序时获得更高的可用性，请跨机架（使用反亲和性）或跨区域（如果使用多区域集群）扩展应用程序。  自愿干扰的频率各不相同。在一个基本的 Kubernetes 集群中，根本没有自愿干扰。然而，集群管理 或托管提供商可能运行一些可能导致自愿干扰的额外服务。例如，节点软 更新可能导致自愿干扰。另外，集群（节点）自动缩放的某些 现可能导致碎片整理和紧缩节点的自愿干扰。集群 理员或托管提供商应该已经记录了各级别的自愿干扰（如果有的话）。</description>
    </item>
    
    <item>
      <title>授权概述</title>
      <link>https://lijun.in/reference/access-authn-authz/authorization/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/access-authn-authz/authorization/</guid>
      <description>了解有关 Kubernetes 授权的更多信息，包括使用支持的授权模块创建策略的详细信息。
在 Kubernetes 中，您必须在授权（授予访问权限）之前进行身份验证（登录），有关身份验证的信息， 请参阅 访问控制概述.
Kubernetes 期望 REST API 请求中常见的属性。 这意味着 Kubernetes 授权适用于现有的组织范围或云提供商范围的访问控制系统， 除了 Kubernetes API 之外，它还可以处理其他 API。
确定是允许还是拒绝请求 Kubernetes 使用 API ​​服务器授权 API 请求。它根据所有策略评估所有请求属性来决定允许或拒绝请求。 一个 API 请求的所有部分必须被某些策略允许才能继续。这意味着默认情况下拒绝权限。
（尽管 Kubernetes 使用 API ​​服务器，但是依赖于特定种类对象的特定字段的访问控制和策略由准入控制器处理。）
配置多个授权模块时，将按顺序检查每个模块。 如果任何授权模块批准或拒绝请求，则立即返回该决定，并且不会与其他授权模块协商。 如果所有模块对请求没有意见，则拒绝该请求。一个拒绝响应返回 HTTP 状态代码 403 。
审查您的请求属性 Kubernetes 仅审查以下 API 请求属性：
 user - 身份验证期间提供的 user 字符串。 group - 经过身份验证的用户所属的组名列表。 extra - 由身份验证层提供的任意字符串键到字符串值的映射。 API - 指示请求是否针对 API 资源。 Request path - 各种非资源端点的路径，如 /api 或 /healthz。 API request verb - API 动词 get，list，create，update，patch，watch，proxy，redirect，delete 和 deletecollection 用于资源请求。要确定资源 API 端点的请求动词，请参阅确定请求动词。 HTTP request verb - HTTP 动词 get，post，put 和 delete 用于非资源请求。 Resource - 正在访问的资源的 ID 或名称（仅限资源请求） - 对于使用 get，update，patch 和 delete 动词的资源请求，您必须提供资源名称。 Subresource - 正在访问的子资源（仅限资源请求）。 Namespace - 正在访问的对象的名称空间（仅适用于命名空间资源请求）。 API group - 正在访问的 API 组（仅限资源请求）。空字符串表示核心 API 组。  确定请求动词 要确定资源 API 端点的请求谓词，请检查所使用的 HTTP 动词以及请求是否对单个资源或资源集合起作用：</description>
    </item>
    
    <item>
      <title>日志架构</title>
      <link>https://lijun.in/concepts/cluster-administration/logging/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/concepts/cluster-administration/logging/</guid>
      <description>应用和系统日志可以让您了解集群内部的运行状况。日志对调试问题和监控集群活动非常有用。大部分现代化应用都有某种日志记录机制；同样地，大多数容器引擎也被设计成支持某种日志记录机制。针对容器化应用，最简单且受欢迎的日志记录方式就是写入标准输出和标准错误流。
但是，由容器引擎或 runtime 提供的原生功能通常不足以满足完整的日志记录方案。例如，如果发生容器崩溃、pod 被逐出或节点宕机等情况，您仍然想访问到应用日志。因此，日志应该具有独立的存储和生命周期，与节点、pod 或容器的生命周期相独立。这个概念叫 集群级的日志 。集群级日志方案需要一个独立的后台来存储、分析和查询日志。Kubernetes 没有为日志数据提供原生存储方案，但是您可以集成许多现有的日志解决方案到 Kubernetes 集群中。
集群级日志架构假定在集群内部或者外部有一个日志后台。如果您对集群级日志不感兴趣，您仍会发现关于如何在节点上存储和处理日志的描述对您是有用的。
Kubernetes 中的基本日志记录 本节，您会看到一个kubernetes 中生成基本日志的例子，该例子中数据被写入到标准输出。 这里通过一个特定的 pod 规约 演示创建一个容器，并令该容器每秒钟向标准输出写入数据。
用下面的命令运行 pod：
kubectl apply -f https://k8s.io/examples/debug/counter-pod.yaml 输出结果为：
pod/counter created 使用 kubectl logs 命令获取日志:
kubectl logs counter 输出结果为：
0: Mon Jan 1 00:00:00 UTC 2001 1: Mon Jan 1 00:00:01 UTC 2001 2: Mon Jan 1 00:00:02 UTC 2001 ... 一旦发生容器崩溃，您可以使用命令 kubectl logs 和参数 --previous 检索之前的容器日志。 如果 pod 中有多个容器，您应该向该命令附加一个容器名以访问对应容器的日志。 详见 kubectl logs 文档。</description>
    </item>
    
    <item>
      <title>知名标签（Label）、注解（Annotation）和 Taints</title>
      <link>https://lijun.in/reference/kubernetes-api/labels-annotations-taints/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/kubernetes-api/labels-annotations-taints/</guid>
      <description>Kubernetes 保留了 kubernetes.io 命名空间下的所有标签和注解。
本文既作为这些标签和注解的参考，也就这些标签和注解的赋值进行了说明。
kubernetes.io/arch 示例：kubernetes.io/arch=amd64
用于：Node
Kubelet 用 Go 中定义的 runtime.GOARCH 值来填充该标签。这在诸如混用 arm 和 x86 节点的情况下很有用。
kubernetes.io/os 示例：kubernetes.io/os=linux
用于：Node
Kubelet 用该 Go 中定义的 runtime.GOOS 值来填充该标签。这在集群中存在不同操作系统的节点时很有用（例如：混合 Linux 和 Windows 操作系统的节点）。
beta.kubernetes.io/arch (已弃用) 该标签已被弃用。请使用 kubernetes.io/arch。
beta.kubernetes.io/os (已弃用) 该标签已被弃用。请使用 kubernetes.io/arch。
kubernetes.io/hostname 示例：kubernetes.io/hostname=ip-172-20-114-199.ec2.internal
用于：Node
Kubelet 用 hostname 值来填充该标签。注意：可以通过向 kubelet 传入 --hostname-override 参数对 “真正的” hostname 进行修改。
beta.kubernetes.io/instance-type (已弃用) . note &amp;gt;}}
从 kubernetes 1.17 版本开始，不推荐使用此标签，而推荐使用node.kubernetes.io/instance-type。 . /note &amp;gt;}}
node.kubernetes.io/instance-type 示例：node.kubernetes.io/instance-type=m3.medium
用于：Node
Kubelet 用 cloudprovider 中定义的实例类型来填充该标签。未使用 cloudprovider 时不会设置该标签。该标签在想要将某些负载定向到特定实例类型的节点上时会很有用，但通常用户更希望依赖 Kubernetes 调度器来执行基于资源的调度，所以用户应该致力于基于属性而不是实例类型来进行调度(例如：需要一个 GPU，而不是 g2.</description>
    </item>
    
    <item>
      <title>调度框架</title>
      <link>https://lijun.in/concepts/scheduling-eviction/scheduling-framework/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/concepts/scheduling-eviction/scheduling-framework/</guid>
      <description>feature-state for_k8s_version=&amp;quot;1.15&amp;rdquo; state=&amp;quot;alpha&amp;rdquo; &amp;gt;}}
调度框架是 Kubernetes Scheduler 的一种可插入架构，可以简化调度器的自定义。它向现有的调度器增加了一组新的“插件” API。插件被编译到调度器程序中。这些 API 允许大多数调度功能以插件的形式实现，同时使调度“核心”保持简单且可维护。请参考调度框架的设计提案获取框架设计的更多技术信息。
框架工作流程 调度框架定义了一些扩展点。调度器插件注册后在一个或多个扩展点处被调用。这些插件中的一些可以改变调度决策，而另一些仅用于提供信息。
每次调度一个 Pod 的尝试都分为两个阶段，即 调度周期 和 绑定周期。
调度周期和绑定周期 调度周期为 Pod 选择一个节点，绑定周期将该决策应用于集群。调度周期和绑定周期一起被称为“调度上下文”。
调度周期是串行运行的，而绑定周期可能是同时运行的。
如果确定 Pod 不可调度或者存在内部错误，则可以终止调度周期或绑定周期。Pod 将返回队列并重试。
扩展点 下图显示了一个 Pod 的调度上下文以及调度框架公开的扩展点。在此图片中，“过滤器”等同于“断言”，“评分”相当于“优先级函数”。
一个插件可以在多个扩展点处注册，以执行更复杂或有状态的任务。
figure src=&amp;rdquo;/images/docs/scheduling-framework-extensions.png&amp;rdquo; title=&amp;quot;调度框架扩展点&amp;rdquo; &amp;gt;}}
队列排序 队列排序插件用于对调度队列中的 Pod 进行排序。队列排序插件本质上提供 &amp;ldquo;less(Pod1, Pod2)&amp;rdquo; 函数。一次只能启动一个队列插件。
前置过滤 前置过滤插件用于预处理 Pod 的相关信息，或者检查集群或 Pod 必须满足的某些条件。如果 PreFilter 插件返回错误，则调度周期将终止。
过滤 过滤插件用于过滤出不能运行该 Pod 的节点。对于每个节点，调度器将按照其配置顺序调用这些过滤插件。如果任何过滤插件将节点标记为不可行，则不会为该节点调用剩下的过滤插件。节点可以被同时进行评估。
前置评分 前置评分插件用于执行 “前置评分” 工作，即生成一个可共享状态供评分插件使用。如果 PreScore 插件返回错误，则调度周期将终止。
评分 评分插件用于对通过过滤阶段的节点进行排名。调度器将为每个节点调用每个评分插件。将有一个定义明确的整数范围，代表最小和最大分数。在标准化评分阶段之后，调度器将根据配置的插件权重合并所有插件的节点分数。
标准化评分 标准化评分插件用于在调度器计算节点的排名之前修改分数。在此扩展点注册的插件将使用同一插件的评分 结果被调用。每个插件在每个调度周期调用一次。
例如，假设一个 BlinkingLightScorer 插件基于具有的闪烁指示灯数量来对节点进行排名。</description>
    </item>
    
    <item>
      <title>配置 Pod 以使用 PersistentVolume 作为存储</title>
      <link>https://lijun.in/tasks/configure-pod-container/configure-persistent-volume-storage/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/configure-pod-container/configure-persistent-volume-storage/</guid>
      <description>本文介绍如何配置 Pod 使用 PersistentVolumeClaim 作为存储。 以下是该过程的总结：
  集群管理员创建由物理存储支持的 PersistentVolume。管理员不将卷与任何 Pod 关联。
  群集用户创建一个 PersistentVolumeClaim，它将自动绑定到合适的 PersistentVolume。
  用户创建一个使用 PersistentVolumeClaim 作为存储的 Pod。
  . heading &amp;ldquo;prerequisites&amp;rdquo; %}}   您需要一个包含单个节点的 Kubernetes 集群，并且必须配置 kubectl 命令行工具以便与集群交互。 如果还没有单节点集群，可以使用 Minikube 创建一个。
  熟悉持久卷中的材料。
  在你的节点上创建一个 index.html 文件 打开集群中节点的一个 shell。 如何打开 shell 取决于集群的设置。 例如，如果您正在使用 Minikube，那么可以通过输入 minikube ssh 来打开节点的 shell。
在 shell 中，创建一个 /mnt/data 目录：
mkdir /mnt/data  在 /mnt/data 目录中创建一个 index.html 文件：</description>
    </item>
    
    <item>
      <title>配置命名空间下pod总数</title>
      <link>https://lijun.in/tasks/administer-cluster/manage-resources/quota-pod-namespace/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/administer-cluster/manage-resources/quota-pod-namespace/</guid>
      <description>本文主要描述如何配置一个命名空间下可运行的pod总数。资源配额详细信息可查看：资源配额 。
. heading &amp;ldquo;prerequisites&amp;rdquo; %}} . include &amp;ldquo;task-tutorial-prereqs.md&amp;rdquo; &amp;gt;}} . version-check &amp;gt;}}
创建一个命名空间 首先创建一个命名空间，这样可以将本次操作中创建的资源与集群其他资源隔离开来。
kubectl create namespace quota-pod-example 创建资源配额 下面是一个资源配额的配置文件：
. codenew file=&amp;quot;admin/resource/quota-pod.yaml&amp;rdquo; &amp;gt;}}
创建这个资源配额：
kubectl apply -f https://k8s.io/examples/admin/resource/quota-pod.yaml --namespace=quota-pod-example 查看资源配额的详细信息：
kubectl get resourcequota pod-demo --namespace=quota-pod-example --output=yaml 从输出的信息我们可以看到，该命名空间下pod的配额是2个，目前创建的pods数为0，配额使用率为0。
spec: hard: pods: &amp;#34;2&amp;#34; status: hard: pods: &amp;#34;2&amp;#34; used: pods: &amp;#34;0&amp;#34; 下面是一个Deployment的配置文件：
. codenew file=&amp;quot;admin/resource/quota-pod-deployment.yaml&amp;rdquo; &amp;gt;}}
在配置文件中， replicas: 3 告诉kubernetes尝试创建三个pods，且运行相同的应用。
创建这个Deployment：
kubectl apply -f https://k8s.io/examples/admin/resource/quota-pod-deployment.yaml --namespace=quota-pod-example 查看Deployment的详细信息：
kubectl get deployment pod-quota-demo --namespace=quota-pod-example --output=yaml 从输出的信息我们可以看到，尽管尝试创建三个pod，但是由于配额的限制，只有两个pod能被成功创建。</description>
    </item>
    
    <item>
      <title>已完成资源的 TTL 控制器</title>
      <link>https://lijun.in/concepts/workloads/controllers/ttlafterfinished/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/concepts/workloads/controllers/ttlafterfinished/</guid>
      <description>feature-state for_k8s_version=&amp;quot;v1.12&amp;rdquo; state=&amp;quot;alpha&amp;rdquo; &amp;gt;}}
TTL 控制器提供了一种 TTL 机制来限制已完成执行的资源对象的生命周期。TTL 控制器目前只处理 Job，可能以后会扩展以处理将完成执行的其他资源，例如 Pod 和自定义资源。
Alpha 免责声明：此功能目前是 alpha 版，并且可以通过 kube-apiserver 和 kube-controller-manager 特性开关 TTLAfterFinished 启用。
TTL 控制器 TTL 控制器现在只支持 Job。集群操作员可以通过指定 Job 的 .spec.ttlSecondsAfterFinished 字段来自动清理已结束的作业（Complete 或 Failed），如下所示的示例。
TTL 控制器假设资源能在执行完成后的 TTL 秒内被清理，也就是当 TTL 过期后。当 TTL 控制器清理资源时，它将做级联删除操作，如删除资源对象的同时也删除其依赖对象。注意，当资源被删除时，由该资源的生命周期保证其终结器（finalizers）等被执行。
可以随时设置 TTL 秒。以下是设置 Job 的 .spec.ttlSecondsAfterFinished 字段的一些示例：
 在资源清单（manifest）中指定此字段，以便 Job 在完成后的某个时间被自动清除。 将此字段设置为存在的、已完成的资源，以采用此新功能。 在创建资源时使用 mutating admission webhook 动态设置该字段。集群管理员可以使用它对完成的资源强制执行 TTL 策略。 使用 mutating admission webhook 在资源完成后动态设置该字段，并根据资源状态、标签等选择不同的 TTL 值。  警告 更新 TTL 秒 请注意，在创建资源或已经执行结束后，仍可以修改其 TTL 周期，例如 Job 的 .</description>
    </item>
    
    <item>
      <title>IPv4/IPv6 双协议栈</title>
      <link>https://lijun.in/concepts/services-networking/dual-stack/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/concepts/services-networking/dual-stack/</guid>
      <description>feature-state for_k8s_version=&amp;quot;v1.16&amp;rdquo; state=&amp;quot;alpha&amp;rdquo; &amp;gt;}}
IPv4/IPv6 双协议栈能够将 IPv4 和 IPv6 地址分配给 text=&amp;quot;Pods&amp;rdquo; term_id=&amp;quot;pod&amp;rdquo; &amp;gt;}} 和 text=&amp;quot;Services&amp;rdquo; term_id=&amp;quot;service&amp;rdquo; &amp;gt;}}。
如果你为 Kubernetes 集群启用了 IPv4/IPv6 双协议栈网络，则该集群将支持同时分配 IPv4 和 IPv6 地址。
支持的功能 在 Kubernetes 集群上启用 IPv4/IPv6 双协议栈可提供下面的功能：
 双协议栈 pod 网络 (每个 pod 分配一个 IPv4 和 IPv6 地址) IPv4 和 IPv6 启用的服务 (每个服务必须是一个单独的地址族) Pod 的集群外出口通过 IPv4 和 IPv6 路由  先决条件 为了使用 IPv4/IPv6 双栈的 Kubernetes 集群，需要满足以下先决条件：
 Kubernetes 1.16 版本及更高版本 提供商支持双协议栈网络（云提供商或其他提供商必须能够为 Kubernetes 节点提供可路由的 IPv4/IPv6 网络接口） 支持双协议栈的网络插件（如 Kubenet 或 Calico）  启用 IPv4/IPv6 双协议栈 要启用 IPv4/IPv6 双协议栈，为集群的相关组件启用 IPv6DualStack 特性门控，并且设置双协议栈的集群网络分配：</description>
    </item>
    
    <item>
      <title>kubeadm 令牌</title>
      <link>https://lijun.in/reference/setup-tools/kubeadm/kubeadm-token/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/setup-tools/kubeadm/kubeadm-token/</guid>
      <description>如使用引导令牌进行身份验证所描述的，引导令牌用于在即将加入集群的节点和主节点间建立双向认证。
kubeadm init 创建了一个有效期为 24 小时的令牌，下面的命令允许您管理令牌，也可以创建和管理新的令牌。
kubeadm token create . include &amp;ldquo;generated/kubeadm_token_create.md&amp;rdquo; &amp;gt;}}
kubeadm token delete . include &amp;ldquo;generated/kubeadm_token_delete.md&amp;rdquo; &amp;gt;}}
kubeadm token generate . include &amp;ldquo;generated/kubeadm_token_generate.md&amp;rdquo; &amp;gt;}}
kubeadm token list . include &amp;ldquo;generated/kubeadm_token_list.md&amp;rdquo; &amp;gt;}}
. heading &amp;ldquo;whatsnext&amp;rdquo; %}}  kubeadm join 引导 Kubernetes 工作节点并将其加入群集  </description>
    </item>
    
    <item>
      <title>使用 kubeadm 创建一个高可用 etcd 集群</title>
      <link>https://lijun.in/setup/production-environment/tools/kubeadm/setup-ha-etcd-with-kubeadm/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/setup/production-environment/tools/kubeadm/setup-ha-etcd-with-kubeadm/</guid>
      <description>. note &amp;gt;}}
在本指南中，当 kubeadm 用作为外部 etcd 节点管理工具，请注意 kubeadm 不计划支持此类节点的证书更换或升级。对于长期规划是使用 etcdadm 增强工具来管理这方面。 . /note &amp;gt;}}
默认情况下，kubeadm 运行单成员的 etcd 集群，该集群由控制面节点上的 kubelet 以静态 Pod 的方式进行管理。由于 etcd 集群只包含一个成员且不能在任一成员不可用时保持运行，所以这不是一种高可用设置。本任务，将告诉您如何在使用 kubeadm 创建一个 kubernetes 集群时创建一个外部 etcd：有三个成员的高可用 etcd 集群。
. heading &amp;ldquo;prerequisites&amp;rdquo; %}}  三个可以通过 2379 和 2380 端口相互通信的主机。本文档使用这些作为默认端口。不过，它们可以通过 kubeadm 的配置文件进行自定义。   每个主机必须 安装有 docker、kubelet 和 kubeadm。   一些可以用来在主机间复制文件的基础设施。例如 ssh 和 scp 就可以满足需求。  建立集群 一般来说，是在一个节点上生成所有证书并且只分发这些必要的文件到其它节点上。
. note &amp;gt;}}
kubeadm 包含生成下述证书所需的所有必要的密码学工具；在这个例子中，不需要其他加密工具。 . /note &amp;gt;}}
  将 kubelet 配置为 etcd 的服务管理器。</description>
    </item>
    
    <item>
      <title>使用 RBAC 鉴权</title>
      <link>https://lijun.in/reference/access-authn-authz/rbac/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/access-authn-authz/rbac/</guid>
      <description>基于角色（Role）的访问控制（RBAC）是一种基于企业中用户的角色来调节控制对计算机或网络资源的访问方法。
RBAC 使用 rbac.authorization.k8s.io . glossary_tooltip text=&amp;quot;API 组&amp;rdquo; term_id=&amp;quot;api-group&amp;rdquo; &amp;gt;}} 来驱动鉴权操作，允许管理员通过 Kubernetes API 动态配置策略。
在 1.8 版本中，RBAC 模式是稳定的并通过 rbac.authorization.k8s.io/v1 API 提供支持。
要启用 RBAC，在启动 API 服务器时添加 --authorization-mode=RBAC 参数。
API 概述 本节介绍 RBAC API 所声明的四种顶级类型。用户可以像与其他 API 资源交互一样， （通过 kubectl、API 调用等方式）与这些资源交互。例如， 命令 kubectl apply -f (resource).yml 可以用在这里的任何一个例子之上。 尽管如此，建议读者循序渐进阅读下面的章节，由浅入深。
Role 和 ClusterRole 在 RBAC API 中，一个角色包含一组相关权限的规则。权限是纯粹累加的（不存在拒绝某操作的规则）。 角色可以用 Role 来定义到某个命名空间上， 或者用 ClusterRole 来定义到整个集群作用域。
一个 Role 只可以用来对某一命名空间中的资源赋予访问权限。 下面的 Role 示例定义到名称为 &amp;ldquo;default&amp;rdquo; 的命名空间，可以用来授予对该命名空间中的 Pods 的读取权限：
apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: namespace: default name: pod-reader rules: - apiGroups: [&amp;#34;&amp;#34;] # &amp;#34;&amp;#34; 指定核心 API 组 resources: [&amp;#34;pods&amp;#34;] verbs: [&amp;#34;get&amp;#34;, &amp;#34;watch&amp;#34;, &amp;#34;list&amp;#34;] ClusterRole 可以授予的权限和 Role 相同， 但是因为 ClusterRole 属于集群范围，所以它也可以授予以下访问权限：</description>
    </item>
    
    <item>
      <title>强制删除 StatefulSet 类型的 Pods</title>
      <link>https://lijun.in/tasks/run-application/force-delete-stateful-set-pod/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/run-application/force-delete-stateful-set-pod/</guid>
      <description>本文介绍了如何删除 StatefulSet 管理的部分 pods，并且解释了这样操作时需要记住的注意事项。
. heading &amp;ldquo;prerequisites&amp;rdquo; %}}  这是一项相当高级的任务，并且可能会违反 StatefulSet 固有的某些属性。 继续任务之前，请熟悉下面列举的注意事项。  StatefulSet 注意事项 在 StatefulSet 的正常操作中，永远不需要强制删除 StatefulSet 管理的 pod。StatefulSet 控制器负责创建，扩容和删除 StatefulSet 管理的 pods。它尝试确保从序号 0 到 N-1 指定数量的 pods 处于活动状态并准备就绪。StatefulSet 确保在任何时候，集群中最多只有一个具有给定标识的 pod。这就是所谓的由 StatefulSet 提供的最多一个的语义。
应谨慎进行手动强制删除操作，因为它可能会违反 StatefulSet 固有的至多一个的语义。StatefulSets 可用于运行分布式和集群级的应用，这些应用需要稳定的网络标识和可靠的存储。这些应用通常配置为具有固定标识固定数量的成员集合。具有相同身份的多个成员可能是灾难性的，并且可能导致数据丢失 (e.g. 基于 quorum 系统中的脑裂场景)。
删除 Pods 您可以使用下面的命令执行优雅地删除 pod:
kubectl delete pods &amp;lt;pod&amp;gt; 为了使上面的方法能够正常终止，Pod 一定不能设置 pod.Spec.TerminationGracePeriodSeconds 为 0。将 pod.Spec.TerminationGracePeriodSeconds 设置为 0s 的做法是不安全的，强烈建议 StatefulSet 类型的 pods 不要使用。优雅删除是安全的，并且会在 kubelet 从 apiserver 中删除名称之前确保 优雅地关闭 pod 。</description>
    </item>
    
    <item>
      <title>调度器性能调优</title>
      <link>https://lijun.in/concepts/scheduling-eviction/scheduler-perf-tuning/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/concepts/scheduling-eviction/scheduler-perf-tuning/</guid>
      <description>feature-state for_k8s_version=&amp;quot;1.14&amp;rdquo; state=&amp;quot;beta&amp;rdquo; &amp;gt;}}
作为 kubernetes 集群的默认调度器，kube-scheduler 主要负责将 Pod 调度到集群的 Node 上。
在一个集群中，满足一个 Pod 调度请求的所有 Node 称之为 可调度 Node。调度器先在集群中找到一个 Pod 的可调度 Node，然后根据一系列函数对这些可调度 Node打分，之后选出其中得分最高的 Node 来运行 Pod。最后，调度器将这个调度决定告知 kube-apiserver，这个过程叫做 绑定。
这篇文章将会介绍一些在大规模 Kubernetes 集群下调度器性能优化的方式。
在大规模集群中，你可以调节调度器的表现来平衡调度的延迟（新 Pod 快速就位）和精度（调度器很少做出糟糕的放置决策）。
你可以通过设置 kube-scheduler 的 percentageOfNodesToScore 来配置这个调优设置。这个 KubeSchedulerConfiguration 设置决定了调度集群中节点的阈值。
设置阈值 percentageOfNodesToScore 选项接受从 0 到 100 之间的整数值。0 值比较特殊，表示 kube-scheduler 应该使用其编译后的默认值。 如果你设置 percentageOfNodesToScore 的值超过了 100，kube-scheduler 的表现等价于设置值为 100。
要修改这个值，编辑 kube-scheduler 的配置文件（通常是 /etc/kubernetes/config/kube-scheduler.yaml），然后重启调度器。
修改完成后，你可以执行
kubectl get componentstatuses 来检查该 kube-scheduler 组件是否健康。输出类似如下：
NAME STATUS MESSAGE ERROR controller-manager Healthy ok scheduler Healthy ok .</description>
    </item>
    
    <item>
      <title>配置 kubelet 垃圾回收策略</title>
      <link>https://lijun.in/concepts/cluster-administration/kubelet-garbage-collection/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/concepts/cluster-administration/kubelet-garbage-collection/</guid>
      <description>垃圾回收是 kubelet 的一个有用功能，它将清理未使用的镜像和容器。
Kubelet 将每分钟对容器执行一次垃圾回收，每五分钟对镜像执行一次垃圾回收。
不建议使用外部垃圾收集工具，因为这些工具可能会删除原本期望存在的容器进而破坏 kubelet 的行为。
镜像回收 Kubernetes 借助于 cadvisor 通过 imageManager 来管理所有镜像的生命周期。
镜像垃圾回收策略只考虑两个因素：HighThresholdPercent 和 LowThresholdPercent。
磁盘使用率超过上限阈值（HighThresholdPercent）将触发垃圾回收。
垃圾回收将删除最近最少使用的镜像，直到磁盘使用率满足下限阈值（LowThresholdPercent）。
容器回收 容器垃圾回收策略考虑三个用户定义变量。MinAge 是容器可以被执行垃圾回收的最小生命周期。MaxPerPodContainer 是每个 pod 内允许存在的死亡容器的最大数量。 MaxContainers 是全部死亡容器的最大数量。可以分别独立地通过将 MinAge 设置为 0，以及将 MaxPerPodContainer 和 MaxContainers 设置为小于 0 来禁用这些变量。
Kubelet 将处理无法辨识的、已删除的以及超出前面提到的参数所设置范围的容器。最老的容器通常会先被移除。 MaxPerPodContainer 和 MaxContainer 在某些场景下可能会存在冲突，例如在保证每个 pod 内死亡容器的最大数量（MaxPerPodContainer）的条件下可能会超过允许存在的全部死亡容器的最大数量（MaxContainer）。 MaxPerPodContainer 在这种情况下会被进行调整：最坏的情况是将 MaxPerPodContainer 降级为 1，并驱逐最老的容器。 此外，pod 内已经被删除的容器一旦年龄超过 MinAge 就会被清理。
不被 kubelet 管理的容器不受容器垃圾回收的约束。
用户配置 用户可以使用以下 kubelet 参数调整相关阈值来优化镜像垃圾回收：
  image-gc-high-threshold，触发镜像垃圾回收的磁盘使用率百分比。默认值为 85%。
  image-gc-low-threshold，镜像垃圾回收试图释放资源后达到的磁盘使用率百分比。默认值为 80%。
  我们还允许用户通过以下 kubelet 参数自定义垃圾收集策略：</description>
    </item>
    
    <item>
      <title>配置 Pod 使用投射卷作存储</title>
      <link>https://lijun.in/tasks/configure-pod-container/configure-projected-volume-storage/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/configure-pod-container/configure-projected-volume-storage/</guid>
      <description>本文介绍怎样通过投射 卷将现有的多个卷资源挂载到相同的目录。 当前，secret、configMap、downwardAPI 和 serviceAccountToken 卷可以被投射。
. note &amp;gt;}}
serviceAccountToken 不是一种卷类型 . /note &amp;gt;}}
. heading &amp;ldquo;prerequisites&amp;rdquo; %}} . include &amp;ldquo;task-tutorial-prereqs.md&amp;rdquo; &amp;gt;}} . version-check &amp;gt;}}
为 Pod 配置投射卷 本练习中，您将从本地文件来创建包含有用户名和密码的 Secret。然后创建运行一个容器的 Pod，该 Pod 使用投射 卷将 Secret 挂载到相同的路径下。
下面是 Pod 的配置文件：
. codenew file=&amp;quot;pods/storage/projected.yaml&amp;rdquo; &amp;gt;}}
    &amp;lt;!--# Create files containing the username and password:--&amp;gt;# 创建包含用户名和密码的文件: echo -n &amp;#34;admin&amp;#34; &amp;gt; ./username.txt echo -n &amp;#34;1f2d1e2e67df&amp;#34; &amp;gt; ./password.txt--&amp;gt; &amp;lt;!--# Package these files into secrets:--&amp;gt;# 将上述文件引用到 Secret： kubectl create secret generic user --from-file=.</description>
    </item>
    
    <item>
      <title>Kubernetes 中调度 Windows 容器的指南</title>
      <link>https://lijun.in/setup/production-environment/windows/user-guide-windows-containers/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/setup/production-environment/windows/user-guide-windows-containers/</guid>
      <description>Windows 应用程序构成了许多组织中运行的服务和应用程序的很大一部分。本指南将引导您完成在 Kubernetes 中配置和部署 Windows 容器的步骤。
目标  配置一个示例 deployment 以在 Windows 节点上运行 Windows 容器 （可选）使用组托管服务帐户（GMSA）为您的 Pod 配置 Active Directory 身份  在你开始之前  创建一个 Kubernetes 集群，其中包括一个运行 Windows Server 的主节点和工作节点 重要的是要注意，对于 Linux 和 Windows 容器，在 Kubernetes 上创建和部署服务和工作负载的行为几乎相同。与集群接口的 Kubectl 命令相同。提供以下部分中的示例只是为了快速启动 Windows 容器的使用体验。  入门：部署 Windows 容器 要在 Kubernetes 上部署 Windows 容器，您必须首先创建一个示例应用程序。下面的示例 YAML 文件创建了一个简单的 Web 服务器应用程序。创建一个名为 win-webserver.yaml 的服务规约，其内容如下：
apiVersion: v1 kind: Service metadata: name: win-webserver labels: app: win-webserver spec: ports: # the port that this service should serve on - port: 80 targetPort: 80 selector: app: win-webserver type: NodePort --- apiVersion: apps/v1 kind: Deployment metadata: labels: app: win-webserver name: win-webserver spec: replicas: 2 selector: matchLabels: app: win-webserver template: metadata: labels: app: win-webserver name: win-webserver spec: containers: - name: windowswebserver image: mcr.</description>
    </item>
    
    <item>
      <title>CronJob</title>
      <link>https://lijun.in/concepts/workloads/controllers/cron-jobs/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/concepts/workloads/controllers/cron-jobs/</guid>
      <description>feature-state for_k8s_version=&amp;quot;v1.8&amp;rdquo; state=&amp;quot;beta&amp;rdquo; &amp;gt;}}
Cron Job 创建基于时间调度的 Jobs。
一个 CronJob 对象就像 crontab (cron table) 文件中的一行。它用 Cron 格式进行编写，并周期性地在给定的调度时间执行 Job。
所有 CronJob 的 schedule: 时间都是基于初始 Job 的主控节点的时区。
如果你的控制平面在 Pod 或是裸容器中运行了主控程序 (kube-controller-manager)， 那么为该容器设置的时区将会决定定时任务的控制器所使用的时区。
为 CronJob 资源创建清单时，请确保创建的名称不超过 52 个字符。这是因为 CronJob 控制器将自动在提供的作业名称后附加 11 个字符，并且存在一个限制，即作业名称的最大长度不能超过 63 个字符。
有关创建和使用 CronJob 的说明及规范文件的示例，请参见使用 CronJob 运行自动化任务。
CronJob 限制 CronJob 创建 Job 对象，每个 Job 的执行次数大约为一次。 我们之所以说 &amp;ldquo;大约&amp;rdquo;，是因为在某些情况下，可能会创建两个 Job，或者不会创建任何 Job。 我们试图使这些情况尽量少发生，但不能完全杜绝。因此，Job 应该是 幂等的。
如果 startingDeadlineSeconds 设置为很大的数值或未设置（默认），并且 concurrencyPolicy 设置为 Allow，则作业将始终至少运行一次。
对于每个 CronJob，CronJob 控制器 检查从上一次调度的时间点到现在所错过了调度次数。如果错过的调度次数超过 100 次，那么它就不会启动这个任务，并记录这个错误:</description>
    </item>
    
    <item>
      <title>kubeadm version</title>
      <link>https://lijun.in/reference/setup-tools/kubeadm/kubeadm-version/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/setup-tools/kubeadm/kubeadm-version/</guid>
      <description>此命令用来查询 kubeadm 的版本。
. include &amp;ldquo;generated/kubeadm_version.md&amp;rdquo; &amp;gt;}}</description>
    </item>
    
    <item>
      <title>临时容器</title>
      <link>https://lijun.in/concepts/workloads/pods/ephemeral-containers/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/concepts/workloads/pods/ephemeral-containers/</guid>
      <description>feature-state state=&amp;quot;alpha&amp;rdquo; for_k8s_version=&amp;quot;v1.16&amp;rdquo; &amp;gt;}}
此页面概述了临时容器：一种特殊的容器，该容器在现有 glossary_tooltip term_id=&amp;quot;pod&amp;rdquo; &amp;gt;}} 中临时运行，为了完成用户启动的操作，例如故障排查。使用临时容器来检查服务，而不是构建应用程序。
临时容器处于早期的 alpha 阶段，不适用于生产环境集群。应该预料到临时容器在某些情况下不起作用，例如在定位容器的命名空间时。根据 Kubernetes 弃用政策，该 alpha 功能将来可能发生重大变化或完全删除。
了解临时容器 glossary_tooltip text=&amp;quot;Pods&amp;rdquo; term_id=&amp;quot;pod&amp;rdquo; &amp;gt;}} 是 Kubernetes 应用程序的基本构建块。由于 pod 是一次性且可替换的，因此一旦 Pod 创建，就无法将容器加入到 Pod 中。取而代之的是，通常使用 glossary_tooltip text=&amp;quot;deployments&amp;rdquo; term_id=&amp;quot;deployment&amp;rdquo; &amp;gt;}} 以受控的方式来删除并替换 Pod。
有时有必要检查现有 Pod 的状态，例如，对于难以复现的故障进行排查。在这些场景中，可以在现有 Pod 中运行临时容器来检查其状态并运行任意命令。
什么是临时容器？ 临时容器与其他容器的不同之处在于，它们缺少对资源或执行的保证，并且永远不会自动重启，因此不适用于构建应用程序。临时容器使用与常规容器相同的 ContainerSpec 段进行描述，但许多字段是不相容且不允许的。
 临时容器没有端口配置，因此像 ports，livenessProbe，readinessProbe 这样的字段是不允许的。 Pod 资源分配是不可变的，因此 resources 配置是不允许的。 有关允许字段的完整列表，请参见[临时容器参考文档](/docs/reference/generated/kubernetes-api/&amp;lt; param &amp;ldquo;version&amp;rdquo; &amp;gt;}}/#ephemeralcontainer-v1-core)。  临时容器是使用 API 中的一种特殊的 ephemeralcontainers 处理器进行创建的，而不是直接添加到 pod.spec 段，因此无法使用 kubectl edit 来添加一个临时容器。
与常规容器一样，将临时容器添加到 Pod 后，将不能更改或删除临时容器。</description>
    </item>
    
    <item>
      <title>使用 kubeadm 配置集群中的每个 kubelet</title>
      <link>https://lijun.in/setup/production-environment/tools/kubeadm/kubelet-integration/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/setup/production-environment/tools/kubeadm/kubelet-integration/</guid>
      <description>. feature-state for_k8s_version=&amp;quot;1.11&amp;rdquo; state=&amp;quot;stable&amp;rdquo; &amp;gt;}}
kubeadm CLI 工具的生命周期与 kubelet解耦，它是一个守护程序，在 Kubernetes 集群中的每个节点上运行。 当 Kubernetes 初始化或升级时，kubeadm CLI 工具由用户执行，而 kubelet 始终在后台运行。
由于kubelet是守护程序，因此需要通过某种初始化系统或服务管理器进行维护。 当使用 DEB 或 RPM 安装 kubelet 时，配置系统去管理 kubelet。 您可以改用其他服务管理器，但需要手动地配置。
集群中涉及的所有 kubelet 的一些配置细节都必须相同，而其他配置方面则需要基于每个 kubelet 进行设置，以适应给定机器的不同特性，例如操作系统、存储和网络。 您可以手动地管理 kubelet 的配置，但是 kubeadm 现在提供一种 KubeletConfiguration API 类型，用于集中管理 kubelet 的配置。
Kubelet 配置模式 以下各节讲述了通过使用 kubeadm 简化 kubelet 配置模式，而不是在每个节点上手动地管理 kubelet 配置。
将集群级配置传播到每个 kubelet 中 您可以通过使用 kubeadm init 和 kubeadm join 命令为 kubelet 提供默认值。 有趣的示例包括使用其他 CRI 运行时或通过服务器设置不同的默认子网。
如果您想使用子网 10.96.0.0/12 作为默认的服务，您可以给 kubeadm 传递 --service-cidr 参数：</description>
    </item>
    
    <item>
      <title>创建一个外部负载均衡器</title>
      <link>https://lijun.in/tasks/access-application-cluster/create-external-load-balancer/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/access-application-cluster/create-external-load-balancer/</guid>
      <description>本文展示如何创建一个外部负载均衡器。
. note &amp;gt;}}
此功能仅适用于支持外部负载均衡器的云提供商或环境。 . /note &amp;gt;}}
创建服务时，您可以选择自动创建云网络负载均衡器。这提供了一个外部可访问的 IP 地址，可将流量分配到集群节点上的正确端口上 假设集群在支持的环境中运行，并配置了正确的云负载平衡器提供商包。
有关如何配置和使用 Ingress 资源为服务提供外部可访问的 URL、负载均衡流量、终止 SSL 等功能，请查看 Ingress 文档。
. heading &amp;ldquo;prerequisites&amp;rdquo; %}}  . include &amp;ldquo;task-tutorial-prereqs.md&amp;rdquo; &amp;gt;}} . version-check &amp;gt;}}  配置文件 要创建外部负载均衡器，请将以下内容添加到 服务配置文件：
type: LoadBalancer 您的配置文件可能会如下所示：
apiVersion: v1 kind: Service metadata: name: example-service spec: selector: app: example ports: - port: 8765 targetPort: 9376 type: LoadBalancer 使用 kubectl 您也可以使用 kubectl expose 命令及其 --type=LoadBalancer 参数创建服务：
kubectl expose rc example --port=8765 --target-port=9376 \  --name=example-service --type=LoadBalancer 此命令通过使用与引用资源（在上面的示例的情况下，名为 example 的 replication controller）相同的选择器来创建一个新的服务。</description>
    </item>
    
    <item>
      <title>基于Replication Controller执行滚动升级</title>
      <link>https://lijun.in/tasks/run-application/rolling-update-replication-controller/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/run-application/rolling-update-replication-controller/</guid>
      <description>概述 注: 创建副本应用的首选方法是使用[Deployment](/docs/api-reference/. param &amp;ldquo;version&amp;rdquo; &amp;gt;}}/#deployment-v1beta1-apps)，Deployment使用[ReplicaSet](/docs/api-reference/. param &amp;ldquo;version&amp;rdquo; &amp;gt;}}/#replicaset-v1beta1-extensions)来进行副本控制。 更多信息, 查看使用Deployment运行一个无状态应用。
为了在更新服务的同时不中断业务， kubectl 支持&amp;lsquo;滚动更新&amp;rsquo;，它一次更新一个pod，而不是同时停止整个服务。 有关更多信息，请参阅 滚动更新设计文档 和 滚动更新示例。
请注意， kubectl rolling-update 仅支持Replication Controllers。 但是，如果使用Replication Controllers部署应用，请考虑将其切换到Deployments. Deployment是一种被推荐使用的更高级别的控制器，它可以对应用进行声明性的自动滚动更新。 如果您仍然希望保留您的Replication Controllers并使用 kubectl rolling-update进行滚动更新， 请继续往下阅读：
滚动更新可以对replication controller所管理的Pod的配置进行变更，变更可以通过一个新的配置文件来进行，或者，如果只更新镜像，则可以直接指定新的容器镜像。
滚动更新的工作流程：
 通过新的配置创建一个replication controller 在新的控制器上增加副本数，在旧的上面减少副本数，直到副本数达到期望值 删除之前的replication controller  使用kubectl rolling-update命令来进行滚动更新：
$ kubectl rolling-update NAME \ ([NEW_NAME] --image=IMAGE | -f FILE)  通过配置文件更新 通过配置文件来进行滚动更新，需要在kubectl rolling-update命令后面带上新的配置文件：
$ kubectl rolling-update NAME -f FILE  这个配置文件必须满足以下条件：
  指定不同的metadata.name值
  至少要修改spec.</description>
    </item>
    
    <item>
      <title>kubeadm alpha</title>
      <link>https://lijun.in/reference/setup-tools/kubeadm/kubeadm-alpha/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/setup-tools/kubeadm/kubeadm-alpha/</guid>
      <description>. caution &amp;gt;}}
kubeadm alpha 提供了一组可用于收集社区反馈的功能的预览。请尝试一下这些功能并给我们反馈！ . /caution &amp;gt;}}
kubeadm alpha certs renew 使用 all 子命令来更新所有 Kubernetes 证书或有选择性地更新它们。有关证书到期和续订的更多详细信息，请参见证书管理文档。
. tabs name=&amp;quot;tab-certs-renew&amp;rdquo; &amp;gt;}} . tab name=&amp;quot;renew&amp;rdquo; include=&amp;quot;generated/kubeadm_alpha_certs_renew.md&amp;rdquo; /&amp;gt;}} . tab name=&amp;quot;all&amp;rdquo; include=&amp;quot;generated/kubeadm_alpha_certs_renew_all.md&amp;rdquo; /&amp;gt;}} . tab name=&amp;quot;admin.conf&amp;rdquo; include=&amp;quot;generated/kubeadm_alpha_certs_renew_admin.conf.md&amp;rdquo; /&amp;gt;}} . tab name=&amp;quot;apiserver-etcd-client&amp;rdquo; include=&amp;quot;generated/kubeadm_alpha_certs_renew_apiserver-etcd-client.md&amp;rdquo; /&amp;gt;}} . tab name=&amp;quot;apiserver-kubelet-client&amp;rdquo; include=&amp;quot;generated/kubeadm_alpha_certs_renew_apiserver-kubelet-client.md&amp;rdquo; /&amp;gt;}} . tab name=&amp;quot;apiserver&amp;rdquo; include=&amp;quot;generated/kubeadm_alpha_certs_renew_apiserver.md&amp;rdquo; /&amp;gt;}} . tab name=&amp;quot;controller-manager.conf&amp;rdquo; include=&amp;quot;generated/kubeadm_alpha_certs_renew_controller-manager.conf.md&amp;rdquo; /&amp;gt;}} . tab name=&amp;quot;etcd-healthcheck-client&amp;rdquo; include=&amp;quot;generated/kubeadm_alpha_certs_renew_etcd-healthcheck-client.md&amp;rdquo; /&amp;gt;}} . tab name=&amp;quot;etcd-peer&amp;rdquo; include=&amp;quot;generated/kubeadm_alpha_certs_renew_etcd-peer.md&amp;rdquo; /&amp;gt;}} . tab name=&amp;quot;etcd-server&amp;rdquo; include=&amp;quot;generated/kubeadm_alpha_certs_renew_etcd-server.</description>
    </item>
    
    <item>
      <title>kubeadm init phase</title>
      <link>https://lijun.in/reference/setup-tools/kubeadm/kubeadm-init-phase/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/setup-tools/kubeadm/kubeadm-init-phase/</guid>
      <description>kubeadm init phase 能确保调用引导过程的原子步骤。因此，如果希望自定义应用，则可以让 kubeadm 做一些工作，然后填补空白。
kubeadm init phase 与 kubeadm init 工作流程一致，后台都使用相同的代码。
kubeadm init phase preflight 使用此命令可以在控制平面节点上执行启动前检查。
. tabs name=&amp;quot;tab-preflight&amp;rdquo; &amp;gt;}} . tab name=&amp;quot;preflight&amp;rdquo; include=&amp;quot;generated/kubeadm_init_phase_preflight.md&amp;rdquo; /&amp;gt;}} . /tabs &amp;gt;}}
kubeadm init phase kubelet-start 此阶段将检查 kubelet 配置文件和环境文件，然后启动 kubelet。
. tabs name=&amp;quot;tab-kubelet-start&amp;rdquo; &amp;gt;}} . tab name=&amp;quot;kubelet-start&amp;rdquo; include=&amp;quot;generated/kubeadm_init_phase_kubelet-start.md&amp;rdquo; /&amp;gt;}} . /tabs &amp;gt;}}
kubeadm init phase certs 该阶段可用于创建 kubeadm 所需的所有证书。
. tabs name=&amp;quot;tab-certs&amp;rdquo; &amp;gt;}} . tab name=&amp;quot;certs&amp;rdquo; include=&amp;quot;generated/kubeadm_init_phase_certs.md&amp;rdquo; /&amp;gt;}} . tab name=&amp;quot;all&amp;rdquo; include=&amp;quot;generated/kubeadm_init_phase_certs_all.md&amp;rdquo; /&amp;gt;}} .</description>
    </item>
    
    <item>
      <title>kubeadm join phase</title>
      <link>https://lijun.in/reference/setup-tools/kubeadm/kubeadm-join-phase/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/setup-tools/kubeadm/kubeadm-join-phase/</guid>
      <description>kubeadm join phase 使您能够调用 join 过程的基本原子步骤。因此，如果希望执行自定义操作，可以让 kubeadm 做一些工作，然后由用户来补足剩余操作。
kubeadm join phase 与 kubeadm join 工作流程 一致，后台都使用相同的代码。
kubeadm join phase . tabs name=&amp;quot;tab-phase&amp;rdquo; &amp;gt;}} . tab name=&amp;quot;phase&amp;rdquo; include=&amp;quot;generated/kubeadm_join_phase.md&amp;rdquo; /&amp;gt;}} . /tabs &amp;gt;}}
kubeadm join phase preflight 使用此命令可以在即将加入集群的节点上执行启动前检查。
. tabs name=&amp;quot;tab-preflight&amp;rdquo; &amp;gt;}} . tab name=&amp;quot;preflight&amp;rdquo; include=&amp;quot;generated/kubeadm_join_phase_preflight.md&amp;rdquo; /&amp;gt;}} . /tabs &amp;gt;}}
kubeadm join phase control-plane-prepare 使用此阶段，您可以准备一个作为控制平面的节点。
. tabs name=&amp;quot;tab-control-plane-prepare&amp;rdquo; &amp;gt;}} . tab name=&amp;quot;control-plane-prepare&amp;rdquo; include=&amp;quot;generated/kubeadm_join_phase_control-plane-prepare.md&amp;rdquo; /&amp;gt;}} . tab name=&amp;quot;all&amp;rdquo; include=&amp;quot;generated/kubeadm_join_phase_control-plane-prepare_all.md&amp;rdquo; /&amp;gt;}} . tab name=&amp;quot;download-certs&amp;rdquo; include=&amp;quot;generated/kubeadm_join_phase_control-plane-prepare_download-certs.</description>
    </item>
    
    <item>
      <title>kubeadm reset phase</title>
      <link>https://lijun.in/reference/setup-tools/kubeadm/kubeadm-reset-phase/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/setup-tools/kubeadm/kubeadm-reset-phase/</guid>
      <description>kubeadm reset phase 使您能够调用 reset 过程的基本原子步骤。因此，如果希望执行自定义操作，可以让 kubeadm 做一些工作，然后由用户来补足剩余操作。
kubeadm reset phase 与 kubeadm reset 工作流程 一致，后台都使用相同的代码。
kubeadm reset phase . tabs name=&amp;quot;tab-phase&amp;rdquo; &amp;gt;}} . tab name=&amp;quot;phase&amp;rdquo; include=&amp;quot;generated/kubeadm_reset_phase.md&amp;rdquo; /&amp;gt;}} . /tabs &amp;gt;}}
kubeadm reset phase preflight 使用此阶段，您可以在要重置的节点上执行启动前检查阶段。
. tabs name=&amp;quot;tab-preflight&amp;rdquo; &amp;gt;}} . tab name=&amp;quot;preflight&amp;rdquo; include=&amp;quot;generated/kubeadm_reset_phase_preflight.md&amp;rdquo; /&amp;gt;}} . /tabs &amp;gt;}}
kubeadm reset phase update-cluster-status 使用此阶段，您可以从 ClusterStatus 对象中删除此控制平面节点。
. tabs name=&amp;quot;tab-update-cluster-status&amp;rdquo; &amp;gt;}} . tab name=&amp;quot;update-cluster-status&amp;rdquo; include=&amp;quot;generated/kubeadm_reset_phase_update-cluster-status.md&amp;rdquo; /&amp;gt;}} . /tabs &amp;gt;}}
kubeadm reset phase remove-etcd-member 使用此阶段，您可以从 etcd 集群中删除此控制平面节点的 etcd 成员。</description>
    </item>
    
    <item>
      <title>kubeadm upgrade phase</title>
      <link>https://lijun.in/reference/setup-tools/kubeadm/kubeadm-upgrade-phase/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/setup-tools/kubeadm/kubeadm-upgrade-phase/</guid>
      <description>在 Kubernetes v1.15.0 版本中，kubeadm 引入了对 kubeadm upgrade node 阶段的初步支持。其他 kubeadm upgrade 子命令如 apply 等阶段将在未来发行版中添加。
kubeadm upgrade node phase 使用此阶段，可以选择执行辅助控制平面或工作节点升级的单独步骤。请注意，kubeadm upgrade apply 命令仍然必须在主控制平面节点上调用。
. tabs name=&amp;quot;tab-phase&amp;rdquo; &amp;gt;}} . tab name=&amp;quot;phase&amp;rdquo; include=&amp;quot;generated/kubeadm_upgrade_node_phase.md&amp;rdquo; /&amp;gt;}} . tab name=&amp;quot;control-plane&amp;rdquo; include=&amp;quot;generated/kubeadm_upgrade_node_phase_control-plane.md&amp;rdquo; /&amp;gt;}} . tab name=&amp;quot;kubelet-config&amp;rdquo; include=&amp;quot;generated/kubeadm_upgrade_node_phase_kubelet-config.md&amp;rdquo; /&amp;gt;}} . /tabs &amp;gt;}}
接下来  kubeadm init 引导一个 Kubernetes 控制平面节点 kubeadm join 将节点加入到群集 kubeadm reset 还原 kubeadm init 或 kubeadm join 命令对主机所做的任何更改 kubeadm upgrade 升级 kubeadm 节点 kubeadm alpha 尝试实验性功能  </description>
    </item>
    
    <item>
      <title>Pod 水平自动伸缩</title>
      <link>https://lijun.in/tasks/run-application/horizontal-pod-autoscale/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/run-application/horizontal-pod-autoscale/</guid>
      <description>Pod 水平自动伸缩（Horizontal Pod Autoscaler）特性， 可以基于CPU利用率自动伸缩 replication controller、deployment和 replica set 中的 pod 数量，（除了 CPU 利用率）也可以 基于其他应程序提供的度量指标custom metrics。 pod 自动缩放不适用于无法缩放的对象，比如 DaemonSets。
Pod 水平自动伸缩特性由 Kubernetes API 资源和控制器实现。资源决定了控制器的行为。 控制器会周期性的获取平均 CPU 利用率，并与目标值相比较后来调整 replication controller 或 deployment 中的副本数量。
Pod 水平自动伸缩工作机制 Pod 水平自动伸缩的实现是一个控制循环，由 controller manager 的 --horizontal-pod-autoscaler-sync-period 参数 指定周期（默认值为15秒）。
每个周期内，controller manager 根据每个 HorizontalPodAutoscaler 定义中指定的指标查询资源利用率。 controller manager 可以从 resource metrics API（每个pod 资源指标）和 custom metrics API（其他指标）获取指标。
 对于每个 pod 的资源指标（如 CPU），控制器从资源指标 API 中获取每一个 HorizontalPodAutoscaler 指定 的 pod 的指标，然后，如果设置了目标使用率，控制器获取每个 pod 中的容器资源使用情况，并计算资源使用率。 如果使用原始值，将直接使用原始数据（不再计算百分比）。 然后，控制器根据平均的资源使用率或原始值计算出缩放的比例，进而计算出目标副本数。  需要注意的是，如果 pod 某些容器不支持资源采集，那么控制器将不会使用该 pod 的 CPU 使用率。 下面的算法细节章节将会介绍详细的算法。</description>
    </item>
    
    <item>
      <title>为 Pod 配置服务账户</title>
      <link>https://lijun.in/tasks/configure-pod-container/configure-service-account/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/configure-pod-container/configure-service-account/</guid>
      <description>服务账户为 Pod 中运行的进程提供了一个标识。
本文是服务账户的用户使用介绍。您也可以参考集群管理指南之服务账户。
. note &amp;gt;}}
本文档描述 Kubernetes 项目推荐的集群中服务帐户的行为。 集群管理员也可能已经定制了服务账户在集群中的属性，在这种情况下，本文档可能并不适用。
. /note &amp;gt;}}
当您（人类）访问集群时（例如，使用 kubectl），api 服务器将您的身份验证为特定的用户帐户（当前这通常是 admin，除非您的集群管理员已经定制了您的集群配置）。 Pod 内的容器中的进程也可以与 api 服务器接触。 当它们进行身份验证时，它们被验证为特定的服务帐户（例如，default）。
. heading &amp;ldquo;prerequisites&amp;rdquo; %}} . include &amp;ldquo;task-tutorial-prereqs.md&amp;rdquo; &amp;gt;}} . version-check &amp;gt;}}
使用默认的服务账户访问 API 服务器 当您创建 Pod 时，如果没有指定服务账户，Pod 会被指定命名空间中的default服务账户。 如果您查看 Pod 的原始 json 或 yaml（例如：kubectl get pods/podname -o yaml）， 您可以看到 spec.serviceAccountName 字段已经被自动设置了。
您可以使用自动挂载给 Pod 的服务账户凭据访问 API，访问集群 中有相关描述。 服务账户的 API 许可取决于您所使用的授权插件和策略。
在 1.6 以上版本中，您可以通过在服务账户上设置 automountServiceAccountToken: false 来实现不给服务账号自动挂载 API 凭据：</description>
    </item>
    
    <item>
      <title>使用 Node 鉴权</title>
      <link>https://lijun.in/reference/access-authn-authz/node/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/access-authn-authz/node/</guid>
      <description>节点鉴权是一种特殊用途的鉴权模式，专门对 kubelet 发出的 API 请求进行鉴权。
概述 节点鉴权器允许 kubelet 执行 API 操作。包括：
读取操作：
 services endpoints nodes pods secrets、configmaps、以及绑定到 kubelet 的节点的 pod 的持久卷申领和持久卷  写入操作：
 节点和节点状态（启用 NodeRestriction 准入插件以限制 kubelet 只能修改自己的节点） Pod 和 Pod 状态 (启用 NodeRestriction 准入插件以限制 kubelet 只能修改绑定到自身的 Pod) 事件  鉴权相关操作：
 对于基于 TLS 的启动引导过程时使用的 certificationsigningrequests API 的读/写权限 为委派的身份验证/授权检查创建 tokenreviews 和 subjectaccessreviews 的能力  在将来的版本中，节点鉴权器可能会添加或删除权限，以确保 kubelet 具有正确操作所需的最小权限集。
为了获得节点鉴权器的授权，kubelet 必须使用一个凭证以表示它在 system:nodes 组中，用户名为 system:node:&amp;lt;nodeName&amp;gt;。 上述的组名和用户名格式要与 kubelet TLS 启动引导过程中为每个 kubelet 创建的标识相匹配。</description>
    </item>
    
    <item>
      <title>配置你的云平台防火墙</title>
      <link>https://lijun.in/tasks/access-application-cluster/configure-cloud-provider-firewall/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/access-application-cluster/configure-cloud-provider-firewall/</guid>
      <description>许多云服务提供商（比如 谷歌计算引擎）定义防火墙以防止服务无意间暴露到互联网上。 当暴露服务给外网时，你可能需要在防火墙上开启一个或者更多的端口来支持服务。 本文描述了这个过程，以及其他云服务商的具体信息。
. heading &amp;ldquo;prerequisites&amp;rdquo; %}} . include &amp;ldquo;task-tutorial-prereqs.md&amp;rdquo; &amp;gt;}} . version-check &amp;gt;}}
负载均衡（LoadBalancer）服务的访问限制 当以 spec.type: LoadBalancer 方式配置服务时，你可以使用 spec.loadBalancerSourceRanges 来指定允许访问负载均衡器的 ip 段。 这个字段采用 CIDR 的 IP 段， kubernetes 使用该段配置防火墙。目前只有 谷歌计算引擎，谷歌云原生引擎，亚马逊弹性原生云服务 和 微软云原生平台支持此功能。 如果云服务提供商不支持这个功能，这个字段将被忽略。
假设内部子网为假设10.0.0.0/8，在下面这个例子中，将创建一个仅能由群集内部IP访问的负载均衡器。此负载均衡器不允许来自 kubernetes 集群外部客户端的访问。
apiVersion: v1 kind: Service metadata: name: myapp spec: ports: - port: 8765 targetPort: 9376 selector: app: example type: LoadBalancer loadBalancerSourceRanges: - 10.0.0.0/8 在下面这个例子中，将创建一个只能被 IP 为 130.211.204.1 和 130.211.204.2 的客户端访问的负载据衡器。
apiVersion: v1 kind: Service metadata: name: myapp spec: ports: - port: 8765 targetPort: 9376 selector: app: example type: LoadBalancer loadBalancerSourceRanges: - 130.</description>
    </item>
    
    <item>
      <title>Webhook 模式</title>
      <link>https://lijun.in/reference/access-authn-authz/webhook/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/access-authn-authz/webhook/</guid>
      <description>WebHook 是一种 HTTP 回调：某些条件下触发的 HTTP POST 请求；通过 HTTP POST 发送的简单事件通知。一个基于 web 应用实现的 WebHook 会在特定事件发生时把消息发送给特定的 URL。
具体来说，当在判断用户权限时，Webhook 模式会使 Kubernetes 查询外部的 REST 服务。
配置文件格式 Webhook 模式需要一个 HTTP 配置文件，通过 --authorization-webhook-config-file=SOME_FILENAME 的参数声明。
配置文件的格式使用 kubeconfig。在文件中，&amp;ldquo;users&amp;rdquo; 代表着 API 服务器的 webhook，而 &amp;ldquo;cluster&amp;rdquo; 代表着远程服务。
使用 HTTPS 客户端认证的配置例子：
# Kubernetes API 版本 apiVersion: v1 # API 对象种类 kind: Config # clusters 代表远程服务。 clusters: - name: name-of-remote-authz-service cluster: # 对远程服务进行身份认证的 CA。 certificate-authority: /path/to/ca.pem # 远程服务的查询 URL。必须使用 &amp;#39;https&amp;#39;。 server: https://authz.example.com/authorize # users 代表 API 服务器的 webhook 配置 users: - name: name-of-api-server user: client-certificate: /path/to/cert.</description>
    </item>
    
    <item>
      <title>Horizontal Pod Autoscaler演练</title>
      <link>https://lijun.in/tasks/run-application/horizontal-pod-autoscale-walkthrough/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/run-application/horizontal-pod-autoscale-walkthrough/</guid>
      <description>Horizontal Pod Autoscaler 可以根据CPU利用率自动伸缩 replication controller、deployment 或者 replica set 中的Pod数量 （也可以基于其他应用程序提供的度量指标，目前这一功能处于 beta 版本）。
本文将引导您了解如何为 php-apache 服务器配置和使用 Horizontal Pod Autoscaler。 更多 Horizontal Pod Autoscaler 的信息请参阅 Horizontal Pod Autoscaler user guide。
. heading &amp;ldquo;prerequisites&amp;rdquo; %}} 本文示例需要一个1.2或者更高版本的可运行的 Kubernetes 集群以及 kubectl。 metrics-server 也需要部署到集群中， 它可以通过 resource metrics API 对外提供度量数据，Horizontal Pod Autoscaler 正是根据此 API 来获取度量数据，部署方法请参考 metrics-server 。 如果你正在使用GCE，按照 getting started on GCE guide 操作，metrics-server 会默认启动。
如果需要为 Horizontal Pod Autoscaler 指定多种资源度量指标，您的 Kubernetes 集群以及 kubectl 至少需要达到1.6版本。 此外，如果要使用自定义度量指标，您的Kubernetes 集群还必须能够与提供这些自定义指标的API服务器通信。 最后，如果要使用与 Kubernetes 对象无关的度量指标，则 Kubernetes 集群版本至少需要达到1.</description>
    </item>
    
    <item>
      <title>从私有仓库拉取镜像</title>
      <link>https://lijun.in/tasks/configure-pod-container/pull-image-private-registry/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/configure-pod-container/pull-image-private-registry/</guid>
      <description>本文介绍如何使用 Secret 从私有的 Docker 镜像仓库或代码仓库拉取镜像来创建 Pod。
. heading &amp;ldquo;prerequisites&amp;rdquo; %}}  . include &amp;ldquo;task-tutorial-prereqs.md&amp;rdquo; &amp;gt;}} . version-check &amp;gt;}}  您需要 Docker ID 和密码来进行本练习。
登录 Docker 镜像仓库 在个人电脑上，要想拉取私有镜像必须在镜像仓库上进行身份验证。
docker login 当提示时，输入 Docker 用户名和密码。
登录过程会创建或更新保存有授权令牌的 config.json 文件。
查看 config.json 文件：
cat ~/.docker/config.json 输出结果包含类似于以下内容的部分：
{ &amp;#34;auths&amp;#34;: { &amp;#34;https://index.docker.io/v1/&amp;#34;: { &amp;#34;auth&amp;#34;: &amp;#34;c3R...zE2&amp;#34; } } } . note &amp;gt;}}
如果使用 Docker 凭证仓库，则不会看到 auth 条目，看到的将是以仓库名称作为值的 credsStore 条目。 . /note &amp;gt;}}
在集群中创建保存授权令牌的 Secret Kubernetes 集群使用 docker-registry 类型的 Secret 来通过容器仓库的身份验证，进而提取私有映像。</description>
    </item>
    
    <item>
      <title>列出集群中所有运行容器的镜像</title>
      <link>https://lijun.in/tasks/access-application-cluster/list-all-running-container-images/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/access-application-cluster/list-all-running-container-images/</guid>
      <description>本文展示如何使用 kubectl 来列出集群中所有运行 pod 的容器的镜像
. heading &amp;ldquo;prerequisites&amp;rdquo; %}} . include &amp;ldquo;task-tutorial-prereqs.md&amp;rdquo; &amp;gt;}} . version-check &amp;gt;}}
在本练习中，您将使用 kubectl 来获取集群中运行的所有 Pod，并格式化输出来提取每个 pod 中的容器列表。
列出所有命名空间下的所有容器  使用 kubectl get pods --all-namespaces 获取所有命名空间下的所有 Pod 使用 -o jsonpath={..image} 来格式化输出，以仅包含容器镜像名称。 这将以递归方式从返回的 json 中解析出 image 字段。  参阅 jsonpath reference 来获取更多关于如何使用 jsonpath 的信息。   使用标准化工具来格式化输出：tr, sort, uniq  使用 tr 以用换行符替换空格 使用 sort 来对结果进行排序 使用 uniq 来聚合镜像计数    kubectl get pods --all-namespaces -o jsonpath=&amp;#34;{.</description>
    </item>
    
    <item>
      <title>控制器管理器指标</title>
      <link>https://lijun.in/concepts/cluster-administration/controller-metrics/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/concepts/cluster-administration/controller-metrics/</guid>
      <description>控制器管理器指标为控制器管理器的性能和健康提供了重要的观测手段。
什么是控制器管理器度量 控制器管理器指标为控制器管理器的性能和健康提供了重要的观测手段。 这些度量包括常见的 Go 语言运行时度量，比如 go_routine 计数，以及控制器特定的度量，比如 etcd 请求延迟或 云提供商（AWS、GCE、OpenStack）的 API 延迟，这些参数可以用来测量集群的健康状况。
从 Kubernetes 1.7 版本开始，详细的云提供商指标可用于 GCE、AWS、Vsphere 和 OpenStack 的存储操作。 这些度量可用于监视持久卷操作的健康状况。
例如，在 GCE 中这些指标叫做：
cloudprovider_gce_api_request_duration_seconds { request = &amp;quot;instance_list&amp;quot;} cloudprovider_gce_api_request_duration_seconds { request = &amp;quot;disk_insert&amp;quot;} cloudprovider_gce_api_request_duration_seconds { request = &amp;quot;disk_delete&amp;quot;} cloudprovider_gce_api_request_duration_seconds { request = &amp;quot;attach_disk&amp;quot;} cloudprovider_gce_api_request_duration_seconds { request = &amp;quot;detach_disk&amp;quot;} cloudprovider_gce_api_request_duration_seconds { request = &amp;quot;list_disk&amp;quot;} 配置 在集群中，控制器管理器指标可从它所在的主机上的 http://localhost:10252/metrics 中获得。
这些指标是以 prometheus 格式 发出的，是人类可读的。
在生产环境中，您可能想配置 prometheus 或其他一些指标收集工具，以定期收集这些指标数据，并将它们应用到某种时间序列数据库中。</description>
    </item>
    
    <item>
      <title>配置您的 kubernetes 集群以自托管控制平台</title>
      <link>https://lijun.in/setup/production-environment/tools/kubeadm/self-hosting/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/setup/production-environment/tools/kubeadm/self-hosting/</guid>
      <description>自托管 Kubernetes 控制平台 kubeadm 允许您实验性地创建 self-hosted Kubernetes 控制平面。 这意味着 API 服务器，控制管理器和调度程序之类的关键组件将通过配置 Kubernetes API 以 DaemonSet pods 的身份运行，而不是通过静态文件将 static pods 在 kubelet 中配置。
要创建自托管集群，请参见 kubeadm alpha 自托管枢纽 命令。
警告 . caution &amp;gt;}}
此功能将您的集群设置为不受支持的状态，从而使 kubeadm 无法再管理您的集群。 这包括 kubeadm 升级 。 . /caution &amp;gt;}}
 1.8及更高版本中的自托管功能有一些重要限制。 特别是，自托管集群在没有人工干预的情况下_无法从控制平面节点的重新启动中恢复_ 。   默认情况下，自托管的控制平面 Pod 依赖于从 hostPath 卷加载的凭据。 除初始创建外，这些凭据不由 kubeadm 管理。   控制平面的自托管部分不包括 etcd，后者仍作为静态 Pod 运行。  处理 自托管引导过程记录在 kubeadm 设计文档 中。
总而言之，kubeadm alpha 自托管 的工作原理如下：</description>
    </item>
    
    <item>
      <title>指定应用程序的中断预算（Disruption Budget）</title>
      <link>https://lijun.in/tasks/run-application/configure-pdb/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/run-application/configure-pdb/</guid>
      <description>本文展示了如何限制应用程序的并发中断数量，在允许集群管理员管理集群节点的同时保证高可用。
. heading &amp;ldquo;prerequisites&amp;rdquo; %}}  用户是 Kubernetes 集群中有高可用需求的应用的所有者。   用户应了解如何部署 无状态应用 和/或 有状态应用。   用户应当已经阅读过关于 Pod 中断 的文档。   用户应当与集群所有者或服务提供者确认其遵从 Pod 中断预算（Pod Disruption Budgets）的规则。  用 PodDisruptionBudget 来保护应用  确定想要使用 PodDisruptionBudget (PDB) 来保护的应用。 考虑应用对中断的反应。 以 YAML 文件形式定义 PDB 。 通过 YAML 文件创建 PDB 对象。  确定要保护的应用 用户想要保护通过内置的 Kubernetes 控制器指定的应用，这是最常见的使用场景：
 Deployment ReplicationController ReplicaSet StatefulSet  在这种情况下，在控制器的 .spec.selector 字段中做记录，并在 PDB 的 .spec.selector 字段中加入同样的选择器。
用户也可以用 PDB 来保护不受上述控制器控制的 pod，或任意组（arbitrary groups）的 pod， 但是正如 任意控制器和选择器 中描述的，这里存在一些限制。</description>
    </item>
    
    <item>
      <title>配置存活、就绪和启动探测器</title>
      <link>https://lijun.in/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/</guid>
      <description>这篇文章介绍如何给容器配置存活、就绪和启动探测器。
kubelet 使用存活探测器来知道什么时候要重启容器。例如，存活探测器可以捕捉到死锁（应用程序在运行，但是无法继续执行后面的步骤）。这样的情况下重启容器有助于让应用程序在有问题的情况下更可用。
kubelet 使用就绪探测器可以知道容器什么时候准备好了并可以开始接受请求流量， 当一个 Pod 内的所有容器都准备好了，才能把这个 Pod 看作就绪了。这种信号的一个用途就是控制哪个 Pod 作为 Service 的后端。在 Pod 还没有准备好的时候，会从 Service 的负载均衡器中被剔除的。
kubelet 使用启动探测器可以知道应用程序容器什么时候启动了。如果配置了这类探测器，就可以控制容器在启动成功后再进行存活性和就绪检查，确保这些存活、就绪探测器不会影响应用程序的启动。这可以用于对慢启动容器进行存活性检测，避免它们在启动运行之前就被杀掉。
. heading &amp;ldquo;prerequisites&amp;rdquo; %}} . include &amp;ldquo;task-tutorial-prereqs.md&amp;rdquo; &amp;gt;}} . version-check &amp;gt;}}
定义存活命令 许多长时间运行的应用程序最终会过渡到断开的状态，除非重新启动，否则无法恢复。Kubernetes 提供了存活探测器来发现并补救这种情况。
在这篇练习中，会创建一个 Pod，其中运行一个基于 k8s.gcr.io/busybox 镜像的容器。下面是这个 Pod 的配置文件。
. codenew file=&amp;quot;pods/probe/exec-liveness.yaml&amp;rdquo; &amp;gt;}}
在这个配置文件中，可以看到 Pod 中只有一个容器。periodSeconds 字段指定了 kubelet 应该每 5 秒执行一次存活探测。initialDelaySeconds 字段告诉 kubelet 在执行第一次探测前应该等待 5 秒。kubelet 在容器内执行命令 cat /tmp/healthy 来进行探测。如果命令执行成功并且返回值为 0，kubelet 就会认为这个容器是健康存活的。如果这个命令返回非 0 值，kubelet 会杀死这个容器并重新启动它。
当容器启动时，执行如下的命令：
/bin/sh -c &amp;#34;touch /tmp/healthy; sleep 30; rm -rf /tmp/healthy; sleep 600&amp;#34; 这个容器生命的前 30 秒， /tmp/healthy 文件是存在的。所以在这最开始的 30 秒内，执行命令 cat /tmp/healthy 会返回成功码。30 秒之后，执行命令 cat /tmp/healthy 就会返回失败码。</description>
    </item>
    
    <item>
      <title>为集群配置 DNS</title>
      <link>https://lijun.in/tasks/access-application-cluster/configure-dns-cluster/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/access-application-cluster/configure-dns-cluster/</guid>
      <description>Kubernetes 提供 DNS 集群插件，大多数支持的环境默认情况下都会启用。
有关如何为 Kubernetes 集群配置 DNS 的详细信息，请参阅 Kubernetes DNS 插件示例.</description>
    </item>
    
    <item>
      <title>将 Pod 分配给节点</title>
      <link>https://lijun.in/tasks/configure-pod-container/assign-pods-nodes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/configure-pod-container/assign-pods-nodes/</guid>
      <description>此页面显示如何将 Kubernetes Pod 分配给 Kubernetes 集群中的特定节点。
. heading &amp;ldquo;prerequisites&amp;rdquo; %}} . include &amp;ldquo;task-tutorial-prereqs.md&amp;rdquo; &amp;gt;}} . version-check &amp;gt;}}
给节点添加标签   列出集群中的节点
 kubectl get nodes  输出类似如下：
 NAME STATUS AGE VERSION worker0 Ready 1d v1.6.0+fff5156 worker1 Ready 1d v1.6.0+fff5156 worker2 Ready 1d v1.6.0+fff5156      选择其中一个节点，为它添加标签：
 kubectl label nodes &amp;lt;your-node-name&amp;gt; disktype=ssd  &amp;lt;your-node-name&amp;gt; 是你选择的节点的名称。
    验证你选择的节点是否有 disktype=ssd 标签：
 kubectl get nodes --show-labels  输出类似如下：</description>
    </item>
    
    <item>
      <title>将 CoreDNS 设置为联邦集群的 DNS 提供者</title>
      <link>https://lijun.in/tasks/federation/set-up-coredns-provider-federation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/federation/set-up-coredns-provider-federation/</guid>
      <description>. deprecationfilewarning &amp;gt;}} . include &amp;ldquo;federation-deprecation-warning-note.md&amp;rdquo; &amp;gt;}} . /deprecationfilewarning &amp;gt;}}
此页面显示如何配置和部署 CoreDNS，将其用作联邦集群的 DNS 提供者
. heading &amp;ldquo;objectives&amp;rdquo; %}}  配置和部署 CoreDNS 服务器 使用 CoreDNS 作为 dns 提供者设置联邦 在 nameserver 查找链中设置 CoreDNS 服务器  . heading &amp;ldquo;prerequisites&amp;rdquo; %}}  你需要有一个正在运行的 Kubernetes 集群（作为主机集群引用）。请参阅入门指南，了解平台的安装说明。 必须在联邦的集群成员中支持 LoadBalancer 服务，用来支持跨联邦集群的 CoreDNS 服务发现。  部署 CoreDNS 和 etcd 图表 CoreDNS 可以部署在各种配置中。下面解释的是一个参考，可以根据平台和联邦集群的需要进行调整。
为了部署 CoreDNS，我们将利用图表。 CoreDNS 将部署 etcd 作为后端，并且应该预先安装。etcd 也可以使用图表进行部署。下面显示了部署 etcd 的说明。
helm install --namespace my-namespace --name etcd-operator stable/etcd-operator helm upgrade --namespace my-namespace --set cluster.</description>
    </item>
    
    <item>
      <title>配置 Pod 初始化</title>
      <link>https://lijun.in/tasks/configure-pod-container/configure-pod-initialization/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/configure-pod-container/configure-pod-initialization/</guid>
      <description>本文介绍在应用容器运行前，怎样利用 Init 容器初始化 Pod。
. heading &amp;ldquo;prerequisites&amp;rdquo; %}} . include &amp;ldquo;task-tutorial-prereqs.md&amp;rdquo; &amp;gt;}} . version-check &amp;gt;}}
创建一个包含 Init 容器的 Pod 本例中您将创建一个包含一个应用容器和一个 Init 容器的 Pod。Init 容器在应用容器启动前运行完成。
下面是 Pod 的配置文件：
. codenew file=&amp;quot;pods/init-containers.yaml&amp;rdquo; &amp;gt;}}
配置文件中，您可以看到应用容器和 Init 容器共享了一个卷。
Init 容器将共享卷挂载到了 /work-dir 目录，应用容器将共享卷挂载到了 /usr/share/nginx/html 目录。 Init 容器执行完下面的命令就终止：
wget -O /work-dir/index.html http://kubernetes.io  请注意 Init 容器在 nginx 服务器的根目录写入 index.html。
创建 Pod：
kubectl create -f https://k8s.io/examples/pods/init-containers.yaml  检查 nginx 容器运行正常：
kubectl get pod init-demo  结果表明 nginx 容器运行正常：</description>
    </item>
    
    <item>
      <title>为容器的生命周期事件设置处理函数</title>
      <link>https://lijun.in/tasks/configure-pod-container/attach-handler-lifecycle-event/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/configure-pod-container/attach-handler-lifecycle-event/</guid>
      <description>这个页面将演示如何为容器的生命周期事件挂接处理函数。Kubernetes 支持 postStart 和 preStop 事件。 当一个容器启动后，Kubernetes 将立即发送 postStart 事件；在容器被终结之前， Kubernetes 将发送一个 preStop 事件。
. heading &amp;ldquo;prerequisites&amp;rdquo; %}} . include &amp;ldquo;task-tutorial-prereqs.md&amp;rdquo; &amp;gt;}} . version-check &amp;gt;}}
定义 postStart 和 preStop 处理函数 在本练习中，你将创建一个包含一个容器的 Pod，该容器为 postStart 和 preStop 事件提供对应的处理函数。
下面是对应 Pod 的配置文件
. codenew file=&amp;quot;pods/lifecycle-events.yaml&amp;rdquo; &amp;gt;}}
在上述配置文件中，你可以看到 postStart 命令在容器的 /usr/share 目录下写入文件 message。 命令 preStop 负责优雅地终止 nginx 服务。当因为失效而导致容器终止时，这一处理方式很有用。```
创建 Pod：
kubectl apply -f https://k8s.io/examples/pods/lifecycle-events.yaml  验证 Pod 中的容器已经运行：
kubectl get pod lifecycle-demo  使用 shell 连接到你的 Pod 里的容器：</description>
    </item>
    
    <item>
      <title>使用联合服务来实现跨集群的服务发现</title>
      <link>https://lijun.in/tasks/federation/federation-service-discovery/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/federation/federation-service-discovery/</guid>
      <description>. deprecationfilewarning &amp;gt;}} . include &amp;ldquo;federation-deprecation-warning-note.md&amp;rdquo; &amp;gt;}} . /deprecationfilewarning &amp;gt;}}
本指南说明了如何使用 Kubernetes 联合服务跨多个 Kubernetes 集群部署通用服务。这样可以轻松实现 Kubernetes 应用程序的跨集群服务发现和可用区容错。
联合服务的创建与传统服务几乎相同 Kubernetes Services 即通过 API 调用来指定所需的服务属性。对于联合服务，此 API 调用定向到联合身份验证 API 接入点，而不是 Kubernetes 集群 API 接入点。联合服务的 API 与传统 Kubernetes 服务的 API 是 100% 兼容的。
创建后，联合服务会自动:
 在基础集群联合的每个集群中创建匹配的 Kubernetes 服务, 监视那些服务 &amp;ldquo;分片&amp;rdquo;(及其驻留的集群)的运行状况,以及 在公共 DNS 提供商(例如 Google Cloud DNS 或 AWS Route 53)中管理一组 DNS 记录,即使在集群可用区域中断的情况下，也能确保您联合服务的客户端始终可以无缝地定位合适的健康服务接入点。  如果存在健康的分片，联合 Kubernetes 集群(即 Pods )中的客户端将自动在其中找到联合服务的本地分片集群或者集群中最接近的健康分片;如果不存在，则使用最接近的其他集群的健康分片。
. toc &amp;gt;}}
. heading &amp;ldquo;prerequisites&amp;rdquo; %}} .</description>
    </item>
    
    <item>
      <title>使用 ConfigMap 配置 Pod</title>
      <link>https://lijun.in/tasks/configure-pod-container/configure-pod-configmap/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/configure-pod-container/configure-pod-configmap/</guid>
      <description>ConfigMap 允许您将配置文件与镜像文件分离，以使容器化的应用程序具有可移植性。该页面提供了一系列使用示例，这些示例演示了如何使用存储在 ConfigMap 中的数据创建 ConfigMap 和配置 Pod。
. heading &amp;ldquo;prerequisites&amp;rdquo; %}} . include &amp;ldquo;task-tutorial-prereqs.md&amp;rdquo; &amp;gt;}} . version-check &amp;gt;}}
创建 ConfigMap 您可以在 kustomization.yaml 中使用 kubectl create configmap 或 ConfigMap 生成器来创建ConfigMap。注意，从 1.14 版本开始， kubectl 开始支持 kustomization.yaml。
使用 kubectl 创建 ConfigMap 在目录, 文件, 或者文字值中使用 kubectl create configmap 命令创建configmap：
kubectl create configmap &amp;lt;map-name&amp;gt; &amp;lt;data-source&amp;gt; 其中， &amp;lt;map-name&amp;gt; 是要分配给 ConfigMap 的名称，&amp;lt;data-source&amp;gt; 是要从中提取数据的目录，文件或者文字值。
数据源对应于 ConfigMap 中的 key-value (键值对)
 key = 您在命令行上提供的文件名或者密钥 value = 您在命令行上提供的文件内容或者文字值  您可以使用kubectl describe或者 kubectl get检索有关 ConfigMap 的信息。</description>
    </item>
    
    <item>
      <title>在 Pod 中的容器之间共享进程命名空间</title>
      <link>https://lijun.in/tasks/configure-pod-container/share-process-namespace/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/configure-pod-container/share-process-namespace/</guid>
      <description>. feature-state state=&amp;quot;stable&amp;rdquo; for_k8s_version=&amp;quot;v1.17&amp;rdquo; &amp;gt;}}
此页面展示如何为 pod 配置进程命名空间共享。 当启用进程命名空间共享时，容器中的进程对该 pod 中的所有其他容器都是可见的。
您可以使用此功能来配置协作容器，比如日志处理 sidecar 容器，或者对那些不包含诸如 shell 等调试实用工具的镜像进行故障排查。
. heading &amp;ldquo;prerequisites&amp;rdquo; %}} . include &amp;ldquo;task-tutorial-prereqs.md&amp;rdquo; &amp;gt;}} . version-check &amp;gt;}}
配置 Pod 进程命名空间共享使用 v1.PodSpec 中的 ShareProcessNamespace 字段启用。例如：
. codenew file=&amp;quot;pods/share-process-namespace.yaml&amp;rdquo; &amp;gt;}}
  在集群中创建 nginx pod：
kubectl apply -f https://k8s.io/examples/pods/share-process-namespace.yaml   获取容器 shell，执行 ps：
kubectl attach -it nginx -c shell 如果没有看到命令提示符，请按 enter 回车键。
/ # ps ax PID USER TIME COMMAND 1 root 0:00 /pause 8 root 0:00 nginx: master process nginx -g daemon off; 14 101 0:00 nginx: worker process 15 root 0:00 sh 21 root 0:00 ps ax   您可以在其他容器中对进程发出信号。例如，发送 SIGHUP 到 nginx 以重启工作进程。这需要 SYS_PTRACE 功能。</description>
    </item>
    
    <item>
      <title>创建静态 Pod</title>
      <link>https://lijun.in/tasks/configure-pod-container/static-pod/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/configure-pod-container/static-pod/</guid>
      <description>静态 Pod 在指定的节点上由 kubelet 守护进程直接管理，不需要 . glossary_tooltip text=&amp;quot;API 服务&amp;rdquo; term_id=&amp;quot;kube-apiserver&amp;rdquo; &amp;gt;}} 监管。 不像 Pod 是由控制面管理的（例如，. glossary_tooltip text=&amp;quot;Deployment&amp;rdquo; term_id=&amp;quot;deployment&amp;rdquo; &amp;gt;}}）；相反 kubelet 监视每个静态 Pod（在它崩溃之后重新启动）。
静态 Pod 永远都会绑定到一个指定节点上的 . glossary_tooltip term_id=&amp;quot;kubelet&amp;rdquo; &amp;gt;}}。
kubelet 会尝试通过 Kubernetes API 服务器为每个静态 Pod 自动创建一个 . glossary_tooltip text=&amp;quot;镜像 Pod&amp;rdquo; term_id=&amp;quot;mirror-pod&amp;rdquo; &amp;gt;}}。 这意味着节点上运行的静态 Pod 对 API 服务来说是不可见的，但是不能通过 API 服务器来控制。
. note &amp;gt;}} 如果你在运行一个 Kubernetes 集群，并且在每个节点上都运行一个静态 Pod，就可能需要考虑使用 . glossary_tooltip text=&amp;quot;DaemonSet&amp;rdquo; term_id=&amp;quot;daemonset&amp;rdquo; &amp;gt;}} 替代这种方式。 . /note &amp;gt;}}
. heading &amp;ldquo;prerequisites&amp;rdquo; %}} . include &amp;ldquo;task-tutorial-prereqs.</description>
    </item>
    
    <item>
      <title>将 Docker Compose 文件转换为 Kubernetes 资源</title>
      <link>https://lijun.in/tasks/configure-pod-container/translate-compose-kubernetes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/configure-pod-container/translate-compose-kubernetes/</guid>
      <description>Kompose 是什么？它是个转换工具，可将 compose（即 Docker Compose）所组装的所有内容转换成容器编排器（Kubernetes 或 OpenShift）可识别的形式。
更多信息请参考 Kompose 官网 http://kompose.io。
. heading &amp;ldquo;prerequisites&amp;rdquo; %}} . include &amp;ldquo;task-tutorial-prereqs.md&amp;rdquo; &amp;gt;}} . version-check &amp;gt;}}
安装 Kompose 我们有很多种方式安装 Kompose。首选方式是从最新的 GitHub 发布页面下载二进制文件。
GitHub 发布版本 Kompose 通过 GitHub 发布版本，发布周期为三星期。您可以在GitHub 发布页面上看到所有当前版本。
# Linux curl -L https://github.com/kubernetes/kompose/releases/download/v1.16.0/kompose-linux-amd64 -o kompose # macOS curl -L https://github.com/kubernetes/kompose/releases/download/v1.16.0/kompose-darwin-amd64 -o kompose # Windows curl -L https://github.com/kubernetes/kompose/releases/download/v1.16.0/kompose-windows-amd64.exe -o kompose.exe chmod +x kompose sudo mv ./kompose /usr/local/bin/kompose 或者，您可以下载 tarball。
Go 用 go get 命令从主分支拉取最新的开发变更的方法安装 Kompose。</description>
    </item>
    
    <item>
      <title></title>
      <link>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm/</guid>
      <description>摘要 ┌──────────────────────────────────────────────────────────┐ │ KUBEADM │ │ 轻松创建一个安全的 Kubernetes 集群 │ │ │ │ 给我们反馈意见的地址： │ │ https://github.com/kubernetes/kubeadm/issues │ └──────────────────────────────────────────────────────────┘ 用途示例：
创建一个有两台机器的集群，包含一个主节点（用来控制集群），和一个工作节点（运行您的工作负载，像 Pod 和 Deployment）。
┌──────────────────────────────────────────────────────────┐ │ 在第一台机器上： │ ├──────────────────────────────────────────────────────────┤ │ control-plane# kubeadm init │ └──────────────────────────────────────────────────────────┘ ┌──────────────────────────────────────────────────────────┐ │ 在第二台机器上： │ ├──────────────────────────────────────────────────────────┤ │ worker# kubeadm join &amp;amp;lt;arguments-returned-from-init&amp;amp;gt;│ └──────────────────────────────────────────────────────────┘ 您可以重复第二步，向集群添加更多机器。
选项 </description>
    </item>
    
    <item>
      <title></title>
      <link>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_alpha/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_alpha/</guid>
      <description>概要 kubeadm 实验子命令
选项 从父命令继承的选项 </description>
    </item>
    
    <item>
      <title></title>
      <link>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_alpha_certs_certificate-key/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_alpha_certs_certificate-key/</guid>
      <description>概要 该命令将打印出可以与 &amp;ldquo;init&amp;rdquo; 命令一起使用的安全的随机生成的证书密钥。
您也可以使用 kubeadm init --upload-certs 而无需指定证书密钥，它将为您生成并打印一个证书密钥。
kubeadm alpha certs certificate-key [flags] 选项 从父命令继承的选项 </description>
    </item>
    
    <item>
      <title></title>
      <link>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_alpha_certs_check-expiration/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_alpha_certs_check-expiration/</guid>
      <description>概要 检查 kubeadm 管理的本地 PKI 中证书的到期时间。
kubeadm alpha certs check-expiration [flags] 选项 继承于父命令的选项 </description>
    </item>
    
    <item>
      <title></title>
      <link>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_alpha_certs_renew/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_alpha_certs_renew/</guid>
      <description>概要 此命令并非设计用来单独运行。请参阅可用子命令列表。
kubeadm alpha certs renew [flags] 选项 从父命令继承的选项 </description>
    </item>
    
    <item>
      <title></title>
      <link>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_alpha_certs_renew_admin.conf/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_alpha_certs_renew_admin.conf/</guid>
      <description>概要 续订 kubeconfig 文件中嵌入的证书，供管理员 和 kubeadm 自身使用。
无论证书的到期日期如何，续订都是无条件进行的；SAN 等额外属性将基于现有文件/证书，因此无需重新提供它们。
默认情况下，续订会尝试使用由 kubeadm 管理的本地 PKI 中的证书机构；作为替代方案， 也可以使用 K8s certificate API 进行证书续订，或者（作为最后一种选择）生成 CSR 请求。
续订后，为了使更改生效，需要重新启动控制平面组件，并最终重新分发更新的证书，以防证书文件在其他地方使用。
kubeadm alpha certs renew admin.conf [flags] 选项 从父命令继承的选项 </description>
    </item>
    
    <item>
      <title></title>
      <link>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_alpha_certs_renew_all/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_alpha_certs_renew_all/</guid>
      <description>概要 续订运行控制平面所需的所有已知证书。续订是无条件进行的，与到期日期无关。续订也可以单独运行以进行更多控制。
kubeadm alpha certs renew all [flags] 选项 从父命令继承的选项 </description>
    </item>
    
    <item>
      <title></title>
      <link>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_alpha_certs_renew_apiserver-etcd-client/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_alpha_certs_renew_apiserver-etcd-client/</guid>
      <description>概要 续订 apiserver 用于访问 etcd 的证书。
无论证书的到期日期如何，续订都会无条件地进行；SAN 等额外属性将基于现有文件/证书，因此无需重新提供它们。
默认情况下，续订尝试在 kubeadm 管理的本地 PKI 中使用证书颁发机构；作为替代方案， 可以使用 K8s 证书 API 进行证书更新，或者作为最后一个选择来生成 CSR 请求。
续订后，为了使更改生效，需要重新启动控制平面组件，并最终重新分发更新的证书，以防文件在其他地方使用。
kubeadm alpha certs renew apiserver-etcd-client [flags] 选项 从父命令继承的选项 </description>
    </item>
    
    <item>
      <title></title>
      <link>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_alpha_certs_renew_apiserver-kubelet-client/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_alpha_certs_renew_apiserver-kubelet-client/</guid>
      <description>概要 续订 apiserver 用于连接 kubelet 的证书。
无论证书的到期日期如何，续订都会无条件地进行；SAN 等额外属性将基于现有文件/证书，因此无需重新提供它们。
默认情况下，续订尝试使用位于 kubeadm 所管理的本地 PKI 中的证书颁发机构；作为替代方案， 也可能调用 K8s 证书 API 进行证书更新；亦或者，作为最后一个选择，生成 CSR 请求。
续订后，为了使更改生效，需要重新启动控制平面组件，并最终重新分发更新的证书，以防文件在其他地方使用。
kubeadm alpha certs renew apiserver-kubelet-client [flags] 选项 从父命令继承的选项 </description>
    </item>
    
    <item>
      <title></title>
      <link>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_alpha_certs_renew_apiserver/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_alpha_certs_renew_apiserver/</guid>
      <description>概要 续订用于提供 Kubernetes API 的证书。
无论证书的到期日期如何，续订都会无条件地进行；SAN 等额外属性将基于现有文件/证书，因此无需重新提供它们。
默认情况下，续订尝试在 kubeadm 管理的本地 PKI 中使用证书颁发机构；作为替代方案， 可以使用 K8s 证书 API 进行证书更新，或者作为最后一个选择来生成 CSR 请求。
续订后，为了使更改生效，需要重新启动控制平面组件，并最终重新分发更新的证书，以防文件在其他地方使用。
kubeadm alpha certs renew apiserver [flags] 选项 从父命令继承的选项 </description>
    </item>
    
    <item>
      <title></title>
      <link>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_alpha_certs_renew_controller-manager.conf/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_alpha_certs_renew_controller-manager.conf/</guid>
      <description>概要 续订 kubeconfig 文件中嵌入的证书，以供控制器管理器（controller manager）使用。
续订无条件地进行，与证书的到期日期无关；SAN 等额外属性将基于现有的文件/证书，因此无需重新提供它们。
默认情况下，续订会尝试在 kubeadm 管理的本地 PKI 中使用证书颁发机构；作为替代方案， 可以使用 K8s 证书 API 进行证书续订；亦或者，作为最后一种选择，生成 CSR 请求。
续订后，为了使更改生效，需要重新启动控制平面组件，并最终重新分发更新的证书，以防文件在其他地方使用。
kubeadm alpha certs renew controller-manager.conf [flags] 选项 从父命令继承的选项 </description>
    </item>
    
    <item>
      <title></title>
      <link>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_alpha_certs_renew_etcd-healthcheck-client/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_alpha_certs_renew_etcd-healthcheck-client/</guid>
      <description>概要 续订 etcd 健康检查的活跃性探针的证书。
无论证书的到期日期如何，续订都是无条件进行的；SAN 等额外属性将基于现有文件/证书，因此无需重新提供它们。
默认情况下，续订会尝试使用由 kubeadm 管理的本地 PKI 中的证书机构；作为替代方案，也可以使用 K8s certificate API 进行证书续订，或者（作为最后一种选择）生成 CSR 请求。
续订后，为了使更改生效，需要重新启动控制平面组件，并最终重新分发更新的证书，以防证书文件在其他地方使用。
kubeadm alpha certs renew etcd-healthcheck-client [flags] 选项 从父命令继承的选项 </description>
    </item>
    
    <item>
      <title></title>
      <link>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_alpha_certs_renew_etcd-peer/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_alpha_certs_renew_etcd-peer/</guid>
      <description>概要 续订 etcd 节点间用来相互通信的证书。
无论证书的到期日期如何，续订都是无条件进行的；SAN 等额外属性将基于现有文件/证书，因此无需重新提供它们。
默认情况下，续订会尝试使用由 kubeadm 管理的本地 PKI 中的证书机构；作为替代方案，也可以使用 K8s certificate API 进行证书续订，或者（作为最后一种选择）生成 CSR 请求。
续订后，为了使更改生效，需要重新启动控制平面组件，并最终重新分发更新的证书，以防证书文件在其他地方使用。
kubeadm alpha certs renew etcd-peer [flags] 选项 从父命令继承的选项 </description>
    </item>
    
    <item>
      <title></title>
      <link>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_alpha_certs_renew_etcd-server/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_alpha_certs_renew_etcd-server/</guid>
      <description>概要 续订用于提供 etcd 的证书。
续订无条件地进行，与证书的到期日期无关；SAN 等额外属性将基于现有的文件/证书，因此无需重新提供它们。
默认情况下，续订会尝试在 kubeadm 管理的本地 PKI 中使用证书颁发机构；作为替代方案，可以使用 K8s 证书 API 进行证书续订，或者作为最后一种选择来生成 CSR 请求。
续订后，为了使更改生效，需要重新启动控制平面组件，并最终重新分发更新的证书，以防文件在其他地方使用。
kubeadm alpha certs renew etcd-server [flags] 选项 从父命令继承的选项 </description>
    </item>
    
    <item>
      <title></title>
      <link>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_alpha_certs_renew_front-proxy-client/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_alpha_certs_renew_front-proxy-client/</guid>
      <description>概要 为前端代理客户端续订证书。
无论证书的到期日期如何，续订都会无条件地进行；SAN 等额外属性将基于现有文件/证书，因此无需重新提供它们。
默认情况下，续订尝试使用位于 kubeadm 所管理的本地 PKI 中的证书颁发机构；作为替代方案， 也可以使用 K8s 证书 API 进行证书续订；亦或者，作为最后一种方案，生成 CSR 请求。
续订后，为了使更改生效，需要重新启动控制平面组件，并最终重新分发更新的证书，以防文件在其他地方使用。
kubeadm alpha certs renew front-proxy-client [flags] 选项 从父命令继承的选项 </description>
    </item>
    
    <item>
      <title></title>
      <link>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_alpha_certs_renew_scheduler.conf/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_alpha_certs_renew_scheduler.conf/</guid>
      <description>概要 续订 kubeconfig 文件中嵌入的证书，以供调度管理器使用。
续订无条件地进行，与证书的到期日期无关；SAN 等额外属性将基于现有的文件/证书，因此无需重新提供它们。
默认情况下，续订会尝试在 kubeadm 管理的本地 PKI 中使用证书颁发机构；作为替代方案，可以使用 K8s 证书 API 进行证书续订；亦或者，作为最后一种选择，生成 CSR 请求。
续订后，为了使更改生效，需要重新启动控制平面组件，并最终重新分发更新的证书，以防文件在其他地方使用。
kubeadm alpha certs renew scheduler.conf [flags] 选项 从父命令继承的选项 </description>
    </item>
    
    <item>
      <title></title>
      <link>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_alpha_kubeconfig/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_alpha_kubeconfig/</guid>
      <description>概要 kubeconfig 文件应用程序。
Alpha 免责声明：此命令当前为 alpha 功能。
选项 从父命令继承的选项 </description>
    </item>
    
    <item>
      <title></title>
      <link>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_alpha_kubeconfig_user/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_alpha_kubeconfig_user/</guid>
      <description>概要 为其他用户输出 kubeconfig 文件。
Alpha 免责声明：此命令当前为 Alpha 功能。
kubeadm alpha kubeconfig user [flags] 示例 # 为名为 foo 的其他用户输出 kubeconfig 文件 kubeadm alpha kubeconfig user --client-name=foo 选项 从父命令继承的选项 </description>
    </item>
    
    <item>
      <title></title>
      <link>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_alpha_kubelet/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_alpha_kubelet/</guid>
      <description>概要 此命令并非设计用来单独运行。请参阅可用子命令列表。
选项 从父命令继承的选项 </description>
    </item>
    
    <item>
      <title></title>
      <link>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_alpha_kubelet_config/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_alpha_kubelet_config/</guid>
      <description>概要 此命令并非设计用来单独运行。请参阅可用子命令列表。
选项 从父命令继承的选项 </description>
    </item>
    
    <item>
      <title></title>
      <link>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_alpha_kubelet_config_download/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_alpha_kubelet_config_download/</guid>
      <description>概要 从集群中形式为 &amp;ldquo;kubelet-config-1.X&amp;rdquo; 的 ConfigMap 中下载 kubelet 配置，其中 X 是 kubelet 的次要版本。 kubeadm 要么通过执行 &amp;ldquo;kubelet &amp;ndash;version&amp;rdquo; 自动检测 kubelet 版本，要么传递 &amp;ndash;kubelet-version 参数。
Alpha 免责声明：此命令当前为 Alpha 功能。
kubeadm alpha kubelet config download [flags] 示例 # 从集群中的 ConfigMap 下载 kubelet 配置。自动检测 kubelet 版本。 kubeadm alpha phase kubelet config download # 从集群中的 ConfigMap 下载 kubelet 配置。使用特定的所需 kubelet 版本。 kubeadm alpha phase kubelet config download --kubelet-version 1.16.0 选项 从父命令继承的选项 </description>
    </item>
    
    <item>
      <title></title>
      <link>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_alpha_kubelet_config_enable-dynamic/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_alpha_kubelet_config_enable-dynamic/</guid>
      <description>概要 针对集群中的 kubelet-config-1.X ConfigMap 启用或更新节点的动态 kubelet 配置，其中 X 是所需 kubelet 版本的次要版本。
警告：此功能仍处于试验阶段，默认情况下处于禁用状态。仅当知道自己在做什么时才启用它，因为在此阶段它可能会产生令人惊讶的副作用。
Alpha 免责声明：此命令当前为 Alpha 功能。
kubeadm alpha kubelet config enable-dynamic [flags] 示例  # 为节点启用动态 kubelet 配置。 kubeadm alpha phase kubelet enable-dynamic-config --node-name node-1 --kubelet-version 1.16.0 WARNING: This feature is still experimental, and disabled by default. Enable only if you know what you are doing, as it may have surprising side-effects at this stage. 选项 从父命令继承的选项 </description>
    </item>
    
    <item>
      <title></title>
      <link>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_alpha_selfhosting/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_alpha_selfhosting/</guid>
      <description>概要 此命令并非设计用来单独运行。请参阅可用子命令列表。
选项 从父命令继承的选项 </description>
    </item>
    
    <item>
      <title></title>
      <link>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_alpha_selfhosting_pivot/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_alpha_selfhosting_pivot/</guid>
      <description>概要 将用于控制平面组件的静态 Pod 文件转换为通过 Kubernetes API 配置的自托管 DaemonSet。
有关自托管的限制，请参阅相关文档。
Alpha 免责声明：此命令当前为 alpha 功能。
kubeadm alpha selfhosting pivot [flags] 示例 # 将静态 Pod 托管的控制平面转换为自托管的控制平面。 kubeadm alpha phase self-hosting convert-from-staticpods 选项 从父命令继承的选项 </description>
    </item>
    
    <item>
      <title></title>
      <link>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_completion/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_completion/</guid>
      <description>概要 为指定的 shell（bash 或 zsh）输出 shell 自动补全代码。 必须激活 shell 代码以提供交互式 kubeadm 命令补全。这可以通过加载 .bash_profile 文件完成。
注意: 此功能依赖于 bash-completion 框架。
在 Mac 上使用 homebrew 安装:
brew install bash-completion 安装后，必须激活 bash_completion。这可以通过在 .bash_profile 文件中添加下面的命令行来完成
source $(brew --prefix)/etc/bash_completion 如果在 Linux 上没有安装 bash-completion，请通过您的发行版的包管理器安装 bash-completion 软件包。
zsh 用户注意事项：[1] zsh 自动补全仅在 &amp;gt;=v5.2 及以上版本中支持。
kubeadm completion SHELL [flags] 示例 # 在 Mac 上使用 homebrew 安装 bash completion brew install bash-completion printf &amp;quot;\n# Bash completion support\nsource $(brew --prefix)/etc/bash_completion\n&amp;quot; &amp;gt;&amp;gt; $HOME/.bash_profile source $HOME/.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_config/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_config/</guid>
      <description>概要 kube-system 命名空间里有一个名为 &amp;ldquo;kubeadm-config&amp;rdquo; 的 ConfigMap，kubeadm 用它来存储有关集群的内部配置。 kubeadm CLI v1.8.0+ 通过一个配置自动创建该 ConfigMap，这个配置是和 &amp;lsquo;kubeadm init&amp;rsquo; 共用的。 但是您如果使用 kubeadm v1.7.x 或更低的版本初始化集群，那么必须使用 &amp;lsquo;config upload&amp;rsquo; 命令创建该 ConfigMap。 这是必要的操作，目的是使 &amp;lsquo;kubeadm upgrade&amp;rsquo; 能够正确地配置升级后的集群。
kubeadm config [flags] 选项 从父命令继承的选项 </description>
    </item>
    
    <item>
      <title></title>
      <link>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_config_images/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_config_images/</guid>
      <description>概要 与 kubeadm 使用的容器镜像交互。
kubeadm config images [flags] 选项 从父命令继承的选项 </description>
    </item>
    
    <item>
      <title></title>
      <link>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_config_images_list/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_config_images_list/</guid>
      <description>概要 打印 kubeadm 要使用的镜像列表。配置文件用于自定义任何镜像或镜像存储库。
kubeadm config images list [flags] 选项 从父命令继承的选项 </description>
    </item>
    
    <item>
      <title></title>
      <link>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_config_images_pull/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_config_images_pull/</guid>
      <description>概要 拉取 kubeadm 使用的镜像。
kubeadm config images pull [flags] 选项 从父命令继承的选项 </description>
    </item>
    
    <item>
      <title></title>
      <link>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_config_migrate/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_config_migrate/</guid>
      <description>概要 此命令允许您在 CLI 工具中将本地旧版本的配置对象转换为最新支持的版本，而无需变更集群中的任何内容。在此版本的 kubeadm 中，支持以下 API 版本：
 kubeadm.k8s.io/v1beta2  因此，无论您在此处传递 &amp;ndash;old-config 参数的版本是什么，当写入到 stdout 或 &amp;ndash;new-config （如果已指定）时， 都会读取、反序列化、默认、转换、验证和重新序列化 API 对象。
换句话说，如果您将此文件传递给 &amp;ldquo;kubeadm init&amp;rdquo;，则该命令的输出就是 kubeadm 实际上在内部读取的内容。
kubeadm config migrate [flags] 选项 从父命令继承的选项 </description>
    </item>
    
    <item>
      <title></title>
      <link>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_config_print/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_config_print/</guid>
      <description>概要 此命令显示所提供子命令的配置。 有关详细信息，请参阅：https://godoc.org/k8s.io/kubernetes/cmd/kubeadm/app/apis/kubeadm/v1beta2
kubeadm config print [flags] 选项 从父命令继承的选项 </description>
    </item>
    
    <item>
      <title></title>
      <link>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_config_print_init-defaults/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_config_print_init-defaults/</guid>
      <description>概要 此命令打印对象，例如用于 &amp;lsquo;kubeadm init&amp;rsquo; 的默认 init 配置对象。
请注意，Bootstrap Token 字段之类的敏感值已替换为 {&amp;ldquo;abcdef.0123456789abcdef&amp;rdquo; &amp;quot;&amp;rdquo; &amp;ldquo;nil&amp;rdquo; &amp;lt;nil&amp;gt; [] []} 之类的占位符值以通过验证，但不执行创建令牌的实际计算。
kubeadm config print init-defaults [flags] 选项 从父命令继承的选项 </description>
    </item>
    
    <item>
      <title></title>
      <link>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_config_print_join-defaults/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_config_print_join-defaults/</guid>
      <description>概要 此命令打印对象，例如用于 &amp;lsquo;kubeadm join&amp;rsquo; 的默认 join 配置对象。
请注意，诸如启动引导令牌字段之类的敏感值已替换为 {&amp;ldquo;abcdef.0123456789abcdef&amp;rdquo; &amp;quot;&amp;rdquo; &amp;ldquo;nil&amp;rdquo; &amp;lt;nil&amp;gt; [] []} 之类的占位符值以通过验证，但不执行创建令牌的实际计算。
kubeadm config print join-defaults [flags] 选项 从父命令继承的选项 </description>
    </item>
    
    <item>
      <title></title>
      <link>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_config_view/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_config_view/</guid>
      <description>概要 使用此命令，可以查看 kubeadm 配置的集群中的 ConfigMap。 该配置位于 &amp;ldquo;kube-system&amp;rdquo; 命名空间中的名为 &amp;ldquo;kubeadm-config&amp;rdquo; 的 ConfigMap 中。
kubeadm config view [flags] 选项 继承于父命令的选项 </description>
    </item>
    
    <item>
      <title></title>
      <link>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_init/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_init/</guid>
      <description>概要 运行此命令来搭建 Kubernetes 控制平面节点。
&amp;ldquo;init&amp;rdquo; 命令执行以下阶段：
preflight Run pre-flight checks kubelet-start Write kubelet settings and (re)start the kubelet certs Certificate generation /ca Generate the self-signed Kubernetes CA to provision identities for other Kubernetes components /apiserver Generate the certificate for serving the Kubernetes API /apiserver-kubelet-client Generate the certificate for the API server to connect to kubelet /front-proxy-ca Generate the self-signed CA to provision identities for front proxy /front-proxy-client Generate the certificate for the front proxy client /etcd-ca Generate the self-signed CA to provision identities for etcd /etcd-server Generate the certificate for serving etcd /etcd-peer Generate the certificate for etcd nodes to communicate with each other /etcd-healthcheck-client Generate the certificate for liveness probes to healthcheck etcd /apiserver-etcd-client Generate the certificate the apiserver uses to access etcd /sa Generate a private key for signing service account tokens along with its public key kubeconfig Generate all kubeconfig files necessary to establish the control plane and the admin kubeconfig file /admin Generate a kubeconfig file for the admin to use and for kubeadm itself /kubelet Generate a kubeconfig file for the kubelet to use *only* for cluster bootstrapping purposes /controller-manager Generate a kubeconfig file for the controller manager to use /scheduler Generate a kubeconfig file for the scheduler to use control-plane Generate all static Pod manifest files necessary to establish the control plane /apiserver Generates the kube-apiserver static Pod manifest /controller-manager Generates the kube-controller-manager static Pod manifest /scheduler Generates the kube-scheduler static Pod manifest etcd Generate static Pod manifest file for local etcd /local Generate the static Pod manifest file for a local, single-node local etcd instance upload-config Upload the kubeadm and kubelet configuration to a ConfigMap /kubeadm Upload the kubeadm ClusterConfiguration to a ConfigMap /kubelet Upload the kubelet component config to a ConfigMap upload-certs Upload certificates to kubeadm-certs mark-control-plane Mark a node as a control-plane bootstrap-token Generates bootstrap tokens used to join a node to a cluster addon Install required addons for passing Conformance tests /coredns Install the CoreDNS addon to a Kubernetes cluster /kube-proxy Install the kube-proxy addon to a Kubernetes cluster kubeadm init [flags] 选项 从父命令继承的选项 </description>
    </item>
    
    <item>
      <title></title>
      <link>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_init_phase/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_init_phase/</guid>
      <description>概要 使用此命令可以调用 init 工作流程的单个阶段
选项 继承于父命令的选择项 </description>
    </item>
    
    <item>
      <title></title>
      <link>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_init_phase_addon/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_init_phase_addon/</guid>
      <description>概要 此命令并非设计用来单独运行。请参阅可用子命令列表。
kubeadm init phase addon [flags] 选项 继承于父命令的选项 </description>
    </item>
    
    <item>
      <title></title>
      <link>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_init_phase_addon_all/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_init_phase_addon_all/</guid>
      <description>概要 安装所有插件（addon）
kubeadm init phase addon all [flags] 选项 继承于父命令的选项 </description>
    </item>
    
    <item>
      <title></title>
      <link>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_init_phase_addon_coredns/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_init_phase_addon_coredns/</guid>
      <description>概要 通过 API 服务器安装 CoreDNS 附加组件。请注意，即使 DNS 服务器已部署，在安装 CNI 之前 DNS 服务器不会被调度执行。
kubeadm init phase addon coredns [flags] 选项 继承于父命令的选项 </description>
    </item>
    
    <item>
      <title></title>
      <link>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_init_phase_addon_kube-proxy/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_init_phase_addon_kube-proxy/</guid>
      <description>概要 通过 API 服务器安装 kube-proxy 附加组件。
kubeadm init phase addon kube-proxy [flags] 选项 继承于父命令的选项 </description>
    </item>
    
    <item>
      <title></title>
      <link>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_init_phase_bootstrap-token/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_init_phase_bootstrap-token/</guid>
      <description>概要 启动引导令牌（bootstrap token）用于在即将加入集群的节点和控制平面节点之间建立双向信任。
该命令使启动引导令牌（bootstrap token）所需的所有配置生效，然后创建初始令牌。
kubeadm init phase bootstrap-token [flags] 示例 # 进行所有引导令牌配置，并创建一个初始令牌，功能上与 kubeadm init 生成的令牌等效。 kubeadm init phase bootstrap-token 选项 继承于父命令的选项 </description>
    </item>
    
    <item>
      <title></title>
      <link>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_init_phase_certs/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_init_phase_certs/</guid>
      <description>概要 此命令并非设计用来单独运行。请参阅可用子命令列表。
kubeadm init phase certs [flags] 选项 从父指令中继承的选项 </description>
    </item>
    
    <item>
      <title></title>
      <link>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_init_phase_certs_all/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_init_phase_certs_all/</guid>
      <description>概要 生成所有证书
kubeadm init phase certs all [flags] 选项 继承于父命令的选项 </description>
    </item>
    
    <item>
      <title></title>
      <link>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_init_phase_certs_apiserver-etcd-client/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_init_phase_certs_apiserver-etcd-client/</guid>
      <description>概要 生成 apiserver 用于访问 etcd 的证书，并将其保存到 apiserver-etcd-client.cert 和 apiserver-etcd-client.key 文件中。
如果两个文件都已存在，则 kubeadm 将跳过生成步骤，使用现有文件。
Alpha 免责声明：此命令当前为 Alpha 功能。
kubeadm init phase certs apiserver-etcd-client [flags] 选项 继承于父命令的选项 </description>
    </item>
    
    <item>
      <title></title>
      <link>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_init_phase_certs_apiserver-kubelet-client/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_init_phase_certs_apiserver-kubelet-client/</guid>
      <description>概要 生成供 API 服务器连接 kubelet 的证书，并将其保存到 apiserver-kubelet-client.cert 和 apiserver-kubelet-client.key 文件中。
如果两个文件都已存在，则 kubeadm 将跳过生成步骤，使用现有文件。
Alpha 免责声明：此命令当前为 Alpha 功能。
kubeadm init phase certs apiserver-kubelet-client [flags] 选项 继承于父命令的选项 </description>
    </item>
    
    <item>
      <title></title>
      <link>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_init_phase_certs_apiserver/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_init_phase_certs_apiserver/</guid>
      <description>概要 生成用于服务 Kubernetes API 的证书，并将其保存到 apiserver.cert 和 apiserver.key 文件中。
默认 SAN 是 kubernetes、kubernetes.default、kubernetes.default.svc、kubernetes.default.svc.cluster.local、10.96.0.1、127.0.0.1。
如果两个文件都已存在，则 kubeadm 将跳过生成步骤，使用现有文件。
Alpha 免责声明：此命令当前为 Alpha 功能。
kubeadm init phase certs apiserver [flags] 选项 继承于父命令的选项 </description>
    </item>
    
    <item>
      <title></title>
      <link>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_init_phase_certs_ca/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_init_phase_certs_ca/</guid>
      <description>概要 生成自签名的 Kubernetes CA 以提供其他 Kubernetes 组件的身份，并将其保存到 ca.cert 和 ca.key 文件中。
如果两个文件都已存在，则 kubeadm 将跳过生成步骤，使用现有文件。
Alpha 免责声明：此命令当前为 Alpha 功能。
kubeadm init phase certs ca [flags] 选项 继承于父命令的选项 </description>
    </item>
    
    <item>
      <title></title>
      <link>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_init_phase_certs_etcd-ca/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_init_phase_certs_etcd-ca/</guid>
      <description>概要 生成用于为 etcd 设置身份的自签名 CA，并将其保存到 etcd/ca.cert 和 etcd/ca.key 文件中。
如果两个文件都已存在，则 kubeadm 将跳过生成步骤，使用现有文件。
Alpha 免责声明：此命令当前为 Alpha 功能。
kubeadm init phase certs etcd-ca [flags] 选项 继承于父命令的选项 </description>
    </item>
    
    <item>
      <title></title>
      <link>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_init_phase_certs_etcd-healthcheck-client/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_init_phase_certs_etcd-healthcheck-client/</guid>
      <description>概要 生成用于 etcd 健康检查的活跃性探针的证书，并将其保存到 healthcheck-client.cert 和 etcd/healthcheck-client.key 文件中。
如果两个文件都已存在，则 kubeadm 将跳过生成步骤，使用现有文件。
Alpha 免责声明：此命令当前为 alpha 功能。
kubeadm init phase certs etcd-healthcheck-client [flags] 选项 继承于父命令的选项 </description>
    </item>
    
    <item>
      <title></title>
      <link>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_init_phase_certs_etcd-peer/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_init_phase_certs_etcd-peer/</guid>
      <description>概要 生成 etcd 节点相互通信的证书，并将其保存到 etcd/peer.cert 和 etcd/peer.key 文件中。
默认 SAN 为 localhost、127.0.0.1、127.0.0.1、:: 1
如果两个文件都已存在，则 kubeadm 将跳过生成步骤，使用现有文件。
Alpha 免责声明：此命令当前为 alpha 功能。
kubeadm init phase certs etcd-peer [flags] 选项 继承于父命令的选项 </description>
    </item>
    
    <item>
      <title></title>
      <link>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_init_phase_certs_etcd-server/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_init_phase_certs_etcd-server/</guid>
      <description>概要 生成用于提供 etcd 服务的证书，并将其保存到 etcd/server.cert 和 etcd/server.key 文件中。
默认 SAN 为 localhost、127.0.0.1、127.0.0.1、:: 1
如果两个文件都已存在，则 kubeadm 将跳过生成步骤，使用现有文件。
Alpha 免责声明：此命令当前为 alpha 功能。
kubeadm init phase certs etcd-server [flags] 选项 继承于父命令的选项 </description>
    </item>
    
    <item>
      <title></title>
      <link>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_init_phase_certs_front-proxy-ca/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_init_phase_certs_front-proxy-ca/</guid>
      <description>概要 生成自签名 CA 来提供前端代理的身份，并将其保存到 front-proxy-ca.cert 和 front-proxy-ca.key 文件中。
如果两个文件都已存在，kubeadm 将跳过生成步骤并将使用现有文件。
Alpha 免责声明：此命令目前是 alpha 阶段。
kubeadm init phase certs front-proxy-ca [flags] 选项 从父命令继承的选项 </description>
    </item>
    
    <item>
      <title></title>
      <link>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_init_phase_certs_front-proxy-client/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_init_phase_certs_front-proxy-client/</guid>
      <description>概要 为前端代理客户端生成证书，并将其保存到 front-proxy-client.cert 和 front-proxy-client.key 文件中。 如果两个文件都已存在，kubeadm 将跳过生成步骤并将使用现有文件。 Alpha 免责声明：此命令目前是 alpha 阶段。
kubeadm init phase certs front-proxy-client [flags] 选项 从父命令继承的选项 </description>
    </item>
    
    <item>
      <title></title>
      <link>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_init_phase_certs_sa/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_init_phase_certs_sa/</guid>
      <description>概要 生成用于签名 service account 令牌的私钥及其公钥，并将其保存到 sa.key 和 sa.pub 文件中。如果两个文件都已存在，则 kubeadm 会跳过生成步骤，而将使用现有文件。
Alpha 免责声明：此命令当前为 alpha 阶段。
kubeadm init phase certs sa [flags] 选项 继承于父命令的选项 </description>
    </item>
    
    <item>
      <title></title>
      <link>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_init_phase_control-plane/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_init_phase_control-plane/</guid>
      <description>概要 此命令并非设计用来单独运行。请参阅可用子命令列表。
kubeadm init phase control-plane [flags] 选项 继承于父命令的选项 </description>
    </item>
    
    <item>
      <title></title>
      <link>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_init_phase_control-plane_all/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_init_phase_control-plane_all/</guid>
      <description>概要 生成所有的静态 Pod 清单文件
kubeadm init phase control-plane all [flags] 示例 # 为 etcd 生成静态 Pod 清单文件，其功能等效于 kubeadm init 生成的文件。 kubeadm init phase control-plane all # 使用从配置文件读取的选项为 etcd 生成静态 Pod 清单文件。 kubeadm init phase control-plane all --config config.yaml 选项 从父指令继承的选项 </description>
    </item>
    
    <item>
      <title></title>
      <link>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_init_phase_control-plane_apiserver/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_init_phase_control-plane_apiserver/</guid>
      <description>概要 生成 kube-apiserver 静态 Pod 清单
kubeadm init phase control-plane apiserver [flags] 选项 继承于父命令的选项 </description>
    </item>
    
    <item>
      <title></title>
      <link>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_init_phase_control-plane_controller-manager/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_init_phase_control-plane_controller-manager/</guid>
      <description>概要 生成 kube-controller-manager 静态 Pod 清单
kubeadm init phase control-plane controller-manager [flags] 选项 从父命令继承的选项 </description>
    </item>
    
    <item>
      <title></title>
      <link>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_init_phase_control-plane_scheduler/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_init_phase_control-plane_scheduler/</guid>
      <description>概要 生成 kube-scheduler 静态 Pod 清单
kubeadm init phase control-plane scheduler [flags] 选项 继承于父命令的选项 </description>
    </item>
    
    <item>
      <title></title>
      <link>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_init_phase_etcd/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_init_phase_etcd/</guid>
      <description>概要 此命令并非设计用来单独运行。请参阅可用子命令列表。
kubeadm init phase etcd [flags] 选项 继承于父命令的选项 </description>
    </item>
    
    <item>
      <title></title>
      <link>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_init_phase_etcd_local/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_init_phase_etcd_local/</guid>
      <description>概要 为本地单节点 etcd 实例生成静态 Pod 清单文件
kubeadm init phase etcd local [flags] 示例 # 为 etcd 生成静态 Pod 清单文件，其功能等效于 kubeadm init 生成的文件。 kubeadm init phase etcd local # 使用从配置文件读取的选项为 etcd 生成静态 Pod 清单文件。 kubeadm init phase etcd local --config config.yaml 选项 继承于父命令的选项 </description>
    </item>
    
    <item>
      <title></title>
      <link>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_init_phase_kubeconfig/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_init_phase_kubeconfig/</guid>
      <description>概要 此命令并非设计用来单独运行。请阅读可用子命令列表。
kubeadm init phase kubeconfig [flags] 选项 从父命令继承的选项 </description>
    </item>
    
    <item>
      <title></title>
      <link>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_init_phase_kubeconfig_admin/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_init_phase_kubeconfig_admin/</guid>
      <description>概要 为管理员和 kubeadm 本身生成 kubeconfig 文件，并将其保存到 admin.conf 文件中。
kubeadm init phase kubeconfig admin [flags] 选项 继承于父命令的选项 </description>
    </item>
    
    <item>
      <title></title>
      <link>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_init_phase_kubeconfig_all/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_init_phase_kubeconfig_all/</guid>
      <description>概要 生成所有 kubeconfig 文件
kubeadm init phase kubeconfig all [flags] 选项 继承于父命令的选项 </description>
    </item>
    
    <item>
      <title></title>
      <link>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_init_phase_kubeconfig_controller-manager/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_init_phase_kubeconfig_controller-manager/</guid>
      <description>概要 生成控制器管理器要使用的 kubeconfig 文件，并保存到 controller-manager.conf 文件中。
kubeadm init phase kubeconfig controller-manager [flags] 选项 继承于父命令的选项 </description>
    </item>
    
    <item>
      <title></title>
      <link>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_init_phase_kubeconfig_kubelet/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_init_phase_kubeconfig_kubelet/</guid>
      <description>概要 生成 kubelet 要使用的 kubeconfig 文件，并将其保存到 kubelet.conf 文件。
请注意，该操作目的是仅应用于引导集群。在控制平面启动之后，应该从 CSR API 请求所有 kubelet 凭据。
kubeadm init phase kubeconfig kubelet [flags] 选项 继承于父命令的选项 </description>
    </item>
    
    <item>
      <title></title>
      <link>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_init_phase_kubeconfig_scheduler/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_init_phase_kubeconfig_scheduler/</guid>
      <description>概要 生成调度器（scheduler）要使用的 kubeconfig 文件，并保存到 scheduler.conf 文件中。
kubeadm init phase kubeconfig scheduler [flags] 选项 继承于父命令的选项 </description>
    </item>
    
    <item>
      <title></title>
      <link>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_init_phase_kubelet-start/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_init_phase_kubelet-start/</guid>
      <description>概要 使用 kubelet 配置文件编写一个文件，并使用特定节点的 kubelet 设置编写一个环境文件，然后（重新）启动 kubelet。
kubeadm init phase kubelet-start [flags] 示例 # 从 InitConfiguration 文件中写入带有 kubelet 参数的动态环境文件。 kubeadm init phase kubelet-start --config config.yaml 选项 从父命令继承的选项 </description>
    </item>
    
    <item>
      <title></title>
      <link>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_init_phase_mark-control-plane/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_init_phase_mark-control-plane/</guid>
      <description>概要 标记 Node 节点为控制平面节点
kubeadm init phase mark-control-plane [flags] 示例 # 将控制平面标签和污点应用于当前节点，其功能等效于 kubeadm init执行的操作。 kubeadm init phase mark-control-plane --config config.yml # 将控制平面标签和污点应用于特定节点 kubeadm init phase mark-control-plane --node-name myNode 选项 从父命令继承的选项 </description>
    </item>
    
    <item>
      <title></title>
      <link>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_init_phase_preflight/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_init_phase_preflight/</guid>
      <description>概要 运行 kubeadm init 前的启动检查。
kubeadm init phase preflight [flags] 案例 # 使用配置文件对 kubeadm init 进行启动检查。 kubeadm init phase preflight --config kubeadm-config.yml 选项 继承于父命令的选项 </description>
    </item>
    
    <item>
      <title></title>
      <link>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_init_phase_upload-certs/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_init_phase_upload-certs/</guid>
      <description>概要 此命令并非设计用来单独运行。请参阅可用子命令列表。
kubeadm init phase upload-certs [flags] 选项 从父命令继承的选项 </description>
    </item>
    
    <item>
      <title></title>
      <link>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_init_phase_upload-config/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_init_phase_upload-config/</guid>
      <description>概要 此命令并非设计用来单独运行。请参阅可用的子命令列表。
kubeadm init phase upload-config [flags] 选项 从父命令中继承的选项 </description>
    </item>
    
    <item>
      <title></title>
      <link>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_init_phase_upload-config_all/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_init_phase_upload-config_all/</guid>
      <description>概要 将所有配置上传到 ConfigMap
kubeadm init phase upload-config all [flags] 选项 从父命令继承的选项 </description>
    </item>
    
    <item>
      <title></title>
      <link>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_init_phase_upload-config_kubeadm/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_init_phase_upload-config_kubeadm/</guid>
      <description>概要 将 kubeadm ClusterConfiguration 上传到 kube-system 命名空间中名为 kubeadm-config 的 ConfigMap 中。 这样就可以正确配置系统组件，并在升级时提供无缝的用户体验。
另外，可以使用 kubeadm 配置。
kubeadm init phase upload-config kubeadm [flags] 示例 # 上传集群配置 kubeadm init phase upload-config --config=myConfig.yaml 选项 从父命令继承的选项 </description>
    </item>
    
    <item>
      <title></title>
      <link>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_init_phase_upload-config_kubelet/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_init_phase_upload-config_kubelet/</guid>
      <description>概要 将从 kubeadm InitConfiguration 对象提取的 kubelet 配置上传到集群中 kubelet-config-1.X 形式的 ConfigMap，其中 X 是当前（API 服务器）Kubernetes 版本的次要版本。
kubeadm init phase upload-config kubelet [flags]示例 # 将 kubelet 配置从 kubeadm 配置文件上传到集群中的 ConfigMap。kubeadm init phase upload-config kubelet --config kubeadm.yaml选项 从父命令继承的选项 </description>
    </item>
    
    <item>
      <title></title>
      <link>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_join/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_join/</guid>
      <description>摘要 当节点加入 kubeadm 初始化的集群时，我们需要建立双向信任。 这个过程可以分解为发现（让待加入节点信任 Kubernetes 控制平面节点）和 TLS 引导（让Kubernetes 控制平面节点信任待加入节点）两个部分。
有两种主要的发现方案。 第一种方法是使用共享令牌和 API 服务器的 IP 地址。 第二种是提供一个文件 - 标准 kubeconfig 文件的一个子集。 该文件可以是本地文件，也可以通过 HTTPS URL 下载。 格式是 kubeadm join --discovery-token abcdef.1234567890abcdef 1.2.3.4:6443、kubeadm join--discovery-file path/to/file.conf 或者kubeadm join --discovery-file https://url/file.conf。 只能使用其中一种。 如果发现信息是从 URL 加载的，必须使用 HTTPS。 此外，在这种情况下，主机安装的 CA 包用于验证连接。
如果使用共享令牌进行发现，还应该传递 &amp;ndash;discovery-token-ca-cert-hash 参数来验证 Kubernetes 控制平面节点提供的根证书颁发机构（CA）的公钥。 此参数的值指定为 &amp;ldquo;&amp;lt;hash-type&amp;gt;:&amp;lt;hex-encoded-value&amp;gt;&amp;quot;，其中支持的哈希类型为 &amp;ldquo;sha256&amp;rdquo;。哈希是通过 Subject Public Key Info（SPKI）对象的字节计算的（如 RFC7469）。 这个值可以从 &amp;ldquo;kubeadm init&amp;rdquo; 的输出中获得，或者可以使用标准工具进行计算。 可以多次重复 &amp;ndash;discovery-token-ca-cert-hash 参数以允许多个公钥。
如果无法提前知道 CA 公钥哈希，则可以通过 &amp;ndash;discovery-token-unsafe-skip-ca-verification 参数禁用此验证。 这削弱了kubeadm 安全模型，因为其他节点可能会模仿 Kubernetes 控制平面节点。</description>
    </item>
    
    <item>
      <title></title>
      <link>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_join_phase/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_join_phase/</guid>
      <description>概要 使用此命令来调用 join 工作流程的某个阶段
选项 从父命令中继承的选项 </description>
    </item>
    
    <item>
      <title></title>
      <link>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_join_phase_control-plane-join/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_join_phase_control-plane-join/</guid>
      <description>概要 添加作为控制平面实例的机器
kubeadm join phase control-plane-join [flags] 示例 # 将机器作为控制平面实例加入 kubeadm join phase control-plane-join all 选项 从父命令中继承的选项 </description>
    </item>
    
    <item>
      <title></title>
      <link>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_join_phase_control-plane-join_all/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_join_phase_control-plane-join_all/</guid>
      <description>概要 添加作为控制平面实例的机器
kubeadm join phase control-plane-join all [flags] 选项 从父命令继承的选项 </description>
    </item>
    
    <item>
      <title></title>
      <link>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_join_phase_control-plane-join_etcd/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_join_phase_control-plane-join_etcd/</guid>
      <description>概要 添加新的本地 etcd 成员
kubeadm join phase control-plane-join etcd [flags] 选项 从父命令中继承的选项 </description>
    </item>
    
    <item>
      <title></title>
      <link>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_join_phase_control-plane-join_mark-control-plane/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_join_phase_control-plane-join_mark-control-plane/</guid>
      <description>概要 将 Node 节点标记为控制平面节点
kubeadm join phase control-plane-join mark-control-plane [flags] 选项 从父命令中继承的选项 </description>
    </item>
    
    <item>
      <title></title>
      <link>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_join_phase_control-plane-join_update-status/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_join_phase_control-plane-join_update-status/</guid>
      <description>概要 将新的控制平面节点注册到 kubeadm-config ConfigMap 维护的 ClusterStatus 中
kubeadm join phase control-plane-join update-status [flags] 选项 从父命令中继承的选项 </description>
    </item>
    
    <item>
      <title></title>
      <link>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_join_phase_control-plane-prepare/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_join_phase_control-plane-prepare/</guid>
      <description>概要 准备为控制平面服务的机器
kubeadm join phase control-plane-prepare [flags] 示例 # 准备为控制平面服务的机器 kubeadm join phase control-plane-prepare all 选项 从父命令中继承的选项 </description>
    </item>
    
    <item>
      <title></title>
      <link>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_join_phase_control-plane-prepare_all/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_join_phase_control-plane-prepare_all/</guid>
      <description>概要 准备为控制平面服务的机器
kubeadm join phase control-plane-prepare all [api-server-endpoint] [flags] 选项 从父命令继承的选项 </description>
    </item>
    
    <item>
      <title></title>
      <link>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_join_phase_control-plane-prepare_certs/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_join_phase_control-plane-prepare_certs/</guid>
      <description>概要 为新的控制平面组件生成证书
kubeadm join phase control-plane-prepare certs [api-server-endpoint] [flags] 选项 从父命令继承的选项 </description>
    </item>
    
    <item>
      <title></title>
      <link>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_join_phase_control-plane-prepare_control-plane/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_join_phase_control-plane-prepare_control-plane/</guid>
      <description>概要 为新的控制平面组件生成清单（manifest）
kubeadm join phase control-plane-prepare control-plane [flags] 选项 从父命令中继承的选项 </description>
    </item>
    
    <item>
      <title></title>
      <link>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_join_phase_control-plane-prepare_download-certs/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_join_phase_control-plane-prepare_download-certs/</guid>
      <description>概要 [实验]从 kubeadm-certs Secret 下载控制平面节点之间共享的证书
kubeadm join phase control-plane-prepare download-certs [api-server-endpoint] [flags] 选项 从父命令中继承的选项 </description>
    </item>
    
    <item>
      <title></title>
      <link>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_join_phase_control-plane-prepare_kubeconfig/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_join_phase_control-plane-prepare_kubeconfig/</guid>
      <description>概要 为新的控制平面组件生成 kubeconfig
kubeadm join phase control-plane-prepare kubeconfig [api-server-endpoint] [flags] 选项 从父命令中继承的选项 </description>
    </item>
    
    <item>
      <title></title>
      <link>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_join_phase_kubelet-start/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_join_phase_kubelet-start/</guid>
      <description>概要 生成一个包含 KubeletConfiguration 的文件和一个包含特定于节点的 kubelet 配置的环境文件，然后（重新）启动 kubelet。
kubeadm join phase kubelet-start [api-server-endpoint] [flags] 选项 从父命令继承的选项 </description>
    </item>
    
    <item>
      <title></title>
      <link>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_join_phase_preflight/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_join_phase_preflight/</guid>
      <description>概要 运行 kubeadm join 命令添加节点前检查。
kubeadm join phase preflight [api-server-endpoint] [flags] 示例 # 使用配置文件运行 kubeadm join 命令添加节点前检查。 kubeadm join phase preflight --config kubeadm-config.yml 选项 从父命令继承的选项 </description>
    </item>
    
    <item>
      <title></title>
      <link>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_reset/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_reset/</guid>
      <description>概要 尽最大努力还原通过 &amp;lsquo;kubeadm init&amp;rsquo; 或者 &amp;lsquo;kubeadm join&amp;rsquo; 操作对主机所做的更改
&amp;ldquo;reset&amp;rdquo; 命令执行以下阶段：
preflight Run reset pre-flight checks update-cluster-status Remove this node from the ClusterStatus object. remove-etcd-member Remove a local etcd member. cleanup-node Run cleanup node. kubeadm reset [flags] 选项 从父命令继承的选项 </description>
    </item>
    
    <item>
      <title></title>
      <link>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_reset_phase/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_reset_phase/</guid>
      <description>概要 使用此命令来调用 reset 工作流程的某个阶段
选项 从父命令继承的选项 </description>
    </item>
    
    <item>
      <title></title>
      <link>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_reset_phase_cleanup-node/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_reset_phase_cleanup-node/</guid>
      <description>概要 执行 cleanup node（清理节点）操作。
kubeadm reset phase cleanup-node [flags] 选项 从父命令继承的选项 </description>
    </item>
    
    <item>
      <title></title>
      <link>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_reset_phase_preflight/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_reset_phase_preflight/</guid>
      <description>概要 kubeadm reset（重置）前运行启动前检查。
kubeadm reset phase preflight [flags] 选项 从父命令继承的选项 </description>
    </item>
    
    <item>
      <title></title>
      <link>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_reset_phase_remove-etcd-member/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_reset_phase_remove-etcd-member/</guid>
      <description>概要 上传关于当前状态的配置，以便 &amp;lsquo;kubeadm upgrade&amp;rsquo; 以后可以知道如何配置升级后的集群。
kubeadm config upload [flags] 选项 从父命令继承的选项 </description>
    </item>
    
    <item>
      <title></title>
      <link>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_reset_phase_update-cluster-status/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_reset_phase_update-cluster-status/</guid>
      <description>概要 如果该节点是控制平面节点，从 ClusterStatus 对象中删除该节点。
kubeadm reset phase update-cluster-status [flags] 选项 从父命令继承的选项 </description>
    </item>
    
    <item>
      <title></title>
      <link>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_token/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_token/</guid>
      <description>概要 此命令管理引导令牌（bootstrap token）。它是可选的，仅适用于高级用例。
简而言之，引导令牌（bootstrap token）用于在客户端和服务器之间建立双向信任。 当客户端（例如，即将加入集群的节点）需要时，可以使用引导令牌相信正在与之通信的服务器。 然后可以使用具有 “签名” 的引导令牌。
引导令牌还可以作为一种允许对 API 服务器进行短期身份验证的方法（令牌用作 API 服务器信任客户端的方式），例如用于执行 TLS 引导程序。
引导令牌准确来说是什么？
 它是位于 kube-system 命名空间中类型为 “bootstrap.kubernetes.io/token” 的一个 Secret。 引导令牌的格式必须为 “[a-z0-9]{6}.[a-z0-9]{16}”，前一部分是公共令牌 ID，而后者是令牌秘钥，必须在任何情况下都保密！ 必须将 Secret 的名称命名为 “bootstrap-token-(token-id)”。  您可以在此处阅读有关引导令牌（bootstrap token）的更多信息： /docs/admin/bootstrap-tokens/
kubeadm token [flags] 选项 从父命令继承的选项 </description>
    </item>
    
    <item>
      <title></title>
      <link>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_token_create/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_token_create/</guid>
      <description>概要 这个命令将为你创建一个引导令牌。 您可以设置此令牌的用途，&amp;ldquo;有效时间&amp;rdquo; 和可选的人性化的描述。
这里的 [token] 是指将要生成的实际令牌。 该令牌应该是一个通过安全机制生成的随机令牌，形式为 &amp;ldquo;[a-z0-9]{6}.[a-z0-9]{16}&amp;quot;。 如果没有给出 [token]，kubeadm 将生成一个随机令牌。
kubeadm token create [token] 选项 从父命令继承的选项 </description>
    </item>
    
    <item>
      <title></title>
      <link>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_token_delete/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_token_delete/</guid>
      <description>概要 这个命令将为你删除指定的引导令牌列表。
[token-value] 是要删除的 &amp;ldquo;[a-z0-9]{6}.[a-z0-9]{16}&amp;rdquo; 形式的完整令牌或者是 &amp;ldquo;[a-z0-9]{6}&amp;rdquo; 形式的的令牌 ID。
kubeadm token delete [token-value] ... 选项 从父命令继承的选项 </description>
    </item>
    
    <item>
      <title></title>
      <link>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_token_generate/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_token_generate/</guid>
      <description>概要 此命令将打印一个随机生成的可以被 &amp;ldquo;init&amp;rdquo; 和 &amp;ldquo;join&amp;rdquo; 命令使用的引导令牌。 您不必使用此命令来生成令牌。你可以自己设定，只要格式符合 &amp;ldquo;[a-z0-9]{6}.[a-z0-9]{16}&amp;quot;。这个命令提供是为了方便生成规定格式的令牌。 您也可以使用 &amp;ldquo;kubeadm init&amp;rdquo; 并且不指定令牌，该命令会生成一个令牌并打印出来。
kubeadm token generate [flags] 选项 从父命令继承的选项 </description>
    </item>
    
    <item>
      <title></title>
      <link>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_token_list/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_token_list/</guid>
      <description>概要 此命令将为您列出所有的引导令牌。
kubeadm token list [flags] 选项 从父命令继承的选项 </description>
    </item>
    
    <item>
      <title></title>
      <link>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_upgrade/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_upgrade/</guid>
      <description>概要 此命令能将集群平滑升级到新版本
kubeadm upgrade [flags] 选项 继承于父命令的选项 </description>
    </item>
    
    <item>
      <title></title>
      <link>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_upgrade_apply/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_upgrade_apply/</guid>
      <description>概要 将 Kubernetes 集群升级到指定版本
kubeadm upgrade apply [version] 选项 从父命令继承的选项 </description>
    </item>
    
    <item>
      <title></title>
      <link>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_upgrade_diff/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_upgrade_diff/</guid>
      <description>概述 显示哪些差异将被应用于现有的静态 pod 资源清单。参考: kubeadm upgrade apply &amp;ndash;dry-run
kubeadm upgrade diff [version] [flags] 选项 从父命令继承的选项 </description>
    </item>
    
    <item>
      <title></title>
      <link>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_upgrade_node/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_upgrade_node/</guid>
      <description>概要 升级集群中某个节点的命令
&amp;ldquo;node&amp;rdquo; 命令执行以下阶段：
control-plane 如果存在的话，升级部署在该节点上的管理面实例 kubelet-config 更新该节点上的 kubelet 配置 kubeadm upgrade node [flags] 选项 从父命令继承的选项 </description>
    </item>
    
    <item>
      <title></title>
      <link>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_upgrade_node_phase/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_upgrade_node_phase/</guid>
      <description>概要 使用此命令调用 node 工作流的某个阶段
选项 从父命令继承的选项 </description>
    </item>
    
    <item>
      <title></title>
      <link>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_upgrade_node_phase_control-plane/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_upgrade_node_phase_control-plane/</guid>
      <description>概要 升级部署在此节点上的控制平面实例，如果有的话
kubeadm upgrade node phase control-plane [flags] 选项 从父命令继承的选项 </description>
    </item>
    
    <item>
      <title></title>
      <link>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_upgrade_node_phase_kubelet-config/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_upgrade_node_phase_kubelet-config/</guid>
      <description>从群集中 &amp;ldquo;kubelet-config-1.X&amp;rdquo; 的 ConfigMap 下载 kubelet 配置，其中 X 是kubelet 的次要版本。 kubeadm 使用 &amp;ndash;kubelet-version 参数来确定所需的 kubelet 版本。
kubeadm upgrade node phase kubelet-config [flags] 选项 从父命令继承的选项 </description>
    </item>
    
    <item>
      <title></title>
      <link>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_upgrade_plan/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_upgrade_plan/</guid>
      <description>概述 检查可升级到哪些版本，并验证您当前的集群是否可升级。 要跳过互联网检查，请传递可选的 [version] 参数
kubeadm upgrade plan [version] [flags] 选项 从父命令继承的选项 </description>
    </item>
    
    <item>
      <title></title>
      <link>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_version/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/setup-tools/kubeadm/generated/kubeadm_version/</guid>
      <description>概要 打印 kubeadm 的版本
kubeadm version [flags] 选项 从父命令继承的选项 </description>
    </item>
    
    <item>
      <title></title>
      <link>https://lijun.in/reference/setup-tools/kubeadm/generated/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/setup-tools/kubeadm/generated/readme/</guid>
      <description>此目录下的所有文件都是从其他仓库自动生成的。 不要人工编辑它们。 您必须在上游仓库中编辑它们</description>
    </item>
    
    <item>
      <title>AppArmor</title>
      <link>https://lijun.in/tutorials/clusters/apparmor/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tutorials/clusters/apparmor/</guid>
      <description>. feature-state for_k8s_version=&amp;quot;v1.4&amp;rdquo; state=&amp;quot;beta&amp;rdquo; &amp;gt;}}
Apparmor 是一个 Linux 内核安全模块，它补充了标准的基于 Linux 用户和组的安全模块将程序限制为有限资源集的权限。AppArmor 可以配置为任何应用程序减少潜在的攻击面，并且提供更加深入的防御。AppArmor 是通过配置文件进行配置的，这些配置文件被调整为报名单，列出了特定程序或者容器所需要的访问权限，如 Linux 功能、网络访问、文件权限等。每个配置文件都可以在强制模式(阻止访问不允许的资源)或投诉模式(仅报告冲突)下运行。
. heading &amp;ldquo;objectives&amp;rdquo; %}}  查看如何在节点上加载配置文件示例 了解如何在 Pod 上强制执行配置文件 了解如何检查配置文件是否已加载 查看违反配置文件时会发生什么情况 查看无法加载配置文件时会发生什么情况  . heading &amp;ldquo;prerequisites&amp;rdquo; %}} 务必：
 Kubernetes 版本至少是 v1.4 &amp;ndash; AppArmor 在 Kubernetes v1.4 版本中才添加了对 AppArmor 的支持。早于 v1.4 版本的 Kubernetes 组件不知道新的 AppArmor 注释，并且将会 默认忽略 提供的任何 AppArmor 设置。为了确保您的 Pods 能够得到预期的保护，必须验证节点的 Kubelet 版本：  kubectl get nodes -o=jsonpath=$&amp;#39;{range .items[*]}{@.metadata.name}: {@.status.nodeInfo.kubeletVersion}\n{end}&amp;#39; gke-test-default-pool-239f5d02-gyn2: v1.4.0 gke-test-default-pool-239f5d02-x1kf: v1.4.0 gke-test-default-pool-239f5d02-xwux: v1.</description>
    </item>
    
    <item>
      <title>Auditing</title>
      <link>https://lijun.in/tasks/debug-application-cluster/audit/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/debug-application-cluster/audit/</guid>
      <description>. feature-state state=&amp;quot;beta&amp;rdquo; &amp;gt;}}
Kubernetes 审计功能提供了与安全相关的按时间顺序排列的记录集，记录单个用户、管理员或系统其他组件影响系统的活动顺序。 它能帮助集群管理员处理以下问题：
 发生了什么？ 什么时候发生的？ 谁触发的？ 活动发生在哪个（些）对象上？ 在哪观察到的？ 它从哪触发的？ 活动的后续处理行为是什么？  . toc &amp;gt;}}
Kube-apiserver 执行审计。每个执行阶段的每个请求都会生成一个事件，然后根据特定策略对事件进行预处理并写入后端。 您可以在 设计方案 中找到更多详细信息。 该策略确定记录的内容并且在后端存储记录。当前的后端支持日志文件和 webhook。
每个请求都可以用相关的 &amp;ldquo;stage&amp;rdquo; 记录。已知的 stage 有：
  RequestReceived - 事件的 stage 将在审计处理器接收到请求后，并且在委托给其余处理器之前生成。
  ResponseStarted - 在响应消息的头部发送后，但是响应消息体发送前。这个 stage 仅为长时间运行的请求生成（例如 watch）。
  ResponseComplete - 当响应消息体完成并且没有更多数据需要传输的时候。
  Panic - 当 panic 发生时生成。
  . note &amp;gt;}}
注意 审计日志记录功能会增加 API server 的内存消耗，因为需要为每个请求存储审计所需的某些上下文。 此外，内存消耗取决于审计日志记录的配置。 . /note &amp;gt;}}</description>
    </item>
    
    <item>
      <title>Credits</title>
      <link>https://lijun.in/credits/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/credits/</guid>
      <description></description>
    </item>
    
    <item>
      <title>DC/OS 上的 Kubernetes</title>
      <link>https://lijun.in/setup/production-environment/on-premises-vm/dcos/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/setup/production-environment/on-premises-vm/dcos/</guid>
      <description>Mesosphere 提供了一个简单的选项来将 Kubernetes 设置到DC/OS上，它提供：
 纯上游 Kubernetes 集群一键部署 默认情况下高度可用且安全 与快速数据平台 (例如 Akka、Cassandra、Kafka、Spark) 一起运行的 Kubernetes  Mesosphere 官方指南 DC/OS 入门的正式来源位于quickstart 仓库中。</description>
    </item>
    
    <item>
      <title>Debug DNS 方案</title>
      <link>https://lijun.in/tasks/administer-cluster/dns-debugging-resolution/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/administer-cluster/dns-debugging-resolution/</guid>
      <description>This page provides hints on diagnosing DNS problems.
&amp;ndash;&amp;gt;
这篇文章提供了一些关于 DNS 问题诊断的方法。
&amp;ndash;&amp;gt;
. heading &amp;ldquo;prerequisites&amp;rdquo; %}}  . include &amp;ldquo;task-tutorial-prereqs.md&amp;rdquo; &amp;gt;}} . version-check &amp;gt;}} Kubernetes 1.6 或者以上版本。 集群必须使用了 coredns (或者 kube-dns)插件。  创建一个简单的 Pod 作为测试环境 新建一个名为 busybox.yaml 的文件并填入下列内容：
. codenew file=&amp;quot;admin/dns/busybox.yaml&amp;rdquo; &amp;gt;}}
然后使用这个文件创建一个 Pod 并验证其状态：
kubectl create -f https://k8s.io/examples/admin/dns/busybox.yaml pod/busybox created kubectl get pods busybox NAME READY STATUS RESTARTS AGE busybox 1/1 Running 0 &amp;lt;some-time&amp;gt; 只要 Pod 处于 running 状态，您就可以在环境里执行 nslookup 。 如果您看到类似下列的内容，则表示 DNS 是正常运行的。</description>
    </item>
    
    <item>
      <title>Init 容器</title>
      <link>https://lijun.in/concepts/workloads/pods/init-containers/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/concepts/workloads/pods/init-containers/</guid>
      <description>本页提供了 Init 容器的概览，它是一种专用的容器，在glossary_tooltip text=&amp;quot;Pod&amp;rdquo; term_id=&amp;quot;pod&amp;rdquo; &amp;gt;}}内的应用容器启动之前运行，并包括一些应用镜像中不存在的实用工具和安装脚本。
你可以在Pod的规格信息中与containers数组同级的位置指定 Init 容器。
理解 Init 容器 glossary_tooltip text=&amp;quot;Pod&amp;rdquo; term_id=&amp;quot;pod&amp;rdquo; &amp;gt;}} 可以包含多个容器，应用运行在这些容器里面，同时 Pod 也可以有一个或多个先于应用容器启动的 Init 容器。
Init 容器与普通的容器非常像，除了如下两点：
 它们总是运行到完成。 每个都必须在下一个启动之前成功完成。  如果 Pod 的 Init 容器失败，Kubernetes 会不断地重启该 Pod，直到 Init 容器成功为止。然而，如果 Pod 对应的 restartPolicy 值为 Never，它不会重新启动。
指定容器为 Init 容器，需要在 Pod 的 spec 中添加 initContainers 字段， 该字段內以[Container](/docs/reference/generated/kubernetes-api/&amp;lt; param &amp;ldquo;version&amp;rdquo; &amp;gt;}}/#container-v1-core) 类型对象数组的形式组织，和应用的 containers 数组同级相邻。 Init 容器的状态在 status.initContainerStatuses 字段中以容器状态数组的格式返回（类似 status.containerStatuses 字段）。
与普通容器的不同之处 Init 容器支持应用容器的全部字段和特性，包括资源限制、数据卷和安全设置。 然而，Init 容器对资源请求和限制的处理稍有不同，在下面 资源 处有说明。</description>
    </item>
    
    <item>
      <title>IP Masquerade Agent 用户指南</title>
      <link>https://lijun.in/tasks/administer-cluster/ip-masq-agent/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/administer-cluster/ip-masq-agent/</guid>
      <description>此页面展示如何配置和启用 ip-masq-agent。
. heading &amp;ldquo;prerequisites&amp;rdquo; %}} . include &amp;ldquo;task-tutorial-prereqs.md&amp;rdquo; &amp;gt;}} . version-check &amp;gt;}}
IP Masquerade Agent 用户指南 ip-masq-agent 配置 iptables 规则以隐藏位于集群节点 IP 地址后面的 pod 的 IP 地址。 这通常在将流量发送到集群的 pod CIDR 范围之外的目的地时使用。
关键词  NAT (网络地址解析) 是一种通过修改 IP 地址头中的源和/或目标地址信息将一个 IP 地址重新映射到另一个 IP 地址的方法。通常由执行 IP 路由的设备执行。   伪装 NAT 的一种形式，通常用于执行多对一地址转换，其中多个源 IP 地址被隐藏在单个地址后面，该地址通常是执行 IP 路由的设备。在 Kubernetes 中，这是节点的 IP 地址。   CIDR (无类别域间路由) 基于可变长度子网掩码，允许指定任意长度的前缀。CIDR 引入了一种新的 IP 地址表示方法，现在通常称为CIDR表示法，其中地址或路由前缀后添加一个后缀，用来表示前缀的位数，例如 192.168.2.0/24。   本地链路 本地链路是仅对网段或主机所连接的广播域内的通信有效的网络地址。IPv4的本地链路地址在 CIDR 表示法的地址块 169.</description>
    </item>
    
    <item>
      <title>kube-apiserver</title>
      <link>https://lijun.in/reference/command-line-tools-reference/kube-apiserver/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/command-line-tools-reference/kube-apiserver/</guid>
      <description>kube-apiserver 概要 Kubernetes API server 为 api 对象验证并配置数据，包括 pods、 services、 replicationcontrollers 和其它 api 对象。API Server 提供 REST 操作和到集群共享状态的前端，所有其他组件通过它进行交互。
kube-apiserver 选项  --admission-control stringSlice 控制资源进入集群的准入控制插件的顺序列表。逗号分隔的 NamespaceLifecycle 列表。（默认值 [AlwaysAdmit]） --admission-control-config-file string 包含准入控制配置的文件。 --advertise-address ip 向集群成员通知 apiserver 消息的 IP 地址。这个地址必须能够被集群中其他成员访问。如果 IP 地址为空，将会使用 --bind-address，如果未指定 --bind-address，将会使用主机的默认接口地址。 --allow-privileged 如果为 true, 将允许特权容器。 --anonymous-auth 启用到 API server 的安全端口的匿名请求。未被其他认证方法拒绝的请求被当做匿名请求。匿名请求的用户名为 system:anonymous，用户组名为 system:unauthenticated。（默认值 true） --apiserver-count int 集群中运行的 apiserver 数量，必须为正数。（默认值 1） --audit-log-maxage int 基于文件名中的时间戳，旧审计日志文件的最长保留天数。 --audit-log-maxbackup int 旧审计日志文件的最大保留个数。 --audit-log-maxsize int 审计日志被轮转前的最大兆字节数。 --audit-log-path string 如果设置该值，所有到 apiserver 的请求都将会被记录到这个文件。&#39;-&#39; 表示记录到标准输出。 --audit-policy-file string 定义审计策略配置的文件的路径。需要打开 &#39;AdvancedAuditing&#39; 特性开关。AdvancedAuditing 需要一个配置来启用审计功能。 --audit-webhook-config-file string 一个具有 kubeconfig 格式文件的路径，该文件定义了审计的 webhook 配置。需要打开 &#39;AdvancedAuditing&#39; 特性开关。 --audit-webhook-mode string 发送审计事件的策略。 Blocking 模式表示正在发送事件时应该阻塞服务器的响应。 Batch 模式使 webhook 异步缓存和发送事件。 Known 模式为 batch,blocking。（默认值 &amp;quot;batch&amp;quot;) --authentication-token-webhook-cache-ttl duration 从 webhook 令牌认证者获取的响应的缓存时长。( 默认值 2m0s) --authentication-token-webhook-config-file string 包含 webhook 配置的文件，用于令牌认证，具有 kubeconfig 格式。API server 将查询远程服务来决定对 bearer 令牌的认证。 --authorization-mode string 在安全端口上进行权限验证的插件的顺序列表。以逗号分隔的列表，包括：AlwaysAllow,AlwaysDeny,ABAC,Webhook,RBAC,Node.</description>
    </item>
    
    <item>
      <title>kube-controller-manager</title>
      <link>https://lijun.in/reference/command-line-tools-reference/kube-controller-manager/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/command-line-tools-reference/kube-controller-manager/</guid>
      <description>kube-controller-manager 概述 Kubernetes 控制器管理器是一个守护进程，嵌入了 Kubernetes 附带的核心控制循环。 在机器人和自动化的应用中，控制回路是一个永不休止的循环，用于调节系统状态。 在 Kubernetes 中，控制器是一个控制循环，它通过 apiserver 监视集群的共享状态，并尝试进行更改以将当前状态转为所需状态。现今，Kubernetes 自带的控制器包括副本控制器，节点控制器，命名空间控制器和serviceaccounts 控制器。
kube-controller-manager [flags] 选项 &amp;lt;tr&amp;gt; &amp;lt;td colspan=&amp;quot;2&amp;quot;&amp;gt;--allocate-node-cidrs&amp;lt;/td&amp;gt; &amp;lt;/tr&amp;gt; &amp;lt;tr&amp;gt; &amp;lt;td&amp;gt;&amp;lt;/td&amp;gt;&amp;lt;td style=&amp;quot;line-height: 130%; word-wrap: break-word;&amp;quot;&amp;gt;Should CIDRs for Pods be allocated and set on the cloud provider.&amp;lt;/td&amp;gt; &amp;lt;/tr&amp;gt; &amp;lt;tr&amp;gt; &amp;lt;td colspan=&amp;quot;2&amp;quot;&amp;gt;--alsologtostderr&amp;lt;/td&amp;gt; &amp;lt;/tr&amp;gt; &amp;lt;tr&amp;gt; &amp;lt;td&amp;gt;&amp;lt;/td&amp;gt;&amp;lt;td style=&amp;quot;line-height: 130%; word-wrap: break-word;&amp;quot;&amp;gt;log to standard error as well as files&amp;lt;/td&amp;gt; &amp;lt;/tr&amp;gt; &amp;lt;tr&amp;gt; &amp;lt;td colspan=&amp;quot;2&amp;quot;&amp;gt;--attach-detach-reconcile-sync-period duration&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;Default: 1m0s&amp;lt;/td&amp;gt; &amp;lt;/tr&amp;gt; &amp;lt;tr&amp;gt; &amp;lt;td&amp;gt;&amp;lt;/td&amp;gt;&amp;lt;td style=&amp;quot;line-height: 130%; word-wrap: break-word;&amp;quot;&amp;gt;The reconciler sync wait time between volume attach detach.</description>
    </item>
    
    <item>
      <title>kubectl 命令</title>
      <link>https://lijun.in/reference/kubectl/kubectl-cmds/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/kubectl/kubectl-cmds/</guid>
      <description>kubectl 命令参考</description>
    </item>
    
    <item>
      <title>kubectl 备忘单</title>
      <link>https://lijun.in/reference/kubectl/cheatsheet/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/kubectl/cheatsheet/</guid>
      <description>也可以看下: Kubectl 概述 和 JsonPath 指南。
本页面是 kubectl 命令的概述。
kubectl - 备忘单 Kubectl 自动补全 BASH ​```bash source &amp;lt;(kubectl completion bash) # 在 bash 中设置当前 shell 的自动补全，要先安装 bash-completion 包。 echo &amp;ldquo;source &amp;lt;(kubectl completion bash)&amp;rdquo; &amp;raquo; ~/.bashrc # 在您的 bash shell 中永久的添加自动补全
 &amp;lt;!-- You can also use a shorthand alias for `kubectl` that also works with completion: --&amp;gt; 您还可以为 `kubectl` 使用一个速记别名，该别名也可以与 completion 一起使用： ```bash alias k=kubectl complete -F __start_kubectl k ZSH ​```bash source &amp;lt;(kubectl completion zsh) # 在 zsh 中设置当前 shell 的自动补全 echo &amp;ldquo;if [ $commands[kubectl] ]; then source &amp;lt;(kubectl completion zsh); fi&amp;rdquo; &amp;raquo; ~/.</description>
    </item>
    
    <item>
      <title>kubectl 的用法约定</title>
      <link>https://lijun.in/reference/kubectl/conventions/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/kubectl/conventions/</guid>
      <description>kubectl 的推荐用法约定
在可重用脚本中使用 kubectl 对于脚本中的稳定输出：
 请求一个面向机器的输出格式，例如 -o name、-o json、-o yaml、-o go template 或 -o jsonpath。 完全限定版本。例如 jobs.v1.batch/myjob。这将确保 kubectl 不会使用其默认版本，该版本会随着时间的推移而更改。 在使用基于生成器的命令（例如 kubectl run 或者 kubectl expose）时，指定 --generator 参数以固定到特定行为。 不要依赖上下文、首选项或其他隐式状态。  最佳实践 kubectl run 若希望 kubectl run 满足基础设施即代码的要求：
 使用特定版本的标签标记镜像，不要将该标签移动到新版本。例如，使用 :v1234、v1.2.3、r03062016-1-4，而不是 :latest（有关详细信息，请参阅配置的最佳实践)。 固定到特定的生成器版本，例如 kubectl run --generator=run-pod/v1。 使用基于版本控制的脚本来记录所使用的参数，或者至少使用 --record 参数以便为所创建的对象添加注解，在使用轻度参数化的镜像时，记录下所使用的命令行。 使用基于版本控制的脚本来运行包含大量参数的镜像。 对于无法通过 kubectl run 参数来表示的功能特性，使用基于源码控制的配置文件，以记录要使用的功能特性。  生成器 您可以使用带有 --generator 参数的 kubectl run 命令创建如下资源：
table caption=&amp;quot;可以使用 kubectl run 创建的资源&amp;rdquo; &amp;gt;}} | 资源 | API 组 | kubectl 命令 | |&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;-|&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;ndash;|&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;ndash; | | Pod | v1 | kubectl run --generator=run-pod/v1 | | ReplicationController (已弃用) | v1 | kubectl run --generator=run/v1 | | Deployment (已弃用) | extensions/v1beta1 | kubectl run --generator=deployment/v1beta1 | | Deployment (已弃用) | apps/v1beta1 | kubectl run --generator=deployment/apps.</description>
    </item>
    
    <item>
      <title>Kubelet authentication/authorization</title>
      <link>https://lijun.in/reference/command-line-tools-reference/kubelet-authentication-authorization/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/command-line-tools-reference/kubelet-authentication-authorization/</guid>
      <description>. toc &amp;gt;}}
Overview A kubelet&amp;rsquo;s HTTPS endpoint exposes APIs which give access to data of varying sensitivity, and allow you to perform operations with varying levels of power on the node and within containers.
This document describes how to authenticate and authorize access to the kubelet&amp;rsquo;s HTTPS endpoint.
Kubelet authentication By default, requests to the kubelet&amp;rsquo;s HTTPS endpoint that are not rejected by other configured authentication methods are treated as anonymous requests, and given a username of system:anonymous and a group of system:unauthenticated.</description>
    </item>
    
    <item>
      <title>Kubernetes API 访问控制</title>
      <link>https://lijun.in/reference/access-authn-authz/controlling-access/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/access-authn-authz/controlling-access/</guid>
      <description>用户通过 kubectl、客户端库或者通过发送 REST 请求访问 API。 用户（自然人）和 Kubernetes 服务账户 都可以被授权进行 API 访问。 请求到达 API 服务器后会经过几个阶段，具体说明如图：
传输层安全 在典型的 Kubernetes 集群中，API 通过 443 端口提供服务。 API 服务器会提供一份证书。 该证书一般是自签名的， 所以用户机器上的 $USER/.kube/config 目录通常 包含该 API 服务器证书的根证书，用来代替系统默认根证书。 当用户使用 kube-up.sh 创建集群时，该证书通常会被自动写入用户的 $USER/.kube/config。 如果集群中存在多个用户，则创建者需要与其他用户共享证书。
认证 一旦 TLS 连接建立，HTTP 请求就进入到了认证的步骤。即图中的步骤 1 。 集群创建脚本或集群管理员会为 API 服务器配置一个或多个认证模块。 更具体的认证相关的描述详见这里。
认证步骤的输入是整个 HTTP 请求，但这里通常只是检查请求头和 / 或客户端证书。
认证模块支持客户端证书，密码和 Plain Tokens， Bootstrap Tokens，以及 JWT Tokens（用于服务账户）。
（管理员）可以同时设置多种认证模块，在设置了多个认证模块的情况下，每个模块会依次尝试认证， 直到其中一个认证成功。
在 GCE 平台中，客户端证书，密码和 Plain Tokens，Bootstrap Tokens，以及 JWT Tokens 同时被启用。
如果请求认证失败，则请求被拒绝，返回 401 状态码。 如果认证成功，则被认证为具体的 username，该用户名可供随后的步骤中使用。一些认证模块还提供了用户的组成员关系，另一些则没有。</description>
    </item>
    
    <item>
      <title>Kubernetes 中的代理</title>
      <link>https://lijun.in/concepts/cluster-administration/proxies/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/concepts/cluster-administration/proxies/</guid>
      <description>本文讲述了 Kubernetes 中所使用的代理。
代理 用户在使用 Kubernetes 的过程中可能遇到几种不同的代理（proxy）：
  kubectl proxy：
 运行在用户的桌面或 pod 中 从本机地址到 Kubernetes apiserver 的代理 客户端到代理使用 HTTP 协议 代理到 apiserver 使用 HTTPS 协议 指向 apiserver 添加认证头信息    apiserver proxy：
 是一个建立在 apiserver 内部的“堡垒” 将集群外部的用户与群集 IP 相连接，这些IP是无法通过其他方式访问的 运行在 apiserver 进程内 客户端到代理使用 HTTPS 协议 (如果配置 apiserver 使用 HTTP 协议，则使用 HTTP 协议) 通过可用信息进行选择，代理到目的地可能使用 HTTP 或 HTTPS 协议 可以用来访问 Node、 Pod 或 Service 当用来访问 Service 时，会进行负载均衡    kube proxy：</description>
    </item>
    
    <item>
      <title>Kubernetes 云管理控制器</title>
      <link>https://lijun.in/tasks/administer-cluster/running-cloud-controller/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/administer-cluster/running-cloud-controller/</guid>
      <description>. feature-state state=&amp;quot;beta&amp;rdquo; &amp;gt;}}
Kubernetes v1.6 包含一个新的二进制文件，叫做 cloud-controller-manager。cloud-controller-manager 是一个嵌入了特定云服务控制循环逻辑的守护进程。这些特定云服务控制循环逻辑最初存在于 kube-controller-manager 中。由于云服务提供商开发和发布的速度与 Kubernetes 项目不同，将服务提供商专用代码从 cloud-controller-manager 二进制中抽象出来有助于云服务厂商在 Kubernetes 核心代码之外独立进行开发。
cloud-controller-manager 可以被链接到任何满足 cloudprovider.Interface 约束的云服务提供商。为了兼容旧版本，Kubernetes 核心项目中提供的 cloud-controller-manager 使用和 kube-controller-manager 相同的云服务类库。已经在 Kubernetes 核心项目中支持的云服务提供商预计将通过使用 in-tree 的 cloud-controller-manager 过渡到 Kubernetes 核心之外。在将来的 Kubernetes 发布中，所有的云管理控制器将在 Kubernetes 核心项目之外，由 sig 领导者或者云服务厂商进行开发。
管理 需求 每个云服务都有一套各自的需求用于系统平台的集成，这不应与运行 kube-controller-manager 的需求有太大差异。作为经验法则，你需要：
 云服务认证 / 授权：您的云服务可能需要使用令牌或者 IAM 规则以允许对其 API 的访问 kubernetes 认证 / 授权：cloud-controller-manager 可能需要 RBAC 规则以访问 kubernetes apiserver 高可用：类似于 kube-controller-manager，您可能希望通过主节点选举（默认开启）配置一个高可用的云管理控制器。  运行云管理控制器 您需要对集群配置做适当的修改以成功地运行云管理控制器：
 一定不要为 kube-apiserver 和 kube-controller-manager 指定 --cloud-provider 标志。这将保证它们不会运行任何云服务专用循环逻辑，这将会由云管理控制器运行。未来这个标记将被废弃并去除。 kubelet 必须使用 --cloud-provider=external 运行。这是为了保证让 kubelet 知道在执行任何任务前，它必须被云管理控制器初始化。  请记住，设置群集使用云管理控制器将用多种方式更改群集行为：</description>
    </item>
    
    <item>
      <title>Kubernetes 在线培训概述</title>
      <link>https://lijun.in/tutorials/online-training/overview/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tutorials/online-training/overview/</guid>
      <description>以下是提供 Kubernetes 在线培训的一些网站:
 [AIOps Essentials (使用 Prometheus 度量标准自动缩放 Kubernetes) 和动手实验室 (Linux 学院)] (https://linuxacademy.com/devops/training/course/name/using-machine-learning-to-scale-kubernetes-clusters)   [带有动手实验室的 Amazon EKS 深入研究(Linux 学院)] (https://linuxacademy.com/amazon-web-services/training/course/name/amazon-eks-deep-dive)   [具有动手实验和实践考试的 Cloud Native 认证 Kubernetes 管理员 (CKA)(Linux 学院)] (https://linuxacademy.com/linux/training/course/name/cloud-native-certified-kubernetes-administrator-cka)   带有实践测试的认证 Kubernetes 管理员准备课程 (KodeKloud)   [经过实践实验室和实践考试认证的 Kubernetes 应用程序开发人员(CKAD) (Linux 学院)] (https://linuxacademy.com/containers/training/course/name/certified-kubernetes-application-developer-ckad/)   经过认证的带有实践测试的 Kubernetes 应用程序开发人员准备课程 (KodeKloud)   Google Kubernetes 引擎入门(Coursera)   [Kubernetes 入门 (Pluralsight)] (https://www.pluralsight.com/courses/getting-started-kubernetes)   OCI Oracle Kubernetes 引擎(OKE)上的 Kubernetes集群入门 (学习库)   [Google Kubernetes 引擎深度学习 (Linux 学院)] (https://linuxacademy.</description>
    </item>
    
    <item>
      <title>Master 节点通信</title>
      <link>https://lijun.in/concepts/architecture/master-node-communication/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/concepts/architecture/master-node-communication/</guid>
      <description>概览 本文对 Master 节点（确切说是 apiserver）和 Kubernetes 集群之间的通信路径进行了分类。目的是为了让用户能够自定义他们的安装，对网络配置进行加固，使得集群能够在不可信的网络上（或者在一个云服务商完全公共的 IP 上）运行。
Cluster -&amp;gt; Master 所有从集群到 master 的通信路径都终止于 apiserver（其它 master 组件没有被设计为可暴露远程服务）。在一个典型的部署中，apiserver 被配置为在一个安全的 HTTPS 端口（443）上监听远程连接并启用一种或多种形式的客户端身份认证机制。一种或多种客户端身份认证机制应该被启用，特别是在允许使用 匿名请求 或 service account tokens 的时候。
应该使用集群的公共根证书开通节点，如此它们就能够基于有效的客户端凭据安全的连接 apiserver。例如：在一个默认的 GCE 部署中，客户端凭据以客户端证书的形式提供给 kubelet。请查看 kubelet TLS bootstrapping 获取如何自动提供 kubelet 客户端证书。
想要连接到 apiserver 的 Pods 可以使用一个 service account 安全的进行连接。这种情况下，当 Pods 被实例化时 Kubernetes 将自动的把公共根证书和一个有效的不记名令牌注入到 pod 里。kubernetes service （所有 namespaces 中）都配置了一个虚拟 IP 地址，用于转发（通过 kube-proxy）请求到 apiserver 的 HTTPS endpoint。
Master 组件通过非安全（没有加密或认证）端口和集群的 apiserver 通信。这个端口通常只在 master 节点的 localhost 接口暴露，这样，所有在相同机器上运行的 master 组件就能和集群的 apiserver 通信。一段时间以后，master 组件将变为使用带身份认证和权限验证的安全端口（查看#13598）。</description>
    </item>
    
    <item>
      <title>oVirt</title>
      <link>https://lijun.in/setup/production-environment/on-premises-vm/ovirt/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/setup/production-environment/on-premises-vm/ovirt/</guid>
      <description>oVirt 是一个虚拟数据中心管理器，可以对多个主机上的多个虚拟机进行强大的管理。 使用 KVM 和 libvirt ，可以将 oVirt 安装在 Fedora、CentOS 或者 Red Hat Enterprise Linux 主机上，以部署和管理您的虚拟数据中心。
oVirt 云驱动的部署 oVirt 云驱动可以轻松发现新 VM 实例并自动将其添加为 Kubernetes 集群的节点。 目前，包括 Kubernetes 在内，尚无社区支持或预加载的 VM 镜像，但可以在 VM 中 导入 或 安装 Project Atomic（或 Fedora）来 生成模版。 包括 Kubernetes 的任何其他 Linux 发行版也可能可行。
必须在寄宿系统中 安装 ovirt-guest-agent，才能将 VM 的 IP 地址和主机名报给 ovirt-engine 并最终报告给 Kubernetes。
一旦 Kubernetes 模版可用，就可以开始创建可由云驱动发现的 VM。
使用 oVirt 云驱动 oVirt 云驱动需要访问 oVirt REST-API 来收集正确的信息，所需的凭据应在 ovirt-cloud.conf 文件中设定：
[connection] uri = https://localhost:8443/ovirt-engine/api username = admin@internal password = admin 在同一文件中，可以指定（使用 filters 节区）搜索查询，用于辨识要报告给 Kubernetes 的 VM：</description>
    </item>
    
    <item>
      <title>Pod 安全策略</title>
      <link>https://lijun.in/concepts/policy/pod-security-policy/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/concepts/policy/pod-security-policy/</guid>
      <description>PodSecurityPolicy 类型的对象能够控制，是否可以向 Pod 发送请求，该 Pod 能够影响被应用到 Pod 和容器的 SecurityContext。 查看 Pod 安全策略建议 获取更多信息。
什么是 Pod 安全策略？ Pod 安全策略 是集群级别的资源，它能够控制 Pod 运行的行为，以及它具有访问什么的能力。 PodSecurityPolicy 对象定义了一组条件，指示 Pod 必须按系统所能接受的顺序运行。 它们允许管理员控制如下方面：
   控制面 字段名称     已授权容器的运行 privileged   为容器添加默认的一组能力 defaultAddCapabilities   为容器去掉某些能力 requiredDropCapabilities   容器能够请求添加某些能力 allowedCapabilities   控制卷类型的使用 volumes   主机网络的使用 hostNetwork   主机端口的使用 hostPorts   主机 PID namespace 的使用 hostPID   主机 IPC namespace 的使用 hostIPC   主机路径的使用 allowedHostPaths   容器的 SELinux 上下文 seLinux   用户 ID runAsUser   配置允许的补充组 supplementalGroups   分配拥有 Pod 数据卷的 FSGroup fsGroup   必须使用一个只读的 root 文件系统 readOnlyRootFilesystem    Pod 安全策略 由设置和策略组成，它们能够控制 Pod 访问的安全特征。这些设置分为如下三类：</description>
    </item>
    
    <item>
      <title>Pod 的生命周期</title>
      <link>https://lijun.in/concepts/workloads/pods/pod-lifecycle/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/concepts/workloads/pods/pod-lifecycle/</guid>
      <description>该页面将描述 Pod 的生命周期。
Pod phase Pod 的 status 定义在 PodStatus 对象中，其中有一个 phase 字段。
Pod 的运行阶段（phase）是 Pod 在其生命周期中的简单宏观概述。该阶段并不是对容器或 Pod 的综合汇总，也不是为了做为综合状态机。
Pod 相位的数量和含义是严格指定的。除了本文档中列举的内容外，不应该再假定 Pod 有其他的 phase 值。
下面是 phase 可能的值：
 挂起（Pending）：Pod 已被 Kubernetes 系统接受，但有一个或者多个容器镜像尚未创建。等待时间包括调度 Pod 的时间和通过网络下载镜像的时间，这可能需要花点时间。 运行中（Running）：该 Pod 已经绑定到了一个节点上，Pod 中所有的容器都已被创建。至少有一个容器正在运行，或者正处于启动或重启状态。 成功（Succeeded）：Pod 中的所有容器都被成功终止，并且不会再重启。 失败（Failed）：Pod 中的所有容器都已终止了，并且至少有一个容器是因为失败终止。也就是说，容器以非0状态退出或者被系统终止。 未知（Unknown）：因为某些原因无法取得 Pod 的状态，通常是因为与 Pod 所在主机通信失败。  Pod 状态 Pod 有一个 PodStatus 对象，其中包含一个 PodCondition 数组。 PodCondition 数组的每个元素都有一个 type 字段和一个 status 字段。type 字段是字符串，可能的值有 PodScheduled、Ready、Initialized 和 Unschedulable。status 字段是一个字符串，可能的值有 True、False 和 Unknown。</description>
    </item>
    
    <item>
      <title>Showcase</title>
      <link>https://lijun.in/showcase/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/showcase/</guid>
      <description></description>
    </item>
    
    <item>
      <title>StackDriver 中的事件</title>
      <link>https://lijun.in/tasks/debug-application-cluster/events-stackdriver/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/debug-application-cluster/events-stackdriver/</guid>
      <description>Kubernetes 事件是一种对象，它为用户提供了洞察集群内发生的事情的能力，例如调度程序做出了什么决定，或者为什么某些 Pod 被逐出节点。 您可以在应用程序自检和调试中阅读有关使用事件调试应用程序的更多信息。
因为事件是 API 对象，所以它们存储在主节点上的 apiserver 中。 为了避免主节点磁盘空间被填满，将强制执行保留策略：事件在最后一次发生的一小时后将会被删除。 为了提供更长的历史记录和聚合能力，应该安装第三方解决方案来捕获事件。
本文描述了一个将 Kubernetes 事件导出为 Stackdriver Logging 的解决方案，在这里可以对它们进行处理和分析。
. note &amp;gt;}}
不能保证集群中发生的所有事件都将导出到 Stackdriver。 事件不能导出的一种可能情况是事件导出器没有运行（例如，在重新启动或升级期间）。 在大多数情况下，可以将事件用于设置 metrics 和 alerts 等目的，但您应该注意潜在的不准确性。 . /note &amp;gt;}}
部署 Google Kubernetes Engine 在 Google Kubernetes Engine 中，如果启用了云日志，那么事件导出器默认部署在主节点运行版本为 1.7 及更高版本的集群中。 为了防止干扰您的工作负载，事件导出器没有设置资源，并且处于尽力而为的 QoS 类型中，这意味着它将在资源匮乏的情况下第一个被杀死。 如果要导出事件，请确保有足够的资源给事件导出器 Pod 使用。 这可能会因为工作负载的不同而有所不同，但平均而言，需要大约 100MB 的内存和 100m 的 CPU。
部署到现有集群 使用下面的命令将事件导出器部署到您的集群：
kubectl create -f https://k8s.io/examples/debug/event-exporter.yaml 由于事件导出器访问 Kubernetes API，因此它需要权限才能访问。 以下的部署配置为使用 RBAC 授权。 它设置服务帐户和集群角色绑定，以允许事件导出器读取事件。 为了确保事件导出器 Pod 不会从节点中退出，您可以另外设置资源请求。 如前所述，100MB 内存和 100m CPU 应该就足够了。</description>
    </item>
    
    <item>
      <title>StatefulSet 基础</title>
      <link>https://lijun.in/tutorials/stateful-application/basic-stateful-set/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tutorials/stateful-application/basic-stateful-set/</guid>
      <description>本教程介绍如何了使用 StatefulSets 来管理应用。演示了如何创建、删除、扩容/缩容和更新 StatefulSets 的 Pods。
. heading &amp;ldquo;prerequisites&amp;rdquo; %}} 在开始本教程之前，你应该熟悉以下 Kubernetes 的概念：
 Pods Cluster DNS Headless Services PersistentVolumes [PersistentVolume Provisioning](https://github.com/kubernetes/examples/tree/. param &amp;ldquo;githubbranch&amp;rdquo; &amp;gt;}}/staging/persistent-volume-provisioning/) StatefulSets kubectl CLI  本教程假设你的集群被配置为动态的提供 PersistentVolumes。如果没有这样配置，在开始本教程之前，你需要手动准备 2 个 1 GiB 的存储卷。
. heading &amp;ldquo;objectives&amp;rdquo; %}} StatefulSets 旨在与有状态的应用及分布式系统一起使用。然而在 Kubernetes 上管理有状态应用和分布式系统是一个宽泛而复杂的话题。为了演示 StatefulSet 的基本特性，并且不使前后的主题混淆，你将会使用 StatefulSet 部署一个简单的 web 应用。
在阅读本教程后，你将熟悉以下内容：
 如何创建 StatefulSet StatefulSet 怎样管理它的 Pods 如何删除 StatefulSet 如何对 StatefulSet 进行扩容/缩容 如何更新一个 StatefulSet 的 Pods  创建 StatefulSet 作为开始，使用如下示例创建一个 StatefulSet。它和 StatefulSets 概念中的示例相似。它创建了一个 Headless Service nginx 用来发布 StatefulSet web 中的 Pod 的 IP 地址。</description>
    </item>
    
    <item>
      <title>TLS bootstrapping</title>
      <link>https://lijun.in/reference/command-line-tools-reference/kubelet-tls-bootstrapping/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/command-line-tools-reference/kubelet-tls-bootstrapping/</guid>
      <description>. toc &amp;gt;}}
Overview This document describes how to set up TLS client certificate bootstrapping for kubelets. Kubernetes 1.4 introduced an API for requesting certificates from a cluster-level Certificate Authority (CA). The original intent of this API is to enable provisioning of TLS client certificates for kubelets. The proposal can be found here and progress on the feature is being tracked as feature #43.
kube-apiserver configuration The API server should be configured with an authenticator that can authenticate tokens as a user in the system:bootstrappers group.</description>
    </item>
    
    <item>
      <title>为 Kubernetes 运行 etcd 集群</title>
      <link>https://lijun.in/tasks/administer-cluster/configure-upgrade-etcd/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/administer-cluster/configure-upgrade-etcd/</guid>
      <description>. glossary_definition term_id=&amp;quot;etcd&amp;rdquo; length=&amp;quot;all&amp;rdquo; prepend=&amp;quot;etcd 是一个&amp;quot;&amp;gt;}}
. heading &amp;ldquo;prerequisites&amp;rdquo; %}} . include &amp;ldquo;task-tutorial-prereqs.md&amp;rdquo; &amp;gt;}} . version-check &amp;gt;}}
先决条件   运行的 etcd 集群个数成员为奇数。
  etcd 是一个 leader-based 分布式系统。确保主节点定期向所有从节点发送心跳，以保持集群稳定。
  确保不发生资源不足。
集群的性能和稳定性对网络和磁盘 IO 非常敏感。任何资源匮乏都会导致心跳超时，从而导致集群的不稳定。不稳定的情况表明没有选出任何主节点。在这种情况下，集群不能对其当前状态进行任何更改，这意味着不能调度新的 pod。
  保持稳定的 etcd 集群对 Kubernetes 集群的稳定性至关重要。因此，请在专用机器或隔离环境上运行 etcd 集群，以满足所需资源需求。
  在生产中运行的 etcd 的最低推荐版本是 3.2.10+。
  资源要求 使用有限的资源运行 etcd 只适合测试目的。为了在生产中部署，需要先进的硬件配置。在生产中部署 etcd 之前，请查看所需资源参考文档。
启动 etcd 集群 本节介绍如何启动单节点和多节点 etcd 集群。
单节点 etcd 集群 只为测试目的使用单节点 etcd 集群。</description>
    </item>
    
    <item>
      <title>为容器设置环境变量</title>
      <link>https://lijun.in/tasks/inject-data-application/define-environment-variable-container/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/inject-data-application/define-environment-variable-container/</guid>
      <description>本页将展示如何为 kubernetes Pod 下的容器设置环境变量。
. heading &amp;ldquo;prerequisites&amp;rdquo; %}} . include &amp;ldquo;task-tutorial-prereqs.md&amp;rdquo; &amp;gt;}} . version-check &amp;gt;}}
为容器设置一个环境变量 创建 Pod 时，可以为其下的容器设置环境变量。通过配置文件的 env 或者 envFrom 字段来设置环境变量。
本示例中，将创建一个只包含单个容器的 Pod。Pod 的配置文件中设置环境变量的名称为 DEMO_GREETING， 其值为 &amp;quot;Hello from the environment&amp;quot;。下面是 Pod 的配置文件内容：
. codenew file=&amp;quot;pods/inject/envars.yaml&amp;rdquo; &amp;gt;}}
  基于 YAML 文件创建一个 Pod：
kubectl apply -f https://k8s.io/examples/pods/inject/envars.yaml     获取一下当前正在运行的 Pods 信息：
kubectl get pods -l purpose=demonstrate-envars 查询结果应为：
NAME READY STATUS RESTARTS AGE envar-demo 1/1 Running 0 9s     进入该 Pod 下的容器并打开一个命令终端：</description>
    </item>
    
    <item>
      <title>为系统守护进程预留计算资源</title>
      <link>https://lijun.in/tasks/administer-cluster/reserve-compute-resources/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/administer-cluster/reserve-compute-resources/</guid>
      <description>Kubernetes 的节点可以按照 Capacity 调度。默认情况下 pod 能够使用节点全部可用容量。这是个问题，因为节点自己通常运行了不少驱动 OS 和 Kubernetes 的系统守护进程。除非为这些系统守护进程留出资源，否则它们将与 pod 争夺资源并导致节点资源短缺问题。
kubelet 公开了一个名为 Node Allocatable 的特性，有助于为系统守护进程预留计算资源。Kubernetes 推荐集群管理员按照每个节点上的工作负载密度配置 Node Allocatable。
. heading &amp;ldquo;prerequisites&amp;rdquo; %}} . include &amp;ldquo;task-tutorial-prereqs.md&amp;rdquo; &amp;gt;}} . version-check &amp;gt;}}
可分配的节点 Node Capacity --------------------------- | kube-reserved | |-------------------------| | system-reserved | |-------------------------| | eviction-threshold | |-------------------------| | | | allocatable | | (available for pods) | | | | | --------------------------- Kubernetes 节点上的 Allocatable 被定义为 pod 可用计算资源量。调度器不会超额申请 Allocatable。目前支持 CPU, memory 和 ephemeral-storage 这几个参数。</description>
    </item>
    
    <item>
      <title>为节点发布扩展资源</title>
      <link>https://lijun.in/tasks/administer-cluster/extended-resource-node/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/administer-cluster/extended-resource-node/</guid>
      <description>本文展示了如何为节点指定扩展资源。 扩展资源允许集群管理员发布节点级别的资源，这些资源在不进行发布的情况下无法被 Kubernetes 感知。
. heading &amp;ldquo;prerequisites&amp;rdquo; %}} . include &amp;ldquo;task-tutorial-prereqs.md&amp;rdquo; &amp;gt;}} . version-check &amp;gt;}}
获取您的节点名称 kubectl get nodes 选择您的一个节点用于此练习。
在您的一个节点上发布一种新的扩展资源 为在一个节点上发布一种新的扩展资源，需要发送一个 HTTP PATCH 请求到 Kubernetes API server。 例如：假设您的一个节点上带有四个 dongle 资源。下面是一个 PATCH 请求的示例， 该请求为您的节点发布四个 dongle 资源。
PATCH /api/v1/nodes/&amp;lt;your-node-name&amp;gt;/status HTTP/1.1 Accept: application/json Content-Type: application/json-patch+json Host: k8s-master:8080 [ { &amp;#34;op&amp;#34;: &amp;#34;add&amp;#34;, &amp;#34;path&amp;#34;: &amp;#34;/status/capacity/example.com~1dongle&amp;#34;, &amp;#34;value&amp;#34;: &amp;#34;4&amp;#34; } ] 注意：Kubernetes 不需要了解 dongle 资源的含义和用途。 前面的 PATCH 请求仅仅告诉 Kubernetes 您的节点拥有四个您称之为 dongle 的东西。
启动一个代理（proxy），以便您可以很容易地向 Kubernetes API server 发送请求：</description>
    </item>
    
    <item>
      <title>使用 ABAC 鉴权</title>
      <link>https://lijun.in/reference/access-authn-authz/abac/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/access-authn-authz/abac/</guid>
      <description>基于属性的访问控制（Attribute-based access control - ABAC）定义了访问控制范例，其中通过使用将属性组合在一起的策略来向用户授予访问权限。
策略文件格式 基于 ABAC 模式，可以这样指定策略文件 --authorization-policy-file=SOME_FILENAME。
此文件格式是 JSON Lines，不应存在封闭的列表或映射，每行一个映射。
每一行都是一个策略对象，策略对象是具有以下属性的映射：
 版本控制属性：  apiVersion，字符串类型：有效值为abac.authorization.kubernetes.io/v1beta1，允许对策略格式进行版本控制和转换。 kind，字符串类型：有效值为 Policy，允许对策略格式进行版本控制和转换。   spec 配置为具有以下映射的属性：  主体匹配属性：  user，字符串类型；来自 --token-auth-file 的用户字符串，如果你指定 user，它必须与验证用户的用户名匹配。 group，字符串类型；如果指定 group，它必须与经过身份验证的用户的一个组匹配，system:authenticated匹配所有经过身份验证的请求。system:unauthenticated匹配所有未经过身份验证的请求。     资源匹配属性：  apiGroup，字符串类型；一个 API 组。  例： extensions 通配符：*匹配所有 API 组。   namespace，字符串类型；一个命名空间。  例如：kube-system 通配符：*匹配所有资源请求。   resource，字符串类型；资源类型。  例：pods 通配符：*匹配所有资源请求。     非资源匹配属性：  nonResourcePath，字符串类型；非资源请求路径。  例如：/version或 /apis 通配符：  * 匹配所有非资源请求。 /foo/* 匹配 /foo/ 的所有子路径。       readonly，键入布尔值，如果为 true，则表示该策略仅适用于 get、list 和 watch 操作。  .</description>
    </item>
    
    <item>
      <title>使用 ConfigMap 来配置 Redis</title>
      <link>https://lijun.in/tutorials/configuration/configure-redis-using-configmap/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tutorials/configuration/configure-redis-using-configmap/</guid>
      <description>这篇文档基于使用 ConfigMap 来配置 Containers 这个任务，提供了一个使用 ConfigMap 来配置 Redis 的真实案例。
. heading &amp;ldquo;objectives&amp;rdquo; %}}    创建一个包含以下内容的 kustomization.yaml 文件： 一个 ConfigMap 生成器 一个使用 ConfigMap 的 Pod 资源配置   使用 kubectl apply -k ./ 应用整个路径的配置 验证配置已经被正确应用。  . heading &amp;ldquo;prerequisites&amp;rdquo; %}}  . include &amp;ldquo;task-tutorial-prereqs.md&amp;rdquo; &amp;gt;}} . version-check &amp;gt;}}   此页面上显示的示例适用于 kubectl 1.14和在其以上的版本。 理解使用ConfigMap来配置Containers。  真实世界的案例：使用 ConfigMap 来配置 Redis 按照下面的步骤，您可以使用ConfigMap中的数据来配置Redis缓存。
 根据docs/user-guide/configmap/redis/redis-config来创建一个ConfigMap：  . codenew file=&amp;quot;pods/config/redis-config&amp;rdquo; &amp;gt;}}
curl -OL https://k8s.</description>
    </item>
    
    <item>
      <title>使用 CoreDNS 进行服务发现</title>
      <link>https://lijun.in/tasks/administer-cluster/coredns/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/administer-cluster/coredns/</guid>
      <description>此页面介绍了 CoreDNS 升级过程以及如何安装 CoreDNS 而不是 kube-dns。
. heading &amp;ldquo;prerequisites&amp;rdquo; %}} . include &amp;ldquo;task-tutorial-prereqs.md&amp;rdquo; &amp;gt;}} . version-check &amp;gt;}}
关于 CoreDNS CoreDNS 是一个灵活可扩展的 DNS 服务器，可以作为 Kubernetes 集群 DNS。与 Kubernetes 一样，CoreDNS 项目由 . glossary_tooltip text=&amp;quot;CNCF&amp;rdquo; term_id=&amp;quot;cncf&amp;rdquo; &amp;gt;}} 托管。
通过在现有的集群中替换 kube-dns，可以在集群中使用 CoreDNS 代替 kube-dns 部署，或者使用 kubeadm 等工具来为您部署和升级集群。
安装 CoreDNS 有关手动部署或替换 kube-dns，请参阅 CoreDNS GitHub 工程。
迁移到 CoreDNS 使用 kubeadm 升级现有集群 在 Kubernetes 1.10 及更高版本中，当您使用 kubeadm 升级使用 kube-dns 的集群时，您还可以迁移到 CoreDNS。 在本例中 kubeadm 将生成 CoreDNS 配置（&amp;ldquo;Corefile&amp;rdquo;）基于 kube-dns ConfigMap，保存联邦、存根域和上游名称服务器的配置。</description>
    </item>
    
    <item>
      <title>使用 crictl 对 Kubernetes 节点进行调试</title>
      <link>https://lijun.in/tasks/debug-application-cluster/crictl/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/debug-application-cluster/crictl/</guid>
      <description>. feature-state for_k8s_version=&amp;quot;v1.11&amp;rdquo; state=&amp;quot;stable&amp;rdquo; &amp;gt;}}
crictl 是 CRI 兼容的容器运行时命令行接口。 您可以使用它来检查和调试 Kubernetes 节点上的容器运行时和应用程序。 crictl和它的源代码在 cri-tools 代码库。
. heading &amp;ldquo;prerequisites&amp;rdquo; %}} crictl 需要带有 CRI 运行时的 Linux 操作系统。
安装 crictl 您可以从 cri-tools 发布页面下载一个压缩的 crictl 归档文件，用于几种不同的架构。 下载与您的 kubernetes 版本相对应的版本。 提取它并将其移动到系统路径上的某个位置，例如/usr/local/bin/。
一般用法 crictl 命令有几个子命令和运行时参数。 有关详细信息，请使用 crictl help 或 crictl &amp;lt;subcommand&amp;gt; help 获取帮助信息。
crictl 默认连接到 unix:///var/run/dockershim.sock。 对于其他的运行时，您可以用多种不同的方法设置端点：
 通过设置参数 --runtime-endpoint 和 --image-endpoint 通过设置环境变量 CONTAINER_RUNTIME_ENDPOINT 和 IMAGE_SERVICE_ENDPOINT 通过在配置文件中设置端点 --config=/etc/crictl.yaml  您还可以在连接到服务器并启用或禁用调试时指定超时值，方法是在配置文件中指定 timeout 或 debug 值，或者使用 --timeout 和 --debug 命令行参数。</description>
    </item>
    
    <item>
      <title>使用 ElasticSearch 和 Kibana 进行日志管理</title>
      <link>https://lijun.in/tasks/debug-application-cluster/logging-elasticsearch-kibana/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/debug-application-cluster/logging-elasticsearch-kibana/</guid>
      <description>在 Google Compute Engine (GCE) 平台上，默认的日志管理支持目标是 Stackdriver Logging，在 使用 Stackdriver Logging 管理日志中详细描述了这一点。
本文介绍了如何设置一个集群，将日志导入Elasticsearch，并使用 Kibana 查看日志，作为在 GCE 上运行应用时使用 Stackdriver Logging 管理日志的替代方案。
. note &amp;gt;}}
您不能在 Google Kubernetes Engine 平台运行的 Kubernetes 集群上自动的部署 Elasticsearch 和 Kibana。您必须手动部署它们。 . /note &amp;gt;}}
要使用 Elasticsearch 和 Kibana 处理集群日志，您应该在使用 kube-up.sh 脚本创建集群时设置下面所示的环境变量：
KUBE_LOGGING_DESTINATION=elasticsearch 您还应该确保设置了 KUBE_ENABLE_NODE_LOGGING=true （这是 GCE 平台的默认设置）。
现在，当您创建集群时，将有一条消息将指示每个节点上运行的 Fluentd 日志收集守护进程以 ElasticSearch 为日志输出目标：
$ cluster/kube-up.sh ... Project: kubernetes-satnam Zone: us-central1-b ... calling kube-up Project: kubernetes-satnam Zone: us-central1-b +++ Staging server tars to Google Storage: gs://kubernetes-staging-e6d0e81793/devel +++ kubernetes-server-linux-amd64.</description>
    </item>
    
    <item>
      <title>使用 Falco 审计</title>
      <link>https://lijun.in/tasks/debug-application-cluster/falco/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/debug-application-cluster/falco/</guid>
      <description>使用 Falco 采集审计事件 Falco是一个开源项目，用于为云原生平台提供入侵和异常检测。本节介绍如何设置 Falco、如何将审计事件发送到 Falco 公开的 Kubernetes Audit 端点、以及 Falco 如何应用一组规则来自动检测可疑行为。
安装 Falco 使用以下方法安装 Falco ：
 [独立安装 Falco][falco_installation] [Kubernetes DaemonSet][falco_installation] [Falco Helm Chart][falco_helm_chart]  安装完成 Falco 后，请确保将其配置为公开 Audit Webhook。为此，请使用以下配置：
webserver: enabled: true listen_port: 8765 k8s_audit_endpoint: /k8s_audit ssl_enabled: false ssl_certificate: /etc/falco/falco.pem 此配置通常位于 /etc/falco/falco.yaml 文件中。如果 Falco 作为 Kubernetes DaemonSet 安装，请编辑 falco-config ConfigMap 并添加此配置。
配置 Kubernetes 审计   为 kube-apiserver webhook 审计后端创建一个kubeconfig文件。
 cat &amp;lt;&amp;lt;EOF &amp;gt; /etc/kubernetes/audit-webhook-kubeconfig apiVersion: v1 kind: Config clusters: - cluster: server: http://&amp;lt;ip_of_falco&amp;gt;:8765/k8s_audit name: falco contexts: - context: cluster: falco user: &amp;quot;&amp;quot; name: default-context current-context: default-context preferences: {} users: [] EOF     使用以下选项启动 kube-apiserver：</description>
    </item>
    
    <item>
      <title>使用 Helm 安装 Service Catalog</title>
      <link>https://lijun.in/tasks/service-catalog/install-service-catalog-using-helm/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/service-catalog/install-service-catalog-using-helm/</guid>
      <description>. glossary_definition term_id=&amp;quot;service-catalog&amp;rdquo; length=&amp;quot;all&amp;rdquo; prepend=&amp;quot;Service Catalog is&amp;rdquo; &amp;gt;}}
使用 Helm 在 Kubernetes 集群上安装 Service Catalog。 要获取有关此过程的最新信息，请浏览 kubernetes-incubator/service-catalog 仓库。
. heading &amp;ldquo;prerequisites&amp;rdquo; %}}  理解 Service Catalog 的关键概念。 Service Catalog 需要 Kubernetes 集群版本在 1.7 或更高版本。 您必须启用 Kubernetes 集群的 DNS 功能。  如果使用基于云的 Kubernetes 集群或 . glossary_tooltip text=&amp;quot;Minikube&amp;rdquo; term_id=&amp;quot;minikube&amp;rdquo; &amp;gt;}}，则可能已经启用了集群 DNS。 如果您正在使用 hack/local-up-cluster.sh，请确保设置了 KUBE_ENABLE_CLUSTER_DNS 环境变量，然后运行安装脚本。   安装和设置 v1.7 或更高版本的 kubectl，确保将其配置为连接到 Kubernetes 集群。 安装 v2.7.0 或更高版本的 Helm。  遵照 Helm 安装说明。 如果已经安装了适当版本的 Helm，请执行 helm init 来安装 Helm 的服务器端组件 Tiller。    添加 service-catalog Helm 仓库 安装 Helm 后，通过执行以下命令将 service-catalog Helm 存储库添加到本地计算机：</description>
    </item>
    
    <item>
      <title>使用 IBM Cloud Private 在多个云上运行 Kubernetes</title>
      <link>https://lijun.in/setup/production-environment/turnkey/icp/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/setup/production-environment/turnkey/icp/</guid>
      <description>IBM® Cloud Private 是一个 一站式云解决方案并且是一个本地的一站式云解决方案。 IBM Cloud Private 提供纯上游 Kubernetes，以及运行实际企业工作负载所需的典型管理组件。这些工作负载包括健康管理、日志管理、审计跟踪以及用于跟踪平台上工作负载使用情况的计量。
IBM Cloud Private 提供了社区版和全支持的企业版。可从 Docker Hub 免费获得社区版本。企业版支持高可用性拓扑，并包括 IBM 对 Kubernetes 和 IBM Cloud Private 管理平台的商业支持。如果您想尝试 IBM Cloud Private，您可以使用托管试用版、教程或自我指导演示。您也可以尝试免费的社区版。有关详细信息，请参阅 IBM Cloud Private 入门。
有关更多信息，请浏览以下资源：
 IBM Cloud Private IBM Cloud Private 参考架构 IBM Cloud Private 文档  IBM Cloud Private 和 Terraform 您可以利用一下模块使用 Terraform 部署 IBM Cloud Private：
 AWS：将 IBM Cloud Private 部署到 AWS Azure：将 IBM Cloud Private 部署到 Azure IBM Cloud：将 IBM Cloud Private 集群部署到 IBM Cloud OpenStack：将IBM Cloud Private 部署到 OpenStack Terraform 模块：在任何支持的基础架构供应商上部署 IBM Cloud Private VMware：将 IBM Cloud Private 部署到 VMware  AWS 上的 IBM Cloud Private 您可以使用 AWS CloudFormation 或 Terraform 在 Amazon Web Services（AWS）上部署 IBM Cloud Private 集群。</description>
    </item>
    
    <item>
      <title>使用 KMS 提供商进行数据加密</title>
      <link>https://lijun.in/tasks/administer-cluster/kms-provider/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/administer-cluster/kms-provider/</guid>
      <description>本页展示了如何配置秘钥管理服务—— Key Management Service (KMS) 提供商和插件以启用数据加密。
. heading &amp;ldquo;prerequisites&amp;rdquo; %}}  . include &amp;ldquo;task-tutorial-prereqs.md&amp;rdquo; &amp;gt;}} . version-check &amp;gt;}}   需要 Kubernetes 1.10.0 或更新版本   需要 etcd v3 或更新版本  . feature-state for_k8s_version=&amp;quot;v1.12&amp;rdquo; state=&amp;quot;beta&amp;rdquo; &amp;gt;}}
KMS 加密提供商使用封套加密模型来加密 etcd 中的数据。数据使用数据加密秘钥（DEK）加密；每次加密都生成一个新的 DEK。这些 DEK 经一个秘钥加密秘钥（KEK）加密后在一个远端的 KMS 中存储和管理。KMS 提供商使用 gRPC 与一个特定的 KMS 插件通信。这个 KMS 插件作为一个 gRPC 服务器被部署在 Kubernetes 主服务器的同一个主机上，负责与远端 KMS 的通信。
配置 KMS 提供商 为了在 API 服务器上配置 KMS 提供商，在加密配置文件中的提供商数组中加入一个类型为 kms 的提供商，并设置下列属性：
 name: KMS 插件的显示名称。 endpoint: gRPC 服务器（KMS 插件）的监听地址。该端点是一个 UNIX 的套接字。 cachesize: 以明文缓存的数据加密秘钥（DEKs）的数量。一旦被缓存，就可以直接使用 DEKs 而无需另外调用 KMS；而未被缓存的 DEKs 需要调用一次 KMS 才能解包。 timeout: 在返回一个错误之前，kube-apiserver 等待 kms-plugin 响应的时间（默认是 3 秒）。  参见 理解静态数据加密配置</description>
    </item>
    
    <item>
      <title>使用 kubeadm 进行证书管理</title>
      <link>https://lijun.in/tasks/administer-cluster/kubeadm/kubeadm-certs/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/administer-cluster/kubeadm/kubeadm-certs/</guid>
      <description>. feature-state for_k8s_version=&amp;quot;v1.15&amp;rdquo; state=&amp;quot;stable&amp;rdquo; &amp;gt;}}
由 kubeadm 生成的客户端证书在 1 年后到期。 本页说明如何使用 kubeadm 管理证书续订。
. heading &amp;ldquo;prerequisites&amp;rdquo; %}} 熟悉 Kubernetes 中的 PKI证书和要求。
您应该熟悉Kubernetes 中的 PKI 证书和要求。
检查证书是否过期 check-expiration 能被用来检查证书是否过期
kubeadm alpha certs check-expiration 输出类似于以下内容：
CERTIFICATE EXPIRES RESIDUAL TIME EXTERNALLY MANAGED admin.conf May 15, 2020 13:03 UTC 364d false apiserver May 15, 2020 13:00 UTC 364d false apiserver-etcd-client May 15, 2020 13:00 UTC 364d false apiserver-kubelet-client May 15, 2020 13:00 UTC 364d false controller-manager.</description>
    </item>
    
    <item>
      <title>使用 Kubernetes API 访问集群</title>
      <link>https://lijun.in/tasks/administer-cluster/access-cluster-api/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/administer-cluster/access-cluster-api/</guid>
      <description>本页展示了如何使用 Kubernetes API 访问集群
. heading &amp;ldquo;prerequisites&amp;rdquo; %}} . include &amp;ldquo;task-tutorial-prereqs.md&amp;rdquo; &amp;gt;}} . version-check &amp;gt;}}
访问集群 API 使用 kubectl 进行首次访问 首次访问 Kubernetes API 时，请使用 Kubernetes 命令行工具 kubectl 。
要访问集群，您需要知道集群位置并拥有访问它的凭证。通常，当您完成入门指南时，这会自动设置完成，或者由其他人设置好集群并将凭证和位置提供给您。
使用此命令检查 kubectl 已知的位置和凭证：
kubectl config view 许多[样例](https://github.com/kubernetes/examples/tree/. param &amp;ldquo;githubbranch&amp;rdquo; &amp;gt;}}/)提供了使用 kubectl 的介绍。完整文档请见 kubectl 手册。
直接访问 REST API kubectl 处理对 API 服务器的定位和身份验证。如果您想通过 http 客户端（如 curl 或 wget，或浏览器）直接访问 REST API，您可以通过多种方式对 API 服务器进行定位和身份验证：
 以代理模式运行 kubectl（推荐）。 推荐使用此方法，因为它用存储的 apiserver 位置并使用自签名证书验证 API 服务器的标识。使用这种方法无法进行中间人（MITM）攻击。 另外，您可以直接为 http 客户端提供位置和身份认证。这适用于被代理混淆的客户端代码。为防止中间人攻击，您需要将根证书导入浏览器。  使用 Go 或 Python 客户端库可以在代理模式下访问 kubectl。</description>
    </item>
    
    <item>
      <title>使用 Minikube 安装 Kubernetes</title>
      <link>https://lijun.in/setup/learning-environment/minikube/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/setup/learning-environment/minikube/</guid>
      <description>Minikube 是一种可以让您在本地轻松运行 Kubernetes 的工具。Minikube 在笔记本电脑上的虚拟机（VM）中运行单节点 Kubernetes 集群，供那些希望尝试 Kubernetes 或进行日常开发的用户使用。
Minikube 功能 Minikube 支持以下 Kubernetes 功能：
 DNS NodePorts ConfigMaps 和 Secrets Dashboards 容器运行时: Docker、CRI-O 以及 containerd 启用 CNI （容器网络接口） Ingress  安装 请参阅安装 Minikube。
快速开始 这个简短的演示将指导您如何在本地启动、使用和删除 Minikube。请按照以下步骤开始探索 Minikube。
  启动 Minikube 并创建一个集群：
minikube start 输出类似于：
Starting local Kubernetes cluster... Running pre-create checks... Creating machine... Starting local Kubernetes cluster... 有关使用特定 Kubernetes 版本、VM 或容器运行时启动集群的详细信息，请参阅启动集群。
  现在，您可以使用 kubectl 与集群进行交互。有关详细信息，请参阅与集群交互。
让我们使用名为 echoserver 的镜像创建一个 Kubernetes Deployment，并使用 --port 在端口 8080 上暴露服务。echoserver 是一个简单的 HTTP 服务器。</description>
    </item>
    
    <item>
      <title>使用 PodPreset 将信息注入 Pods</title>
      <link>https://lijun.in/tasks/inject-data-application/podpreset/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/inject-data-application/podpreset/</guid>
      <description>在 pod 创建时，用户可以使用 podpreset 对象将 secrets、卷挂载和环境变量等信息注入其中。 本文展示了一些 PodPreset 资源使用的示例。 用户可以从理解 Pod Presets 中了解 PodPresets 的整体情况。
. toc &amp;gt;}}
创建 Pod Preset 简单的 Pod Spec 示例 这里是一个简单的示例，展示了如何通过 Pod Preset 修改 Pod spec 。
. codenew file=&amp;quot;podpreset/preset.yaml&amp;rdquo; &amp;gt;}}
创建 PodPreset：
kubectl apply -f https://k8s.io/examples/podpreset/preset.yaml 检查所创建的 PodPreset：
kubectl get podpreset NAME AGE allow-database 1m 新的 PodPreset 会对所有具有标签 role: frontend 的 Pods 采取行动。
用户提交的 pod spec：
. codenew file=&amp;quot;podpreset/pod.yaml&amp;rdquo; &amp;gt;}}
创建 Pod：
kubectl create -f https://k8s.</description>
    </item>
    
    <item>
      <title>使用 SC 安装服务目录</title>
      <link>https://lijun.in/tasks/service-catalog/install-service-catalog-using-sc/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/service-catalog/install-service-catalog-using-sc/</guid>
      <description>. glossary_definition term_id=&amp;quot;service-catalog&amp;rdquo; length=&amp;quot;all&amp;rdquo; prepend=&amp;quot;Service Catalog is&amp;rdquo; &amp;gt;}}
使用服务目录安装程序工具可以轻松地在 Kubernetes 集群上安装或卸载服务目录。 这个 CLI 工具以 sc 命令形式被安装在您的本地环境中。
. heading &amp;ldquo;prerequisites&amp;rdquo; %}}   了解服务目录的主要概念。
  安装 Go 1.6+ 以及设置 GOPATH。
  安装生成 SSL 工件所需的 cfssl 工具。
  服务目录需要 Kubernetes 1.7+ 版本。
  安装和设置 kubectl，以便将其配置为连接到 Kubernetes v1.7+ 集群。
  要安装服务目录，kubectl 用户必须绑定到 cluster-admin 角色。为了确保这是正确的，请运行以下命令：
 kubectl create clusterrolebinding cluster-admin-binding --clusterrole=cluster-admin --user=&amp;lt;user-name&amp;gt;    在本地环境中安装 sc 使用 go get 命令安装 sc CLI 工具：</description>
    </item>
    
    <item>
      <title>使用 Secret 安全地分发凭证</title>
      <link>https://lijun.in/tasks/inject-data-application/distribute-credentials-secure/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/inject-data-application/distribute-credentials-secure/</guid>
      <description>本文展示如何安全地将敏感数据（如密码和加密密钥）注入到 Pods 中。
. heading &amp;ldquo;prerequisites&amp;rdquo; %}} . include &amp;ldquo;task-tutorial-prereqs.md&amp;rdquo; &amp;gt;}} . version-check &amp;gt;}}
将 secret 数据转换为 base-64 形式 假设用户想要有两条 secret 数据：用户名 my-app 和密码 39528$vdg7Jb。 首先使用 Base64 编码 将用户名和密码转化为 base-64 形式。 这里是一个 Linux 示例：
```shell echo -n &#39;my-app&#39; | base64 echo -n &#39;39528$vdg7Jb&#39; | base64 ```  结果显示 base-64 形式的用户名为 bXktYXBw， base-64 形式的密码为 Mzk1MjgkdmRnN0pi。
创建 Secret 这里是一个配置文件，可以用来创建存有用户名和密码的 Secret:
. codenew file=&amp;quot;pods/inject/secret.yaml&amp;rdquo; &amp;gt;}}
  创建 Secret
kubectl create -f https://k8s.io/examples/pods/inject/secret.yaml .</description>
    </item>
    
    <item>
      <title>使用 Service 把前端连接到后端</title>
      <link>https://lijun.in/tasks/access-application-cluster/connecting-frontend-backend/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/access-application-cluster/connecting-frontend-backend/</guid>
      <description>本任务会描述如何创建前端微服务和后端微服务。后端微服务是一个 hello 欢迎程序。 前端和后端的连接是通过 Kubernetes 服务对象（Service object）完成的。
. heading &amp;ldquo;objectives&amp;rdquo; %}}  使用部署对象（Deployment object）创建并运行一个微服务 从后端将流量路由到前端 使用服务对象把前端应用连接到后端应用  . heading &amp;ldquo;prerequisites&amp;rdquo; %}}   . include &amp;ldquo;task-tutorial-prereqs.md&amp;rdquo; &amp;gt;}} . version-check &amp;gt;}}
  本任务使用 外部负载均衡服务， 所以需要对应的可支持此功能的环境。如果你的环境不能支持，你可以使用 NodePort 类型的服务代替。
  使用部署对象（Deployment）创建后端 后端是一个简单的 hello 欢迎微服务应用。这是后端应用的 Deployment 配置文件：
. codenew file=&amp;quot;service/access/hello.yaml&amp;rdquo; &amp;gt;}}
创建后端 Deployment：
kubectl apply -f https://k8s.io/examples/service/access/hello.yaml 查看后端的 Deployment 信息：
kubectl describe deployment hello 输出类似于：
Name: hello Namespace: default CreationTimestamp: Mon, 24 Oct 2016 14:21:02 -0700 Labels: app=hello tier=backend track=stable Selector: app=hello,tier=backend,track=stable Replicas: 7 updated | 7 total | 7 available | 0 unavailable StrategyType: RollingUpdate MinReadySeconds: 0 RollingUpdateStrategy: 1 max unavailable, 1 max surge OldReplicaSets: &amp;lt;none&amp;gt; NewReplicaSet: hello-3621623197 (7/7 replicas created) Events: .</description>
    </item>
    
    <item>
      <title>使用 Source IP</title>
      <link>https://lijun.in/tutorials/services/source-ip/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tutorials/services/source-ip/</guid>
      <description>Kubernetes 集群中运行的应用通过 Service 抽象来互相查找、通信和与外部世界沟通。本文介绍被发送到不同类型 Services 的数据包源 IP 的变化过程，你可以根据你的需求改变这些行为。
. heading &amp;ldquo;prerequisites&amp;rdquo; %}} . include &amp;ldquo;task-tutorial-prereqs.md&amp;rdquo; &amp;gt;}} . version-check &amp;gt;}}
术语表 本文使用了下列术语：
 NAT: 网络地址转换 Source NAT: 替换数据包的源 IP, 通常为节点的 IP Destination NAT: 替换数据包的目的 IP, 通常为 Pod 的 IP VIP: 一个虚拟 IP, 例如分配给每个 Kubernetes Service 的 IP Kube-proxy: 一个网络守护程序，在每个节点上协调 Service VIP 管理  准备工作 你必须拥有一个正常工作的 Kubernetes 1.5 集群来运行此文档中的示例。该示例使用一个简单的 nginx webserver，通过一个HTTP消息头返回它接收到请求的源IP。你可以像下面这样创建它：
kubectl run source-ip-app --image=k8s.gcr.io/echoserver:1.4 输出结果为
deployment.apps/source-ip-app created . heading &amp;ldquo;objectives&amp;rdquo; %}}  通过多种类型的 Services 暴露一个简单应用 理解每种 Service 类型如何处理源 IP NAT 理解保留源IP所涉及的折中  Type=ClusterIP 类型 Services 的 Source IP 如果你的 kube-proxy 运行在 iptables 模式下，从集群内部发送到 ClusterIP 的包永远不会进行源地址 NAT，这从 Kubernetes 1.</description>
    </item>
    
    <item>
      <title>使用Deployment运行一个无状态应用</title>
      <link>https://lijun.in/tasks/run-application/run-stateless-application-deployment/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/run-application/run-stateless-application-deployment/</guid>
      <description>本文介绍通过Kubernetes Deployment对象如何去运行一个应用.
. heading &amp;ldquo;objectives&amp;rdquo; %}}  创建一个nginx deployment. 使用kubectl列举关于deployment信息. 更新deployment.  . heading &amp;ldquo;prerequisites&amp;rdquo; %}} . include &amp;ldquo;task-tutorial-prereqs.md&amp;rdquo; &amp;gt;}} . version-check &amp;gt;}}
创建和探究一个nginx deployment 你可以通过创建一个Kubernetes Deployment对象来运行一个应用, 可以在一个YAML文件中描述Deployment. 例如, 下面这个YAML文件描述了一个运行nginx:1.7.9 Docker镜像的Deployment:
. codenew file=&amp;quot;application/deployment.yaml&amp;rdquo; &amp;gt;}}
  通过YAML文件创建一个Deployment:
 kubectl apply -f https://k8s.io/examples/application/deployment.yaml    展示Deployment相关信息:
kubectl describe deployment nginx-deployment user@computer:~/website$ kubectl describe deployment nginx-deployment Name: nginx-deployment Namespace: default CreationTimestamp: Tue, 30 Aug 2016 18:11:37 -0700 Labels: app=nginx Annotations: deployment.</description>
    </item>
    
    <item>
      <title>使用启动引导令牌（Bootstrap Tokens）认证</title>
      <link>https://lijun.in/reference/access-authn-authz/bootstrap-tokens/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/access-authn-authz/bootstrap-tokens/</guid>
      <description>. toc &amp;gt;}}
概述 启动引导令牌是一种简单的持有者令牌（Bearer Token），这种令牌是在新建集群或者在现有集群中添加新节点时使用的。 它被设计成能够支持 kubeadm，但是也可以被用在其他的案例中以便用户在 不使用 kubeadm 的情况下启动集群。它也被设计成可以通过 RBAC 策略，结合 Kubelet TLS Bootstrapping 系统进行工作。
启动引导令牌被定义成一个特定类型的 secrets(bootstrap.kubernetes.io/token)，并存在于 kube-system 命名空间中。然后这些 secrets 会被 API 服务器上的启动引导的认证器读取。 控制器管理器中的控制器 TokenCleaner 能够删除过期的令牌。在节点发现的过程中 Kubernetes 会使用特殊的 ConfigMap 对象。 控制器管理器中的 BootstrapSigner 控制器也会使用启动引导令牌为这类对象生成签名信息。
目前，启动引导令牌处于 alpha 阶段，但是预期也不会有大的突破性变化。
令牌格式 启动引导令牌使用 abcdef.0123456789abcdef 的形式。 更加规范地说，它们必须符合正则表达式 [a-z0-9]{6}\.[a-z0-9]{16}。
令牌的第一部分是 &amp;ldquo;Token ID&amp;rdquo; ，它是公共信息。用于引用某个令牌，并确保不会泄露认证所使用的秘密信息。 第二部分是“令牌秘密（Token Secret）”，它应该被共享给收信的第三方。
启用启动引导令牌 所有与启动引导令牌相关的特性在 Kubernetes v1.6 版本中默认都是禁用的。
你可以在 API 服务器上通过 --enable-bootstrap-token-auth 参数启用启动引导令牌。 你可以设置控制管理器的 --controllers 参数来启用启动引导令牌相关的控制器，例如 --controllers=*,tokencleaner,bootstrapsigner 。 在使用 kubeadm 时，这是自动完成的。
HTTPS 调用中的令牌是这样使用的：</description>
    </item>
    
    <item>
      <title>关键插件 Pod 的调度保证</title>
      <link>https://lijun.in/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/</guid>
      <description>除了在主机上运行的 Kubernetes 核心组件（如 api-server 、scheduler 、controller-manager）之外，还有许多插件，由于各种原因， 必须在常规集群节点（而不是 Kubernetes 主节点）上运行。 其中一些插件对于功能完备的群集至关重要，例如 Heapster、DNS 和 UI。 如果关键插件被逐出（手动或作为升级等其他操作的副作用）或者变成挂起状态，群集可能会停止正常工作。 关键插件进入挂起状态的例子有：集群利用率过高；被逐出的关键插件 Pod 释放了空间，但该空间被之前悬决的 Pod 占用；由于其它原因导致节点上可用资源的总量发生变化。
标记关键 Pod 要将 pod 标记为关键性（critical），pod 必须在 kube-system 命名空间中运行（可通过参数配置）。 同时，需要将 priorityClassName 设置为 system-cluster-critical 或 system-node-critical ，后者是整个群集的最高级别。 或者，也可以为 Pod 添加名为 scheduler.alpha.kubernetes.io/critical-pod、值为空字符串的注解。 不过，这一注解从 1.13 版本开始不再推荐使用，并将在 1.14 中删除。</description>
    </item>
    
    <item>
      <title>升级 kubeadm 集群</title>
      <link>https://lijun.in/tasks/administer-cluster/kubeadm/kubeadm-upgrade/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/administer-cluster/kubeadm/kubeadm-upgrade/</guid>
      <description>本页介绍了如何将 kubeadm 创建的 Kubernetes 集群从 1.16.x 版本升级到 1.17.x 版本，以及从版本 1.17.x 升级到 1.17.y ，其中 y &amp;gt; x。
要查看 kubeadm 创建的有关旧版本集群升级的信息，请参考以下页面：
 将 kubeadm 集群从 1.15 升级到 1.16 将 kubeadm 集群从 1.14 升级到 1.15 将 kubeadm 集群从 1.13 升级到 1.14  高版本升级工作流如下：
 升级主控制平面节点。 升级其他控制平面节点。 升级工作节点。  . heading &amp;ldquo;prerequisites&amp;rdquo; %}}  您需要有一个由 kubeadm 创建并运行着 1.16.0 或更高版本的 Kubernetes 集群。 禁用 Swap。 集群应使用静态的控制平面和 etcd pod 或者 外部 etcd。 务必仔细认真阅读[发行说明](. latest-release-notes &amp;gt;}})。 务必备份所有重要组件，例如存储在数据库中应用层面的状态。 kubeadm upgrade 不会影响您的工作负载，只会涉及 Kubernetes 内部的组件，但备份终究是好的。  附加信息  升级后，因为容器 spec 哈希值已更改，所以所有容器都会重新启动。 您只能从一个次版本升级到下一个次版本，或者同样次版本的补丁版。也就是说，升级时无法跳过版本。 例如，您只能从 1.</description>
    </item>
    
    <item>
      <title>同 Pod 内的容器使用共享卷通信</title>
      <link>https://lijun.in/tasks/access-application-cluster/communicate-containers-same-pod-shared-volume/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/access-application-cluster/communicate-containers-same-pod-shared-volume/</guid>
      <description>本文旨在说明如何让一个 Pod 内的两个容器使用一个卷（Volume）进行通信。
. heading &amp;ldquo;prerequisites&amp;rdquo; %}} . include &amp;ldquo;task-tutorial-prereqs.md&amp;rdquo; &amp;gt;}} . version-check &amp;gt;}}
创建一个包含两个容器的 Pod 在这个练习中，你会创建一个包含两个容器的 Pod。两个容器共享一个卷用于他们之间的通信。 Pod 的配置文件如下：
. codenew file=&amp;quot;pods/two-container-pod.yaml&amp;rdquo; &amp;gt;}}
在配置文件中，你可以看到 Pod 有一个共享卷，名为 shared-data。
配置文件中的第一个容器运行了一个 nginx 服务器。共享卷的挂载路径是 /usr/share/nginx/html。 第二个容器是基于 debian 镜像的，有一个 /pod-data 的挂载路径。第二个容器运行了下面的命令然后终止。
echo Hello from the debian container &amp;gt; /pod-data/index.html  注意，第二个容器在 nginx 服务器的根目录下写了 index.html 文件。
创建一个包含两个容器的 Pod：
kubectl apply -f https://k8s.io/examples/pods/two-container-pod.yaml  查看 Pod 和容器的信息：
kubectl get pod two-containers --output=yaml  这是输出的一部分：
apiVersion: v1 kind: Pod metadata: .</description>
    </item>
    
    <item>
      <title>启用端点切片</title>
      <link>https://lijun.in/tasks/administer-cluster/enabling-endpointslices/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/administer-cluster/enabling-endpointslices/</guid>
      <description>本页提供启用 Kubernetes 端点切片的总览
. heading &amp;ldquo;prerequisites&amp;rdquo; %}} . include &amp;ldquo;task-tutorial-prereqs.md&amp;rdquo; &amp;gt;}} . version-check &amp;gt;}}
介绍 端点切片为 Kubernetes 端点提供了可伸缩和可扩展的替代方案。它们建立在端点提供的功能基础之上，并以可伸缩的方式进行扩展。当服务具有大量（&amp;gt;100）网络端点， 它们将被分成多个较小的端点切片资源，而不是单个大型端点资源。
启用端点切片 . feature-state for_k8s_version=&amp;quot;v1.16&amp;rdquo; state=&amp;quot;alpha&amp;rdquo; &amp;gt;}}
. note &amp;gt;}}
尽管端点切片最终可能会取代端点，但许多 Kubernetes 组件仍然依赖于端点。目前，启用端点切片应该被视为集群中端点的补充，而不是它们的替代。
. /note &amp;gt;}}
作为 Alpha 功能，默认情况下，Kubernetes 中未启用端点切片。启用端点切片需要对 Kubernetes 集群进行多达 3 项配置修改。
要启用包括端点切片的 Discovery API 组，请使用运行时配置标志（--runtime-config=discovery.k8s.io/v1alpha1=true）。
该逻辑负责监视服务，pod 和节点以及创建或更新与之关联，在端点切片控制器内的端点切片。 默认情况下，此功能处于禁用状态，但可以通过启用在 kube-controller-manager 控制器的标志（--controllers=endpointslice）来开启。
对于像 kube-proxy 这样的 Kubernetes 组件真正开始使用端点切片，需要开启端点切片功能标志（--feature-gates=EndpointSlice=true）。
使用端点切片 在集群中完全启用端点切片的情况下，您应该看到对应的每个端点资源的端点切片资源。除了兼容现有的端点功能，端点切片应包括拓扑等新的信息。它们将使集群中网络端点具有更强的可伸缩性，可扩展性。</description>
    </item>
    
    <item>
      <title>命名空间演练</title>
      <link>https://lijun.in/tasks/administer-cluster/namespaces-walkthrough/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/administer-cluster/namespaces-walkthrough/</guid>
      <description>Kubernetes . glossary_tooltip text=&amp;quot;命名空间&amp;rdquo; term_id=&amp;quot;namespace&amp;rdquo; &amp;gt;}} 有助于不同的项目、团队或客户去共享 Kubernetes 集群。
名字空间通过以下方式实现这点：
 为名字设置作用域. 为集群中的部分资源关联鉴权和策略的机制。  使用多个命名空间是可选的。
此示例演示了如何使用 Kubernetes 命名空间细分群集。
. heading &amp;ldquo;prerequisites&amp;rdquo; %}} . include &amp;ldquo;task-tutorial-prereqs.md&amp;rdquo; &amp;gt;}} . version-check &amp;gt;}}
环境准备 此示例作如下假设：
 您已拥有一个 配置好的 Kubernetes 集群。 您已对 Kubernetes 的 Pods, Services 和 Deployments 有基本理解。   理解默认命名空间  默认情况下，Kubernetes 集群会在配置集群时实例化一个默认命名空间，用以存放集群所使用的默认 Pods、Services 和 Deployments 集合。
假设您有一个新的集群，您可以通过执行以下操作来检查可用的命名空间：
kubectl get namespaces NAME STATUS AGE default Active 13m 创建新的命名空间 在本练习中，我们将创建两个额外的 Kubernetes 命名空间来保存我们的内容。
我们假设一个场景，某组织正在使用共享的 Kubernetes 集群来支持开发和生产：</description>
    </item>
    
    <item>
      <title>在 AWS EC2 上运行 Kubernetes</title>
      <link>https://lijun.in/setup/production-environment/turnkey/aws/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/setup/production-environment/turnkey/aws/</guid>
      <description>本页面介绍了如何在 AWS 上安装 Kubernetes 集群。
. heading &amp;ldquo;prerequisites&amp;rdquo; %}} 在 AWS 上创建 Kubernetes 集群，您将需要 AWS 的 Access Key ID 和 Secret Access Key。
支持的生产级别工具  conjure-up 是 Kubernetes 的开源安装程序，可在 Ubuntu 上创建与原生 AWS 集成的 Kubernetes 集群。   Kubernetes Operations - 生产级 K8s 的安装、升级和管理。支持在 AWS 运行 Debian、Ubuntu、CentOS 和 RHEL。   CoreOS Tectonic 包括开源的 Tectonic 安装程序，它用于在 AWS 上创建带有 Container Linux 节点的 Kubernetes 集群。   起源于 CoreOS，Kubernetes Incubator 维护的 CLI 工具， kube-aws ，该工具使用 Container Linux 节点创建和管理 Kubernetes 集群，它使用了 AWS 工具：EC2、CloudFormation 和 Autoscaling。   KubeOne 是一个开源集群生命周期管理工具，它可用于创建，升级和管理高可用 Kubernetes 集群。  集群入门 命令行管理工具：kubectl 集群启动脚本将在您的工作站上为您提供一个 kubernetes 目录。 或者，您可以从此页面下载最新的 Kubernetes 版本。</description>
    </item>
    
    <item>
      <title>在 Azure 上运行 Kubernetes</title>
      <link>https://lijun.in/setup/production-environment/turnkey/azure/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/setup/production-environment/turnkey/azure/</guid>
      <description>Azure Kubernetes 服务 (AKS) Azure Kubernetes 服务提供了简单的 Kubernetes 集群部署方式。
有关通过 Azure Kubernetes 服务将 Kubernetes 集群部署到 Azure 的示例：
微软 Azure Kubernetes 服务
定制部署：AKS 引擎 Azure Kubernetes 服务的核心是开源，并且可以在 GitHub 上让社区使用和参与贡献：AKS 引擎。旧版 ACS 引擎 代码库已被弃用，以支持AKS-engine。
如果您需要在 Azure Kubernetes 服务正式支持的范围之外对部署进行自定义，则 AKS 引擎是一个不错的选择。这些自定义包括部署到现有虚拟网络中，利用多个代理程序池等。一些社区对 AKS 引擎的贡献甚至可能成为 Azure Kubernetes 服务的特性。
AKS 引擎的输入是一个描述 Kubernetes 集群的 apimodel JSON 文件。它和用于直接通过 Azure Kubernetes 服务部署集群的 Azure 资源管理器（ARM）模板语法相似。产生的输出是一个 ARM 模板，可以将其签入源代码管理，并使用它将 Kubernetes 集群部署到 Azure。
您可以按照 **AKS 引擎 Kubernetes 教程**开始使用。
适用于 Azure 的 CoreOS Tectonic 适用于 Azure 的 CoreOS Tectonic Installer 是开源的，它可以让社区在 GitHub 上使用和参与贡献：Tectonic Installer。</description>
    </item>
    
    <item>
      <title>在 Kubernetes 集群中使用 NodeLocal DNSCache</title>
      <link>https://lijun.in/tasks/administer-cluster/nodelocaldns/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/administer-cluster/nodelocaldns/</guid>
      <description>本页概述了 Kubernetes 中的 NodeLocal DNSCache 功能。
. heading &amp;ldquo;prerequisites&amp;rdquo; %}} . include &amp;ldquo;task-tutorial-prereqs.md&amp;rdquo; &amp;gt;}} . version-check &amp;gt;}}
引言 NodeLocal DNSCache 通过在集群节点上作为 DaemonSet 运行 dns 缓存代理来提高集群 DNS 性能。 在当今的体系结构中，处于 ClusterFirst DNS 模式的 Pod 可以连接到 kube-dns serviceIP 进行 DNS 查询。 通过 kube-proxy 添加的 iptables 规则将其转换为 kube-dns/CoreDNS 端点。 借助这种新架构，Pods 将可以访问在同一节点上运行的 dns 缓存代理，从而避免了 iptables DNAT 规则和连接跟踪。 本地缓存代理将查询 kube-dns 服务以获取集群主机名的缓存缺失（默认为 cluster.local 后缀）。
动机  使用当前的 DNS 体系结构，如果没有本地 kube-dns/CoreDNS 实例，则具有最高 DNS QPS 的 Pod 可能必须延伸到另一个节点。 在这种脚本下，拥有本地缓存将有助于改善延迟。   跳过 iptables DNAT 和连接跟踪将有助于减少 conntrack 竞争并避免 UDP DNS 条目填满 conntrack 表。   从本地缓存代理到 kube-dns 服务的连接可以升级到 TCP 。 TCP conntrack 条目将在连接关闭时被删除，相反 UDP 条目必须超时(默认 nf_conntrack_udp_timeout 是 30 秒)   将 DNS 查询从 UDP 升级到 TCP 将减少归因于丢弃的 UDP 数据包和 DNS 超时的尾部等待时间，通常长达 30 秒（3 次重试+ 10 秒超时）。   在节点级别对 dns 请求的度量和可见性。   可以重新启用负缓存，从而减少对 kube-dns 服务的查询数量。  架构图 启用 NodeLocal DNSCache 之后，这是 DNS 查询所遵循的路径：</description>
    </item>
    
    <item>
      <title>在 Kubernetes 集群中使用 sysctl</title>
      <link>https://lijun.in/tasks/administer-cluster/sysctl-cluster/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/administer-cluster/sysctl-cluster/</guid>
      <description>. feature-state for_k8s_version=&amp;quot;v1.11&amp;rdquo; state=&amp;quot;beta&amp;rdquo; &amp;gt;}}
本文档介绍如何通过 sysctl 接口在 Kubernetes 集群中配置和使用内核参数。
. heading &amp;ldquo;prerequisites&amp;rdquo; %}} . include &amp;ldquo;task-tutorial-prereqs.md&amp;rdquo; &amp;gt;}} . version-check &amp;gt;}}
获取 Sysctl 的参数列表 在 Linux 中，管理员可以通过 sysctl 接口修改内核运行时的参数。在 /proc/sys/ 虚拟文件系统下存放许多内核参数。这些参数涉及了多个内核子系统，如：
 内核子系统 (通常前缀为: kernel.) 网络子系统 (通常前缀为: net.) 虚拟内存子系统 (通常前缀为: vm.) MDADM 子系统 (通常前缀为: dev.) 更多子系统请参见 内核文档。  若要获取完整的参数列表，请执行以下命令
$ sudo sysctl -a 启用非安全的 Sysctl 参数 sysctl 参数分为 安全 和 非安全的。安全 sysctl 参数除了需要设置恰当的命名空间外，在同一 node 上的不同 Pod 之间也必须是 相互隔离的。这意味着在 Pod 上设置 安全 sysctl 参数</description>
    </item>
    
    <item>
      <title>在实时集群上重新配置节点的 Kubelet</title>
      <link>https://lijun.in/tasks/administer-cluster/reconfigure-kubelet/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/administer-cluster/reconfigure-kubelet/</guid>
      <description>. feature-state for_k8s_version=&amp;quot;v1.11&amp;rdquo; state=&amp;quot;beta&amp;rdquo; &amp;gt;}}
动态Kubelet配置 引导你在一个运行的 Kubernetes 集群上更改每一个 Kubelet 的配置，通过部署 ConfigMap 并配置每个节点来使用它。
. warning &amp;gt;}}
警告：所有Kubelet配置参数都可以动态地更改，但这对某些参数来说是不安全的。在决定动态更改参数之前，你需要深刻理解这种变化将如何影响你的集群的行为。 在把一组节点推广到集群范围之前，都要仔细地测试这些节点上的配置变化。与配置相关的建议可以在具体的文件下找到，内联 KubeletConfiguration 类型文档。 . /warning &amp;gt;}}
. heading &amp;ldquo;prerequisites&amp;rdquo; %}}  Kubernetes v1.11 或者更高版本配置在主节点和节点上 kubectl v1.11 或者更高版本和集群配置通信 The Kubelet --dynamic-config-dir flag 必须设置在节点的可写目录上  在你集群中的一个实时节点上配置Kubelet 基本工作流程概述 在实时集群中配置 Kubelet 的基本工作流程如下：
 编写一个 YAML 或 JSON 的配置文件包含 Kubelet 的配置。 将此文件包装在 ConfigMap 中并将其保存到 Kubernetes 控制平面。 更新 Kubelet 的相应节点对象以使用此 ConfigMap。  每个 Kubelet 都会在其各自的节点对象上查看配置引用。当此引用更改时，Kubelet 将下载新配置， 更新本地引用以引用该文件，然后退出。 要想使该功能正常地工作，您必须运行操作系统级别的服务管理器（如systemd），如果退出，将重启Kubelet。 当Kubelet重新启动时，它将开始使用新配置。</description>
    </item>
    
    <item>
      <title>在本地开发和调试服务</title>
      <link>https://lijun.in/tasks/debug-application-cluster/local-debugging/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/debug-application-cluster/local-debugging/</guid>
      <description>Kubernetes 应用程序通常由多个独立的服务组成，每个服务都在自己的容器中运行。 在远端的 Kubernetes 集群上开发和调试这些服务可能很麻烦，需要在运行的容器上打开 shell，然后在远端 shell 中运行您所需的工具。
telepresence 是一种工具，用于在本地轻松开发和调试服务，同时将服务代理到远程 Kubernetes 集群。 使用 telepresence 可以为本地服务使用自定义工具（如调试器和 IDE），并提供对 Configmap、Secrets 和远程集群上运行的服务的完全访问。
 Kubernetes 集群安装完毕 配置好 kubectl 与集群交互 Telepresence 安装完毕  打开终端，不带参数运行 telepresence，以打开 telepresence shell。这个 shell 在本地运行，使您可以完全访问本地文件系统。
telepresence shell 的使用方式多种多样。 例如，在你的笔记本电脑上写一个 shell 脚本，然后直接在 shell 中实时运行它。 您也可以在远端 shell 上执行此操作，但这样可能无法使用首选的代码编辑器，并且在容器终止时脚本将被删除。
开发和调试现有的服务 在 Kubernetes 上开发应用程序时，通常对单个服务进行编程或调试。 服务可能需要访问其他服务以进行测试和调试。 一种选择是使用连续部署管道，但即使最快的部署管道也会在程序或调试周期中引入延迟。
使用 --swap-deployment 选项将现有部署与 Telepresence 代理交换。交换允许您在本地运行服务并能够连接到远端的 Kubernetes 集群。远端的集群中的服务现在就可以访问本地运行的实例。
到运行 telepresence 并带有 --swap-deployment 选项，请输入：
telepresence --swap-deployment $DEPLOYMENT_NAME
这里的 $DEPLOYMENT_NAME 是您现有的部署名称。
运行此命令将生成 shell。在 shell 中，启动您的服务。 然后，您就可以在本地对源代码进行编辑、保存并能看到更改立即生效。您还可以在调试器或任何其他本地开发工具中运行服务。</description>
    </item>
    
    <item>
      <title>在联邦中设置放置策略</title>
      <link>https://lijun.in/tasks/federation/set-up-placement-policies-federation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/federation/set-up-placement-policies-federation/</guid>
      <description>. deprecationfilewarning &amp;gt;}} . include &amp;ldquo;federation-deprecation-warning-note.md&amp;rdquo; &amp;gt;}} . /deprecationfilewarning &amp;gt;}}
此页面显示如何使用外部策略引擎对联邦资源强制执行基于策略的放置决策。
. heading &amp;ldquo;prerequisites&amp;rdquo; %}} 您需要一个正在运行的 Kubernetes 集群(它被引用为主机集群)。有关您的平台的安装说明，请参阅入门指南。
Deploying 联邦并配置外部策略引擎 可以使用 kubefed init 部署联邦控制平面。
Deploying 联邦控制平面之后，必须在联邦 API 服务器中配置一个准入控制器，该控制器强制执行从外部策略引擎接收到的放置决策。
kubectl create -f scheduling-policy-admission.yaml  下图是准入控制器的 ConfigMap 示例：
. codenew file=&amp;quot;federation/scheduling-policy-admission.yaml&amp;rdquo; &amp;gt;}}
ConfigMap 包含三个文件：
 config.yml 指定 调度策略 准入控制器配置文件的位置。 scheduling-policy-config.yml 指定与外部策略引擎联系所需的 kubeconfig 文件的位置。 该文件还可以包含一个 retryBackoff 值，该值以毫秒为单位控制初始重试 backoff 延迟。 opa-kubeconfig 是一个标准的 kubeconfig，包含联系外部策略引擎所需的 URL 和凭证。  编辑联邦 API 服务器部署以启用 SchedulingPolicy 准入控制器。
kubectl -n federation-system edit deployment federation-apiserver  更新 Federation API 服务器命令行参数以启用准入控制器， 并将 ConfigMap 挂载到容器中。如果存在现有的 -enable-admissionplugins 参数，则追加 SchedulingPolicy 而不是添加另一行。</description>
    </item>
    
    <item>
      <title>在腾讯云容器服务上运行 Kubernetes</title>
      <link>https://lijun.in/setup/production-environment/turnkey/tencent/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/setup/production-environment/turnkey/tencent/</guid>
      <description>腾讯云容器服务 腾讯云容器服务（TKE）提供本地 Kubernetes 容器管理服务。您只需几个步骤即可使用 TKE 部署和管理 Kubernetes 集群。有关详细说明，请参阅部署腾讯云容器服务。
TKE 是认证的 Kubernetes 产品。它与原生 Kubernetes API 完全兼容。
定制部署 腾讯 Kubernetes Engine 的核心是开源的，并且可以在 GitHub 上使用。
使用 TKE 创建 Kubernetes 集群时，可以选择托管模式或独立部署模式。另外，您可以根据需要自定义部署。例如，您可以选择现有的 Cloud Virtual Machine 实例来创建集群，也可以在 IPVS 模式下启用 Kube-proxy。
下一步 要了解更多信息，请参阅 TKE 文档。</description>
    </item>
    
    <item>
      <title>在阿里云上运行 Kubernetes</title>
      <link>https://lijun.in/setup/production-environment/turnkey/alibaba-cloud/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/setup/production-environment/turnkey/alibaba-cloud/</guid>
      <description>阿里云容器服务 阿里云容器服务使您可以在阿里云 ECS 实例集群上运行和管理 Docker 应用程序。它支持流行的开源容器编排引擎：Docker Swarm 和 Kubernetes。
为了简化集群的部署和管理，请使用 容器服务 Kubernetes 版。您可以按照 Kubernetes 演练快速入门，其中有一些使用中文书写的容器服务 Kubernetes 版教程。
要使用自定义二进制文件或开源版本的 Kubernetes，请按照以下说明进行操作。
自定义部署 阿里云 Kubernetes Cloud Provider 实现 的源代码是开源的，可在 GitHub 上获得。
有关更多信息，请参阅中文版本快速部署 Kubernetes - 阿里云上的VPC环境和英文版本。</description>
    </item>
    
    <item>
      <title>声明网络策略</title>
      <link>https://lijun.in/tasks/administer-cluster/declare-network-policy/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/administer-cluster/declare-network-policy/</guid>
      <description>本文可以帮助您开始使用 Kubernetes 的 NetworkPolicy API 声明网络策略去管理 Pod 之间的通信
. heading &amp;ldquo;prerequisites&amp;rdquo; %}} 您首先需要有一个支持网络策略的 Kubernetes 集群。已经有许多支持 NetworkPolicy 的网络提供商，包括：
 Calico Romana Weave 网络  注意：以上列表是根据产品名称按字母顺序排序，而不是按推荐或偏好排序。下面示例对于使用了上面任何提供商的 Kubernetes 集群都是有效的
创建一个nginx deployment 并且通过服务将其暴露 为了查看 Kubernetes 网络策略是怎样工作的，可以从创建一个nginx deployment 并且通过服务将其暴露开始
$ kubectl run nginx --image=nginx --replicas=2 deployment &amp;quot;nginx&amp;quot; created $ kubectl expose deployment nginx --port=80 service &amp;quot;nginx&amp;quot; exposed 在 default 命名空间下运行了两个 nginx pod，而且通过一个名字为 nginx 的服务进行了暴露
$ kubectl get svc,pod NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE svc/kubernetes 10.100.0.1 &amp;lt;none&amp;gt; 443/TCP 46m svc/nginx 10.</description>
    </item>
    
    <item>
      <title>安装扩展（Addons）</title>
      <link>https://lijun.in/concepts/cluster-administration/addons/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/concepts/cluster-administration/addons/</guid>
      <description>Add-ons 扩展了 Kubernetes 的功能。
本文列举了一些可用的 add-ons 以及到它们各自安装说明的链接。
每个 add-ons 按字母顺序排序 - 顺序不代表任何优先地位。
网络和网络策略  ACI 通过 Cisco ACI 提供集成的容器网络和安全网络。 Calico 是一个安全的 L3 网络和网络策略提供者。 Canal 结合 Flannel 和 Calico，提供网络和网络策略。 Cilium 是一个 L3 网络和网络策略插件，能够透明的实施 HTTP/API/L7 策略。同时支持路由（routing）和叠加/封装（overlay/encapsulation）模式。 CNI-Genie 使 Kubernetes 无缝连接到一种 CNI 插件，例如：Flannel、Calico、Canal、Romana 或者 Weave。 Contiv 为多种用例提供可配置网络（使用 BGP 的原生 L3，使用 vxlan 的 overlay，经典 L2 和 Cisco-SDN/ACI）和丰富的策略框架。Contiv 项目完全开源。安装工具同时提供基于和不基于 kubeadm 的安装选项。 基于 Tungsten Fabric 的 Contrail，是一个开源的多云网络虚拟化和策略管理平台，Contrail 和 Tungsten Fabric 与业务流程系统（例如 Kubernetes、OpenShift、OpenStack和Mesos）集成在一起，并为虚拟机、容器或 Pod 以及裸机工作负载提供了隔离模式。 Flannel 是一个可以用于 Kubernetes 的 overlay 网络提供者。 Knitter 是为 kubernetes 提供复合网络解决方案的网络组件。 Multus 是一个多插件，可在 Kubernetes 中提供多种网络支持，以支持所有 CNI 插件（例如 Calico，Cilium，Contiv，Flannel），而且包含了在 Kubernetes 中基于 SRIOV、DPDK、OVS-DPDK 和 VPP 的工作负载。 NSX-T 容器插件（ NCP ）提供了 VMware NSX-T 与容器协调器（例如 Kubernetes）之间的集成，以及 NSX-T 与基于容器的 CaaS / PaaS 平台（例如关键容器服务（PKS）和 OpenShift）之间的集成。 Nuage 是一个 SDN 平台，可在 Kubernetes Pods 和非 Kubernetes 环境之间提供基于策略的联网，并具有可视化和安全监控。 Romana 是一个 pod 网络的层 3 解决方案，并且支持 NetworkPolicy API。Kubeadm add-on 安装细节可以在这里找到。 Weave Net 提供了在网络分组两端参与工作的网络和网络策略，并且不需要额外的数据库。  服务发现  CoreDNS 是一种灵活的，可扩展的 DNS 服务器，可以安装为集群内的 Pod 提供 DNS 服务。  可视化管理  Dashboard 是一个 Kubernetes 的 web 控制台界面。 Weave Scope 是一个图形化工具，用于查看你的 containers、 pods、services 等。 请和一个 Weave Cloud account 一起使用，或者自己运行 UI。  基础设施  KubeVirt 是可以让 Kubernetes 运行虚拟机的 add-ons。通常运行在裸机群集上。  遗留 Add-ons 还有一些其它 add-ons 归档在已废弃的 cluster/addons 路径中。</description>
    </item>
    
    <item>
      <title>容器环境变量</title>
      <link>https://lijun.in/concepts/containers/container-environment-variables/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/concepts/containers/container-environment-variables/</guid>
      <description>本文介绍容器环境中对容器可用的资源。
容器环境 Kubernetes 容器环境为容器提供了几类重要的资源：
 一个文件系统，其中包含一个镜像和一个或多个卷。 容器本身相关的信息。 集群中其他对象相关的信息。  容器信息 容器的 hostname 是容器所在的 Pod 名称。 可以通过 hostname 命令或调用 libc 中的 gethostname 函数来获取。
Pod 名称和名字空间可以通过 downward API 以环境变量方式访问。
与 Docker 镜像中静态指定的环境变量一样，Pod 中用户定义的环境变量也可用于容器。
集群信息 容器创建时运行的所有服务的列表都会作为环境变量提供给容器。 这些环境变量与 Docker 链接语法相匹配。
对一个名为 foo ，映射到名为 bar 的容器端口的服务， 会定义如下变量：
FOO_SERVICE_HOST=&amp;lt;服务所在的主机地址&amp;gt; FOO_SERVICE_PORT=&amp;lt;服务所启用的端口&amp;gt; 服务具有专用 IP 地址，如果启用了 [DNS 插件](http://releases.k8s.io/ param &amp;ldquo;githubbranch&amp;rdquo; &amp;gt;}}/cluster/addons/dns/)，还可以在容器中通过 DNS 进行访问。
  查看容器生命周期挂钩（hooks）了解更多。 获取为容器生命周期事件附加处理程序的实践经验。  </description>
    </item>
    
    <item>
      <title>对 DaemonSet 执行回滚</title>
      <link>https://lijun.in/tasks/manage-daemon/rollback-daemon-set/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/manage-daemon/rollback-daemon-set/</guid>
      <description>本文展示了如何对 DaemonSet 执行回滚。
. heading &amp;ldquo;prerequisites&amp;rdquo; %}}  DaemonSet 滚动升级历史和 DaemonSet 回滚特性仅在 Kubernetes 1.7 及以后版本的 kubectl 中支持。 确保您了解如何 对 DaemonSet 执行滚动升级。  对 DaemonSet 执行回滚 步骤 1： 找到想要 DaemonSet 回滚到的历史版本（revision） 如果只想回滚到最后一个版本，可以跳过这一步。
列出 DaemonSet 的所有版本：
kubectl rollout history daemonset &amp;lt;daemonset-name&amp;gt; 该命令返回 DaemonSet 版本列表：
daemonsets &amp;#34;&amp;lt;daemonset-name&amp;gt;&amp;#34; REVISION CHANGE-CAUSE 1 ... 2 ... ...  在创建时，DaemonSet 的变化原因从 kubernetes.io/change-cause 注解（annotation）复制到其版本中。 用户可以在 kubectl 中指定 --record=true ，将执行的命令记录在变化原因注解中。  执行以下命令，来查看指定版本的详细信息：
kubectl rollout history daemonset &amp;lt;daemonset-name&amp;gt; --revision=1 该命令返回相应版本的详细信息：</description>
    </item>
    
    <item>
      <title>对 DaemonSet 执行滚动更新</title>
      <link>https://lijun.in/tasks/manage-daemon/update-daemon-set/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/manage-daemon/update-daemon-set/</guid>
      <description>本文介绍了如何对 DaemonSet 执行滚动更新。
. heading &amp;ldquo;prerequisites&amp;rdquo; %}}  Kubernetes 1.6 或者更高版本中才支持 DaemonSet 滚动更新功能。  DaemonSet 更新策略 DaemonSet 有两种更新策略：
 OnDelete: 使用 OnDelete 更新策略时，在更新 DaemonSet 模板后，只有当您手动删除老的 DaemonSet pods 之后，新的 DaemonSet pods 才会被自动创建。跟 Kubernetes 1.6 以前的版本类似。 RollingUpdate: 这是默认的更新策略。使用 RollingUpdate 更新策略时，在更新 DaemonSet 模板后，老的 DaemonSet pods 将被终止，并且将以受控方式自动创建新的 DaemonSet pods。  执行滚动更新 要启用 DaemonSet 的滚动更新功能，必须设置 .spec.updateStrategy.type 为 RollingUpdate。
您可能想设置.spec.updateStrategy.rollingUpdate.maxUnavailable (默认为 1) 和.spec.minReadySeconds (默认为 0)。
步骤 1: 检查 DaemonSet 的滚动更新策略 首先，检查 DaemonSet 的更新策略，确保已经将其设置为 RollingUpdate:
kubectl get ds/&amp;lt;daemonset-name&amp;gt; -o go-template=&amp;#39;{{.</description>
    </item>
    
    <item>
      <title>工具</title>
      <link>https://lijun.in/reference/tools/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/tools/</guid>
      <description>Kubernetes 包含一些内置工具，可以帮助用户更好的使用 Kubernetes 系统。
Kubectl kubectl 是 Kubernetes 命令行工具，可以用来操控 Kubernetes 集群。
Kubeadm kubeadm 是一个命令行工具，可以用来在物理机、云服务器或虚拟机（目前处于 alpha 阶段）上轻松部署一个安全可靠的 Kubernetes 集群。
Kubefed kubefed 是一个命令行工具，可以用来帮助用户管理联邦集群。
Minikube minikube 是一个可以方便用户在其工作站点本地部署一个单节点 Kubernetes 集群的工具，用于开发和测试。
Dashboard Dashboard, 是 Kubernetes 基于 Web 的用户管理界面，允许用户部署容器化应用到 Kubernetes 集群，进行故障排查以及管理集群和集群资源。
Helm Kubernetes Helm 是一个管理预先配置 Kubernetes 资源包的工具，这里的资源在 Helm 中也被称作 Kubernetes charts。
使用 Helm：
 查找并使用已经打包为 Kubernetes charts 的流行软件 分享您自己的应用作为 Kubernetes charts 为 Kubernetes 应用创建可重复执行的构建 为您的 Kubernetes 清单文件提供更智能化的管理 管理 Helm 软件包的发布  Kompose Kompose 一个转换工具，用来帮助 Docker Compose 用户迁移至 Kubernetes。</description>
    </item>
    
    <item>
      <title>应用故障排查</title>
      <link>https://lijun.in/tasks/debug-application-cluster/debug-application/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/debug-application-cluster/debug-application/</guid>
      <description>本指南帮助用户来调试kubernetes上那些没有正常运行的应用。 本指南不能调试集群。如果想调试集群的话，请参阅这里。
. toc &amp;gt;}}
诊断问题 故障排查的第一步是先给问题分下类。这个问题是什么？Pods，Replication Controller或者Service？
 Debugging Pods Debugging Replication Controllers Debugging Services  Debugging Pods 调试pod的第一步是看一下这个pod的信息，用如下命令查看一下pod的当前状态和最近的事件：
$ kubectl describe pods ${POD_NAME} 查看一下pod中的容器所处的状态。这些容器的状态都是Running吗？最近有没有重启过？
后面的调试都是要依靠pods的状态的。
pod停留在pending状态 如果一个pod卡在Pending状态，则表示这个pod没有被调度到一个节点上。通常这是因为资源不足引起的。 敲一下kubectl describe ...这个命令，输出的信息里面应该有显示为什么没被调度的原因。 常见原因如下：
  资源不足: 你可能耗尽了集群上所有的CPU和内存，此时，你需要删除pods，调整资源请求，或者增加节点。 更多信息请参阅Compute Resources document
  使用了hostPort: 如果绑定一个pod到hostPort，那么能创建的pod个数就有限了。 多数情况下，hostPort是非必要的，而应该采用服务来暴露pod。 如果确实需要使用hostPort，那么能创建的pod的数量就是节点的个数。
  pod停留在waiting状态 如果一个pod卡在Waiting状态，则表示这个pod已经调试到节点上，但是没有运行起来。 再次敲一下kubectl describe ...这个命令来查看相关信息。 最常见的原因是拉取镜像失败。可以通过以下三种方式来检查：
 使用的镜像名字正确吗？ 镜像仓库里有没有这个镜像？ 用docker pull &amp;lt;image&amp;gt;命令手动拉下镜像试试。  pod处于crashing状态或者unhealthy 首先，看一下容器的log:
$ kubectl logs ${POD_NAME} ${CONTAINER_NAME} 如果容器是crashed的，用如下命令可以看到crash的log:
$ kubectl logs --previous ${POD_NAME} ${CONTAINER_NAME} 或者，用exec在容器内运行一些命令：</description>
    </item>
    
    <item>
      <title>应用自测与调试</title>
      <link>https://lijun.in/tasks/debug-application-cluster/debug-application-introspection/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/debug-application-cluster/debug-application-introspection/</guid>
      <description>运行应用时，不可避免的需要定位问题。 前面我们介绍了如何使用 kubectl get pods 来查询 pod 的简单信息。 除此之外，还有一系列的方法来获取应用的更详细信息。
使用 kubectl describe pod 命令获取 pod 详情 与之前的例子类似，我们使用一个 Deployment 来创建两个 pod。
. codenew file=&amp;quot;application/nginx-with-request.yaml&amp;rdquo; &amp;gt;}}
使用如下命令创建 deployment：
kubectl apply -f https://k8s.io/examples/application/nginx-with-request.yaml deployment.apps/nginx-deployment created 使用如下命令查看 pod 状态：
kubectl get pods NAME READY STATUS RESTARTS AGE nginx-deployment-1006230814-6winp 1/1 Running 0 11s nginx-deployment-1006230814-fmgu3 1/1 Running 0 11s 我们可以使用 kubectl describe pod 命令来查询每个 pod 的更多信息，比如：
kubectl describe pod nginx-deployment-1006230814-6winp Name:	nginx-deployment-1006230814-6winp Namespace:	default Node:	kubernetes-node-wul5/10.240.0.9 Start Time:	Thu, 24 Mar 2016 01:39:49 +0000 Labels:	app=nginx,pod-template-hash=1006230814 Annotations: kubernetes.</description>
    </item>
    
    <item>
      <title>开发云控制器管理器</title>
      <link>https://lijun.in/tasks/administer-cluster/developing-cloud-controller-manager/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/administer-cluster/developing-cloud-controller-manager/</guid>
      <description>. feature-state for_k8s_version=&amp;quot;v1.11&amp;rdquo; state=&amp;quot;beta&amp;rdquo; &amp;gt;}}
在即将发布的版本中，云控制器管理器将是把 Kubernetes 与任何云集成的首选方式。 这将确保驱动可以独立于核心 Kubernetes 发布周期开发其功能。
. feature-state for_k8s_version=&amp;quot;1.8&amp;rdquo; state=&amp;quot;alpha&amp;rdquo; &amp;gt;}}
在讨论如何构建自己的云控制器管理器之前，了解有关它如何工作的一些背景知识是有帮助的。云控制器管理器是来自 kube-controller-manager 的代码，利用 Go 接口允许插入任何云的实现。大多数框架和通用控制器的实现在 core，但只要满足 云提供者接口，它就会始终执行它所提供的云接口。
为了深入了解实施细节，所有云控制器管理器都将从 Kubernetes 核心导入依赖包，唯一的区别是每个项目都会通过调用 cloudprovider.RegisterCloudProvider 来注册自己的驱动，更新可用驱动的全局变量。
开发 Out of Tree 要为您的云构建一个 out-of-tree 云控制器管理器，请按照下列步骤操作：
 使用满足 cloudprovider.Interface 的实现创建一个 go 包。 使用来自 Kubernetes 核心包的 cloud-controller-manager 中的 main.go 作为 main.go 的模板。如上所述，唯一的区别应该是将导入的云包。 在 main.go 中导入你的云包，确保你的包有一个 init 块来运行 cloudprovider.RegisterCloudProvider。  用现有的 out-of-tree 云驱动作为例子可能会有所帮助。你可以在这里找到 清单。
In Tree 对于 in-tree 驱动，您可以将 in-tree 云控制器管理器作为群集中的 Daemonset 运行。有关详细信息，请参阅 运行的云控制器管理器文档。</description>
    </item>
    
    <item>
      <title>弹缩StatefulSet</title>
      <link>https://lijun.in/tasks/run-application/scale-stateful-set/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/run-application/scale-stateful-set/</guid>
      <description>本文介绍如何弹缩StatefulSet.
. heading &amp;ldquo;prerequisites&amp;rdquo; %}}  StatefulSets仅适用于Kubernetes1.5及以上版本. 不是所有Stateful应用都适合弹缩. 在弹缩前您的应用前. 您必须充分了解您的应用, 不适当的弹缩StatefulSet或许会造成应用自身功能的不稳定. 仅当您确定该Stateful应用的集群是完全健康才可执行弹缩操作.  使用 kubectl 弹缩StatefulSets 弹缩请确认 kubectl 已经升级到Kubernetes1.5及以上版本. 如果不确定, 执行 kubectl version 命令并检查使用的 Client Version.
kubectl 弹缩 首先, 找到您想要弹缩的StatefulSet. 记住, 您需先清楚是否能弹缩该应用.
kubectl get statefulsets &amp;lt;stateful-set-name&amp;gt; 改变StatefulSet副本数量:
kubectl scale statefulsets &amp;lt;stateful-set-name&amp;gt; --replicas=&amp;lt;new-replicas&amp;gt; 可使用其他命令: kubectl apply / kubectl edit / kubectl patch 另外, 您可以 in-place updates StatefulSets.
如果您的StatefulSet开始由 kubectl apply 或 kubectl create --save-config 创建,更新StatefulSet manifests中的 .spec.replicas, 然后执行命令 kubectl apply:
kubectl apply -f &amp;lt;stateful-set-file-updated&amp;gt; 除此之外, 可以通过命令 kubectl edit 编辑该字段:</description>
    </item>
    
    <item>
      <title>排错</title>
      <link>https://lijun.in/tasks/debug-application-cluster/troubleshooting/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/debug-application-cluster/troubleshooting/</guid>
      <description>有时候事情会出错。本指南旨在正确解决这些问题。它包含两个部分：
 应用排错 - 用于部署代码到 Kubernetes 并想知道代码为什么不能正常运行的用户。 集群排错 - 用于集群管理员以及 Kubernetes 集群表现异常的用户。  您也应该查看所用版本的已知问题。
获取帮助 如果您的问题在上述指南中没有得到答案，您还有另外几种方式从 Kubernetes 团队获得帮助。
提问 网站上的文档针对回答各类问题进行了结构化组织和分类。 概念部分解释了 Kubernetes 体系结构以及每个组件的工作方式，安装部分提供了入门的实用说明。 任务部分展示了如何完成常用任务，入门部分则是对现实世界、特定行业或端到端开发场景的更全面的演练。 参考部分提供了详细的 [Kubernetes API](/docs/reference/generated/kubernetes-api/. param &amp;ldquo;version&amp;rdquo; &amp;gt;}}/) 文档和命令行 (CLI) 接口，例如kubectl。
您还可以找到堆栈溢出相关的主题：
 Kubernetes Google Kubernetes Engine  求救！我的问题还没有解决！我需要立即得到帮助！ 堆栈溢出 社区中的其他人可能已经问过和您类似的问题，这或许能帮助解决您的问题。 Kubernetes 团队还会监视带有 Kubernetes 标签的帖子。 如果现有的问题对您没有帮助，请问一个新问题!
Slack Kubernetes 团队在 Slack 中建有 #kubernetes-users 频道。 您可以在这里参加与 Kubernetes 团队的讨论。 Slack 需要注册，但 Kubernetes 团队公开邀请任何人在这里注册。 欢迎您随时来问任何问题。
一旦注册完成，您就可以浏览各种感兴趣的频道列表。 例如，Kubernetes 新人可能还想加入 #kubernetes-novice 频道。作为另一个例子，开发人员应该加入 #kubernetes-dev 频道。</description>
    </item>
    
    <item>
      <title>控制节点上的 CPU 管理策略</title>
      <link>https://lijun.in/tasks/administer-cluster/cpu-management-policies/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/administer-cluster/cpu-management-policies/</guid>
      <description>. feature-state for_k8s_version=&amp;quot;v1.12&amp;rdquo; state=&amp;quot;beta&amp;rdquo; &amp;gt;}}
按照设计，Kubernetes 对 pod 执行相关的很多方面进行了抽象，使得用户不必关心。然 而，为了正常运行，有些工作负载要求在延迟和/或性能方面有更强的保证。 为此，kubelet 提供方法来实现更复杂的负载放置策略，同时保持抽象，避免显式的放置指令。
. heading &amp;ldquo;prerequisites&amp;rdquo; %}} . include &amp;ldquo;task-tutorial-prereqs.md&amp;rdquo; &amp;gt;}} . version-check &amp;gt;}}
CPU 管理策略 默认情况下，kubelet 使用 CFS 配额 来执行 pod 的 CPU 约束。当节点上运行了很多 CPU 密集的 pod 时，工作负载可能会迁移到不同的 CPU 核，这取决于调度时 pod 是否被扼制，以及哪些 CPU 核是可用的。许多工作负载对这种迁移不敏感，因此无需任何干预即可正常工作。
然而，有些工作负载的性能明显地受到 CPU 缓存亲和性以及调度延迟的影响，对此，kubelet 提供了可选的 CPU 管理策略，来确定节点上的一些分配偏好。
配置 CPU 管理器（CPU Manager）作为 alpha 特性引入 Kubernetes 1.8 版本。从 1.10 版本开始，作为 beta 特性默认开启。
CPU 管理策略通过 kubelet 参数 --cpu-manager-policy 来指定。支持两种策略：
 none: 默认策略，表示现有的调度行为。 static: 允许为节点上具有某些资源特征的 pod 赋予增强的 CPU 亲和性和独占性。  CPU 管理器定期通过 CRI 写入资源更新，以保证内存中 CPU 分配与 cgroupfs 一致。同步频率通过新增的 Kubelet 配置参数 --cpu-manager-reconcile-period 来设置。 如果不指定，默认与 --node-status-update-frequency 的周期相同。</description>
    </item>
    
    <item>
      <title>控制节点上的拓扑管理策略</title>
      <link>https://lijun.in/tasks/administer-cluster/topology-manager/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/administer-cluster/topology-manager/</guid>
      <description>. feature-state state=&amp;quot;alpha&amp;rdquo; &amp;gt;}}
越来越多的系统利用 CPU 和硬件加速器的组合来支持对延迟要求较高的任务和高吞吐量的并行计算。 这类负载包括电信、科学计算、机器学习、金融服务和数据分析等。 此类混合系统即用于构造这些高性能环境。
为了获得最佳性能，需要进行与 CPU 隔离、内存和设备局部性有关的优化。 但是，在 Kubernetes 中，这些优化由各自独立的组件集合来处理。
拓扑管理器（Topology Manager） 是一个 Kubelet 的一部分，旨在协调负责这些优化的一组组件。
. heading &amp;ldquo;prerequisites&amp;rdquo; %}} . include &amp;ldquo;task-tutorial-prereqs.md&amp;rdquo; &amp;gt;}} . version-check &amp;gt;}}
拓扑管理器如何工作 在引入拓扑管理器之前， Kubernetes 中的 CPU 和设备管理器相互独立地做出资源分配决策。 这可能会导致在多处理系统上出现并非期望的资源分配；由于这些与期望相左的分配，对性能或延迟敏感的应用将受到影响。 这里的不符合期望意指，例如， CPU 和设备是从不同的 NUMA 节点分配的，因此会导致额外的延迟。
拓扑管理器是一个 Kubelet 组件，扮演信息源的角色，以便其他 Kubelet 组件可以做出与拓扑结构相对应的资源分配决定。
拓扑管理器为组件提供了一个称为 建议供应者（Hint Providers） 的接口，以发送和接收拓扑信息。 拓扑管理器具有一组节点级策略，具体说明如下。
拓扑管理器从 建议提供者 接收拓扑信息，作为表示可用的 NUMA 节点和首选分配指示的位掩码。 拓扑管理器策略对所提供的建议执行一组操作，并根据策略对提示进行约减以得到最优解；如果存储了与预期不符的建议，则该建议的优选字段将被设置为 false。 在当前策略中，首选的是最窄的优选掩码。 所选建议将被存储为拓扑管理器的一部分。 取决于所配置的策略，所选建议可用来决定节点接受或拒绝 Pod 。 之后，建议会被存储在拓扑管理器中，供 建议提供者 进行资源分配决策时使用。
拓扑管理器策略 当前拓扑管理器：</description>
    </item>
    
    <item>
      <title>推荐使用的标签</title>
      <link>https://lijun.in/concepts/overview/working-with-objects/common-labels/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/concepts/overview/working-with-objects/common-labels/</guid>
      <description>除了 kubectl 和 dashboard 之外，您可以使用其他工具来可视化和管理 Kubernetes 对象。一组通用的标签可以让多个工具之间相互操作，用所有工具都能理解的通用方式描述对象。
除了支持工具外，推荐的标签还以一种可以查询的方式描述了应用程序。
元数据围绕 应用（application） 的概念进行组织。Kubernetes 不是 平台即服务（PaaS），没有或强制执行正式的应用程序概念。 相反，应用程序是非正式的，并使用元数据进行描述。应用程序包含的定义是松散的。
这些是推荐的标签。它们使管理应用程序变得更容易但不是任何核心工具所必需的。
共享标签和注解都使用同一个前缀：app.kubernetes.io。没有前缀的标签是用户私有的。共享前缀可以确保共享标签不会干扰用户自定义的标签。
标签 为了充分利用这些标签，应该在每个资源对象上都使用它们。
   键 描述 示例 类型     app.kubernetes.io/name 应用程序的名称 mysql 字符串   app.kubernetes.io/instance 用于唯一确定应用实例的名称 wordpress-abcxzy 字符串   app.kubernetes.io/version 应用程序的当前版本（例如，语义版本，修订版哈希等） 5.7.21 字符串   app.kubernetes.io/component 架构中的组件 database 字符串   app.kubernetes.io/part-of 此级别的更高级别应用程序的名称 wordpress 字符串   app.kubernetes.io/managed-by 用于管理应用程序的工具 helm 字符串    为说明这些标签的实际使用情况，请看下面的 StatefulSet 对象：
apiVersion: apps/v1 kind: StatefulSet metadata: labels: app.</description>
    </item>
    
    <item>
      <title>搭建高可用的 Kubernetes Masters</title>
      <link>https://lijun.in/tasks/administer-cluster/highly-available-master/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/administer-cluster/highly-available-master/</guid>
      <description>. feature-state for_k8s_version=&amp;quot;1.5&amp;rdquo; state=&amp;quot;alpha&amp;rdquo; &amp;gt;}}
您可以在谷歌计算引擎（GCE）的 kubeup 或 kube-down 脚本中复制 Kubernetes Master。 本文描述了如何使用 kube-up/down 脚本来管理高可用（HA）的 Master，以及如何使用 GCE 实现高可用 Master。
. heading &amp;ldquo;prerequisites&amp;rdquo; %}} . include &amp;ldquo;task-tutorial-prereqs.md&amp;rdquo; &amp;gt;}} . version-check &amp;gt;}}
启动一个兼容高可用的集群 要创建一个新的兼容高可用的集群，您必须在 kubeup 脚本中设置以下标志:
  MULTIZONE=true - 为了防止从不同于 Master 默认区域的区域中删除 kubelets 副本。如果您希望在不同的区域运行 Master 副本，那么这一项是必需并且推荐的。
  ENABLE_ETCD_QUORUM_READ=true - 确保从所有 API 服务器读取数据时将返回最新的数据。如果为 true，读操作将被定向到 leader etcd 副本。可以选择将这个值设置为 true，那么读取将更可靠，但也会更慢。
  您还可以指定一个 GCE 区域，在这里创建第一个 Master 副本。设置以下标志:
 KUBE_GCE_ZONE=zone - 将运行第一个 Master 副本的区域。  下面的命令演示在 GCE europe-west1-b 区域中设置一个兼容高可用的集群:</description>
    </item>
    
    <item>
      <title>改变默认 StorageClass</title>
      <link>https://lijun.in/tasks/administer-cluster/change-default-storage-class/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/administer-cluster/change-default-storage-class/</guid>
      <description>本文展示了如何改变默认的 Storage Class，它用于为没有特殊需求的 PersistentVolumeClaims 配置 volumes。
. heading &amp;ldquo;prerequisites&amp;rdquo; %}} . include &amp;ldquo;task-tutorial-prereqs.md&amp;rdquo; &amp;gt;}} . version-check &amp;gt;}}
为什么要改变默认 storage class？ 取决于安装模式，您的 Kubernetes 集群可能和一个被标记为默认的已有 StorageClass 一起部署。这个默认的 StorageClass 以后将被用于动态的为没有特定 storage class 需求的 PersistentVolumeClaims 配置存储。更多细节请查看 PersistentVolumeClaim 文档。
预先安装的默认 StorageClass 可能不能很好的适应您期望的工作负载；例如，它配置的存储可能太过昂贵。如果是这样的话，您可以改变默认 StorageClass，或者完全禁用它以防止动态配置存储。
简单的删除默认 StorageClass 可能行不通，因为它可能会被您集群中的扩展管理器自动重建。请查阅您的安装文档中关于扩展管理器的细节，以及如何禁用单个扩展。
改变默认 StorageClass   列出您集群中的 StorageClasses：
kubectl get storageclass 输出类似这样：
NAME PROVISIONER AGE standard (default) kubernetes.io/gce-pd 1d gold kubernetes.io/gce-pd 1d 默认 StorageClass 以 (default) 标记。
  标记默认 StorageClass 非默认：</description>
    </item>
    
    <item>
      <title>更改 PersistentVolume 的回收策略</title>
      <link>https://lijun.in/tasks/administer-cluster/change-pv-reclaim-policy/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/administer-cluster/change-pv-reclaim-policy/</guid>
      <description>本文展示了如何更改 Kubernetes PersistentVolume 的回收策略。
. heading &amp;ldquo;prerequisites&amp;rdquo; %}} . include &amp;ldquo;task-tutorial-prereqs.md&amp;rdquo; &amp;gt;}} . version-check &amp;gt;}}
为什么要更改 PersistentVolume 的回收策略 PersistentVolumes 可以有多种回收策略，包括 &amp;ldquo;Retain&amp;rdquo;、&amp;ldquo;Recycle&amp;rdquo; 和 &amp;ldquo;Delete&amp;rdquo;。对于动态配置的 PersistentVolumes 来说，默认回收策略为 &amp;ldquo;Delete&amp;rdquo;。这表示当用户删除对应的 PersistentVolumeClaim 时，动态配置的 volume 将被自动删除。如果 volume 包含重要数据时，这种自动行为可能是不合适的。那种情况下，更适合使用 &amp;ldquo;Retain&amp;rdquo; 策略。使用 &amp;ldquo;Retain&amp;rdquo; 时，如果用户删除 PersistentVolumeClaim，对应的 PersistentVolume 不会被删除。相反，它将变为 Released 状态，表示所有的数据可以被手动恢复。
更改 PersistentVolume 的回收策略   列出你集群中的 PersistentVolumes
kubectl get pv  输出类似于这样：
 NAME CAPACITY ACCESSMODES RECLAIMPOLICY STATUS CLAIM REASON AGE pvc-b6efd8da-b7b5-11e6-9d58-0ed433a7dd94 4Gi RWO Delete Bound default/claim1 10s pvc-b95650f8-b7b5-11e6-9d58-0ed433a7dd94 4Gi RWO Delete Bound default/claim2 6s pvc-bb3ca71d-b7b5-11e6-9d58-0ed433a7dd94 4Gi RWO Delete Bound default/claim3 3s  这个列表同样包含了绑定到每个 volume 的 claims 名称，以便更容易的识别动态配置的 volumes。</description>
    </item>
    
    <item>
      <title>特定于节点的卷数限制</title>
      <link>https://lijun.in/concepts/storage/storage-limits/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/concepts/storage/storage-limits/</guid>
      <description>此页面描述了各个云供应商可关联至一个节点的最大卷数。
谷歌、亚马逊和微软等云供应商通常对可以关联到节点的卷数量进行限制。 Kubernetes 需要尊重这些限制。 否则，在节点上调度的 Pod 可能会卡住去等待卷的关联。
Kubernetes 的默认限制 The Kubernetes 调度器对关联于一个节点的卷数有默认限制：
自定义限制 您可以通过设置 KUBE_MAX_PD_VOLS 环境变量的值来设置这些限制，然后再启动调度器。 CSI 驱动程序可能具有不同的过程，关于如何自定义其限制请参阅相关文档。
如果设置的限制高于默认限制，请谨慎使用。请参阅云提供商的文档以确保节点可支持您设置的限制。
此限制应用于整个集群，所以它会影响所有节点。
动态卷限制 feature-state state=&amp;quot;stable&amp;rdquo; for_k8s_version=&amp;quot;v1.17&amp;rdquo; &amp;gt;}}
以下卷类型支持动态卷限制。
 Amazon EBS Google Persistent Disk Azure Disk CSI  对于由内建插件管理的卷，Kubernetes 会自动确定节点类型并确保节点上可关联的卷数目合规。 例如：
  在 Google Compute Engine环境中, 根据节点类型最多可以将127个卷关联到节点。
  对于 M5、C5、R5、T3 和 Z1D 类型实例的 Amazon EBS 磁盘，Kubernetes 仅允许 25 个卷关联到节点。 对于 ec2 上的其他实例类型 Amazon Elastic Compute Cloud (EC2), Kubernetes 允许 39 个卷关联至节点。</description>
    </item>
    
    <item>
      <title>确定 Pod 失败的原因</title>
      <link>https://lijun.in/tasks/debug-application-cluster/determine-reason-pod-failure/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/debug-application-cluster/determine-reason-pod-failure/</guid>
      <description>本文介绍如何编写和读取容器的终止消息。
终止消息为容器提供了一种方法，可以将有关致命事件的信息写入某个位置，在该位置可以通过仪表板和监控软件等工具轻松检索和显示致命事件。 在大多数情况下，您放入终止消息中的信息也应该写入常规 Kubernetes 日志。
. heading &amp;ldquo;prerequisites&amp;rdquo; %}} . include &amp;ldquo;task-tutorial-prereqs.md&amp;rdquo; &amp;gt;}} . version-check &amp;gt;}}
读写终止消息 在本练习中，您将创建运行一个容器的 Pod。 配置文件指定在容器启动时要运行的命令。
. codenew file=&amp;quot;debug/termination.yaml&amp;rdquo; &amp;gt;}}
   kubectl create -f https://k8s.io/examples/debug/termination.yaml  YAML 文件中，在 cmd 和 args 字段，你可以看到容器休眠 10 秒然后将 &amp;ldquo;Sleep expired&amp;rdquo; 写入 /dev/termination-log 文件。 容器写完 &amp;ldquo;Sleep expired&amp;rdquo; 消息后，它就终止了。
   kubectl get pod termination-demo  重复前面的命令直到 Pod 不再运行。
   kubectl get pod --output=yaml   apiVersion: v1 kind: Pod .</description>
    </item>
    
    <item>
      <title>示例任务的模板</title>
      <link>https://lijun.in/tasks/example-task-template/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/example-task-template/</guid>
      <description>. note &amp;gt;}} 还要确保为新文档在目录中创建一个条目。 . /note &amp;gt;}}
这个页面展示了如何&amp;hellip;
. heading &amp;ldquo;prerequisites&amp;rdquo; %}}  . include &amp;ldquo;task-tutorial-prereqs.md&amp;rdquo; &amp;gt;}} . version-check &amp;gt;}} 做这个。 也做这个。  做&amp;hellip;  做这个。 接下来做这个。可能需要阅读一下这个相关解释.  理解 &amp;hellip; [可选部分]
关于你刚才所做的过程，有一点是需要知道的。
. heading &amp;ldquo;whatsnext&amp;rdquo; %}} [可选部分]
 了解更多关于撰写新主题. 查看使用页面模板-任务模板 for how to use this template.  </description>
    </item>
    
    <item>
      <title>示例：使用 Stateful Sets 部署 Cassandra</title>
      <link>https://lijun.in/tutorials/stateful-application/cassandra/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tutorials/stateful-application/cassandra/</guid>
      <description>目录  准备工作 Cassandra docker 镜像 快速入门 步骤1：创建 Cassandra Headless Service 步骤2：使用 StatefulSet 创建 Cassandra Ring 环 步骤3：验证并修改 Cassandra StatefulSet 步骤4：删除 Cassandra StatefulSet 步骤5：使用 Replication Controller 创建 Cassandra 节点 pods 步骤6：Cassandra 集群扩容 步骤7：删除 Replication Controller 步骤8：使用 DaemonSet 替换 Replication Controller 步骤9：资源清理 Seed Provider Source  下文描述了在 Kubernetes 上部署一个_云原生_ Cassandra 的过程。当我们说_云原生_时，指的是一个应用能够理解它运行在一个集群管理器内部，并且使用这个集群的管理基础设施来帮助实现这个应用。特别的，本例使用了一个自定义的 Cassandra SeedProvider 帮助 Cassandra 发现新加入集群 Cassandra 节点。
本示例也使用了Kubernetes的一些核心组件：
 Pods Services Replication Controllers Stateful Sets Daemon Sets  准备工作 本示例假设你已经安装运行了一个 Kubernetes集群（版本 &amp;gt;=1.</description>
    </item>
    
    <item>
      <title>管理 Service Accounts</title>
      <link>https://lijun.in/reference/access-authn-authz/service-accounts-admin/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/access-authn-authz/service-accounts-admin/</guid>
      <description>这是一篇针对service accounts（服务账户）的集群管理员指南。 它呈现了 User Guide to Service Accounts中的信息。
对授权和用户账户的支持已在规划中，当前并不完备，为了更好地描述 service accounts，有时这些不完善的特性也会被提及。
用户账户与服务账户 Kubernetes 区分用户账户和服务账户的概念主要基于以下原因：
 用户账户是针对人而言的。 服务账户是针对运行在 pod 中的进程而言的。 用户账户是全局性的。 其名称在集群各 namespace 中都是全局唯一的，未来的用户资源不会做 namespace 隔离， 服务账户是 namespace 隔离的。 通常情况下，集群的用户账户可能会从企业数据库进行同步，其创建需要特殊权限，并且涉及到复杂的业务流程。 服务账户创建的目的是为了更轻量，允许集群用户为了具体的任务创建服务账户 ( 即权限最小化原则 )。 对人员和服务账户审计所考虑的因素可能不同。 针对复杂系统的配置可能包含系统组件相关的各种服务账户的定义。 因为服务账户可以定制化地创建，并且有 namespace 级别的名称，这种配置是很轻量的。  服务账户的自动化 三个独立组件协作完成服务账户相关的自动化 :
 服务账户准入控制器（Service account admission controller） Token 控制器（Token controller） 服务账户控制器（Service account controller）  服务账户准入控制器 对 pod 的改动通过一个被称为 Admission Controller 的插件来实现。它是 apiserver 的一部分。 当 pod 被创建或更新时，它会同步地修改 pod。 当该插件处于激活状态 ( 在大多数发行版中都是默认的 )，当 pod 被创建或更新时它会进行以下动作：</description>
    </item>
    
    <item>
      <title>管理集群中的 TLS 认证</title>
      <link>https://lijun.in/tasks/tls/managing-tls-in-a-cluster/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/tls/managing-tls-in-a-cluster/</guid>
      <description>Kubernetes提供一个 certificates.k8s.io API，可让您配置 由您控制的证书颁发机构（CA）签名的TLS证书。 您的工作负载可以使用这些CA和证书来建立信任。
certificates.k8s.io API使用的协议类似于ACME 草稿。
. note &amp;gt;}}
使用certificates.k8s.io API创建的证书由 指定 CA 颁发。 将集群配置为使用集群根目录 CA 可以达到这个目的，但是您永远不要依赖它。不要以为 这些证书将针对群根目录 CA 进行验证。 . /note &amp;gt;}}
. heading &amp;ldquo;prerequisites&amp;rdquo; %}} . include &amp;ldquo;task-tutorial-prereqs.md&amp;rdquo; &amp;gt;}} . version-check &amp;gt;}}
集群中的 TLS 信任 让 Pod 中运行的应用程序信任集群根 CA 通常需要一些额外的应用程序配置。您将需要将 CA 证书包添加到 TLS 客户端或服务器信任的 CA 证书列表中。例如，您可以使用 golang TLS 配置通过解析证书链并将解析的证书添加到 tls.Config 结构中的 RootCAs 字段中。
CA 证书捆绑包将使用默认服务账户自动加载到 pod 中，路径为 /var/run/secrets/kubernetes.io/serviceaccount/ca.crt。如果您没有使用默认服务账户，请请求集群管理员构建包含您有权访问使用的证书包的 configmap。
请求认证 以下部分演示如何为通过 DNS 访问的 Kubernetes 服务创建 TLS 证书。</description>
    </item>
    
    <item>
      <title>联邦</title>
      <link>https://lijun.in/concepts/cluster-administration/federation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/concepts/cluster-administration/federation/</guid>
      <description>本页面阐明了为何以及如何使用联邦创建Kubernetes集群。
为何使用联邦 联邦可以使多个集群的管理简单化。它提供了两个主要构件模块：
 跨集群同步资源：联邦能够让资源在多个集群中同步。例如，你可以确保在多个集群中存在同样的部署。 跨集群发现：联邦能够在所有集群的后端自动配置DNS服务和负载均衡。例如，通过多个集群的后端，你可以确保全局的VIP或DNS记录可用。  联邦技术的其他应用场景：
 高可用性：通过跨集群分摊负载，自动配置DNS服务和负载均衡，联邦将集群失败所带来的影响降到最低。 避免供应商锁定：跨集群使迁移应用程序变得更容易，联邦服务避免了供应商锁定。  只有在多个集群的场景下联邦服务才是有帮助的。这里列出了一些你会使用多个集群的原因：
 降低延迟：在多个区域含有集群，可使用离用户最近的集群来服务用户，从而最大限度降低延迟。 故障隔离：对于故障隔离，也许有多个小的集群比有一个大的集群要更好一些（例如：一个云供应商的不同可用域里有多个集群）。详细信息请参阅多集群指南。 可伸缩性：对于单个kubernetes集群是有伸缩性限制的（但对于大多数用户来说并非如此。更多细节参考Kubernetes扩展和性能目标）。 混合云：可以有多个集群，它们分别拥有不同的云供应商或者本地数据中心。  注意事项 虽然联邦有很多吸引人的场景，但这里还是有一些需要关注的事项：
 增加网络的带宽和损耗：联邦控制面会监控所有的集群，来确保集群的当前状态与预期一致。那么当这些集群运行在一个或者多个云提供者的不同区域中，则会带来重大的网络损耗。 降低集群的隔离：当联邦控制面中存在一个故障时，会影响所有的集群。把联邦控制面的逻辑降到最小可以缓解这个问题。 无论何时，它都是kubernetes集群里控制面的代表。设计和实现也使其变得更安全,避免多集群运行中断。 完整性：联邦项目相对较新，还不是很成熟。不是所有资源都可用，且很多资源才刚刚开始。Issue 38893 列举了一些团队正忙于解决的系统已知问题。  混合云的能力 Kubernetes集群里的联邦包括运行在不同云供应商上的集群（例如，谷歌云、亚马逊），和本地部署的集群（例如，OpenStack）。只需在适当的云供应商和/或位置创建所需的所有集群，并将每个集群的API endpoint和凭据注册到您的联邦API服务中（详情参考联邦管理指南）。
在此之后，您的API资源就可以跨越不同的集群和云供应商。
建立联邦 若要能联合多个集群，首先需要建立一个联邦控制面。参照安装指南 建立联邦控制面。
API资源 控制面建立完成后，就可以开始创建联邦API资源了。 以下指南详细介绍了一些资源：
 Cluster ConfigMap DaemonSets Deployment Events Ingress Namespaces ReplicaSets Secrets Services  API参考文档列举了联邦API服务支持的所有资源。
级联删除 Kubernetes1.6版本支持联邦资源级联删除。使用级联删除，即当删除联邦控制面的一个资源时，也删除了所有底层集群中的相应资源。
当使用REST API时，级联删除功能不是默认开启的。若使用REST API从联邦控制面删除一个资源时，要开启级联删除功能，即需配置选项 DeleteOptions.orphanDependents=false。使用kubectl delete使级联删除功能默认开启。使用kubectl delete --cascade=false禁用级联删除功能。
注意：Kubernetes1.5版本开始支持联邦资源子集的级联删除。
单个集群的范围 对于IaaS供应商如谷歌计算引擎或亚马逊网络服务，一个虚拟机存在于一个域或可用域中。 我们建议一个Kubernetes集群里的所有虚机应该在相同的可用域里，因为：
 与单一的全局Kubernetes集群对比，该方式有较少的单点故障。 与跨可用域的集群对比，该方式更容易推断单区域集群的可用性属性。 当Kubernetes开发者设计一个系统(例如，对延迟、带宽或相关故障进行假设)，他们也会假设所有的机器都在一个单一的数据中心，或者以其他方式紧密相连。  每个可用区域里包含多个集群当然是可以的，但是总的来说我们认为集群数越少越好。 偏爱较少集群数的原因是：</description>
    </item>
    
    <item>
      <title>联邦 ConfigMap</title>
      <link>https://lijun.in/tasks/federation/administer-federation/configmap/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/federation/administer-federation/configmap/</guid>
      <description>. deprecationfilewarning &amp;gt;}} . include &amp;ldquo;federation-deprecation-warning-note.md&amp;rdquo; &amp;gt;}} . /deprecationfilewarning &amp;gt;}}
本指南介绍如何在联邦控制平面中使用 ConfigMap。
联邦 ConfigMap 与传统 Kubernetes ConfigMap 非常相似且提供相同的功能。 在联邦控制平面中创建它们可以确保它们在联邦的所有集群中同步。
. heading &amp;ldquo;prerequisites&amp;rdquo; %}}  . include &amp;ldquo;federated-task-tutorial-prereqs.md&amp;rdquo; &amp;gt;}}   通常我们还期望您拥有基本的 Kubernetes 应用知识， 特别是 ConfigMap 相关的应用知识。  创建联邦 ConfigMap 联邦 ConfigMap 的 API 100% 兼容传统 Kubernetes ConfigMap 的 API。您可以通过向联邦 apiserver 发送请求来创建 ConfigMap。 您可以通过使用 kubectl 运行下面的指令来创建联邦 ConfigMap：
kubectl --context=federation-cluster create -f myconfigmap.yaml --context=federation-cluster 参数告诉 kubectl 将请求提交到联邦 apiserver 而不是发送给某一个 Kubernetes 集群。
一旦联邦 ConfigMap 被创建，联邦控制平面就会在所有底层 Kubernetes 集群中创建匹配的 ConfigMap。 您可以通过检查底层每个集群来对其进行验证，例如：</description>
    </item>
    
    <item>
      <title>联邦 DaemonSet</title>
      <link>https://lijun.in/tasks/federation/administer-federation/daemonset/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/federation/administer-federation/daemonset/</guid>
      <description>. deprecationfilewarning &amp;gt;}} . include &amp;ldquo;federation-deprecation-warning-note.md&amp;rdquo; &amp;gt;}} . /deprecationfilewarning &amp;gt;}}
本指南说明了如何在联邦控制平面中使用 DaemonSet。
联邦控制平面中的 DaemonSet（在本指南中称为 “联邦 DaemonSet”）与传统的 Kubernetes DaemonSet 非常类似，并提供相同的功能。在联邦控制平面中创建联邦 DaemonSet 可以确保它们同步到联邦的所有集群中。
. heading &amp;ldquo;prerequisites&amp;rdquo; %}}  . include &amp;ldquo;federated-task-tutorial-prereqs.md&amp;rdquo; &amp;gt;}}   你还应该具备基本的 Kubernetes 应用知识，特别是 DaemonSet 相关的应用知识。  创建联邦 Daemonset 联邦 Daemonset 的 API 和传统的 Kubernetes Daemonset API 是 100% 兼容的。您可以通过向联邦 apiserver 发送请求来创建一个 DaemonSet。
您可以通过使用 kubectl 运行下面的指令来创建联邦 Daemonset：
kubectl --context=federation-cluster create -f mydaemonset.yaml --context=federation-cluster 参数告诉 kubectl 发送请求到联邦 apiserver 而不是某个 Kubernetes 集群。</description>
    </item>
    
    <item>
      <title>联邦 Deployment</title>
      <link>https://lijun.in/tasks/federation/administer-federation/deployment/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/federation/administer-federation/deployment/</guid>
      <description>. deprecationfilewarning &amp;gt;}} . include &amp;ldquo;federation-deprecation-warning-note.md&amp;rdquo; &amp;gt;}} . /deprecationfilewarning &amp;gt;}}
本指南说明了如何在联邦控制平面中使用 Deployment。
联邦控制平面中的 Deployment（在本指南中称为 “联邦 Deployment”）与传统的 Kubernetes Deployment 非常类似，并提供相同的功能。在联邦控制平面中创建联邦 Deployment 确保所需的副本数存在于注册的群集中。
. feature-state for_k8s_version=&amp;quot;1.5&amp;rdquo; state=&amp;quot;alpha&amp;rdquo; &amp;gt;}}
一些特性（例如完整的 rollout 兼容性）仍在开发中。
. heading &amp;ldquo;prerequisites&amp;rdquo; %}}  . include &amp;ldquo;federated-task-tutorial-prereqs.md&amp;rdquo; &amp;gt;}}   您还应当拥有基本的 Kubernetes 应用知识，特别是在 Deployments 方面。  创建联邦 Deployment 联邦 Deployment 的 API 和传统的 Kubernetes Deployment API 是兼容的。 您可以通过向联邦 apiserver 发送请求来创建一个 Deployment。
您可以通过使用 kubectl 运行下面的指令：
kubectl --context=federation-cluster create -f mydeployment.yaml --context=federation-cluster 参数告诉 kubectl 发送请求到联邦 apiserver 而不是某个 Kubernetes 集群。</description>
    </item>
    
    <item>
      <title>联邦 Job</title>
      <link>https://lijun.in/tasks/federation/administer-federation/job/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/federation/administer-federation/job/</guid>
      <description>. deprecationfilewarning &amp;gt;}} . include &amp;ldquo;federation-deprecation-warning-note.md&amp;rdquo; &amp;gt;}} . /deprecationfilewarning &amp;gt;}}
本指南解释了如何在联邦控制平面中使用 job。
联邦控制平面中的一次性任务（在本指南中称为“联邦一次性任务”）类似于传统的 Kubernetes 一次性任务，并且提供相同的功能。 在联邦控制平面中创建 job 可以确保在已注册的集群中存在所需的并行性和完成数。
. heading &amp;ldquo;prerequisites&amp;rdquo; %}}  . include &amp;ldquo;federated-task-tutorial-prereqs.md&amp;rdquo; &amp;gt;}} 你需要具备基本的 Kubernetes 的工作知识，特别是 job。  创建一个联邦 job 用于联邦 job 的 API 与用于传统 Kubernetes job 的 API 完全兼容。您可以通过向联邦 apiserver 发送请求来创建 job。
你可以使用 kubectl 来运行：
kubectl --context=federation-cluster create -f myjob.yaml --context=federation-cluster 参数告诉 kubectl 将请求提交到联邦 API 服务器，而不是发送到 Kubernetes 集群。
一旦创建了联邦 job，联邦控制平面将在所有底层 Kubernetes 集群中创建一个 job。 你可以通过检查每个集群底层来验证这一点，例如：
kubectl --context=gce-asia-east1a get job myjob 前面的示例假设你的客户端中为该区域中的集群配置了一个名为 gce-asia-east1a 的上下文。</description>
    </item>
    
    <item>
      <title>联邦 ReplicaSet</title>
      <link>https://lijun.in/tasks/federation/administer-federation/replicaset/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/federation/administer-federation/replicaset/</guid>
      <description>. deprecationfilewarning &amp;gt;}} . include &amp;ldquo;federation-deprecation-warning-note.md&amp;rdquo; &amp;gt;}} . /deprecationfilewarning &amp;gt;}}
本指南阐述了如何在联邦控制平面中使用 ReplicaSet。 在联邦控制平面中的 ReplicaSet (在本指南中称为”联邦 ReplicaSet”) 和传统的 Kubernetes ReplicaSet 很相似，提供了一样的功能。在联邦控制平面中创建联邦 ReplicaSet 可以确保在联邦的所有集群中都有预期数量的副本。
. heading &amp;ldquo;prerequisites&amp;rdquo; %}}  . include &amp;ldquo;federated-task-tutorial-prereqs.md&amp;rdquo; &amp;gt;}}   你还应该具备基本的 Kubernetes 应用知识，特别是 ReplicaSets 相关的应用知识。  创建联邦 ReplicaSet 联邦 ReplicaSet 的 API 和传统的 Kubernetes ReplicaSet API 是 100% 兼容的。您可以通过请求联邦 apiserver 来创建联邦 ReplicaSet。
您可以通过使用 kubectl 运行下面的指令来创建联邦 ReplicaSet：
kubectl --context=federation-cluster create -f myrs.yaml --context=federation-cluster 参数告诉 kubectl 发送请求到联邦 apiserver 而不是某个 Kubernetes 集群。</description>
    </item>
    
    <item>
      <title>联邦 Secret</title>
      <link>https://lijun.in/tasks/federation/administer-federation/secret/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/federation/administer-federation/secret/</guid>
      <description>. deprecationfilewarning &amp;gt;}} . include &amp;ldquo;federation-deprecation-warning-note.md&amp;rdquo; &amp;gt;}} . /deprecationfilewarning &amp;gt;}}
本指南解释了如何在联邦控制平面中使用 secret。
联邦控制平面中的 Secret（在本指南中称为“联邦 secret”）与提供相同功能的传统 Kubernetes Secret 非常相似。 在联邦控制平面中创建它们可以确保它们跨联邦中的所有集群同步。
先决条件 本指南假设你有一个正在运行的 Kubernetes 集群联邦安装。 如果没有，请访问联邦管理指南，了解如何启动联邦集群（或者让集群管理员为你做这件事）。 其他教程，例如这里 Kelsey Hightower，也可以帮助您。
你还应该具有一个基本的 Kubernetes 工作知识， 特别是 Secret。
创建联邦 Secret 用于联邦 Secret 的 API 与用于传统的 Kubernetes Secret 的 API 100% 兼容。 您可以通过向联邦 apiserver 发送请求来创建一个 Secret。
你可以使用 kubectl 来运行：
kubectl --context=federation-cluster create -f mysecret.yaml --context=federation-cluster 参数通知 kubectl 将请求提交给联邦 apiserver，而不是将其发送到 Kubernetes 集群。
创建联邦命名空间后，联邦控制平面将在所有基础 Kubernetes 集群中创建匹配的命名空间。您可以通过检查每个基础集群来验证这一点，例如：
kubectl --context=gce-asia-east1a get secret mysecret 以上假设您在客户端中为该区域中的集群配置了名为 “gce-asia-east1a” 的上下文。 集群底层中的这些 secret 将与联邦 secret 匹配。</description>
    </item>
    
    <item>
      <title>联邦事件</title>
      <link>https://lijun.in/tasks/federation/administer-federation/events/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/federation/administer-federation/events/</guid>
      <description>. deprecationfilewarning &amp;gt;}} . include &amp;ldquo;federation-deprecation-warning-note.md&amp;rdquo; &amp;gt;}} . /deprecationfilewarning &amp;gt;}}
本指南介绍如何在联邦控制平面中使用事件来帮助调试。
先决条件 本指南假定您正在运行 Kubernetes 集群联邦安装。 如果没有，请转到联邦管理员指南，了解如何启动集群联邦（或让集群管理员为您执行此操作）。 其他教程，例如这个由 Kelsey Hightower，也可为您提供帮助。
你还应该具备 kubernetes 基本工作知识。
查看联邦事件 联邦控制平面中的事件（本指南中称为“联邦事件”）与提供相同功能的传统 Kubernetes 事件非常相似。 联邦事件仅存储在联邦控制平面中，不会传递给基础 Kubernetes 集群。
联邦控制器在处理 API 资源时创建事件，以便向用户显示它们所处的状态。您可以通过运行以下命令从联邦 apiserver 获取所有事件：
kubectl --context=federation-cluster get events 标准的 kubectl get，update，delete 命令都可以正常工作。</description>
    </item>
    
    <item>
      <title>联邦命名空间</title>
      <link>https://lijun.in/tasks/federation/administer-federation/namespaces/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/federation/administer-federation/namespaces/</guid>
      <description>. deprecationfilewarning &amp;gt;}} . include &amp;ldquo;federation-deprecation-warning-note.md&amp;rdquo; &amp;gt;}} . /deprecationfilewarning &amp;gt;}}
本指南介绍如何在联邦控制平面中使用命名空间。
联邦控制平面中的命名空间（本指南中称为“联邦命名空间”）与提供相同功能的传统 Kubernetes 命名空间非常相似。 在联邦控制平面中创建它们可确保它们在联邦中的所有集群之间同步
. heading &amp;ldquo;prerequisites&amp;rdquo; %}}  . include &amp;ldquo;federated-task-tutorial-prereqs.md&amp;rdquo; &amp;gt;}} 您还需要具备基本的 Kubernetes 工作知识， 特别是命名空间。  创建联邦命名空间 联邦命名空间的 API 与传统 Kubernetes 命名空间的 API 100％ 兼容。您可以通过向联邦身份验证程序发送请求来创建命名空间。
您可以通过运行以下命令使用 kubectl 执行此操作：
kubectl --context=federation-cluster create -f myns.yaml --context=federation-cluster 参数通知 kubectl 将请求提交给联邦 apiserver，而不是将其发送到 Kubernetes 集群。
创建联邦命名空间后，联邦控制平面将在所有基础 Kubernetes 集群中创建匹配的命名空间。您可以通过检查每个基础集群来验证这一点，例如：
kubectl --context=gce-asia-east1a get namespaces myns 以上假设您在客户端中为该区域中的集群配置了名为 “gce-asia-east1a” 的上下文。 基础命名空间的名称和规范将与您在上面创建的联邦命名空间的名称和规范相匹配。
更新联邦命名空间 您可以像更新 Kubernetes 命名空间一样更新联邦命名空间，只需将请求发送到联邦身份验证程序，而不是将其发送到指定的 Kubernetes 集群。 联邦控制平面将确保每当更新联邦命名空间时，它都会更新所有基础集群中的相应命名空间以与其匹配。</description>
    </item>
    
    <item>
      <title>自定义 DNS 服务</title>
      <link>https://lijun.in/tasks/administer-cluster/dns-custom-nameservers/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/administer-cluster/dns-custom-nameservers/</guid>
      <description>本页说明如何配置 DNS Pod 和自定义 DNS 解析过程。 在 Kubernetes 1.11 和更高版本中，CoreDNS 位于 GA 并且默认情况下与 kubeadm 一起安装。 请参见CoreDNS 的 ConfigMap 选项 and 使用 CoreDNS 进行服务发现。
. heading &amp;ldquo;prerequisites&amp;rdquo; %}}  . include &amp;ldquo;task-tutorial-prereqs.md&amp;rdquo; &amp;gt;}} . version-check &amp;gt;}} Kubernetes 版本 1.6 或更新。如果与 CoreDNS 匹配，版本 1.9 或更新。 合适的 add-on 插件: kube-dns 或 CoreDNS. 使用 kubeadm 安装，请参见 kubeadm 帮助文档.  介绍 DNS 是使用插件管理器[集群 add-on](http://releases.k8s.io/. param &amp;ldquo;githubbranch&amp;rdquo; &amp;gt;}}/cluster/addons/README.md)自动启动的内置的 Kubernetes 服务。
从 Kubernetes v1.12 开始，CoreDNS 是推荐的 DNS 服务器，取代了kube-dns。 但是，默认情况下，某些 Kubernetes 安装程序工具仍可能安装 kube-dns。 请参阅安装程序提供的文档，以了解默认情况下安装了哪个 DNS 服务器。</description>
    </item>
    
    <item>
      <title>节点健康监测</title>
      <link>https://lijun.in/tasks/debug-application-cluster/monitor-node-health/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/debug-application-cluster/monitor-node-health/</guid>
      <description>节点问题探测器 是一个 DaemonSet 用来监控节点健康。它从各种守护进程收集节点问题，并以NodeCondition 和 [Event](/docs/reference/generated/kubernetes-api/. param &amp;ldquo;version&amp;rdquo; &amp;gt;}}/#event-v1-core) 的形式报告给 apiserver 。
它现在支持一些已知的内核问题检测，并将随着时间的推移，检测更多节点问题。
目前，Kubernetes 不会对节点问题检测器监测到的节点状态和事件采取任何操作。将来可能会引入一个补救系统来处理这些节点问题。
更多信息请参阅 这里。
. heading &amp;ldquo;prerequisites&amp;rdquo; %}} . include &amp;ldquo;task-tutorial-prereqs.md&amp;rdquo; &amp;gt;}} . version-check &amp;gt;}}
局限性  节点问题检测器的内核问题检测现在只支持基于文件类型的内核日志。 它不支持像 journald 这样的命令行日志工具。   节点问题检测器的内核问题检测对内核日志格式有一定要求，现在它只适用于 Ubuntu 和 Debian。但是，将其扩展为 支持其它日志格式 也很容易。  在 GCE 集群中启用/禁用 节点问题检测器在 gce 集群中以集群插件的形式默认启用。
您可以在运行 kube-up.sh 之前，以设置环境变量 KUBE_ENABLE_NODE_PROBLEM_DETECTOR 的形式启用/禁用它。
在其它环境中使用 要在 GCE 之外的其他环境中启用节点问题检测器，您可以使用 kubectl 或插件 pod。
Kubectl 这是在 GCE 之外启动节点问题检测器的推荐方法。它的管理更加灵活，例如覆盖默认配置以使其适合您的环境或检测自定义节点问题。
 步骤 1: node-problem-detector.yaml:  .</description>
    </item>
    
    <item>
      <title>获取正在运行容器的 Shell</title>
      <link>https://lijun.in/tasks/debug-application-cluster/get-shell-running-container/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/debug-application-cluster/get-shell-running-container/</guid>
      <description>本文介绍怎样使用 kubectl exec 命令获取正在运行容器的 Shell。
.% heading &amp;ldquo;prerequisites&amp;rdquo; %}} . include &amp;ldquo;task-tutorial-prereqs.md&amp;rdquo; &amp;gt;}} . version-check &amp;gt;}}
获取容器的 Shell 在本练习中，你将创建包含一个容器的 Pod。容器运行 nginx 镜像。下面是 Pod 的配置文件：
. codenew file=&amp;quot;application/shell-demo.yaml&amp;rdquo; &amp;gt;}}
创建 Pod：
kubectl create -f https://k8s.io/examples/application/shell-demo.yaml 检查容器是否运行正常：
kubectl get pod shell-demo 获取正在运行容器的 Shell：
kubectl exec -it shell-demo -- /bin/bash . note &amp;gt;}}
双破折号 &amp;ldquo;&amp;ndash;&amp;rdquo; 用于将要传递给命令的参数与 kubectl 的参数分开。 note &amp;gt;}}
在 shell 中，打印根目录：
root@shell-demo:/# ls / 在 shell 中，实验其他命令。下面是一些示例：
root@shell-demo:/# ls / root@shell-demo:/# cat /proc/mounts root@shell-demo:/# cat /proc/1/maps root@shell-demo:/# apt-get update root@shell-demo:/# apt-get install -y tcpdump root@shell-demo:/# tcpdump root@shell-demo:/# apt-get install -y lsof root@shell-demo:/# lsof root@shell-demo:/# apt-get install -y procps root@shell-demo:/# ps aux root@shell-demo:/# ps aux | grep nginx 编写 nginx 的 根页面 在看一下 Pod 的配置文件。该 Pod 有个 emptyDir 卷，容器将该卷挂载到了 /usr/share/nginx/html。</description>
    </item>
    
    <item>
      <title>访问集群上运行的服务</title>
      <link>https://lijun.in/tasks/administer-cluster/access-cluster-services/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/administer-cluster/access-cluster-services/</guid>
      <description>本文展示了如何连接 Kubernetes 集群上运行的服务。
. heading &amp;ldquo;prerequisites&amp;rdquo; %}} . include &amp;ldquo;task-tutorial-prereqs.md&amp;rdquo; &amp;gt;}} . version-check &amp;gt;}}
访问集群上运行的服务 在 Kubernetes 里， nodes、pods 和 services 都有它们自己的 IP。许多情况下，集群上的 node IP、pod IP 和某些 service IP 路由不可达，所以不能从一个集群之外的节点访问它们，例如从你自己的台式机。
连接方式 你有多种从集群外连接 nodes、pods 和 services 的选项：
 通过公共 IP 访问 services。  使用具有 NodePort 或 LoadBalancer 类型的 service，可以从外部访问它们。请查阅 services 和 kubectl expose 文档。 取决于你的集群环境，你可以仅把 service 暴露在你的企业网络环境中，也可以将其暴露在因特网上。需要考虑暴露的 service 是否安全，它是否有自己的用户认证？ 将 pods 放置于 services 背后。如果要访问一个副本集合中特定的 pod，例如用于调试目的时，请给 pod 指定一个独特的标签并创建一个新 service 选择这个标签。 大部分情况下，都不需要应用开发者通过节点 IP 直接访问 nodes。   通过 Proxy Verb 访问 services、nodes 或者 pods。  在访问 Apiserver 远程服务之前是否经过认证和授权？如果你的服务暴露到因特网中不够安全，或者需要获取 node IP 之上的端口，又或者处于调试目的时，请使用这个特性。 Proxies 可能给某些应用带来麻烦。 仅适用于 HTTP/HTTPS。 在这里描述   从集群中的 node 或者 pod 访问。  运行一个 pod，然后使用 kubectl exec 连接到它的一个shell。从那个 shell 连接其他的 nodes、pods 和 services。 某些集群可能允许你 ssh 到集群中的节点。你可能可以从那儿访问集群服务。这是一个非标准的方式，可能在一些集群上能工作，但在另一些上却不能。浏览器和其他工具可能安装或可能不会安装。集群 DNS 可能不会正常工作。    发现内置服务 典型情况下，kube-system 会启动集群中的几个服务。使用 kubectl cluster-info 命令获取它们的列表：</description>
    </item>
    
    <item>
      <title>证书轮换</title>
      <link>https://lijun.in/tasks/tls/certificate-rotation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/tls/certificate-rotation/</guid>
      <description>本文展示如何在 kubelet 中启用并配置证书轮换。
. heading &amp;ldquo;prerequisites&amp;rdquo; %}}   要求 Kubernetes 1.8.0 或更高的版本
  Kubelet 证书轮换在 1.8.0 版本中处于 beta 阶段, 这意味着该特性可能在没有通知的情况下发生变化。
  概述 Kubelet 使用证书进行 Kubernetes API 的认证。 默认情况下，这些证书的签发期限为一年，所以不需要太频繁地进行更新。
Kubernetes 1.8 版本中包含 beta 特性 kubelet 证书轮换， 在当前证书即将过期时， 将自动生成新的秘钥，并从 Kubernetes API 申请新的证书。 一旦新的证书可用，它将被用于与 Kubernetes API 间的连接认证。
启用客户端证书轮换 kubelet 进程接收 --rotate-certificates 参数，该参数决定 kubelet 在当前使用的证书即将到期时， 是否会自动申请新的证书。 由于证书轮换是 beta 特性，必须通过参数 --feature-gates=RotateKubeletClientCertificate=true 进行启用。
kube-controller-manager 进程接收 --experimental-cluster-signing-duration 参数，该参数控制证书签发的有效期限。
理解证书轮换配置 当 kubelet 启动时，如被配置为自举（使用--bootstrap-kubeconfig 参数），kubelet 会使用其初始证书连接到 Kubernetes API ，并发送证书签名的请求。 可以通过以下方式查看证书签名请求的状态：</description>
    </item>
    
    <item>
      <title>调试 Init 容器</title>
      <link>https://lijun.in/tasks/debug-application-cluster/debug-init-containers/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/debug-application-cluster/debug-init-containers/</guid>
      <description>此页显示如何核查与 init 容器执行相关的问题。 下面的示例命令行将 Pod 称为 &amp;lt;pod-name&amp;gt;，而 init 容器称为 &amp;lt;init-container-1&amp;gt; 和 &amp;lt;init-container-2&amp;gt;。
. heading &amp;ldquo;prerequisites&amp;rdquo; %}} . include &amp;ldquo;task-tutorial-prereqs.md&amp;rdquo; &amp;gt;}} . version-check &amp;gt;}}
 您应该熟悉 Init 容器的基础知识。 您应该已经配置好一个 Init 容器。  检查 Init 容器的状态 显示你的 Pod 的状态：
kubectl get pod &amp;lt;pod-name&amp;gt; 例如，状态 Init:1/2 表明两个 Init 容器中的一个已经成功完成：
NAME READY STATUS RESTARTS AGE &amp;lt;pod-name&amp;gt; 0/1 Init:1/2 0 7s 更多状态值及其含义请参考了解 Pod 的状态。
获取 Init 容器详情 查看 Init 容器运行的更多详情：
kubectl describe pod &amp;lt;pod-name&amp;gt; 例如，对于包含两个 Init 容器的 Pod 应该显示如下信息：</description>
    </item>
    
    <item>
      <title>调试 Pods 和 Replication Controllers</title>
      <link>https://lijun.in/tasks/debug-application-cluster/debug-pod-replication-controller/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/debug-application-cluster/debug-pod-replication-controller/</guid>
      <description>此页面告诉您如何调试 Pod 和 ReplicationController。
. heading &amp;ldquo;prerequisites&amp;rdquo; %}} . include &amp;ldquo;task-tutorial-prereqs.md&amp;rdquo; &amp;gt;}} . version-check &amp;gt;}}
 您应该先熟悉 Pods 和 Pod Lifecycle 的基础概念。  调试 Pod 调试一个 pod 的第一步是观察它。使用下面的命令检查这个 pod 的当前状态和最近事件：
kubectl describe pods ${POD_NAME} 看看 pod 中的容器的状态。他们都是 Running 吗？有最近重启了吗？
根据 pod 的状态继续调试。
我的 Pod 卡在 Pending 如果一个 pod 被卡在 Pending 状态，就意味着它不能调度在某个节点上。一般来说，这是因为某种类型的资源不足而 阻止调度。 看看上面的命令 kubectl describe ... 的输出。调度器的消息中应该会包含无法调度 Pod 的原因。 原因包括：
资源不足 您可能已经耗尽了集群中供应的 CPU 或内存。在这个情况下你可以尝试几件事情：
  添加更多节点 到集群。
  终止不需要的 pod 为 pending 中的 pod 提供空间。</description>
    </item>
    
    <item>
      <title>调试 Service</title>
      <link>https://lijun.in/tasks/debug-application-cluster/debug-service/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/debug-application-cluster/debug-service/</guid>
      <description>对于新安装的 Kubernetes，经常出现的一个问题是 Service 没有正常工作。如果您已经运行了 Deployment 并创建了一个 Service，但是当您尝试访问它时没有得到响应，希望这份文档能帮助您找出问题所在。
约定 在整个文档中，您将看到各种可以运行的命令。有些命令需要在 Pod 中运行，有些命令需要在 Kubernetes Node 上运行，还有一些命令可以在您拥有 kubectl 和集群凭证的任何地方运行。为了明确预期的效果，本文档将使用以下约定。
如果命令 &amp;ldquo;COMMAND&amp;rdquo; 期望在 Pod 中运行，并且产生 &amp;ldquo;OUTPUT&amp;rdquo;：
u@pod$ COMMAND OUTPUT 如果命令 &amp;ldquo;COMMAND&amp;rdquo; 期望在 Node 上运行，并且产生 &amp;ldquo;OUTPUT&amp;rdquo;：
u@node$ COMMAND OUTPUT 如果命令是 &amp;ldquo;kubectl ARGS&amp;rdquo;：
$ kubectl ARGS OUTPUT 在 pod 中运行命令 对于这里的许多步骤，您可能希望知道运行在集群中的 Pod 看起来是什么样的。最简单的方法是运行一个交互式的 busybox Pod：
$ kubectl run -it --rm --restart=Never busybox --image=busybox sh 如果你没有看到命令提示符，请尝试按 Enter 键。 / # 如果您已经有了您喜欢使用的正在运行的 Pod，则可以运行一下命令去使用：
$ kubectl exec &amp;lt;POD-NAME&amp;gt; -c &amp;lt;CONTAINER-NAME&amp;gt; -- &amp;lt;COMMAND&amp;gt; 设置 为了完成本次演练的目的，我们先运行几个 Pod。因为可能正在调试您自己的 Service，所以，您可以使用自己的详细信息进行替换，或者，您也可以跟随并开始下面的步骤来获得第二个数据点。</description>
    </item>
    
    <item>
      <title>调试StatefulSet</title>
      <link>https://lijun.in/tasks/debug-application-cluster/debug-stateful-set/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/debug-application-cluster/debug-stateful-set/</guid>
      <description>此任务展示如何调试StatefulSet。
. heading &amp;ldquo;prerequisites&amp;rdquo; %}}  你需要有一个Kubernetes集群，通过必要的配置使kubectl命令行工具与您的集群进行通信。 你应该有一个运行中的StatefulSet，以便用于调试。  调试StatefulSet 由于StatefulSet在创建时设置了app=myapp标签，列出仅属于该StatefulSet的所有pod时，可以使用以下命令：
kubectl get pods -l app=myapp 如果您发现列出的任何Pods长时间处于Unknown 或Terminating状态，关于如何处理它们的说明任务,请参阅删除 StatefulSet Pods。您可以参考调试 Pods指南来调试StatefulSet中的各个Pod。
StatefulSets提供调试机制，可以使用注解来暂停所有控制器在Pod上的操作。在任何StatefulSet Pod上设置pod.alpha.kubernetes.io/initialized注解为&amp;quot;false&amp;quot;将暂停 StatefulSet的所有操作。暂停时，StatefulSet将不执行任何伸缩操作。一旦调试钩子设置完成后，就可以在StatefulSet pod的容器内执行命令，而不会造成伸缩操作的干扰。您可以通过执行以下命令将注解设置为&amp;quot;false&amp;quot;：
kubectl annotate pods &amp;lt;pod-name&amp;gt; pod.alpha.kubernetes.io/initialized=&amp;#34;false&amp;#34; --overwrite 当注解设置为&amp;quot;false&amp;quot;时，StatefulSet在其Pods变得不健康或不可用时将不会响应。StatefulSet不会创建副本Pod直到每个Pod上删除注解或将注解设置为&amp;quot;true&amp;quot;。
逐步初始化 创建StatefulSet之前，您可以通过使用和上文相同的注解，即将yaml文件中.spec.template.metadata.annotations里的pod.alpha.kubernetes.io/initialized字段设置为&amp;quot;false&amp;quot;，对竞态条件的StatefulSet进行调试。
apiVersion: apps/v1beta1 kind: StatefulSet metadata: name: my-app spec: serviceName: &amp;#34;my-app&amp;#34; replicas: 3 template: metadata: labels: app: my-app annotations: pod.alpha.kubernetes.io/initialized: &amp;#34;false&amp;#34; ... ... ... 设置注解后，如果创建了StatefulSet，您可以等待每个Pod来验证它是否正确初始化。StatefulSet将不会创建任何后续的Pods，直到在已经创建的每个Pod上将调试注解设置为&amp;quot;true&amp;quot; (或删除)。 您可以通过执行以下命令将注解设置为&amp;quot;true&amp;quot;：
kubectl annotate pods &amp;lt;pod-name&amp;gt; pod.alpha.kubernetes.io/initialized=&amp;#34;true&amp;#34; --overwrite . heading &amp;ldquo;whatsnext&amp;rdquo; %}} 点击链接调试init-container，了解更多信息。</description>
    </item>
    
    <item>
      <title>资源指标管道</title>
      <link>https://lijun.in/tasks/debug-application-cluster/resource-metrics-pipeline/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/debug-application-cluster/resource-metrics-pipeline/</guid>
      <description>从 Kubernetes 1.8开始，资源使用指标，例如容器 CPU 和内存使用率，可通过 Metrics API 在 Kubernetes 中获得。这些指标可以直接被用户访问，比如使用kubectl top命令行，或者这些指标由集群中的控制器使用，例如，Horizontal Pod Autoscaler，使用这些指标来做决策。
Metrics API 通过 Metrics API，您可以获得指定节点或 pod 当前使用的资源量。此 API 不存储指标值，因此想要获取某个指定节点10分钟前的资源使用量是不可能的。
此 API 与其他 API 没有区别：
 此 API 和其它 Kubernetes API 一起位于同一端点（endpoint）之下，是可发现的，路径为/apis/metrics.k8s.io/ 它提供相同的安全性、可扩展性和可靠性保证  Metrics API 在k8s.io/metrics 仓库中定义。您可以在那里找到有关 Metrics API 的更多信息。
. note &amp;gt;}} Metrics API 需要在集群中部署 Metrics Server。否则它将不可用。 . /note &amp;gt;}}
Metrics Server Metrics Server是集群范围资源使用数据的聚合器。 从 Kubernetes 1.8开始，它作为 Deployment 对象，被默认部署在由kube-up.sh脚本创建的集群中。 如果您使用不同的 Kubernetes 安装方法，则可以使用提供的deployment yamls来部署。它在 Kubernetes 1.7+中得到支持（详见下文）。
Metric server 从每个节点上的 Kubelet 公开的 Summary API 中采集指标信息。</description>
    </item>
    
    <item>
      <title>资源监控工具</title>
      <link>https://lijun.in/tasks/debug-application-cluster/resource-usage-monitoring/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/debug-application-cluster/resource-usage-monitoring/</guid>
      <description>要扩展应用程序并提供可靠的服务，您需要了解应用程序在部署时的行为。 您可以通过检测容器检查 Kubernetes 集群中的应用程序性能，pods, 服务和整个集群的特征。 Kubernetes 在每个级别上提供有关应用程序资源使用情况的详细信息。 此信息使您可以评估应用程序的性能，以及在何处可以消除瓶颈以提高整体性能。
在 Kubernetes 中，应用程序监控不依赖单个监控解决方案。 在新集群上，您可以使用资源度量或完整度量管道来收集监视统计信息。
资源度量管道 资源指标管道提供了一组与集群组件，例如[Horizontal Pod Autoscaler]控制器(/docs/tasks/run-application/horizontal-pod-autoscale)，以及 kubectl top 实用程序相关的有限度量。 这些指标是由轻量级的、短期内存度量服务器收集的，通过 metrics.k8s.io 公开。
度量服务器发现集群中的所有节点，并且查询每个节点的kubelet以获取 CPU 和内存使用情况。 Kubelet 充当 Kubernetes 主节点与节点之间的桥梁，管理机器上运行的 Pod 和容器。 kubelet 将每个 pod 转换为其组成的容器，并在容器运行时通过容器运行时界面获取各个容器使用情况统计信息。 kubelet 从集成的 cAdvisor 获取此信息，以进行旧式 Docker 集成。 然后，它通过 metrics-server Resource Metrics API 公开聚合的 pod 资源使用情况统计信息。 该 API 在 kubelet 的经过身份验证和只读的端口上的/metrics/resource/v1beta1中提供。
完整度量管道 一个完整度量管道可以让您访问更丰富的度量。 Kubernetes 还可以根据集群的当前状态，使用 Pod 水平自动扩缩器等机制，通过自动调用扩展或调整集群来响应这些度量。 监控管道从 kubelet 获取度量，然后通过适配器将它们公开给 Kubernetes，方法是实现 custom.metrics.k8s.io 或 external.metrics.k8s.io API。
Prometheus，一个 CNCF 项目，可以原生监控 Kubernetes、节点和 Prometheus 本身。 完整度量管道项目不属于 CNCF 的一部分，不在 Kubernetes 文档的范围之内。</description>
    </item>
    
    <item>
      <title>运行 ZooKeeper， 一个 CP 分布式系统</title>
      <link>https://lijun.in/tutorials/stateful-application/zookeeper/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tutorials/stateful-application/zookeeper/</guid>
      <description>本教程展示了在 Kubernetes 上使用 PodDisruptionBudgets 和 PodAntiAffinity 特性运行 Apache Zookeeper。
. heading &amp;ldquo;prerequisites&amp;rdquo; %}} 在开始本教程前，你应该熟悉以下 Kubernetes 概念。
 Pods Cluster DNS Headless Services PersistentVolumes [PersistentVolume Provisioning](http://releases.k8s.io/. param &amp;ldquo;githubbranch&amp;rdquo; &amp;gt;}}/examples/persistent-volume-provisioning/) ConfigMaps StatefulSets PodDisruptionBudgets PodAntiAffinity kubectl CLI  你需要一个至少包含四个节点的集群，每个节点至少 2 CPUs 和 4 GiB 内存。在本教程中你将会 cordon 和 drain 集群的节点。这意味着集群节点上所有的 Pods 将会被终止并移除。这些节点也会暂时变为不可调度。在本教程中你应该使用一个独占的集群，或者保证你造成的干扰不会影响其它租户。
本教程假设你的集群配置为动态的提供 PersistentVolumes。如果你的集群没有配置成这样，在开始本教程前，你需要手动准备三个 20 GiB 的卷。
. heading &amp;ldquo;objectives&amp;rdquo; %}} 在学习本教程后，你将熟悉下列内容。
 如何使用 StatefulSet 部署一个 ZooKeeper ensemble。 如何使用 ConfigMaps 一致性配置 ensemble。 如何在 ensemble 中 分布 ZooKeeper 服务的部署。 如何在计划维护中使用 PodDisruptionBudgets 确保服务可用性。  ZooKeeper 基础 Apache ZooKeeper 是一个分布式的开源协调服务，用于分布式系统。ZooKeeper 允许你读取、写入数据和发现数据更新。数据按层次结构组织在文件系统中，并复制到 ensemble（一个 ZooKeeper 服务的集合） 中所有的 ZooKeeper 服务。对数据的所有操作都是原子的和顺序一致的。ZooKeeper 通过 Zab 一致性协议在 ensemble 的所有服务之间复制一个状态机来确保这个特性。</description>
    </item>
    
    <item>
      <title>运行一个单实例有状态应用</title>
      <link>https://lijun.in/tasks/run-application/run-single-instance-stateful-application/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/run-application/run-single-instance-stateful-application/</guid>
      <description>本文介绍在 Kubernetes 中使用 PersistentVolume 和 Deployment 如何运行一个单实例有状态应用. 该应用是 MySQL.
. heading &amp;ldquo;objectives&amp;rdquo; %}}  在环境中通过磁盘创建一个PersistentVolume. 创建一个MySQL Deployment. 在集群内以一个已知的 DNS 名将 MySQL 暴露给其他 pods.  . heading &amp;ldquo;prerequisites&amp;rdquo; %}}   . include &amp;ldquo;task-tutorial-prereqs.md&amp;rdquo; &amp;gt;}} . version-check &amp;gt;}}
  . include &amp;ldquo;default-storage-class-prereqs.md&amp;rdquo; &amp;gt;}}
  部署MySQL 注意: 在配置的 yaml 文件中定义密码的做法是不安全的. 具体安全解决方案请参考 Kubernetes Secrets.
. codenew file=&amp;quot;application/mysql/mysql-deployment.yaml&amp;rdquo; &amp;gt;}} . codenew file=&amp;quot;application/mysql/mysql-pv.yaml&amp;rdquo; &amp;gt;}}
  部署 YAML 文件中定义的 PV 和 PVC：
 kubectl apply -f https://k8s.</description>
    </item>
    
    <item>
      <title>适用于 Docker 用户的 kubectl</title>
      <link>https://lijun.in/reference/kubectl/docker-cli-to-kubectl/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/reference/kubectl/docker-cli-to-kubectl/</guid>
      <description>您可以使用 Kubernetes 命令行工具 kubectl 与 API 服务器进行交互。如果您熟悉 Docker 命令行工具，则使用 kubectl 非常简单。但是，docker 命令和 kubectl 命令之间有一些区别。以下显示了 docker 子命令，并描述了等效的 kubectl 命令。
docker run 要运行 nginx 部署并将其暴露，请参见 kubectl 运行。
docker:
docker run -d --restart=always -e DOMAIN=cluster --name nginx-app -p 80:80 nginx 55c103fa129692154a7652490236fee9be47d70a8dd562281ae7d2f9a339a6db docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 55c103fa1296 nginx &amp;quot;nginx -g &#39;daemon of…&amp;quot; 9 seconds ago Up 9 seconds 0.0.0.0:80-&amp;gt;80/tcp nginx-app kubectl:
# 启动运行 nginx 的 Pod kubectl run --image=nginx nginx-app --port=80 --env=&amp;#34;DOMAIN=cluster&amp;#34; deployment &amp;quot;nginx-app&amp;quot; created note &amp;gt;}}</description>
    </item>
    
    <item>
      <title>通过命名空间共享集群</title>
      <link>https://lijun.in/tasks/administer-cluster/namespaces/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/administer-cluster/namespaces/</guid>
      <description>本页展示了如何查看、使用和删除. glossary_tooltip text=&amp;quot;namespaces&amp;rdquo; term_id=&amp;quot;namespace&amp;rdquo; &amp;gt;}}。本页同时展示了如何使用 Kubernetes 命名空间去细分集群。
. heading &amp;ldquo;prerequisites&amp;rdquo; %}}  您已拥有一个 配置好的 Kubernetes 集群. 您已对 Kubernetes 的 Pods, Services, 和 Deployments 有基本理解。  查看命名空间  列出集群中现有的命名空间：  kubectl get namespaces NAME STATUS AGE default Active 11d kube-system Active 11d kube-public Active 11d 初始状态下，Kubernetes 具有三个名字空间：
 default 无命名空间对象的默认命名空间 kube-system 由 Kubernetes 系统创建的对象的命名空间 kube-public 自动创建且被所有用户可读的命名空间（包括未经身份认证的）。此命名空间通常在某些资源在整个集群中可见且可公开读取时被集群使用。此命名空间的公共方面只是一个约定，而不是一个必要条件。  您还可以通过下列命令获取特定命名空间的摘要：
kubectl get namespaces &amp;lt;name&amp;gt; 或获取详细信息：
kubectl describe namespaces &amp;lt;name&amp;gt; Name: default Labels: &amp;lt;none&amp;gt; Annotations: &amp;lt;none&amp;gt; Status: Active No resource quota.</description>
    </item>
    
    <item>
      <title>通过文件将Pod信息呈现给容器</title>
      <link>https://lijun.in/tasks/inject-data-application/downward-api-volume-expose-pod-information/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/inject-data-application/downward-api-volume-expose-pod-information/</guid>
      <description>此页面描述Pod如何使用DownwardAPIVolumeFile把自己的信息呈现给pod中运行的容器。DownwardAPIVolumeFile可以呈现pod的字段和容器字段。
. heading &amp;ldquo;prerequisites&amp;rdquo; %}} . include &amp;ldquo;task-tutorial-prereqs.md&amp;rdquo; &amp;gt;}} . version-check &amp;gt;}}
Downward API 有两种方式可以将Pod和Container字段呈现给运行中的容器：
 环境变量 DownwardAPIVolumeFile  这两种呈现Pod和Container字段的方式都称为Downward API。
存储Pod字段 在这个练习中，你将创建一个包含一个容器的pod。这是该pod的配置文件：
. codenew file=&amp;quot;pods/inject/dapi-volume.yaml&amp;rdquo; &amp;gt;}}
在配置文件中，你可以看到Pod有一个downwardAPI类型的Volume，并且挂载到容器中的/etc。
查看downwardAPI下面的items数组。每个数组元素都是一个[DownwardAPIVolumeFile](/docs/resources-reference/. param &amp;ldquo;version&amp;rdquo; &amp;gt;}}/#downwardapivolumefile-v1-core)。 第一个元素指示Pod的metadata.labels字段的值保存在名为labels的文件中。 第二个元素指示Pod的annotations字段的值保存在名为annotations的文件中。
. note &amp;gt;}} 本示例中的字段是Pod字段，不是Pod中容器的字段。 . /note &amp;gt;}}
创建 Pod：
kubectl apply -f https://k8s.io/examples/pods/inject/dapi-volume.yaml 验证Pod中的容器运行正常：
kubectl get pods 查看容器的日志：
kubectl logs kubernetes-downwardapi-volume-example 输出显示 labels 和 annotations 文件的内容：
cluster=&amp;#34;test-cluster1&amp;#34; rack=&amp;#34;rack-22&amp;#34; zone=&amp;#34;us-est-coast&amp;#34; build=&amp;#34;two&amp;#34; builder=&amp;#34;john-doe&amp;#34; 进入Pod中运行的容器，打开一个shell：
kubectl exec -it kubernetes-downwardapi-volume-example -- sh 在该shell中，查看labels文件：</description>
    </item>
    
    <item>
      <title>通过环境变量将Pod信息呈现给容器</title>
      <link>https://lijun.in/tasks/inject-data-application/environment-variable-expose-pod-information/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/inject-data-application/environment-variable-expose-pod-information/</guid>
      <description>此页面显示了Pod如何使用环境变量把自己的信息呈现给pod中运行的容器。环境变量可以呈现pod的字段和容器字段。
有两种方式可以将Pod和Container字段呈现给运行中的容器： 环境变量 和[DownwardAPIVolumeFiles](/docs/resources-reference/. param &amp;ldquo;version&amp;rdquo; &amp;gt;}}/#downwardapivolumefile-v1-core). 这两种呈现Pod和Container字段的方式都称为Downward API。
. heading &amp;ldquo;prerequisites&amp;rdquo; %}} . include &amp;ldquo;task-tutorial-prereqs.md&amp;rdquo; &amp;gt;}} . version-check &amp;gt;}}
Downward API 有两种方式可以将Pod和Container字段呈现给运行中的容器：
 环境变量 [DownwardAPIVolumeFiles](/docs/resources-reference/. param &amp;ldquo;version&amp;rdquo; &amp;gt;}}/#downwardapivolumefile-v1-core)  这两种呈现Pod和Container字段的方式都称为Downward API。
用Pod字段作为环境变量的值 在这个练习中，你将创建一个包含一个容器的pod。这是该pod的配置文件：
. codenew file=&amp;quot;pods/inject/dapi-envars-pod.yaml&amp;rdquo; &amp;gt;}}
这个配置文件中，你可以看到五个环境变量。env字段是一个[EnvVars](/docs/resources-reference/. param &amp;ldquo;version&amp;rdquo; &amp;gt;}}/#envvar-v1-core)类型的数组。 数组中第一个元素指定MY_NODE_NAME这个环境变量从Pod的spec.nodeName字段获取变量值。同样，其它环境变量也是从Pod的字段获取它们的变量值。
. note &amp;gt;}} 本示例中的字段是Pod字段，不是Pod中容器的字段。 . /note &amp;gt;}}
创建Pod：
kubectl apply -f https://k8s.io/examples/pods/inject/dapi-envars-pod.yaml 验证Pod中的容器运行正常：
kubectl get pods 查看容器日志：
kubectl logs dapi-envars-fieldref 输出信息显示了所选择的环境变量的值：
minikube dapi-envars-fieldref default 172.17.0.4 default 要了解为什么这些值在日志中，请查看配置文件中的command 和 args字段。 当容器启动时，它将五个环境变量的值写入stdout。每十秒重复执行一次。</description>
    </item>
    
    <item>
      <title>通过配置文件设置 Kubelet 参数</title>
      <link>https://lijun.in/tasks/administer-cluster/kubelet-config-file/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/administer-cluster/kubelet-config-file/</guid>
      <description>. feature-state state=&amp;quot;beta&amp;rdquo; &amp;gt;}}
通过保存在硬盘的配置文件设置 Kubelet 的配置参数子集，可以作为命令行参数的替代。此功能在 v1.10 中为 beta 版。
建议通过配置文件的方式提供参数，因为这样可以简化节点部署和配置管理。
. heading &amp;ldquo;prerequisites&amp;rdquo; %}}  需要安装 1.10 或更高版本的 Kubelet 二进制文件，才能实现 beta 功能。  创建配置文件 KubeletConfiguration 结构体定义了可以通过文件配置的 Kubelet 配置子集，该结构体在 [这里（v1beta1）](https://github.com/kubernetes/kubernetes/blob/. param &amp;ldquo;docsbranch&amp;rdquo; &amp;gt;}}/staging/src/k8s.io/kubelet/config/v1beta1/types.go) 可以找到, 配置文件必须是这个结构体中参数的 JSON 或 YAML 表现形式。
在单独的文件夹中创建一个名为 kubelet 的文件，并保证 Kubelet 可以读取该文件夹及文件。您应该在这个 kubelet 文件中编写 Kubelet 配置。
这是一个 Kubelet 配置文件示例：
kind: KubeletConfiguration apiVersion: kubelet.config.k8s.io/v1beta1 evictionHard: memory.available: &amp;quot;200Mi&amp;quot; 在这个示例中, 当可用内存低于200Mi 时, Kubelet 将会开始驱逐 Pods。 没有声明的其余配置项都将使用默认值, 命令行中的 flags 将会覆盖配置文件中的对应值。
作为一个小技巧，您可以从活动节点生成配置文件，相关方法请查看 重新配置活动集群节点的 Kubelet。</description>
    </item>
    
    <item>
      <title>配置 API 对象配额</title>
      <link>https://lijun.in/tasks/administer-cluster/quota-api-object/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/administer-cluster/quota-api-object/</guid>
      <description>本文讨论如何为 API 对象配置配额，包括 PersistentVolumeClaims 和 Services。 配额限制了可以在命名空间中创建的特定类型对象的数量。 您可以在 [ResourceQuota](/docs/reference/generated/kubernetes-api/. param &amp;ldquo;version&amp;rdquo; &amp;gt;}}/#resourcequota-v1-core) 对象中指定配额。
. heading &amp;ldquo;prerequisites&amp;rdquo; %}} . include &amp;ldquo;task-tutorial-prereqs.md&amp;rdquo; &amp;gt;}} . version-check &amp;gt;}}
创建命名空间 创建一个命名空间以便本例中创建的资源和集群中的其余部分相隔离。
kubectl create namespace quota-object-example 创建 ResourceQuota 下面是一个 ResourceQuota 对象的配置文件：
. codenew file=&amp;quot;admin/resource/quota-objects.yaml&amp;rdquo; &amp;gt;}}
创建 ResourceQuota
kubectl create -f https://k8s.io/examples/admin/resource/quota-objects.yaml --namespace=quota-object-example 查看 ResourceQuota 的详细信息：
kubectl get resourcequota object-quota-demo --namespace=quota-object-example --output=yaml 输出结果表明在 quota-object-example 命名空间中，至多只能有一个 PersistentVolumeClaim，最多两个 LoadBalancer 类型的服务，不能有 NodePort 类型的服务。
status: hard: persistentvolumeclaims: &amp;#34;1&amp;#34; services.loadbalancers: &amp;#34;2&amp;#34; services.nodeports: &amp;#34;0&amp;#34; used: persistentvolumeclaims: &amp;#34;0&amp;#34; services.</description>
    </item>
    
    <item>
      <title>配置多个调度器</title>
      <link>https://lijun.in/tasks/administer-cluster/configure-multiple-schedulers/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/administer-cluster/configure-multiple-schedulers/</guid>
      <description>Kubernetes 自带了一个默认调度器，其详细描述请查阅这里。
如果默认调度器不适合您的需求，您可以实现自己的调度器。
不仅如此，您甚至可以伴随着默认调度器同时运行多个调度器，并告诉 Kubernetes 为每个 pod 使用什么调度器。 让我们通过一个例子讲述如何在 Kubernetes 中运行多个调度器。
关于实现调度器的具体细节描述超出了本文范围。 请参考 kube-scheduler 的实现，规范示例代码位于 [pkg/scheduler](https://github.com/kubernetes/kubernetes/tree/. param &amp;ldquo;githubbranch&amp;rdquo; &amp;gt;}}/pkg/scheduler)。
. heading &amp;ldquo;prerequisites&amp;rdquo; %}} . include &amp;ldquo;task-tutorial-prereqs.md&amp;rdquo; &amp;gt;}} . version-check &amp;gt;}}
打包调度器 将调度器二进制文件打包到容器镜像中。出于示例目的，我们就使用默认调度器（kube-scheduler）作为我们的第二个调度器。
从 Github 克隆 Kubernetes 源代码，并编译构建源代码。
git clone https://github.com/kubernetes/kubernetes.git cd kubernetes make 创建一个包含 kube-scheduler 二进制文件的容器镜像。用于构建镜像的 Dockerfile 内容如下：
FROMbusyboxADD ./_output/dockerized/bin/linux/amd64/kube-scheduler /usr/local/bin/kube-scheduler将文件保存为 Dockerfile，构建镜像并将其推送到镜像仓库。 此示例将镜像推送到 Google 容器镜像仓库（GCR）。
有关详细信息，请阅读 GCR 文档。
docker build -t gcr.io/my-gcp-project/my-kube-scheduler:1.0 . gcloud docker -- push gcr.io/my-gcp-project/my-kube-scheduler:1.0 为调度器定义 Kubernetes Deployment 现在我们将调度器放在容器镜像中，我们可以为它创建一个 pod 配置，并在我们的 Kubernetes 集群中运行它。 但是与其在集群中直接创建一个 pod，不如使用 Deployment。 Deployment 管理一个 Replica Set，Replica Set 再管理 pod，从而使调度器能够适应故障。 以下是 Deployment 配置，被保存为 my-scheduler.</description>
    </item>
    
    <item>
      <title>配置对多集群的访问</title>
      <link>https://lijun.in/tasks/access-application-cluster/configure-access-multiple-clusters/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/access-application-cluster/configure-access-multiple-clusters/</guid>
      <description>本文展示如何使用配置文件来配置对多个集群的访问。 在将集群、用户和上下文定义在一个或多个配置文件中之后，用户可以使用 kubectl config use-context 命令快速地在集群之间进行切换。
. note &amp;gt;}} 用于配置集群访问的文件有时被称为 kubeconfig 文件。 这是一种引用配置文件的通用方式，并不意味着存在一个名为 kubeconfig 的文件。 . /note &amp;gt;}}
. heading &amp;ldquo;prerequisites&amp;rdquo; %}} 需要安装 kubectl 命令行工具。
定义集群、用户和上下文 假设用户有两个集群，一个用于正式开发工作，一个用于其它临时用途（scratch）。 在 development 集群中，前端开发者在名为 frontend 的名字空间下工作， 存储开发者在名为 storage 的名字空间下工作。 在 scratch 集群中， 开发人员可能在默认名字空间下工作，也可能视情况创建附加的名字空间。 访问开发集群需要通过证书进行认证。 访问其它临时用途的集群需要通过用户名和密码进行认证。
创建名为 config-exercise 的目录。 在 config-exercise 目录中，创建名为 config-demo 的文件，其内容为：
apiVersion: v1 kind: Config preferences: {} clusters: - cluster: name: development - cluster: name: scratch users: - name: developer - name: experimenter contexts: - context: name: dev-frontend - context: name: dev-storage - context: name: exp-scratch 配置文件描述了集群、用户名和上下文。 config-demo 文件中含有描述两个集群、两个用户和三个上下文的框架。</description>
    </item>
    
    <item>
      <title>配置资源不足时的处理方式</title>
      <link>https://lijun.in/tasks/administer-cluster/out-of-resource/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/administer-cluster/out-of-resource/</guid>
      <description>本页介绍了如何使用kubelet配置资源不足时的处理方式。
当可用计算资源较少时，kubelet需要保证节点稳定性。这在处理如内存和硬盘之类的不可压缩资源时尤为重要。如果任意一种资源耗尽，节点将会变得不稳定。
驱逐策略 kubelet 能够主动监测和防止计算资源的全面短缺。在那种情况下，kubelet可以主动地结束一个或多个 pod 以回收短缺的资源。当 kubelet 结束一个 pod 时，它将终止 pod 中的所有容器，而 pod 的 PodPhase 将变为 Failed。
如果被驱逐的 Pod 由 Deployment 管理，这个 Deployment 会创建另一个 Pod 给 Kubernetes 来调度。
驱逐信号 kubelet 支持按照以下表格中描述的信号触发驱逐决定。每个信号的值在 description 列描述，基于 kubelet 摘要 API。
   驱逐信号 描述     memory.available memory.available := node.status.capacity[memory] - node.stats.memory.workingSet   nodefs.available nodefs.available := node.stats.fs.available   nodefs.inodesFree nodefs.inodesFree := node.stats.fs.inodesFree   imagefs.available imagefs.available := node.stats.runtime.imagefs.available   imagefs.</description>
    </item>
    
    <item>
      <title>限制存储消耗</title>
      <link>https://lijun.in/tasks/administer-cluster/limit-storage-consumption/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/administer-cluster/limit-storage-consumption/</guid>
      <description>此示例演示了一种限制命名空间中存储使用量的简便方法。
演示中用到了以下资源：ResourceQuota，LimitRange 和 PersistentVolumeClaim。
. heading &amp;ldquo;prerequisites&amp;rdquo; %}}  . include &amp;ldquo;task-tutorial-prereqs.md&amp;rdquo; &amp;gt;}} . version-check &amp;gt;}}  场景：限制存储消耗 集群管理员代表用户群操作集群，管理员希望控制单个名称空间可以消耗多少存储空间以控制成本。
管理员想要限制：
 命名空间中持久卷申领（persistent volume claims）的数量 每个申领（claim）可以请求的存储量 命名空间可以具有的累计存储量  使用 LimitRange 限制存储请求 将 LimitRange 添加到命名空间会为存储请求大小强制设置最小值和最大值。存储是通过 PersistentVolumeClaim 来发起请求的。执行限制范围控制的准入控制器会拒绝任何高于或低于管理员所设阈值的 PVC。
在此示例中，请求 10Gi 存储的 PVC 将被拒绝，因为它超过了最大 2Gi。
apiVersion: v1 kind: LimitRange metadata: name: storagelimits spec: limits: - type: PersistentVolumeClaim max: storage: 2Gi min: storage: 1Gi 当底层存储提供程序需要某些最小值时，将会用到所设置最小存储请求值。例如，AWS EBS volumes 的最低要求为 1Gi。
使用 StorageQuota 限制 PVC 数目和累计存储容量 管理员可以限制某个命名空间中的 PVCs 个数以及这些 PVCs 的累计容量。新 PVCs 请求如果超过任一上限值将被拒绝。</description>
    </item>
    
    <item>
      <title>集群 DNS 服务自动伸缩</title>
      <link>https://lijun.in/tasks/administer-cluster/dns-horizontal-autoscaling/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/administer-cluster/dns-horizontal-autoscaling/</guid>
      <description>本页展示了如何在集群中启用和配置 DNS 服务的自动伸缩功能。
. heading &amp;ldquo;prerequisites&amp;rdquo; %}}  . include &amp;ldquo;task-tutorial-prereqs.md&amp;rdquo; &amp;gt;}} . version-check &amp;gt;}}    本指南假设您的节点使用 AMD64 或 Intel 64 CPU 架构
  确保已启用 DNS 功能本身。
  建议使用 Kubernetes 1.4.0 或更高版本。
  确定是否 DNS 水平 水平自动伸缩特性已经启用 在 kube-system 命名空间中列出集群中的 . glossary_tooltip text=&amp;quot;Deployments&amp;rdquo; term_id=&amp;quot;deployment&amp;rdquo; &amp;gt;}} ：
kubectl get deployment --namespace=kube-system 输出类似如下这样：
NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE ... dns-autoscaler 1 1 1 1 ... ...  如果在输出中看到 “dns-autoscaler”，说明 DNS 水平自动伸缩已经启用，可以跳到 调优自动伸缩参数。</description>
    </item>
    
    <item>
      <title>集群安全</title>
      <link>https://lijun.in/tasks/administer-cluster/securing-a-cluster/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/administer-cluster/securing-a-cluster/</guid>
      <description>本文档涉及与保护集群免受意外或恶意访问有关的主题，并对总体安全性提出建议。
. heading &amp;ldquo;prerequisites&amp;rdquo; %}}  . include &amp;ldquo;task-tutorial-prereqs.md&amp;rdquo; &amp;gt;}} . version-check &amp;gt;}}  控制对 Kubernetes API 的访问 因为 Kubernetes 是完全通过 API 驱动的，所以，控制和限制谁可以通过 API 访问集群，以及允许这些访问者执行什么样的 API 动作，就成为了安全控制的第一道防线。
为 API 交互提供传输层安全 （TLS） Kubernetes 期望集群中所有的 API 通信在默认情况下都使用 TLS 加密，大多数安装方法也允许创建所需的证书并且分发到集群组件中。请注意，某些组件和安装方法可能使用 HTTP 来访问本地端口， 管理员应该熟悉每个组件的设置，以识别潜在的不安全的流量。
API 认证 安装集群时，选择一个 API 服务器的身份验证机制，去使用与之匹配的公共访问模式。例如，小型的单用户集群可能希望使用简单的证书或静态承载令牌方法。更大的集群则可能希望整合现有的、OIDC、LDAP 等允许用户分组的服务器。
所有 API 客户端都必须经过身份验证，即使它是基础设施的一部分，比如节点、代理、调度程序和卷插件。这些客户端通常使用 服务帐户 或 X509 客户端证书，并在集群启动时自动创建或是作为集群安装的一部分进行设置。
如果您希望获取更多信息，请参考 认证参考文档。
API 授权 一旦使用授权，每个 API 的调用都将通过授权检查。Kubernetes 集成 基于访问控制（RBAC） 的组件，将传入的用户或组与一组绑定到角色的权限匹配。这些权限将动作（get，create，delete）和资源（pod，service, node）在命名空间或者集群范围内结合起来，根据客户可能希望执行的操作，提供了一组提供合理的违约责任分离的外包角色。建议您将节点 和 RBAC 一起作为授权者，再与 NodeRestriction 准入插件结合使用。
与身份验证一样，简单而广泛的角色可能适合于较小的集群，但是随着更多的用户与集群交互，可能需要将团队划分成有更多角色限制的单独的命名空间。</description>
    </item>
    
    <item>
      <title>集群故障排查</title>
      <link>https://lijun.in/tasks/debug-application-cluster/debug-cluster/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/debug-application-cluster/debug-cluster/</guid>
      <description>本篇文档是介绍集群故障排查的；我们假设对于你碰到的问题，你已经排除了是由应用程序造成的。 对于应用的调试，请参阅应用故障排查指南。 你也可以访问troubleshooting document来获取更多的信息。
显示出集群的节点列表 调试的第一步是查看所有的节点是否都正确的注册。
运行
kubectl get nodes 接下来，验证你的所有节点都能够显示出来，并且都处于Ready状态。
查看logs 现在，挖掘出集群更深层的信息就需要登录到相关的机器上。下面是相关log文件所在的位置。 (注意，对于基于systemd的系统，你可能需要使用journalctl)
Master  /var/log/kube-apiserver.log - API Server, 提供API服务 /var/log/kube-scheduler.log - Scheduler, 负责调度决策 /var/log/kube-controller-manager.log - 管理replication controllers的控制器  Worker Nodes  /var/log/kubelet.log - Kubelet, 管控节点上运行的容器 /var/log/kube-proxy.log - Kube Proxy, 负责服务的负载均衡  集群故障模式的概述 下面是一个不完整的列表，列举了一些可能出错的场景，以及通过调整集群配置来解决相关问题的方法。
根本原因：
 VM(s)关机 集群之间，或者集群和用户之间网络分裂 Kubernetes软件本身崩溃了 数据丢失或者持久化存储不可用(如:GCE PD 或 AWS EBS卷) 操作错误，如：Kubernetes或者应用程序配置错误  具体情况:
  Apiserver所在的VM关机或者apiserver崩溃
 结果  不能停止，更新，或者启动新的pods，services，replication controller 现有的pods和services在不依赖Kubernetes API的情况下应该能继续正常工作      Apiserver 后端存储丢失</description>
    </item>
    
    <item>
      <title>集群管理</title>
      <link>https://lijun.in/tasks/administer-cluster/cluster-management/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/administer-cluster/cluster-management/</guid>
      <description>. toc &amp;gt;}}
本文描述了和集群生命周期相关的几个主题：创建新集群、更新集群的 master 和 worker 节点、执行节点维护（例如升级内核）以及升级运行中集群的 Kubernetes API 版本。
创建和配置集群 要在一组机器上安装 Kubernetes， 请根据您的环境，查阅现有的 入门指南
升级集群 集群升级当前是配套提供的，某些发布版本在升级时可能需要特殊处理。推荐管理员在升级他们的集群前，同时查阅 发行说明 和版本具体升级说明。
 升级到 1.6  升级 Google Compute Engine 集群 Google Compute Engine Open Source (GCE-OSS) 通过删除和重建 master 来支持 master 升级。通过维持相同的 Persistent Disk (PD) 以保证在升级过程中保留数据。
GCE 的 Node 升级采用 管理实例组，每个节点将被顺序的删除，然后使用新软件重建。任何运行在那个节点上的 Pod 需要用 Replication Controller 控制，或者在扩容之后手动重建。
开源 Google Compute Engine (GCE) 集群上的升级过程由 cluster/gce/upgrade.sh 脚本控制。
运行 cluster/gce/upgrade.sh -h 获取使用说明。
例如，只将 master 升级到一个指定的版本 (v1.0.2):</description>
    </item>
    
    <item>
      <title>静态加密 Secret 数据</title>
      <link>https://lijun.in/tasks/administer-cluster/encrypt-data/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/administer-cluster/encrypt-data/</guid>
      <description>本文展示如何启用和配置静态 Secret 数据的加密
. heading &amp;ldquo;prerequisites&amp;rdquo; %}}  . include &amp;ldquo;task-tutorial-prereqs.md&amp;rdquo; &amp;gt;}} . version-check &amp;gt;}}    需要 Kubernetes 1.7.0 或者更高版本
  需要 etcd v3 或者更高版本
  静态数据加密在 1.7.0 中仍然是 alpha 版本，这意味着它可能会在没有通知的情况下进行更改。在升级到 1.8.0 之前，用户可能需要解密他们的数据。
  . toc &amp;gt;}}
. heading &amp;ldquo;prerequisites&amp;rdquo; %}} . include &amp;ldquo;task-tutorial-prereqs.md&amp;rdquo; &amp;gt;}} . version-check &amp;gt;}}
配置并确定是否已启用静态数据加密 kube-apiserver 的参数 --experimental-encryption-provider-config 控制 API 数据在 etcd 中的加密方式。 下面提供一个配置示例。
理解静态数据加密 kind: EncryptionConfiguration apiVersion: apiserver.config.k8s.io/v1 resources: - resources: - secrets providers: - identity: {} - aesgcm: keys: - name: key1 secret: c2VjcmV0IGlzIHNlY3VyZQ== - name: key2 secret: dGhpcyBpcyBwYXNzd29yZA== - aescbc: keys: - name: key1 secret: c2VjcmV0IGlzIHNlY3VyZQ== - name: key2 secret: dGhpcyBpcyBwYXNzd29yZA== - secretbox: keys: - name: key1 secret: YWJjZGVmZ2hpamtsbW5vcHFyc3R1dnd4eXoxMjM0NTY= 每个 resources 数组项目是一个单独的完整的配置。 resources.</description>
    </item>
    
    <item>
      <title>验证 IPv4/IPv6 双协议栈</title>
      <link>https://lijun.in/tasks/network/validate-dual-stack/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/network/validate-dual-stack/</guid>
      <description>这篇文章分享了如何验证 IPv4/IPv6 双协议栈的 Kubernetes 集群。
. heading &amp;ldquo;prerequisites&amp;rdquo; %}}  Kubernetes 1.16 或更高版本 提供程序对双协议栈网络的支持 (云供应商或其他方式必须能够为 Kubernetes 节点提供可路由的 IPv4/IPv6 网络接口) Kubenet 网络插件 Kube-proxy 在 IPVS 模式下运行 启用双协议栈 集群  验证寻址 验证节点寻址 每个双协议栈节点应分配一个 IPv4 块和一个 IPv6 块。 通过运行以下命令来验证是否配置了 IPv4/IPv6 Pod 地址范围。 将示例节点名称替换为集群中的有效双协议栈节点。 在此示例中，节点的名称为 k8s-linuxpool1-34450317-0：
kubectl get nodes k8s-linuxpool1-34450317-0 -o go-template --template=&amp;#39;{{range .spec.podCIDRs}}{{printf &amp;#34;%s\n&amp;#34; .}}{{end}}&amp;#39; 10.244.1.0/24 a00:100::/24 应该分配一个 IPv4 块和一个 IPv6 块。
验证节点是否检测到 IPv4 和 IPv6 接口（用集群中的有效节点替换节点名称。在此示例中，节点名称为 k8s-linuxpool1-34450317-0）：
kubectl get nodes k8s-linuxpool1-34450317-0 -o go-template --template=&amp;#39;{{range .</description>
    </item>
    
    <item>
      <title>😝 - 用插件扩展 kubectl</title>
      <link>https://lijun.in/tasks/extend-kubectl/kubectl-plugins/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/extend-kubectl/kubectl-plugins/</guid>
      <description>. feature-state state=&amp;quot;stable&amp;rdquo; &amp;gt;}}
本指南演示了如何为 kubectl 安装和编写扩展。 通过将核心 kubectl 命令看作与 Kubernetes 集群交互的基本构建块，集群管理员可以将插件视为一种利用这些构建块创建更复杂行为的方法。 插件用新的子命令扩展了 kubectl，允许新的和自定义的特性不包括在 kubectl 的主要发行版中。
. heading &amp;ldquo;prerequisites&amp;rdquo; %}} 您需要安装一个工作的二进制 kubectl。
. note &amp;gt;}}
插件在 v1.8.0 版本中正式作为 alpha 特性引入。它们已经在 v1.12.0 版本中工作，以支持更广泛的用例。因此，虽然在以前的版本中已经提供了部分插件特性，但如果您遵循这些文档，建议使用 1.12.0 或更高版本的 kubectl。
. /note &amp;gt;}}
安装 kubectl 插件 插件只不过是一个独立的可执行文件，名称以 kubectl- 开头。要安装插件，只需将此可执行文件移动到路径上的任何位置。
. note &amp;gt;}}
Kubernetes 不提供包管理器或任何类似于安装或更新插件的东西。你有责任确保插件可执行文件的文件名以 kubectl- 开头，并将它们放在你路径的某个位置。 . /note &amp;gt;}}
发现插件 kubectl 提供一个命令 kubectl plugin list，用于搜索路径查找有效的插件可执行文件。 执行此命令将遍历路径中的所有文件。任何以 kubectl- 开头的可执行文件都将在这个命令的输出中以它们在路径中出现的顺序显示。 任何以 kubectl- 开头的文件如果不可执行，都将包含一个警告。 对于任何相同的有效插件文件，都将包含一个警告。
限制 目前无法创建覆盖现有 kubectl 命令的插件，例如，创建一个插件 kubectl-version 将导致该插件永远不会被执行，因为现有的 kubectl-version 命令总是优先于它执行。 由于这个限制，也不可能使用插件将新的子命令添加到现有的 kubectl 命令中。例如，通过将插件命名为 kubectl-create-foo 来添加子命令 kubectl create foo 将导致该插件被忽略。对于任何试图这样做的有效插件 kubectl plugin list 的输出中将显示警告。</description>
    </item>
    
    <item>
      <title>😝 - 管理巨页（HugePages）</title>
      <link>https://lijun.in/tasks/manage-hugepages/scheduling-hugepages/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/manage-hugepages/scheduling-hugepages/</guid>
      <description>. feature-state state=&amp;quot;stable&amp;rdquo; &amp;gt;}}
作为 GA 特性，Kubernetes 支持在 Pod 应用中使用预先分配的巨页。本文描述了用户如何使用巨页，以及当前的限制。
. heading &amp;ldquo;prerequisites&amp;rdquo; %}}  为了使节点能够上报巨页容量，Kubernetes 节点必须预先分配巨页。每个节点只能预先分配一种特定规格的巨页。  节点会自动发现全部巨页资源，并作为可供调度的资源进行上报。
API 用户可以通过在容器级别的资源需求中使用资源名称 hugepages-&amp;lt;size&amp;gt; 来使用巨页，其中的 size 是特定节点上支持的以整数值表示的最小二进制单位。 例如，如果节点支持 2048KiB 的页面规格， 它将暴露可供调度的资源 hugepages-2Mi。 与 CPU 或内存不同，巨页不支持过量使用（overcommit）。 注意，在请求巨页资源时，还必须请求内存或 CPU 资源。
apiVersion: v1 kind: Pod metadata: generateName: hugepages-volume- spec: containers: - image: fedora:latest command: - sleep - inf name: example volumeMounts: - mountPath: /hugepages name: hugepage resources: limits: hugepages-2Mi: 100Mi memory: 100Mi requests: memory: 100Mi volumes: - name: hugepage emptyDir: medium: HugePages  巨页的资源请求值必须等于其限制值。该条件在指定了资源限制，而没有指定请求的情况下默认成立。 巨页是被隔离在 pod 作用域的，计划在将来的迭代中实现容器级别的隔离。 巨页可用于 EmptyDir 卷，不过 EmptyDir 卷所使用的巨页数量不能够超出 Pod 请求的巨页数量。 通过带有 SHM_HUGETLB 的 shmget() 使用巨页的应用，必须运行在一个与 proc/sys/vm/hugetlb_shm_group 匹配的补充组下。 通过 ResourceQuota 资源，可以使用 hugepages-&amp;lt;size&amp;gt; 标记控制每个命名空间下的巨页使用量， 类似于使用 cpu 或 memory 来控制其他计算资源。  待实现的特性  在 pod 级别隔离的基础上，支持巨页在容器级别的隔离。 作为服务质量特性，保证巨页的 NUMA 局部性。 支持 LimitRange 。  </description>
    </item>
    
    <item>
      <title>😝 - 调度 GPUs</title>
      <link>https://lijun.in/tasks/manage-gpus/scheduling-gpus/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/manage-gpus/scheduling-gpus/</guid>
      <description>Kubernetes 支持对节点上的 AMD 和 NVIDA GPU 进行管理，目前处于实验状态。对 NVIDIA GPU 的支持在 v1.6 中加入，已经经历了多次不向后兼容的迭代。而对 AMD GPU 的支持则在 v1.9 中通过 设备插件 加入。
这个页面介绍了用户如何在不同的 Kubernetes 版本中使用 GPU，以及当前存在的一些限制。
从 v1.8 起 从 1.8 版本开始，我们推荐通过 设备插件 的方式来使用 GPU。
在 1.10 版本之前，为了通过设备插件开启 GPU 的支持，我们需要在系统中将 DevicePlugins 这一特性开关显式地设置为 true：--feature-gates=&amp;quot;DevicePlugins=true&amp;quot;。不过， 从 1.10 版本开始，我们就不需要这一步骤了。
接着你需要在主机节点上安装对应厂商的 GPU 驱动并运行对应厂商的设备插件 (AMD、NVIDIA)。
当上面的条件都满足，Kubernetes 将会暴露 nvidia.com/gpu 或 amd.com/gpu 来作为 一种可调度的资源。
你也能通过像请求 cpu 或 memory 一样请求 &amp;lt;vendor&amp;gt;.com/gpu 来在容器中使用 GPU。然而，当你要通过指定资源请求来使用 GPU 时，存在着以下几点限制：
 GPU 仅仅支持在 limits 部分被指定，这表明：  你可以仅仅指定 GPU 的 limits 字段而不必须指定 requests 字段，因为 Kubernetes 会默认使用 limit 字段的值来作为 request 字段的默认值。 你能同时指定 GPU 的 limits 和 requests 字段，但这两个值必须相等。 你不能仅仅指定 GPU 的 request 字段而不指定 limits。   容器（以及 pod）并不会共享 GPU，也不存在对 GPU 的过量使用。 每一个容器能够请求一个或多个 GPU。然而只请求一个 GPU 的一部分是不允许的。  下面是一个例子:</description>
    </item>
    
  </channel>
</rss>