<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>😝 - 管理集群 on Hugo 主题的 Learn 文档</title>
    <link>https://lijun.in/tasks/administer-cluster/</link>
    <description>Recent content in 😝 - 管理集群 on Hugo 主题的 Learn 文档</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-US</language>
    
	<atom:link href="https://lijun.in/tasks/administer-cluster/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Debug DNS 方案</title>
      <link>https://lijun.in/tasks/administer-cluster/dns-debugging-resolution/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/administer-cluster/dns-debugging-resolution/</guid>
      <description>This page provides hints on diagnosing DNS problems.
&amp;ndash;&amp;gt;
这篇文章提供了一些关于 DNS 问题诊断的方法。
&amp;ndash;&amp;gt;
. heading &amp;ldquo;prerequisites&amp;rdquo; %}}  . include &amp;ldquo;task-tutorial-prereqs.md&amp;rdquo; &amp;gt;}} . version-check &amp;gt;}} Kubernetes 1.6 或者以上版本。 集群必须使用了 coredns (或者 kube-dns)插件。  创建一个简单的 Pod 作为测试环境 新建一个名为 busybox.yaml 的文件并填入下列内容：
. codenew file=&amp;quot;admin/dns/busybox.yaml&amp;rdquo; &amp;gt;}}
然后使用这个文件创建一个 Pod 并验证其状态：
kubectl create -f https://k8s.io/examples/admin/dns/busybox.yaml pod/busybox created kubectl get pods busybox NAME READY STATUS RESTARTS AGE busybox 1/1 Running 0 &amp;lt;some-time&amp;gt; 只要 Pod 处于 running 状态，您就可以在环境里执行 nslookup 。 如果您看到类似下列的内容，则表示 DNS 是正常运行的。</description>
    </item>
    
    <item>
      <title>IP Masquerade Agent 用户指南</title>
      <link>https://lijun.in/tasks/administer-cluster/ip-masq-agent/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/administer-cluster/ip-masq-agent/</guid>
      <description>此页面展示如何配置和启用 ip-masq-agent。
. heading &amp;ldquo;prerequisites&amp;rdquo; %}} . include &amp;ldquo;task-tutorial-prereqs.md&amp;rdquo; &amp;gt;}} . version-check &amp;gt;}}
IP Masquerade Agent 用户指南 ip-masq-agent 配置 iptables 规则以隐藏位于集群节点 IP 地址后面的 pod 的 IP 地址。 这通常在将流量发送到集群的 pod CIDR 范围之外的目的地时使用。
关键词  NAT (网络地址解析) 是一种通过修改 IP 地址头中的源和/或目标地址信息将一个 IP 地址重新映射到另一个 IP 地址的方法。通常由执行 IP 路由的设备执行。   伪装 NAT 的一种形式，通常用于执行多对一地址转换，其中多个源 IP 地址被隐藏在单个地址后面，该地址通常是执行 IP 路由的设备。在 Kubernetes 中，这是节点的 IP 地址。   CIDR (无类别域间路由) 基于可变长度子网掩码，允许指定任意长度的前缀。CIDR 引入了一种新的 IP 地址表示方法，现在通常称为CIDR表示法，其中地址或路由前缀后添加一个后缀，用来表示前缀的位数，例如 192.168.2.0/24。   本地链路 本地链路是仅对网段或主机所连接的广播域内的通信有效的网络地址。IPv4的本地链路地址在 CIDR 表示法的地址块 169.</description>
    </item>
    
    <item>
      <title>Kubernetes 云管理控制器</title>
      <link>https://lijun.in/tasks/administer-cluster/running-cloud-controller/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/administer-cluster/running-cloud-controller/</guid>
      <description>. feature-state state=&amp;quot;beta&amp;rdquo; &amp;gt;}}
Kubernetes v1.6 包含一个新的二进制文件，叫做 cloud-controller-manager。cloud-controller-manager 是一个嵌入了特定云服务控制循环逻辑的守护进程。这些特定云服务控制循环逻辑最初存在于 kube-controller-manager 中。由于云服务提供商开发和发布的速度与 Kubernetes 项目不同，将服务提供商专用代码从 cloud-controller-manager 二进制中抽象出来有助于云服务厂商在 Kubernetes 核心代码之外独立进行开发。
cloud-controller-manager 可以被链接到任何满足 cloudprovider.Interface 约束的云服务提供商。为了兼容旧版本，Kubernetes 核心项目中提供的 cloud-controller-manager 使用和 kube-controller-manager 相同的云服务类库。已经在 Kubernetes 核心项目中支持的云服务提供商预计将通过使用 in-tree 的 cloud-controller-manager 过渡到 Kubernetes 核心之外。在将来的 Kubernetes 发布中，所有的云管理控制器将在 Kubernetes 核心项目之外，由 sig 领导者或者云服务厂商进行开发。
管理 需求 每个云服务都有一套各自的需求用于系统平台的集成，这不应与运行 kube-controller-manager 的需求有太大差异。作为经验法则，你需要：
 云服务认证 / 授权：您的云服务可能需要使用令牌或者 IAM 规则以允许对其 API 的访问 kubernetes 认证 / 授权：cloud-controller-manager 可能需要 RBAC 规则以访问 kubernetes apiserver 高可用：类似于 kube-controller-manager，您可能希望通过主节点选举（默认开启）配置一个高可用的云管理控制器。  运行云管理控制器 您需要对集群配置做适当的修改以成功地运行云管理控制器：
 一定不要为 kube-apiserver 和 kube-controller-manager 指定 --cloud-provider 标志。这将保证它们不会运行任何云服务专用循环逻辑，这将会由云管理控制器运行。未来这个标记将被废弃并去除。 kubelet 必须使用 --cloud-provider=external 运行。这是为了保证让 kubelet 知道在执行任何任务前，它必须被云管理控制器初始化。  请记住，设置群集使用云管理控制器将用多种方式更改群集行为：</description>
    </item>
    
    <item>
      <title>为 Kubernetes 运行 etcd 集群</title>
      <link>https://lijun.in/tasks/administer-cluster/configure-upgrade-etcd/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/administer-cluster/configure-upgrade-etcd/</guid>
      <description>. glossary_definition term_id=&amp;quot;etcd&amp;rdquo; length=&amp;quot;all&amp;rdquo; prepend=&amp;quot;etcd 是一个&amp;quot;&amp;gt;}}
. heading &amp;ldquo;prerequisites&amp;rdquo; %}} . include &amp;ldquo;task-tutorial-prereqs.md&amp;rdquo; &amp;gt;}} . version-check &amp;gt;}}
先决条件   运行的 etcd 集群个数成员为奇数。
  etcd 是一个 leader-based 分布式系统。确保主节点定期向所有从节点发送心跳，以保持集群稳定。
  确保不发生资源不足。
集群的性能和稳定性对网络和磁盘 IO 非常敏感。任何资源匮乏都会导致心跳超时，从而导致集群的不稳定。不稳定的情况表明没有选出任何主节点。在这种情况下，集群不能对其当前状态进行任何更改，这意味着不能调度新的 pod。
  保持稳定的 etcd 集群对 Kubernetes 集群的稳定性至关重要。因此，请在专用机器或隔离环境上运行 etcd 集群，以满足所需资源需求。
  在生产中运行的 etcd 的最低推荐版本是 3.2.10+。
  资源要求 使用有限的资源运行 etcd 只适合测试目的。为了在生产中部署，需要先进的硬件配置。在生产中部署 etcd 之前，请查看所需资源参考文档。
启动 etcd 集群 本节介绍如何启动单节点和多节点 etcd 集群。
单节点 etcd 集群 只为测试目的使用单节点 etcd 集群。</description>
    </item>
    
    <item>
      <title>为系统守护进程预留计算资源</title>
      <link>https://lijun.in/tasks/administer-cluster/reserve-compute-resources/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/administer-cluster/reserve-compute-resources/</guid>
      <description>Kubernetes 的节点可以按照 Capacity 调度。默认情况下 pod 能够使用节点全部可用容量。这是个问题，因为节点自己通常运行了不少驱动 OS 和 Kubernetes 的系统守护进程。除非为这些系统守护进程留出资源，否则它们将与 pod 争夺资源并导致节点资源短缺问题。
kubelet 公开了一个名为 Node Allocatable 的特性，有助于为系统守护进程预留计算资源。Kubernetes 推荐集群管理员按照每个节点上的工作负载密度配置 Node Allocatable。
. heading &amp;ldquo;prerequisites&amp;rdquo; %}} . include &amp;ldquo;task-tutorial-prereqs.md&amp;rdquo; &amp;gt;}} . version-check &amp;gt;}}
可分配的节点 Node Capacity --------------------------- | kube-reserved | |-------------------------| | system-reserved | |-------------------------| | eviction-threshold | |-------------------------| | | | allocatable | | (available for pods) | | | | | --------------------------- Kubernetes 节点上的 Allocatable 被定义为 pod 可用计算资源量。调度器不会超额申请 Allocatable。目前支持 CPU, memory 和 ephemeral-storage 这几个参数。</description>
    </item>
    
    <item>
      <title>为节点发布扩展资源</title>
      <link>https://lijun.in/tasks/administer-cluster/extended-resource-node/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/administer-cluster/extended-resource-node/</guid>
      <description>本文展示了如何为节点指定扩展资源。 扩展资源允许集群管理员发布节点级别的资源，这些资源在不进行发布的情况下无法被 Kubernetes 感知。
. heading &amp;ldquo;prerequisites&amp;rdquo; %}} . include &amp;ldquo;task-tutorial-prereqs.md&amp;rdquo; &amp;gt;}} . version-check &amp;gt;}}
获取您的节点名称 kubectl get nodes 选择您的一个节点用于此练习。
在您的一个节点上发布一种新的扩展资源 为在一个节点上发布一种新的扩展资源，需要发送一个 HTTP PATCH 请求到 Kubernetes API server。 例如：假设您的一个节点上带有四个 dongle 资源。下面是一个 PATCH 请求的示例， 该请求为您的节点发布四个 dongle 资源。
PATCH /api/v1/nodes/&amp;lt;your-node-name&amp;gt;/status HTTP/1.1 Accept: application/json Content-Type: application/json-patch+json Host: k8s-master:8080 [ { &amp;#34;op&amp;#34;: &amp;#34;add&amp;#34;, &amp;#34;path&amp;#34;: &amp;#34;/status/capacity/example.com~1dongle&amp;#34;, &amp;#34;value&amp;#34;: &amp;#34;4&amp;#34; } ] 注意：Kubernetes 不需要了解 dongle 资源的含义和用途。 前面的 PATCH 请求仅仅告诉 Kubernetes 您的节点拥有四个您称之为 dongle 的东西。
启动一个代理（proxy），以便您可以很容易地向 Kubernetes API server 发送请求：</description>
    </item>
    
    <item>
      <title>使用 CoreDNS 进行服务发现</title>
      <link>https://lijun.in/tasks/administer-cluster/coredns/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/administer-cluster/coredns/</guid>
      <description>此页面介绍了 CoreDNS 升级过程以及如何安装 CoreDNS 而不是 kube-dns。
. heading &amp;ldquo;prerequisites&amp;rdquo; %}} . include &amp;ldquo;task-tutorial-prereqs.md&amp;rdquo; &amp;gt;}} . version-check &amp;gt;}}
关于 CoreDNS CoreDNS 是一个灵活可扩展的 DNS 服务器，可以作为 Kubernetes 集群 DNS。与 Kubernetes 一样，CoreDNS 项目由 . glossary_tooltip text=&amp;quot;CNCF&amp;rdquo; term_id=&amp;quot;cncf&amp;rdquo; &amp;gt;}} 托管。
通过在现有的集群中替换 kube-dns，可以在集群中使用 CoreDNS 代替 kube-dns 部署，或者使用 kubeadm 等工具来为您部署和升级集群。
安装 CoreDNS 有关手动部署或替换 kube-dns，请参阅 CoreDNS GitHub 工程。
迁移到 CoreDNS 使用 kubeadm 升级现有集群 在 Kubernetes 1.10 及更高版本中，当您使用 kubeadm 升级使用 kube-dns 的集群时，您还可以迁移到 CoreDNS。 在本例中 kubeadm 将生成 CoreDNS 配置（&amp;ldquo;Corefile&amp;rdquo;）基于 kube-dns ConfigMap，保存联邦、存根域和上游名称服务器的配置。</description>
    </item>
    
    <item>
      <title>使用 KMS 提供商进行数据加密</title>
      <link>https://lijun.in/tasks/administer-cluster/kms-provider/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/administer-cluster/kms-provider/</guid>
      <description>本页展示了如何配置秘钥管理服务—— Key Management Service (KMS) 提供商和插件以启用数据加密。
. heading &amp;ldquo;prerequisites&amp;rdquo; %}}  . include &amp;ldquo;task-tutorial-prereqs.md&amp;rdquo; &amp;gt;}} . version-check &amp;gt;}}   需要 Kubernetes 1.10.0 或更新版本   需要 etcd v3 或更新版本  . feature-state for_k8s_version=&amp;quot;v1.12&amp;rdquo; state=&amp;quot;beta&amp;rdquo; &amp;gt;}}
KMS 加密提供商使用封套加密模型来加密 etcd 中的数据。数据使用数据加密秘钥（DEK）加密；每次加密都生成一个新的 DEK。这些 DEK 经一个秘钥加密秘钥（KEK）加密后在一个远端的 KMS 中存储和管理。KMS 提供商使用 gRPC 与一个特定的 KMS 插件通信。这个 KMS 插件作为一个 gRPC 服务器被部署在 Kubernetes 主服务器的同一个主机上，负责与远端 KMS 的通信。
配置 KMS 提供商 为了在 API 服务器上配置 KMS 提供商，在加密配置文件中的提供商数组中加入一个类型为 kms 的提供商，并设置下列属性：
 name: KMS 插件的显示名称。 endpoint: gRPC 服务器（KMS 插件）的监听地址。该端点是一个 UNIX 的套接字。 cachesize: 以明文缓存的数据加密秘钥（DEKs）的数量。一旦被缓存，就可以直接使用 DEKs 而无需另外调用 KMS；而未被缓存的 DEKs 需要调用一次 KMS 才能解包。 timeout: 在返回一个错误之前，kube-apiserver 等待 kms-plugin 响应的时间（默认是 3 秒）。  参见 理解静态数据加密配置</description>
    </item>
    
    <item>
      <title>使用 Kubernetes API 访问集群</title>
      <link>https://lijun.in/tasks/administer-cluster/access-cluster-api/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/administer-cluster/access-cluster-api/</guid>
      <description>本页展示了如何使用 Kubernetes API 访问集群
. heading &amp;ldquo;prerequisites&amp;rdquo; %}} . include &amp;ldquo;task-tutorial-prereqs.md&amp;rdquo; &amp;gt;}} . version-check &amp;gt;}}
访问集群 API 使用 kubectl 进行首次访问 首次访问 Kubernetes API 时，请使用 Kubernetes 命令行工具 kubectl 。
要访问集群，您需要知道集群位置并拥有访问它的凭证。通常，当您完成入门指南时，这会自动设置完成，或者由其他人设置好集群并将凭证和位置提供给您。
使用此命令检查 kubectl 已知的位置和凭证：
kubectl config view 许多[样例](https://github.com/kubernetes/examples/tree/. param &amp;ldquo;githubbranch&amp;rdquo; &amp;gt;}}/)提供了使用 kubectl 的介绍。完整文档请见 kubectl 手册。
直接访问 REST API kubectl 处理对 API 服务器的定位和身份验证。如果您想通过 http 客户端（如 curl 或 wget，或浏览器）直接访问 REST API，您可以通过多种方式对 API 服务器进行定位和身份验证：
 以代理模式运行 kubectl（推荐）。 推荐使用此方法，因为它用存储的 apiserver 位置并使用自签名证书验证 API 服务器的标识。使用这种方法无法进行中间人（MITM）攻击。 另外，您可以直接为 http 客户端提供位置和身份认证。这适用于被代理混淆的客户端代码。为防止中间人攻击，您需要将根证书导入浏览器。  使用 Go 或 Python 客户端库可以在代理模式下访问 kubectl。</description>
    </item>
    
    <item>
      <title>关键插件 Pod 的调度保证</title>
      <link>https://lijun.in/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/</guid>
      <description>除了在主机上运行的 Kubernetes 核心组件（如 api-server 、scheduler 、controller-manager）之外，还有许多插件，由于各种原因， 必须在常规集群节点（而不是 Kubernetes 主节点）上运行。 其中一些插件对于功能完备的群集至关重要，例如 Heapster、DNS 和 UI。 如果关键插件被逐出（手动或作为升级等其他操作的副作用）或者变成挂起状态，群集可能会停止正常工作。 关键插件进入挂起状态的例子有：集群利用率过高；被逐出的关键插件 Pod 释放了空间，但该空间被之前悬决的 Pod 占用；由于其它原因导致节点上可用资源的总量发生变化。
标记关键 Pod 要将 pod 标记为关键性（critical），pod 必须在 kube-system 命名空间中运行（可通过参数配置）。 同时，需要将 priorityClassName 设置为 system-cluster-critical 或 system-node-critical ，后者是整个群集的最高级别。 或者，也可以为 Pod 添加名为 scheduler.alpha.kubernetes.io/critical-pod、值为空字符串的注解。 不过，这一注解从 1.13 版本开始不再推荐使用，并将在 1.14 中删除。</description>
    </item>
    
    <item>
      <title>启用端点切片</title>
      <link>https://lijun.in/tasks/administer-cluster/enabling-endpointslices/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/administer-cluster/enabling-endpointslices/</guid>
      <description>本页提供启用 Kubernetes 端点切片的总览
. heading &amp;ldquo;prerequisites&amp;rdquo; %}} . include &amp;ldquo;task-tutorial-prereqs.md&amp;rdquo; &amp;gt;}} . version-check &amp;gt;}}
介绍 端点切片为 Kubernetes 端点提供了可伸缩和可扩展的替代方案。它们建立在端点提供的功能基础之上，并以可伸缩的方式进行扩展。当服务具有大量（&amp;gt;100）网络端点， 它们将被分成多个较小的端点切片资源，而不是单个大型端点资源。
启用端点切片 . feature-state for_k8s_version=&amp;quot;v1.16&amp;rdquo; state=&amp;quot;alpha&amp;rdquo; &amp;gt;}}
. note &amp;gt;}}
尽管端点切片最终可能会取代端点，但许多 Kubernetes 组件仍然依赖于端点。目前，启用端点切片应该被视为集群中端点的补充，而不是它们的替代。
. /note &amp;gt;}}
作为 Alpha 功能，默认情况下，Kubernetes 中未启用端点切片。启用端点切片需要对 Kubernetes 集群进行多达 3 项配置修改。
要启用包括端点切片的 Discovery API 组，请使用运行时配置标志（--runtime-config=discovery.k8s.io/v1alpha1=true）。
该逻辑负责监视服务，pod 和节点以及创建或更新与之关联，在端点切片控制器内的端点切片。 默认情况下，此功能处于禁用状态，但可以通过启用在 kube-controller-manager 控制器的标志（--controllers=endpointslice）来开启。
对于像 kube-proxy 这样的 Kubernetes 组件真正开始使用端点切片，需要开启端点切片功能标志（--feature-gates=EndpointSlice=true）。
使用端点切片 在集群中完全启用端点切片的情况下，您应该看到对应的每个端点资源的端点切片资源。除了兼容现有的端点功能，端点切片应包括拓扑等新的信息。它们将使集群中网络端点具有更强的可伸缩性，可扩展性。</description>
    </item>
    
    <item>
      <title>命名空间演练</title>
      <link>https://lijun.in/tasks/administer-cluster/namespaces-walkthrough/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/administer-cluster/namespaces-walkthrough/</guid>
      <description>Kubernetes . glossary_tooltip text=&amp;quot;命名空间&amp;rdquo; term_id=&amp;quot;namespace&amp;rdquo; &amp;gt;}} 有助于不同的项目、团队或客户去共享 Kubernetes 集群。
名字空间通过以下方式实现这点：
 为名字设置作用域. 为集群中的部分资源关联鉴权和策略的机制。  使用多个命名空间是可选的。
此示例演示了如何使用 Kubernetes 命名空间细分群集。
. heading &amp;ldquo;prerequisites&amp;rdquo; %}} . include &amp;ldquo;task-tutorial-prereqs.md&amp;rdquo; &amp;gt;}} . version-check &amp;gt;}}
环境准备 此示例作如下假设：
 您已拥有一个 配置好的 Kubernetes 集群。 您已对 Kubernetes 的 Pods, Services 和 Deployments 有基本理解。   理解默认命名空间  默认情况下，Kubernetes 集群会在配置集群时实例化一个默认命名空间，用以存放集群所使用的默认 Pods、Services 和 Deployments 集合。
假设您有一个新的集群，您可以通过执行以下操作来检查可用的命名空间：
kubectl get namespaces NAME STATUS AGE default Active 13m 创建新的命名空间 在本练习中，我们将创建两个额外的 Kubernetes 命名空间来保存我们的内容。
我们假设一个场景，某组织正在使用共享的 Kubernetes 集群来支持开发和生产：</description>
    </item>
    
    <item>
      <title>在 Kubernetes 集群中使用 NodeLocal DNSCache</title>
      <link>https://lijun.in/tasks/administer-cluster/nodelocaldns/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/administer-cluster/nodelocaldns/</guid>
      <description>本页概述了 Kubernetes 中的 NodeLocal DNSCache 功能。
. heading &amp;ldquo;prerequisites&amp;rdquo; %}} . include &amp;ldquo;task-tutorial-prereqs.md&amp;rdquo; &amp;gt;}} . version-check &amp;gt;}}
引言 NodeLocal DNSCache 通过在集群节点上作为 DaemonSet 运行 dns 缓存代理来提高集群 DNS 性能。 在当今的体系结构中，处于 ClusterFirst DNS 模式的 Pod 可以连接到 kube-dns serviceIP 进行 DNS 查询。 通过 kube-proxy 添加的 iptables 规则将其转换为 kube-dns/CoreDNS 端点。 借助这种新架构，Pods 将可以访问在同一节点上运行的 dns 缓存代理，从而避免了 iptables DNAT 规则和连接跟踪。 本地缓存代理将查询 kube-dns 服务以获取集群主机名的缓存缺失（默认为 cluster.local 后缀）。
动机  使用当前的 DNS 体系结构，如果没有本地 kube-dns/CoreDNS 实例，则具有最高 DNS QPS 的 Pod 可能必须延伸到另一个节点。 在这种脚本下，拥有本地缓存将有助于改善延迟。   跳过 iptables DNAT 和连接跟踪将有助于减少 conntrack 竞争并避免 UDP DNS 条目填满 conntrack 表。   从本地缓存代理到 kube-dns 服务的连接可以升级到 TCP 。 TCP conntrack 条目将在连接关闭时被删除，相反 UDP 条目必须超时(默认 nf_conntrack_udp_timeout 是 30 秒)   将 DNS 查询从 UDP 升级到 TCP 将减少归因于丢弃的 UDP 数据包和 DNS 超时的尾部等待时间，通常长达 30 秒（3 次重试+ 10 秒超时）。   在节点级别对 dns 请求的度量和可见性。   可以重新启用负缓存，从而减少对 kube-dns 服务的查询数量。  架构图 启用 NodeLocal DNSCache 之后，这是 DNS 查询所遵循的路径：</description>
    </item>
    
    <item>
      <title>在 Kubernetes 集群中使用 sysctl</title>
      <link>https://lijun.in/tasks/administer-cluster/sysctl-cluster/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/administer-cluster/sysctl-cluster/</guid>
      <description>. feature-state for_k8s_version=&amp;quot;v1.11&amp;rdquo; state=&amp;quot;beta&amp;rdquo; &amp;gt;}}
本文档介绍如何通过 sysctl 接口在 Kubernetes 集群中配置和使用内核参数。
. heading &amp;ldquo;prerequisites&amp;rdquo; %}} . include &amp;ldquo;task-tutorial-prereqs.md&amp;rdquo; &amp;gt;}} . version-check &amp;gt;}}
获取 Sysctl 的参数列表 在 Linux 中，管理员可以通过 sysctl 接口修改内核运行时的参数。在 /proc/sys/ 虚拟文件系统下存放许多内核参数。这些参数涉及了多个内核子系统，如：
 内核子系统 (通常前缀为: kernel.) 网络子系统 (通常前缀为: net.) 虚拟内存子系统 (通常前缀为: vm.) MDADM 子系统 (通常前缀为: dev.) 更多子系统请参见 内核文档。  若要获取完整的参数列表，请执行以下命令
$ sudo sysctl -a 启用非安全的 Sysctl 参数 sysctl 参数分为 安全 和 非安全的。安全 sysctl 参数除了需要设置恰当的命名空间外，在同一 node 上的不同 Pod 之间也必须是 相互隔离的。这意味着在 Pod 上设置 安全 sysctl 参数</description>
    </item>
    
    <item>
      <title>在实时集群上重新配置节点的 Kubelet</title>
      <link>https://lijun.in/tasks/administer-cluster/reconfigure-kubelet/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/administer-cluster/reconfigure-kubelet/</guid>
      <description>. feature-state for_k8s_version=&amp;quot;v1.11&amp;rdquo; state=&amp;quot;beta&amp;rdquo; &amp;gt;}}
动态Kubelet配置 引导你在一个运行的 Kubernetes 集群上更改每一个 Kubelet 的配置，通过部署 ConfigMap 并配置每个节点来使用它。
. warning &amp;gt;}}
警告：所有Kubelet配置参数都可以动态地更改，但这对某些参数来说是不安全的。在决定动态更改参数之前，你需要深刻理解这种变化将如何影响你的集群的行为。 在把一组节点推广到集群范围之前，都要仔细地测试这些节点上的配置变化。与配置相关的建议可以在具体的文件下找到，内联 KubeletConfiguration 类型文档。 . /warning &amp;gt;}}
. heading &amp;ldquo;prerequisites&amp;rdquo; %}}  Kubernetes v1.11 或者更高版本配置在主节点和节点上 kubectl v1.11 或者更高版本和集群配置通信 The Kubelet --dynamic-config-dir flag 必须设置在节点的可写目录上  在你集群中的一个实时节点上配置Kubelet 基本工作流程概述 在实时集群中配置 Kubelet 的基本工作流程如下：
 编写一个 YAML 或 JSON 的配置文件包含 Kubelet 的配置。 将此文件包装在 ConfigMap 中并将其保存到 Kubernetes 控制平面。 更新 Kubelet 的相应节点对象以使用此 ConfigMap。  每个 Kubelet 都会在其各自的节点对象上查看配置引用。当此引用更改时，Kubelet 将下载新配置， 更新本地引用以引用该文件，然后退出。 要想使该功能正常地工作，您必须运行操作系统级别的服务管理器（如systemd），如果退出，将重启Kubelet。 当Kubelet重新启动时，它将开始使用新配置。</description>
    </item>
    
    <item>
      <title>声明网络策略</title>
      <link>https://lijun.in/tasks/administer-cluster/declare-network-policy/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/administer-cluster/declare-network-policy/</guid>
      <description>本文可以帮助您开始使用 Kubernetes 的 NetworkPolicy API 声明网络策略去管理 Pod 之间的通信
. heading &amp;ldquo;prerequisites&amp;rdquo; %}} 您首先需要有一个支持网络策略的 Kubernetes 集群。已经有许多支持 NetworkPolicy 的网络提供商，包括：
 Calico Romana Weave 网络  注意：以上列表是根据产品名称按字母顺序排序，而不是按推荐或偏好排序。下面示例对于使用了上面任何提供商的 Kubernetes 集群都是有效的
创建一个nginx deployment 并且通过服务将其暴露 为了查看 Kubernetes 网络策略是怎样工作的，可以从创建一个nginx deployment 并且通过服务将其暴露开始
$ kubectl run nginx --image=nginx --replicas=2 deployment &amp;quot;nginx&amp;quot; created $ kubectl expose deployment nginx --port=80 service &amp;quot;nginx&amp;quot; exposed 在 default 命名空间下运行了两个 nginx pod，而且通过一个名字为 nginx 的服务进行了暴露
$ kubectl get svc,pod NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE svc/kubernetes 10.100.0.1 &amp;lt;none&amp;gt; 443/TCP 46m svc/nginx 10.</description>
    </item>
    
    <item>
      <title>开发云控制器管理器</title>
      <link>https://lijun.in/tasks/administer-cluster/developing-cloud-controller-manager/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/administer-cluster/developing-cloud-controller-manager/</guid>
      <description>. feature-state for_k8s_version=&amp;quot;v1.11&amp;rdquo; state=&amp;quot;beta&amp;rdquo; &amp;gt;}}
在即将发布的版本中，云控制器管理器将是把 Kubernetes 与任何云集成的首选方式。 这将确保驱动可以独立于核心 Kubernetes 发布周期开发其功能。
. feature-state for_k8s_version=&amp;quot;1.8&amp;rdquo; state=&amp;quot;alpha&amp;rdquo; &amp;gt;}}
在讨论如何构建自己的云控制器管理器之前，了解有关它如何工作的一些背景知识是有帮助的。云控制器管理器是来自 kube-controller-manager 的代码，利用 Go 接口允许插入任何云的实现。大多数框架和通用控制器的实现在 core，但只要满足 云提供者接口，它就会始终执行它所提供的云接口。
为了深入了解实施细节，所有云控制器管理器都将从 Kubernetes 核心导入依赖包，唯一的区别是每个项目都会通过调用 cloudprovider.RegisterCloudProvider 来注册自己的驱动，更新可用驱动的全局变量。
开发 Out of Tree 要为您的云构建一个 out-of-tree 云控制器管理器，请按照下列步骤操作：
 使用满足 cloudprovider.Interface 的实现创建一个 go 包。 使用来自 Kubernetes 核心包的 cloud-controller-manager 中的 main.go 作为 main.go 的模板。如上所述，唯一的区别应该是将导入的云包。 在 main.go 中导入你的云包，确保你的包有一个 init 块来运行 cloudprovider.RegisterCloudProvider。  用现有的 out-of-tree 云驱动作为例子可能会有所帮助。你可以在这里找到 清单。
In Tree 对于 in-tree 驱动，您可以将 in-tree 云控制器管理器作为群集中的 Daemonset 运行。有关详细信息，请参阅 运行的云控制器管理器文档。</description>
    </item>
    
    <item>
      <title>控制节点上的 CPU 管理策略</title>
      <link>https://lijun.in/tasks/administer-cluster/cpu-management-policies/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/administer-cluster/cpu-management-policies/</guid>
      <description>. feature-state for_k8s_version=&amp;quot;v1.12&amp;rdquo; state=&amp;quot;beta&amp;rdquo; &amp;gt;}}
按照设计，Kubernetes 对 pod 执行相关的很多方面进行了抽象，使得用户不必关心。然 而，为了正常运行，有些工作负载要求在延迟和/或性能方面有更强的保证。 为此，kubelet 提供方法来实现更复杂的负载放置策略，同时保持抽象，避免显式的放置指令。
. heading &amp;ldquo;prerequisites&amp;rdquo; %}} . include &amp;ldquo;task-tutorial-prereqs.md&amp;rdquo; &amp;gt;}} . version-check &amp;gt;}}
CPU 管理策略 默认情况下，kubelet 使用 CFS 配额 来执行 pod 的 CPU 约束。当节点上运行了很多 CPU 密集的 pod 时，工作负载可能会迁移到不同的 CPU 核，这取决于调度时 pod 是否被扼制，以及哪些 CPU 核是可用的。许多工作负载对这种迁移不敏感，因此无需任何干预即可正常工作。
然而，有些工作负载的性能明显地受到 CPU 缓存亲和性以及调度延迟的影响，对此，kubelet 提供了可选的 CPU 管理策略，来确定节点上的一些分配偏好。
配置 CPU 管理器（CPU Manager）作为 alpha 特性引入 Kubernetes 1.8 版本。从 1.10 版本开始，作为 beta 特性默认开启。
CPU 管理策略通过 kubelet 参数 --cpu-manager-policy 来指定。支持两种策略：
 none: 默认策略，表示现有的调度行为。 static: 允许为节点上具有某些资源特征的 pod 赋予增强的 CPU 亲和性和独占性。  CPU 管理器定期通过 CRI 写入资源更新，以保证内存中 CPU 分配与 cgroupfs 一致。同步频率通过新增的 Kubelet 配置参数 --cpu-manager-reconcile-period 来设置。 如果不指定，默认与 --node-status-update-frequency 的周期相同。</description>
    </item>
    
    <item>
      <title>控制节点上的拓扑管理策略</title>
      <link>https://lijun.in/tasks/administer-cluster/topology-manager/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/administer-cluster/topology-manager/</guid>
      <description>. feature-state state=&amp;quot;alpha&amp;rdquo; &amp;gt;}}
越来越多的系统利用 CPU 和硬件加速器的组合来支持对延迟要求较高的任务和高吞吐量的并行计算。 这类负载包括电信、科学计算、机器学习、金融服务和数据分析等。 此类混合系统即用于构造这些高性能环境。
为了获得最佳性能，需要进行与 CPU 隔离、内存和设备局部性有关的优化。 但是，在 Kubernetes 中，这些优化由各自独立的组件集合来处理。
拓扑管理器（Topology Manager） 是一个 Kubelet 的一部分，旨在协调负责这些优化的一组组件。
. heading &amp;ldquo;prerequisites&amp;rdquo; %}} . include &amp;ldquo;task-tutorial-prereqs.md&amp;rdquo; &amp;gt;}} . version-check &amp;gt;}}
拓扑管理器如何工作 在引入拓扑管理器之前， Kubernetes 中的 CPU 和设备管理器相互独立地做出资源分配决策。 这可能会导致在多处理系统上出现并非期望的资源分配；由于这些与期望相左的分配，对性能或延迟敏感的应用将受到影响。 这里的不符合期望意指，例如， CPU 和设备是从不同的 NUMA 节点分配的，因此会导致额外的延迟。
拓扑管理器是一个 Kubelet 组件，扮演信息源的角色，以便其他 Kubelet 组件可以做出与拓扑结构相对应的资源分配决定。
拓扑管理器为组件提供了一个称为 建议供应者（Hint Providers） 的接口，以发送和接收拓扑信息。 拓扑管理器具有一组节点级策略，具体说明如下。
拓扑管理器从 建议提供者 接收拓扑信息，作为表示可用的 NUMA 节点和首选分配指示的位掩码。 拓扑管理器策略对所提供的建议执行一组操作，并根据策略对提示进行约减以得到最优解；如果存储了与预期不符的建议，则该建议的优选字段将被设置为 false。 在当前策略中，首选的是最窄的优选掩码。 所选建议将被存储为拓扑管理器的一部分。 取决于所配置的策略，所选建议可用来决定节点接受或拒绝 Pod 。 之后，建议会被存储在拓扑管理器中，供 建议提供者 进行资源分配决策时使用。
拓扑管理器策略 当前拓扑管理器：</description>
    </item>
    
    <item>
      <title>搭建高可用的 Kubernetes Masters</title>
      <link>https://lijun.in/tasks/administer-cluster/highly-available-master/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/administer-cluster/highly-available-master/</guid>
      <description>. feature-state for_k8s_version=&amp;quot;1.5&amp;rdquo; state=&amp;quot;alpha&amp;rdquo; &amp;gt;}}
您可以在谷歌计算引擎（GCE）的 kubeup 或 kube-down 脚本中复制 Kubernetes Master。 本文描述了如何使用 kube-up/down 脚本来管理高可用（HA）的 Master，以及如何使用 GCE 实现高可用 Master。
. heading &amp;ldquo;prerequisites&amp;rdquo; %}} . include &amp;ldquo;task-tutorial-prereqs.md&amp;rdquo; &amp;gt;}} . version-check &amp;gt;}}
启动一个兼容高可用的集群 要创建一个新的兼容高可用的集群，您必须在 kubeup 脚本中设置以下标志:
  MULTIZONE=true - 为了防止从不同于 Master 默认区域的区域中删除 kubelets 副本。如果您希望在不同的区域运行 Master 副本，那么这一项是必需并且推荐的。
  ENABLE_ETCD_QUORUM_READ=true - 确保从所有 API 服务器读取数据时将返回最新的数据。如果为 true，读操作将被定向到 leader etcd 副本。可以选择将这个值设置为 true，那么读取将更可靠，但也会更慢。
  您还可以指定一个 GCE 区域，在这里创建第一个 Master 副本。设置以下标志:
 KUBE_GCE_ZONE=zone - 将运行第一个 Master 副本的区域。  下面的命令演示在 GCE europe-west1-b 区域中设置一个兼容高可用的集群:</description>
    </item>
    
    <item>
      <title>改变默认 StorageClass</title>
      <link>https://lijun.in/tasks/administer-cluster/change-default-storage-class/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/administer-cluster/change-default-storage-class/</guid>
      <description>本文展示了如何改变默认的 Storage Class，它用于为没有特殊需求的 PersistentVolumeClaims 配置 volumes。
. heading &amp;ldquo;prerequisites&amp;rdquo; %}} . include &amp;ldquo;task-tutorial-prereqs.md&amp;rdquo; &amp;gt;}} . version-check &amp;gt;}}
为什么要改变默认 storage class？ 取决于安装模式，您的 Kubernetes 集群可能和一个被标记为默认的已有 StorageClass 一起部署。这个默认的 StorageClass 以后将被用于动态的为没有特定 storage class 需求的 PersistentVolumeClaims 配置存储。更多细节请查看 PersistentVolumeClaim 文档。
预先安装的默认 StorageClass 可能不能很好的适应您期望的工作负载；例如，它配置的存储可能太过昂贵。如果是这样的话，您可以改变默认 StorageClass，或者完全禁用它以防止动态配置存储。
简单的删除默认 StorageClass 可能行不通，因为它可能会被您集群中的扩展管理器自动重建。请查阅您的安装文档中关于扩展管理器的细节，以及如何禁用单个扩展。
改变默认 StorageClass   列出您集群中的 StorageClasses：
kubectl get storageclass 输出类似这样：
NAME PROVISIONER AGE standard (default) kubernetes.io/gce-pd 1d gold kubernetes.io/gce-pd 1d 默认 StorageClass 以 (default) 标记。
  标记默认 StorageClass 非默认：</description>
    </item>
    
    <item>
      <title>更改 PersistentVolume 的回收策略</title>
      <link>https://lijun.in/tasks/administer-cluster/change-pv-reclaim-policy/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/administer-cluster/change-pv-reclaim-policy/</guid>
      <description>本文展示了如何更改 Kubernetes PersistentVolume 的回收策略。
. heading &amp;ldquo;prerequisites&amp;rdquo; %}} . include &amp;ldquo;task-tutorial-prereqs.md&amp;rdquo; &amp;gt;}} . version-check &amp;gt;}}
为什么要更改 PersistentVolume 的回收策略 PersistentVolumes 可以有多种回收策略，包括 &amp;ldquo;Retain&amp;rdquo;、&amp;ldquo;Recycle&amp;rdquo; 和 &amp;ldquo;Delete&amp;rdquo;。对于动态配置的 PersistentVolumes 来说，默认回收策略为 &amp;ldquo;Delete&amp;rdquo;。这表示当用户删除对应的 PersistentVolumeClaim 时，动态配置的 volume 将被自动删除。如果 volume 包含重要数据时，这种自动行为可能是不合适的。那种情况下，更适合使用 &amp;ldquo;Retain&amp;rdquo; 策略。使用 &amp;ldquo;Retain&amp;rdquo; 时，如果用户删除 PersistentVolumeClaim，对应的 PersistentVolume 不会被删除。相反，它将变为 Released 状态，表示所有的数据可以被手动恢复。
更改 PersistentVolume 的回收策略   列出你集群中的 PersistentVolumes
kubectl get pv  输出类似于这样：
 NAME CAPACITY ACCESSMODES RECLAIMPOLICY STATUS CLAIM REASON AGE pvc-b6efd8da-b7b5-11e6-9d58-0ed433a7dd94 4Gi RWO Delete Bound default/claim1 10s pvc-b95650f8-b7b5-11e6-9d58-0ed433a7dd94 4Gi RWO Delete Bound default/claim2 6s pvc-bb3ca71d-b7b5-11e6-9d58-0ed433a7dd94 4Gi RWO Delete Bound default/claim3 3s  这个列表同样包含了绑定到每个 volume 的 claims 名称，以便更容易的识别动态配置的 volumes。</description>
    </item>
    
    <item>
      <title>自定义 DNS 服务</title>
      <link>https://lijun.in/tasks/administer-cluster/dns-custom-nameservers/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/administer-cluster/dns-custom-nameservers/</guid>
      <description>本页说明如何配置 DNS Pod 和自定义 DNS 解析过程。 在 Kubernetes 1.11 和更高版本中，CoreDNS 位于 GA 并且默认情况下与 kubeadm 一起安装。 请参见CoreDNS 的 ConfigMap 选项 and 使用 CoreDNS 进行服务发现。
. heading &amp;ldquo;prerequisites&amp;rdquo; %}}  . include &amp;ldquo;task-tutorial-prereqs.md&amp;rdquo; &amp;gt;}} . version-check &amp;gt;}} Kubernetes 版本 1.6 或更新。如果与 CoreDNS 匹配，版本 1.9 或更新。 合适的 add-on 插件: kube-dns 或 CoreDNS. 使用 kubeadm 安装，请参见 kubeadm 帮助文档.  介绍 DNS 是使用插件管理器[集群 add-on](http://releases.k8s.io/. param &amp;ldquo;githubbranch&amp;rdquo; &amp;gt;}}/cluster/addons/README.md)自动启动的内置的 Kubernetes 服务。
从 Kubernetes v1.12 开始，CoreDNS 是推荐的 DNS 服务器，取代了kube-dns。 但是，默认情况下，某些 Kubernetes 安装程序工具仍可能安装 kube-dns。 请参阅安装程序提供的文档，以了解默认情况下安装了哪个 DNS 服务器。</description>
    </item>
    
    <item>
      <title>访问集群上运行的服务</title>
      <link>https://lijun.in/tasks/administer-cluster/access-cluster-services/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/administer-cluster/access-cluster-services/</guid>
      <description>本文展示了如何连接 Kubernetes 集群上运行的服务。
. heading &amp;ldquo;prerequisites&amp;rdquo; %}} . include &amp;ldquo;task-tutorial-prereqs.md&amp;rdquo; &amp;gt;}} . version-check &amp;gt;}}
访问集群上运行的服务 在 Kubernetes 里， nodes、pods 和 services 都有它们自己的 IP。许多情况下，集群上的 node IP、pod IP 和某些 service IP 路由不可达，所以不能从一个集群之外的节点访问它们，例如从你自己的台式机。
连接方式 你有多种从集群外连接 nodes、pods 和 services 的选项：
 通过公共 IP 访问 services。  使用具有 NodePort 或 LoadBalancer 类型的 service，可以从外部访问它们。请查阅 services 和 kubectl expose 文档。 取决于你的集群环境，你可以仅把 service 暴露在你的企业网络环境中，也可以将其暴露在因特网上。需要考虑暴露的 service 是否安全，它是否有自己的用户认证？ 将 pods 放置于 services 背后。如果要访问一个副本集合中特定的 pod，例如用于调试目的时，请给 pod 指定一个独特的标签并创建一个新 service 选择这个标签。 大部分情况下，都不需要应用开发者通过节点 IP 直接访问 nodes。   通过 Proxy Verb 访问 services、nodes 或者 pods。  在访问 Apiserver 远程服务之前是否经过认证和授权？如果你的服务暴露到因特网中不够安全，或者需要获取 node IP 之上的端口，又或者处于调试目的时，请使用这个特性。 Proxies 可能给某些应用带来麻烦。 仅适用于 HTTP/HTTPS。 在这里描述   从集群中的 node 或者 pod 访问。  运行一个 pod，然后使用 kubectl exec 连接到它的一个shell。从那个 shell 连接其他的 nodes、pods 和 services。 某些集群可能允许你 ssh 到集群中的节点。你可能可以从那儿访问集群服务。这是一个非标准的方式，可能在一些集群上能工作，但在另一些上却不能。浏览器和其他工具可能安装或可能不会安装。集群 DNS 可能不会正常工作。    发现内置服务 典型情况下，kube-system 会启动集群中的几个服务。使用 kubectl cluster-info 命令获取它们的列表：</description>
    </item>
    
    <item>
      <title>通过命名空间共享集群</title>
      <link>https://lijun.in/tasks/administer-cluster/namespaces/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/administer-cluster/namespaces/</guid>
      <description>本页展示了如何查看、使用和删除. glossary_tooltip text=&amp;quot;namespaces&amp;rdquo; term_id=&amp;quot;namespace&amp;rdquo; &amp;gt;}}。本页同时展示了如何使用 Kubernetes 命名空间去细分集群。
. heading &amp;ldquo;prerequisites&amp;rdquo; %}}  您已拥有一个 配置好的 Kubernetes 集群. 您已对 Kubernetes 的 Pods, Services, 和 Deployments 有基本理解。  查看命名空间  列出集群中现有的命名空间：  kubectl get namespaces NAME STATUS AGE default Active 11d kube-system Active 11d kube-public Active 11d 初始状态下，Kubernetes 具有三个名字空间：
 default 无命名空间对象的默认命名空间 kube-system 由 Kubernetes 系统创建的对象的命名空间 kube-public 自动创建且被所有用户可读的命名空间（包括未经身份认证的）。此命名空间通常在某些资源在整个集群中可见且可公开读取时被集群使用。此命名空间的公共方面只是一个约定，而不是一个必要条件。  您还可以通过下列命令获取特定命名空间的摘要：
kubectl get namespaces &amp;lt;name&amp;gt; 或获取详细信息：
kubectl describe namespaces &amp;lt;name&amp;gt; Name: default Labels: &amp;lt;none&amp;gt; Annotations: &amp;lt;none&amp;gt; Status: Active No resource quota.</description>
    </item>
    
    <item>
      <title>通过配置文件设置 Kubelet 参数</title>
      <link>https://lijun.in/tasks/administer-cluster/kubelet-config-file/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/administer-cluster/kubelet-config-file/</guid>
      <description>. feature-state state=&amp;quot;beta&amp;rdquo; &amp;gt;}}
通过保存在硬盘的配置文件设置 Kubelet 的配置参数子集，可以作为命令行参数的替代。此功能在 v1.10 中为 beta 版。
建议通过配置文件的方式提供参数，因为这样可以简化节点部署和配置管理。
. heading &amp;ldquo;prerequisites&amp;rdquo; %}}  需要安装 1.10 或更高版本的 Kubelet 二进制文件，才能实现 beta 功能。  创建配置文件 KubeletConfiguration 结构体定义了可以通过文件配置的 Kubelet 配置子集，该结构体在 [这里（v1beta1）](https://github.com/kubernetes/kubernetes/blob/. param &amp;ldquo;docsbranch&amp;rdquo; &amp;gt;}}/staging/src/k8s.io/kubelet/config/v1beta1/types.go) 可以找到, 配置文件必须是这个结构体中参数的 JSON 或 YAML 表现形式。
在单独的文件夹中创建一个名为 kubelet 的文件，并保证 Kubelet 可以读取该文件夹及文件。您应该在这个 kubelet 文件中编写 Kubelet 配置。
这是一个 Kubelet 配置文件示例：
kind: KubeletConfiguration apiVersion: kubelet.config.k8s.io/v1beta1 evictionHard: memory.available: &amp;quot;200Mi&amp;quot; 在这个示例中, 当可用内存低于200Mi 时, Kubelet 将会开始驱逐 Pods。 没有声明的其余配置项都将使用默认值, 命令行中的 flags 将会覆盖配置文件中的对应值。
作为一个小技巧，您可以从活动节点生成配置文件，相关方法请查看 重新配置活动集群节点的 Kubelet。</description>
    </item>
    
    <item>
      <title>配置 API 对象配额</title>
      <link>https://lijun.in/tasks/administer-cluster/quota-api-object/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/administer-cluster/quota-api-object/</guid>
      <description>本文讨论如何为 API 对象配置配额，包括 PersistentVolumeClaims 和 Services。 配额限制了可以在命名空间中创建的特定类型对象的数量。 您可以在 [ResourceQuota](/docs/reference/generated/kubernetes-api/. param &amp;ldquo;version&amp;rdquo; &amp;gt;}}/#resourcequota-v1-core) 对象中指定配额。
. heading &amp;ldquo;prerequisites&amp;rdquo; %}} . include &amp;ldquo;task-tutorial-prereqs.md&amp;rdquo; &amp;gt;}} . version-check &amp;gt;}}
创建命名空间 创建一个命名空间以便本例中创建的资源和集群中的其余部分相隔离。
kubectl create namespace quota-object-example 创建 ResourceQuota 下面是一个 ResourceQuota 对象的配置文件：
. codenew file=&amp;quot;admin/resource/quota-objects.yaml&amp;rdquo; &amp;gt;}}
创建 ResourceQuota
kubectl create -f https://k8s.io/examples/admin/resource/quota-objects.yaml --namespace=quota-object-example 查看 ResourceQuota 的详细信息：
kubectl get resourcequota object-quota-demo --namespace=quota-object-example --output=yaml 输出结果表明在 quota-object-example 命名空间中，至多只能有一个 PersistentVolumeClaim，最多两个 LoadBalancer 类型的服务，不能有 NodePort 类型的服务。
status: hard: persistentvolumeclaims: &amp;#34;1&amp;#34; services.loadbalancers: &amp;#34;2&amp;#34; services.nodeports: &amp;#34;0&amp;#34; used: persistentvolumeclaims: &amp;#34;0&amp;#34; services.</description>
    </item>
    
    <item>
      <title>配置多个调度器</title>
      <link>https://lijun.in/tasks/administer-cluster/configure-multiple-schedulers/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/administer-cluster/configure-multiple-schedulers/</guid>
      <description>Kubernetes 自带了一个默认调度器，其详细描述请查阅这里。
如果默认调度器不适合您的需求，您可以实现自己的调度器。
不仅如此，您甚至可以伴随着默认调度器同时运行多个调度器，并告诉 Kubernetes 为每个 pod 使用什么调度器。 让我们通过一个例子讲述如何在 Kubernetes 中运行多个调度器。
关于实现调度器的具体细节描述超出了本文范围。 请参考 kube-scheduler 的实现，规范示例代码位于 [pkg/scheduler](https://github.com/kubernetes/kubernetes/tree/. param &amp;ldquo;githubbranch&amp;rdquo; &amp;gt;}}/pkg/scheduler)。
. heading &amp;ldquo;prerequisites&amp;rdquo; %}} . include &amp;ldquo;task-tutorial-prereqs.md&amp;rdquo; &amp;gt;}} . version-check &amp;gt;}}
打包调度器 将调度器二进制文件打包到容器镜像中。出于示例目的，我们就使用默认调度器（kube-scheduler）作为我们的第二个调度器。
从 Github 克隆 Kubernetes 源代码，并编译构建源代码。
git clone https://github.com/kubernetes/kubernetes.git cd kubernetes make 创建一个包含 kube-scheduler 二进制文件的容器镜像。用于构建镜像的 Dockerfile 内容如下：
FROMbusyboxADD ./_output/dockerized/bin/linux/amd64/kube-scheduler /usr/local/bin/kube-scheduler将文件保存为 Dockerfile，构建镜像并将其推送到镜像仓库。 此示例将镜像推送到 Google 容器镜像仓库（GCR）。
有关详细信息，请阅读 GCR 文档。
docker build -t gcr.io/my-gcp-project/my-kube-scheduler:1.0 . gcloud docker -- push gcr.io/my-gcp-project/my-kube-scheduler:1.0 为调度器定义 Kubernetes Deployment 现在我们将调度器放在容器镜像中，我们可以为它创建一个 pod 配置，并在我们的 Kubernetes 集群中运行它。 但是与其在集群中直接创建一个 pod，不如使用 Deployment。 Deployment 管理一个 Replica Set，Replica Set 再管理 pod，从而使调度器能够适应故障。 以下是 Deployment 配置，被保存为 my-scheduler.</description>
    </item>
    
    <item>
      <title>配置资源不足时的处理方式</title>
      <link>https://lijun.in/tasks/administer-cluster/out-of-resource/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/administer-cluster/out-of-resource/</guid>
      <description>本页介绍了如何使用kubelet配置资源不足时的处理方式。
当可用计算资源较少时，kubelet需要保证节点稳定性。这在处理如内存和硬盘之类的不可压缩资源时尤为重要。如果任意一种资源耗尽，节点将会变得不稳定。
驱逐策略 kubelet 能够主动监测和防止计算资源的全面短缺。在那种情况下，kubelet可以主动地结束一个或多个 pod 以回收短缺的资源。当 kubelet 结束一个 pod 时，它将终止 pod 中的所有容器，而 pod 的 PodPhase 将变为 Failed。
如果被驱逐的 Pod 由 Deployment 管理，这个 Deployment 会创建另一个 Pod 给 Kubernetes 来调度。
驱逐信号 kubelet 支持按照以下表格中描述的信号触发驱逐决定。每个信号的值在 description 列描述，基于 kubelet 摘要 API。
   驱逐信号 描述     memory.available memory.available := node.status.capacity[memory] - node.stats.memory.workingSet   nodefs.available nodefs.available := node.stats.fs.available   nodefs.inodesFree nodefs.inodesFree := node.stats.fs.inodesFree   imagefs.available imagefs.available := node.stats.runtime.imagefs.available   imagefs.</description>
    </item>
    
    <item>
      <title>限制存储消耗</title>
      <link>https://lijun.in/tasks/administer-cluster/limit-storage-consumption/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/administer-cluster/limit-storage-consumption/</guid>
      <description>此示例演示了一种限制命名空间中存储使用量的简便方法。
演示中用到了以下资源：ResourceQuota，LimitRange 和 PersistentVolumeClaim。
. heading &amp;ldquo;prerequisites&amp;rdquo; %}}  . include &amp;ldquo;task-tutorial-prereqs.md&amp;rdquo; &amp;gt;}} . version-check &amp;gt;}}  场景：限制存储消耗 集群管理员代表用户群操作集群，管理员希望控制单个名称空间可以消耗多少存储空间以控制成本。
管理员想要限制：
 命名空间中持久卷申领（persistent volume claims）的数量 每个申领（claim）可以请求的存储量 命名空间可以具有的累计存储量  使用 LimitRange 限制存储请求 将 LimitRange 添加到命名空间会为存储请求大小强制设置最小值和最大值。存储是通过 PersistentVolumeClaim 来发起请求的。执行限制范围控制的准入控制器会拒绝任何高于或低于管理员所设阈值的 PVC。
在此示例中，请求 10Gi 存储的 PVC 将被拒绝，因为它超过了最大 2Gi。
apiVersion: v1 kind: LimitRange metadata: name: storagelimits spec: limits: - type: PersistentVolumeClaim max: storage: 2Gi min: storage: 1Gi 当底层存储提供程序需要某些最小值时，将会用到所设置最小存储请求值。例如，AWS EBS volumes 的最低要求为 1Gi。
使用 StorageQuota 限制 PVC 数目和累计存储容量 管理员可以限制某个命名空间中的 PVCs 个数以及这些 PVCs 的累计容量。新 PVCs 请求如果超过任一上限值将被拒绝。</description>
    </item>
    
    <item>
      <title>集群 DNS 服务自动伸缩</title>
      <link>https://lijun.in/tasks/administer-cluster/dns-horizontal-autoscaling/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/administer-cluster/dns-horizontal-autoscaling/</guid>
      <description>本页展示了如何在集群中启用和配置 DNS 服务的自动伸缩功能。
. heading &amp;ldquo;prerequisites&amp;rdquo; %}}  . include &amp;ldquo;task-tutorial-prereqs.md&amp;rdquo; &amp;gt;}} . version-check &amp;gt;}}    本指南假设您的节点使用 AMD64 或 Intel 64 CPU 架构
  确保已启用 DNS 功能本身。
  建议使用 Kubernetes 1.4.0 或更高版本。
  确定是否 DNS 水平 水平自动伸缩特性已经启用 在 kube-system 命名空间中列出集群中的 . glossary_tooltip text=&amp;quot;Deployments&amp;rdquo; term_id=&amp;quot;deployment&amp;rdquo; &amp;gt;}} ：
kubectl get deployment --namespace=kube-system 输出类似如下这样：
NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE ... dns-autoscaler 1 1 1 1 ... ...  如果在输出中看到 “dns-autoscaler”，说明 DNS 水平自动伸缩已经启用，可以跳到 调优自动伸缩参数。</description>
    </item>
    
    <item>
      <title>集群安全</title>
      <link>https://lijun.in/tasks/administer-cluster/securing-a-cluster/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/administer-cluster/securing-a-cluster/</guid>
      <description>本文档涉及与保护集群免受意外或恶意访问有关的主题，并对总体安全性提出建议。
. heading &amp;ldquo;prerequisites&amp;rdquo; %}}  . include &amp;ldquo;task-tutorial-prereqs.md&amp;rdquo; &amp;gt;}} . version-check &amp;gt;}}  控制对 Kubernetes API 的访问 因为 Kubernetes 是完全通过 API 驱动的，所以，控制和限制谁可以通过 API 访问集群，以及允许这些访问者执行什么样的 API 动作，就成为了安全控制的第一道防线。
为 API 交互提供传输层安全 （TLS） Kubernetes 期望集群中所有的 API 通信在默认情况下都使用 TLS 加密，大多数安装方法也允许创建所需的证书并且分发到集群组件中。请注意，某些组件和安装方法可能使用 HTTP 来访问本地端口， 管理员应该熟悉每个组件的设置，以识别潜在的不安全的流量。
API 认证 安装集群时，选择一个 API 服务器的身份验证机制，去使用与之匹配的公共访问模式。例如，小型的单用户集群可能希望使用简单的证书或静态承载令牌方法。更大的集群则可能希望整合现有的、OIDC、LDAP 等允许用户分组的服务器。
所有 API 客户端都必须经过身份验证，即使它是基础设施的一部分，比如节点、代理、调度程序和卷插件。这些客户端通常使用 服务帐户 或 X509 客户端证书，并在集群启动时自动创建或是作为集群安装的一部分进行设置。
如果您希望获取更多信息，请参考 认证参考文档。
API 授权 一旦使用授权，每个 API 的调用都将通过授权检查。Kubernetes 集成 基于访问控制（RBAC） 的组件，将传入的用户或组与一组绑定到角色的权限匹配。这些权限将动作（get，create，delete）和资源（pod，service, node）在命名空间或者集群范围内结合起来，根据客户可能希望执行的操作，提供了一组提供合理的违约责任分离的外包角色。建议您将节点 和 RBAC 一起作为授权者，再与 NodeRestriction 准入插件结合使用。
与身份验证一样，简单而广泛的角色可能适合于较小的集群，但是随着更多的用户与集群交互，可能需要将团队划分成有更多角色限制的单独的命名空间。</description>
    </item>
    
    <item>
      <title>集群管理</title>
      <link>https://lijun.in/tasks/administer-cluster/cluster-management/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/administer-cluster/cluster-management/</guid>
      <description>. toc &amp;gt;}}
本文描述了和集群生命周期相关的几个主题：创建新集群、更新集群的 master 和 worker 节点、执行节点维护（例如升级内核）以及升级运行中集群的 Kubernetes API 版本。
创建和配置集群 要在一组机器上安装 Kubernetes， 请根据您的环境，查阅现有的 入门指南
升级集群 集群升级当前是配套提供的，某些发布版本在升级时可能需要特殊处理。推荐管理员在升级他们的集群前，同时查阅 发行说明 和版本具体升级说明。
 升级到 1.6  升级 Google Compute Engine 集群 Google Compute Engine Open Source (GCE-OSS) 通过删除和重建 master 来支持 master 升级。通过维持相同的 Persistent Disk (PD) 以保证在升级过程中保留数据。
GCE 的 Node 升级采用 管理实例组，每个节点将被顺序的删除，然后使用新软件重建。任何运行在那个节点上的 Pod 需要用 Replication Controller 控制，或者在扩容之后手动重建。
开源 Google Compute Engine (GCE) 集群上的升级过程由 cluster/gce/upgrade.sh 脚本控制。
运行 cluster/gce/upgrade.sh -h 获取使用说明。
例如，只将 master 升级到一个指定的版本 (v1.0.2):</description>
    </item>
    
    <item>
      <title>静态加密 Secret 数据</title>
      <link>https://lijun.in/tasks/administer-cluster/encrypt-data/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lijun.in/tasks/administer-cluster/encrypt-data/</guid>
      <description>本文展示如何启用和配置静态 Secret 数据的加密
. heading &amp;ldquo;prerequisites&amp;rdquo; %}}  . include &amp;ldquo;task-tutorial-prereqs.md&amp;rdquo; &amp;gt;}} . version-check &amp;gt;}}    需要 Kubernetes 1.7.0 或者更高版本
  需要 etcd v3 或者更高版本
  静态数据加密在 1.7.0 中仍然是 alpha 版本，这意味着它可能会在没有通知的情况下进行更改。在升级到 1.8.0 之前，用户可能需要解密他们的数据。
  . toc &amp;gt;}}
. heading &amp;ldquo;prerequisites&amp;rdquo; %}} . include &amp;ldquo;task-tutorial-prereqs.md&amp;rdquo; &amp;gt;}} . version-check &amp;gt;}}
配置并确定是否已启用静态数据加密 kube-apiserver 的参数 --experimental-encryption-provider-config 控制 API 数据在 etcd 中的加密方式。 下面提供一个配置示例。
理解静态数据加密 kind: EncryptionConfiguration apiVersion: apiserver.config.k8s.io/v1 resources: - resources: - secrets providers: - identity: {} - aesgcm: keys: - name: key1 secret: c2VjcmV0IGlzIHNlY3VyZQ== - name: key2 secret: dGhpcyBpcyBwYXNzd29yZA== - aescbc: keys: - name: key1 secret: c2VjcmV0IGlzIHNlY3VyZQ== - name: key2 secret: dGhpcyBpcyBwYXNzd29yZA== - secretbox: keys: - name: key1 secret: YWJjZGVmZ2hpamtsbW5vcHFyc3R1dnd4eXoxMjM0NTY= 每个 resources 数组项目是一个单独的完整的配置。 resources.</description>
    </item>
    
  </channel>
</rss>