---
reviewers:
- vishh
content_type: concept
title: ğŸ˜ - è°ƒåº¦ GPUs
---
<!--
---
reviewers:
- vishh
content_type: concept
title: Schedule GPUs
---
--->

<!-- overview -->

<!--
Kubernetes includes **experimental** support for managing AMD and NVIDIA GPUs spread
across nodes. The support for NVIDIA GPUs was added in v1.6 and has gone through
multiple backwards incompatible iterations.  The support for AMD GPUs was added in
v1.9 via [device plugin](#deploying-amd-gpu-device-plugin).

This page describes how users can consume GPUs across different Kubernetes versions
and the current limitations.
--->
Kubernetes æ”¯æŒå¯¹èŠ‚ç‚¹ä¸Šçš„ AMD å’Œ NVIDA GPU è¿›è¡Œç®¡ç†ï¼Œç›®å‰å¤„äº**å®éªŒ**çŠ¶æ€ã€‚å¯¹ NVIDIA GPU çš„æ”¯æŒåœ¨ v1.6 ä¸­åŠ å…¥ï¼Œå·²ç»ç»å†äº†å¤šæ¬¡ä¸å‘åå…¼å®¹çš„è¿­ä»£ã€‚è€Œå¯¹ AMD GPU çš„æ”¯æŒåˆ™åœ¨ v1.9 ä¸­é€šè¿‡ [è®¾å¤‡æ’ä»¶](#deploying-amd-gpu-device-plugin) åŠ å…¥ã€‚

è¿™ä¸ªé¡µé¢ä»‹ç»äº†ç”¨æˆ·å¦‚ä½•åœ¨ä¸åŒçš„ Kubernetes ç‰ˆæœ¬ä¸­ä½¿ç”¨ GPUï¼Œä»¥åŠå½“å‰å­˜åœ¨çš„ä¸€äº›é™åˆ¶ã€‚




<!-- body -->

<!--
## v1.8 onwards

**From 1.8 onwards, the recommended way to consume GPUs is to use [device
plugins](/docs/concepts/cluster-administration/device-plugins).**

To enable GPU support through device plugins before 1.10, the `DevicePlugins`
feature gate has to be explicitly set to true across the system:
`--feature-gates="DevicePlugins=true"`. This is no longer required starting
from 1.10.
--->
## ä» v1.8 èµ·

**ä» 1.8 ç‰ˆæœ¬å¼€å§‹ï¼Œæˆ‘ä»¬æ¨èé€šè¿‡ [è®¾å¤‡æ’ä»¶](/docs/concepts/cluster-administration/device-plugins) çš„æ–¹å¼æ¥ä½¿ç”¨ GPUã€‚**

åœ¨ 1.10 ç‰ˆæœ¬ä¹‹å‰ï¼Œä¸ºäº†é€šè¿‡è®¾å¤‡æ’ä»¶å¼€å¯ GPU çš„æ”¯æŒï¼Œæˆ‘ä»¬éœ€è¦åœ¨ç³»ç»Ÿä¸­å°† `DevicePlugins` è¿™ä¸€ç‰¹æ€§å¼€å…³æ˜¾å¼åœ°è®¾ç½®ä¸º trueï¼š`--feature-gates="DevicePlugins=true"`ã€‚ä¸è¿‡ï¼Œ
ä» 1.10 ç‰ˆæœ¬å¼€å§‹ï¼Œæˆ‘ä»¬å°±ä¸éœ€è¦è¿™ä¸€æ­¥éª¤äº†ã€‚

<!--
Then you have to install GPU drivers from the corresponding vendor on the nodes
and run the corresponding device plugin from the GPU vendor
([AMD](#deploying-amd-gpu-device-plugin), [NVIDIA](#deploying-nvidia-gpu-device-plugin)).

When the above conditions are true, Kubernetes will expose `nvidia.com/gpu` or
`amd.com/gpu` as a schedulable resource.
--->
æ¥ç€ä½ éœ€è¦åœ¨ä¸»æœºèŠ‚ç‚¹ä¸Šå®‰è£…å¯¹åº”å‚å•†çš„ GPU é©±åŠ¨å¹¶è¿è¡Œå¯¹åº”å‚å•†çš„è®¾å¤‡æ’ä»¶ ([AMD](#deploying-amd-gpu-device-plugin)ã€[NVIDIA](#deploying-nvidia-gpu-device-plugin))ã€‚

å½“ä¸Šé¢çš„æ¡ä»¶éƒ½æ»¡è¶³ï¼ŒKubernetes å°†ä¼šæš´éœ² `nvidia.com/gpu` æˆ– `amd.com/gpu` æ¥ä½œä¸º
ä¸€ç§å¯è°ƒåº¦çš„èµ„æºã€‚

<!--
You can consume these GPUs from your containers by requesting
`<vendor>.com/gpu` just like you request `cpu` or `memory`.
However, there are some limitations in how you specify the resource requirements
when using GPUs:
--->
ä½ ä¹Ÿèƒ½é€šè¿‡åƒè¯·æ±‚ `cpu` æˆ– `memory` ä¸€æ ·è¯·æ±‚ `<vendor>.com/gpu` æ¥åœ¨å®¹å™¨ä¸­ä½¿ç”¨ GPUã€‚ç„¶è€Œï¼Œå½“ä½ è¦é€šè¿‡æŒ‡å®šèµ„æºè¯·æ±‚æ¥ä½¿ç”¨ GPU æ—¶ï¼Œå­˜åœ¨ç€ä»¥ä¸‹å‡ ç‚¹é™åˆ¶ï¼š

<!--
- GPUs are only supposed to be specified in the `limits` section, which means:
  * You can specify GPU `limits` without specifying `requests` because
    Kubernetes will use the limit as the request value by default.
  * You can specify GPU in both `limits` and `requests` but these two values
    must be equal.
  * You cannot specify GPU `requests` without specifying `limits`.
- Containers (and pods) do not share GPUs. There's no overcommitting of GPUs.
- Each container can request one or more GPUs. It is not possible to request a
  fraction of a GPU.

Here's an example:
--->
- GPU ä»…ä»…æ”¯æŒåœ¨ `limits` éƒ¨åˆ†è¢«æŒ‡å®šï¼Œè¿™è¡¨æ˜ï¼š
  * ä½ å¯ä»¥ä»…ä»…æŒ‡å®š GPU çš„ `limits` å­—æ®µè€Œä¸å¿…é¡»æŒ‡å®š `requests` å­—æ®µï¼Œå› ä¸º Kubernetes ä¼šé»˜è®¤ä½¿ç”¨ limit å­—æ®µçš„å€¼æ¥ä½œä¸º request å­—æ®µçš„é»˜è®¤å€¼ã€‚
  * ä½ èƒ½åŒæ—¶æŒ‡å®š GPU çš„ `limits` å’Œ `requests` å­—æ®µï¼Œä½†è¿™ä¸¤ä¸ªå€¼å¿…é¡»ç›¸ç­‰ã€‚
  * ä½ ä¸èƒ½ä»…ä»…æŒ‡å®š GPU çš„ `request` å­—æ®µè€Œä¸æŒ‡å®š `limits`ã€‚
- å®¹å™¨ï¼ˆä»¥åŠ podï¼‰å¹¶ä¸ä¼šå…±äº« GPUï¼Œä¹Ÿä¸å­˜åœ¨å¯¹ GPU çš„è¿‡é‡ä½¿ç”¨ã€‚
- æ¯ä¸€ä¸ªå®¹å™¨èƒ½å¤Ÿè¯·æ±‚ä¸€ä¸ªæˆ–å¤šä¸ª GPUã€‚ç„¶è€Œåªè¯·æ±‚ä¸€ä¸ª GPU çš„ä¸€éƒ¨åˆ†æ˜¯ä¸å…è®¸çš„ã€‚

ä¸‹é¢æ˜¯ä¸€ä¸ªä¾‹å­:

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: cuda-vector-add
spec:
  restartPolicy: OnFailure
  containers:
    - name: cuda-vector-add
      # https://github.com/kubernetes/kubernetes/blob/v1.7.11/test/images/nvidia-cuda/Dockerfile
      image: "k8s.gcr.io/cuda-vector-add:v0.1"
      resources:
        limits:
          nvidia.com/gpu: 1 # requesting 1 GPU
```

<!--
### Deploying AMD GPU device plugin

The [official AMD GPU device plugin](https://github.com/RadeonOpenCompute/k8s-device-plugin)
has the following requirements:
--->
### éƒ¨ç½² AMD GPU è®¾å¤‡æ’ä»¶

[å®˜æ–¹çš„ AMD GPU è®¾å¤‡æ’ä»¶](https://github.com/RadeonOpenCompute/k8s-device-plugin) æœ‰ä»¥ä¸‹è¦æ±‚ï¼š

<!--
- Kubernetes nodes have to be pre-installed with AMD GPU Linux driver.

To deploy the AMD device plugin once your cluster is running and the above
requirements are satisfied:
```
# For Kubernetes v1.9
kubectl create -f https://raw.githubusercontent.com/RadeonOpenCompute/k8s-device-plugin/r1.9/k8s-ds-amdgpu-dp.yaml

# For Kubernetes v1.10
kubectl create -f https://raw.githubusercontent.com/RadeonOpenCompute/k8s-device-plugin/r1.10/k8s-ds-amdgpu-dp.yaml
```
--->
- Kubernetes èŠ‚ç‚¹å¿…é¡»é¢„å…ˆå®‰è£… AMD GPU çš„ Linux é©±åŠ¨ã€‚

å¦‚æœä½ çš„é›†ç¾¤å·²ç»å¯åŠ¨å¹¶ä¸”æ»¡è¶³ä¸Šè¿°è¦æ±‚çš„è¯ï¼Œå¯ä»¥è¿™æ ·éƒ¨ç½² AMD è®¾å¤‡æ’ä»¶ï¼š
```
# é’ˆå¯¹ Kubernetes v1.9
kubectl create -f https://raw.githubusercontent.com/RadeonOpenCompute/k8s-device-plugin/r1.9/k8s-ds-amdgpu-dp.yaml

# é’ˆå¯¹ Kubernetes v1.10
kubectl create -f https://raw.githubusercontent.com/RadeonOpenCompute/k8s-device-plugin/r1.10/k8s-ds-amdgpu-dp.yaml
```

<!--
Report issues with this device plugin to [RadeonOpenCompute/k8s-device-plugin](https://github.com/RadeonOpenCompute/k8s-device-plugin).
--->
è¯·åˆ° [RadeonOpenCompute/k8s-device-plugin](https://github.com/RadeonOpenCompute/k8s-device-plugin) æŠ¥å‘Šæœ‰å…³æ­¤è®¾å¤‡æ’ä»¶çš„é—®é¢˜ã€‚

<!--
### Deploying NVIDIA GPU device plugin

There are currently two device plugin implementations for NVIDIA GPUs:

#### Official NVIDIA GPU device plugin

The [official NVIDIA GPU device plugin](https://github.com/NVIDIA/k8s-device-plugin)
has the following requirements:
--->
### éƒ¨ç½² NVIDIA GPU è®¾å¤‡æ’ä»¶

å¯¹äº NVIDIA GPUsï¼Œç›®å‰å­˜åœ¨ä¸¤ç§è®¾å¤‡æ’ä»¶çš„å®ç°ï¼š

#### å®˜æ–¹çš„ NVIDIA GPU è®¾å¤‡æ’ä»¶

[å®˜æ–¹çš„ NVIDIA GPU è®¾å¤‡æ’ä»¶](https://github.com/NVIDIA/k8s-device-plugin) æœ‰ä»¥ä¸‹è¦æ±‚:

<!--
- Kubernetes nodes have to be pre-installed with NVIDIA drivers.
- Kubernetes nodes have to be pre-installed with [nvidia-docker 2.0](https://github.com/NVIDIA/nvidia-docker)
- nvidia-container-runtime must be configured as the [default runtime](https://github.com/NVIDIA/k8s-device-plugin#preparing-your-gpu-nodes)
  for docker instead of runc.
- NVIDIA drivers ~= 361.93

To deploy the NVIDIA device plugin once your cluster is running and the above
requirements are satisfied:
--->
- Kubernetes çš„èŠ‚ç‚¹å¿…é¡»é¢„å…ˆå®‰è£…äº† NVIDIA é©±åŠ¨
- Kubernetes çš„èŠ‚ç‚¹å¿…é¡»é¢„å…ˆå®‰è£… [nvidia-docker 2.0](https://github.com/NVIDIA/nvidia-docker)
- Docker çš„[é»˜è®¤è¿è¡Œæ—¶](https://github.com/NVIDIA/k8s-device-plugin#preparing-your-gpu-nodes)å¿…é¡»è®¾ç½®ä¸º nvidia-container-runtimeï¼Œè€Œä¸æ˜¯ runc
- NVIDIA é©±åŠ¨ç‰ˆæœ¬ ~= 361.93

å¦‚æœä½ çš„é›†ç¾¤å·²ç»å¯åŠ¨å¹¶ä¸”æ»¡è¶³ä¸Šè¿°è¦æ±‚çš„è¯ï¼Œå¯ä»¥è¿™æ ·éƒ¨ç½² NVIDIA è®¾å¤‡æ’ä»¶ï¼š

<!--
```
# For Kubernetes v1.8
kubectl create -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/v1.8/nvidia-device-plugin.yml

# For Kubernetes v1.9
kubectl create -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/v1.9/nvidia-device-plugin.yml
```

Report issues with this device plugin to [NVIDIA/k8s-device-plugin](https://github.com/NVIDIA/k8s-device-plugin).
--->
```
# é’ˆå¯¹ Kubernetes v1.8
kubectl create -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/v1.8/nvidia-device-plugin.yml

# é’ˆå¯¹ Kubernetes v1.9
kubectl create -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/v1.9/nvidia-device-plugin.yml
```

è¯·åˆ° [NVIDIA/k8s-device-plugin](https://github.com/NVIDIA/k8s-device-plugin) æŠ¥å‘Šæœ‰å…³æ­¤è®¾å¤‡æ’ä»¶çš„é—®é¢˜ã€‚

<!--
#### NVIDIA GPU device plugin used by GCE

The [NVIDIA GPU device plugin used by GCE](https://github.com/GoogleCloudPlatform/container-engine-accelerators/tree/master/cmd/nvidia_gpu)
doesn't require using nvidia-docker and should work with any container runtime
that is compatible with the Kubernetes Container Runtime Interface (CRI). It's tested
on [Container-Optimized OS](https://cloud.google.com/container-optimized-os/)
and has experimental code for Ubuntu from 1.9 onwards.
--->
#### GCE ä¸­ä½¿ç”¨çš„ NVIDIA GPU è®¾å¤‡æ’ä»¶

[GCE ä½¿ç”¨çš„ NVIDIA GPU è®¾å¤‡æ’ä»¶](https://github.com/GoogleCloudPlatform/container-engine-accelerators/tree/master/cmd/nvidia_gpu) å¹¶ä¸è¦æ±‚ä½¿ç”¨ nvidia-dockerï¼Œå¹¶ä¸”å¯¹äºä»»ä½•å®ç°äº† Kubernetes CRI çš„å®¹å™¨è¿è¡Œæ—¶ï¼Œéƒ½åº”è¯¥èƒ½å¤Ÿä½¿ç”¨ã€‚è¿™ä¸€å®ç°å·²ç»åœ¨ [Container-Optimized OS](https://cloud.google.com/container-optimized-os/) ä¸Šè¿›è¡Œäº†æµ‹è¯•ï¼Œå¹¶ä¸”åœ¨ 1.9 ç‰ˆæœ¬ä¹‹åä¼šæœ‰å¯¹äº Ubuntu çš„å®éªŒæ€§ä»£ç ã€‚

<!--
On your 1.12 cluster, you can use the following commands to install the NVIDIA drivers and device plugin:

```
# Install NVIDIA drivers on Container-Optimized OS:
kubectl create -f https://raw.githubusercontent.com/GoogleCloudPlatform/container-engine-accelerators/stable/daemonset.yaml

# Install NVIDIA drivers on Ubuntu (experimental):
kubectl create -f https://raw.githubusercontent.com/GoogleCloudPlatform/container-engine-accelerators/stable/nvidia-driver-installer/ubuntu/daemonset.yaml

# Install the device plugin:
kubectl create -f https://raw.githubusercontent.com/kubernetes/kubernetes/release-1.12/cluster/addons/device-plugins/nvidia-gpu/daemonset.yaml
```
--->
åœ¨ä½  1.12 ç‰ˆæœ¬çš„é›†ç¾¤ä¸Šï¼Œä½ èƒ½ä½¿ç”¨ä¸‹é¢çš„å‘½ä»¤æ¥å®‰è£… NVIDIA é©±åŠ¨ä»¥åŠè®¾å¤‡æ’ä»¶ï¼š

```
# åœ¨å®¹å™¨ä¼˜åŒ–çš„æ“ä½œç³»ç»Ÿä¸Šå®‰è£… NVIDIA é©±åŠ¨:
kubectl create -f https://raw.githubusercontent.com/GoogleCloudPlatform/container-engine-accelerators/stable/daemonset.yaml

# åœ¨ Ubuntu ä¸Šå®‰è£… NVIDIA é©±åŠ¨ (å®éªŒæ€§è´¨):
kubectl create -f https://raw.githubusercontent.com/GoogleCloudPlatform/container-engine-accelerators/stable/nvidia-driver-installer/ubuntu/daemonset.yaml

# å®‰è£…è®¾å¤‡æ’ä»¶:
kubectl create -f https://raw.githubusercontent.com/kubernetes/kubernetes/release-1.12/cluster/addons/device-plugins/nvidia-gpu/daemonset.yaml
```

<!--
Report issues with this device plugin and installation method to [GoogleCloudPlatform/container-engine-accelerators](https://github.com/GoogleCloudPlatform/container-engine-accelerators).

Instructions for using NVIDIA GPUs on GKE are
[here](https://cloud.google.com/kubernetes-engine/docs/how-to/gpus)
--->
è¯·åˆ° [GoogleCloudPlatform/container-engine-accelerators](https://github.com/GoogleCloudPlatform/container-engine-accelerators) æŠ¥å‘Šæœ‰å…³æ­¤è®¾å¤‡æ’ä»¶ä»¥åŠå®‰è£…æ–¹æ³•çš„é—®é¢˜ã€‚

<!--
## Clusters containing different types of GPUs

If different nodes in your cluster have different types of GPUs, then you
can use [Node Labels and Node Selectors](/docs/tasks/configure-pod-container/assign-pods-nodes/)
to schedule pods to appropriate nodes.

For example:
--->
## é›†ç¾¤å†…å­˜åœ¨ä¸åŒç±»å‹çš„ NVIDIA GPU

å¦‚æœé›†ç¾¤å†…éƒ¨çš„ä¸åŒèŠ‚ç‚¹ä¸Šæœ‰ä¸åŒç±»å‹çš„ NVIDIA GPUï¼Œé‚£ä¹ˆä½ å¯ä»¥ä½¿ç”¨ [èŠ‚ç‚¹æ ‡ç­¾å’ŒèŠ‚ç‚¹é€‰æ‹©å™¨](/docs/tasks/configure-pod-container/assign-pods-nodes/) æ¥å°† pod è°ƒåº¦åˆ°åˆé€‚çš„èŠ‚ç‚¹ä¸Šã€‚

ä¾‹å¦‚ï¼š

<!--
```shell
# Label your nodes with the accelerator type they have.
kubectl label nodes <node-with-k80> accelerator=nvidia-tesla-k80
kubectl label nodes <node-with-p100> accelerator=nvidia-tesla-p100
```
--->
```shell
# ä¸ºä½ çš„èŠ‚ç‚¹åŠ ä¸Šå®ƒä»¬æ‰€æ‹¥æœ‰çš„åŠ é€Ÿå™¨ç±»å‹çš„æ ‡ç­¾
kubectl label nodes <node-with-k80> accelerator=nvidia-tesla-k80
kubectl label nodes <node-with-p100> accelerator=nvidia-tesla-p100
```

<!--
For AMD GPUs, you can deploy [Node Labeller](https://github.com/RadeonOpenCompute/k8s-device-plugin/tree/master/cmd/k8s-node-labeller), which automatically labels your nodes with GPU properties. Currently supported properties:
--->
å¯¹äº AMD GPUsï¼Œæ‚¨å¯ä»¥éƒ¨ç½² [èŠ‚ç‚¹æ ‡ç­¾å™¨](https://github.com/RadeonOpenCompute/k8s-device-plugin/tree/master/cmd/k8s-node-labeller)ï¼Œå®ƒä¼šè‡ªåŠ¨ç»™èŠ‚ç‚¹æ‰“ä¸Š GPU å±æ€§æ ‡ç­¾ã€‚ç›®å‰æ”¯æŒçš„å±æ€§ï¼š

<!--
* Device ID (-device-id)
* VRAM Size (-vram)
* Number of SIMD (-simd-count)
* Number of Compute Unit (-cu-count)
* Firmware and Feature Versions (-firmware)
* GPU Family, in two letters acronym (-family)
  * SI - Southern Islands
  * CI - Sea Islands
  * KV - Kaveri
  * VI - Volcanic Islands
  * CZ - Carrizo
  * AI - Arctic Islands
  * RV - Raven

Example result:
--->
* è®¾å¤‡ ID (-device-id)
* VRAM å¤§å° (-vram)
* SIMD æ•°é‡(-simd-count)
* è®¡ç®—å•ä½æ•°é‡(-cu-count)
* å›ºä»¶å’Œç‰¹æ€§ç‰ˆæœ¬ (-firmware)
* GPU ç³»åˆ—ï¼Œä¸¤ä¸ªå­—æ¯çš„é¦–å­—æ¯ç¼©å†™(-family)
  * SI - Southern Islands
  * CI - Sea Islands
  * KV - Kaveri
  * VI - Volcanic Islands
  * CZ - Carrizo
  * AI - Arctic Islands
  * RV - Raven

ç¤ºä¾‹:

    $ kubectl describe node cluster-node-23
    Name:               cluster-node-23
    Roles:              <none>
    Labels:             beta.amd.com/gpu.cu-count.64=1
                        beta.amd.com/gpu.device-id.6860=1
                        beta.amd.com/gpu.family.AI=1
                        beta.amd.com/gpu.simd-count.256=1
                        beta.amd.com/gpu.vram.16G=1
                        beta.kubernetes.io/arch=amd64
                        beta.kubernetes.io/os=linux
                        kubernetes.io/hostname=cluster-node-23
    Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock
                        node.alpha.kubernetes.io/ttl: 0
    ......

<!--
Specify the GPU type in the pod spec:
--->
åœ¨ pod çš„ spec å­—æ®µä¸­æŒ‡å®š GPU çš„ç±»å‹ï¼š

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: cuda-vector-add
spec:
  restartPolicy: OnFailure
  containers:
    - name: cuda-vector-add
      # https://github.com/kubernetes/kubernetes/blob/v1.7.11/test/images/nvidia-cuda/Dockerfile
      image: "k8s.gcr.io/cuda-vector-add:v0.1"
      resources:
        limits:
          nvidia.com/gpu: 1
  nodeSelector:
    accelerator: nvidia-tesla-p100 # or nvidia-tesla-k80 etc.
```

<!--
This will ensure that the pod will be scheduled to a node that has the GPU type
you specified.
--->
è¿™èƒ½å¤Ÿä¿è¯ pod èƒ½å¤Ÿè¢«è°ƒåº¦åˆ°ä½ æ‰€æŒ‡å®šç±»å‹çš„ GPU çš„èŠ‚ç‚¹ä¸Šå»ã€‚
